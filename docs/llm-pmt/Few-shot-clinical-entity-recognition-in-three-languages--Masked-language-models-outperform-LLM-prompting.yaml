- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:46:37'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Few shot clinical entity recognition in three languages: Masked language models
    outperform LLM prompting'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.12801](https://ar5iv.labs.arxiv.org/html/2402.12801)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Marco Naguib Xavier Tannier Aurélie Névéol
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Large Language Models are becoming the go-to solution for many natural language
    processing tasks, including in specialized domains where their few-shot capacities
    are expected to yield high performance in low-resource settings. Herein, we aim
    to assess the performance of Large Language Models for few shot clinical entity
    recognition in multiple languages. We evaluate named entity recognition in English,
    French and Spanish using 8 in-domain (clinical) and 6 out-domain gold standard
    corpora. We assess the performance of 10 auto-regressive language models using
    prompting and 16 masked language models used for text encoding in a biLSTM-CRF
    supervised tagger. We create a few-shot set-up by limiting the amount of annotated
    data available to 100 sentences. Our experiments show that although larger prompt-based
    models tend to achieve competitive F-measure for named entity recognition outside
    the clinical domain, this level of performance does not carry over to the clinical
    domain where lighter supervised taggers relying on masked language models perform
    better, even with the performance drop incurred from the few-shot set-up. In all
    experiments, the CO2 impact of masked language models is inferior to that of auto-regressive
    models. Results are consistent over the three languages and suggest that few-shot
    learning using Large language models is not production ready for named entity
    recognition in the clinical domain. Instead, models could be used for speeding-up
    the production of gold standard annotated data.
  prefs: []
  type: TYPE_NORMAL
- en: 'keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Deep Learning , Natural Language Processing , Named Entity Recognition , Few-shot
    Learning , Large Language Models^†^†journal: Journal of Artificial Intelligence
    in Medicine\affiliation'
  prefs: []
  type: TYPE_NORMAL
- en: '[LISN]organization=LISN, CNRS, Université Paris-Saclay, addressline=1 Rue du
    Belvédère, postcode=91400, city=Orsay, country=France \affiliation[LIMICS]organization=LIMICS,
    Sorbonne Université, Inserm, Université Sorbonne Paris-Nord, addressline=15 rue
    de l’école de médecine, postcode=75005, city=Paris, country=France'
  prefs: []
  type: TYPE_NORMAL
- en: 1 INTRODUCTION
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Electronic Health Records (EHR) are rich sources of clinical information [[1](#bib.bib1)],
    which often appear in unstructured text only [[2](#bib.bib2)]. Efficiently extracting
    information from EHRs into a more structured form can help advance clinical research,
    public health surveillance and automatic clinical decision support [[3](#bib.bib3)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Named Entity Recognition (NER) is a critical primary step in information extraction.
    It consists in identifying mentions of relevant entities in text. In the context
    of clinical information extraction from EHRs, these can be mentions of clinical
    entities such as disorders or drugs. Extracting these entities can particularly
    benefit concept normalization [[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6)]
    as well as facilitate interpreting patient profiling and phenotyping [[7](#bib.bib7)].
    While general-domain NER (identifying entities such as persons and locations)
    has received much attention in the Natural Language Processing (NLP) community,
    clinical NER is widely considered as a harder problem : clinical entities are
    often jargon or ambiguous, and clinical texts have a nonstandard phrasal structure
    [[8](#bib.bib8), [9](#bib.bib9)].'
  prefs: []
  type: TYPE_NORMAL
- en: Language Models have progressively become the main approach for tackling NER
    [[10](#bib.bib10), [11](#bib.bib11)]. Prior work has focused on general-domain
    NER [[12](#bib.bib12)] as well as clinical NER [[7](#bib.bib7), [13](#bib.bib13)].
    This work can be mainly divided into two approaches, depending on the type of
    language models used.
  prefs: []
  type: TYPE_NORMAL
- en: The first approach is to use pre-trained Masked Language Models (MLM). This
    type of models is first pre-trained to predict randomly-selected masked words
    in large text corpora using a dense vector representation of every token (e.g.,
    word) in the text [[12](#bib.bib12), [14](#bib.bib14)]. Leveraging these models
    for NER usually involves training a linear projection to map vector representations
    into an NER tagging of the sentence, while jointly fine-tuning the parameters
    of the language model itself for the downstream task of NER. This approach has
    gained a lot of attention in the community since the release of the BERT architecture
    [[12](#bib.bib12)], and became the go-to solution for building robust NER systems.
  prefs: []
  type: TYPE_NORMAL
- en: However, when dealing with clinical NER, this approach encounters two problems.
    First, due to the sensitive nature of EHRs, public corpora are rare, restrictively
    licensed, with limited availability in languages other than English. This leads
    the community to solutions built on MLMs pre-trained mainly on general-domain
    corpora, and thus suffering from the domain shift problem. Second, for fine-tuning
    to be efficient, large corpora of in-domain annotated text is required [[15](#bib.bib15),
    [16](#bib.bib16)]. But clinical NER annotation campaigns are highly expensive
    and time-consuming due to the level of domain expertise needed to perform it [[8](#bib.bib8),
    [17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19)]. Additionally, due to the
    diversity of clinical cases, data annotated for one biomedical application might
    not necessarily be helpful for another. Hence the need for data-efficient clinical
    NER, also known as few-shot NER.
  prefs: []
  type: TYPE_NORMAL
- en: The second, novel, approach is to use pre-trained Causal Language Models (CLM).
    These substantially larger models are pre-trained on (often larger) corpora as
    generative, auto-regressive models. That is, the model is given a series of tokens
    or prompt as input and estimates the most probable following series of tokens.
    To leverage these language models for downstream tasks such as NER, one can formulate
    the task in natural language in a prompt. The prompt is formulated in such a way
    that the continuity of the text involves resolving the task. The language model
    is then used to predict this continuity. This process is often called "In-context
    learning" (ICL) [[20](#bib.bib20)]. Optionally, one can build a prompt featuring
    a few demonstrations of the task resolved for other instances (in this case, task-specific
    NER-labeled instances), along with the new test input [[21](#bib.bib21)]. The
    model thus outputs an estimation of the most probable NER tagging for the test
    input instance. While MLMs have been studied for few-shot NER [[22](#bib.bib22)],
    CLMs seem more naturally adapted to it. ICL learning has in fact proven particular
    success with LLMs in few-shot learning, showing state-of-the-art results in a
    wide set of NLP tasks [[23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25)].
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the superiority of CLMs over MLMs for NER is questionable. First,
    many efforts towards "few-shot learning" with CLMs design prompts based on their
    performance on large held-out validation datasets [[20](#bib.bib20), [26](#bib.bib26),
    [27](#bib.bib27), [28](#bib.bib28)]. This is problematic because ICL is shown
    [[29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31)] to depend greatly on the
    prompt structure : a small change in task phrasing, the examples presented, the
    order of examples, or the tagging format can affect the performance. Therefore,
    making these choices assuming large annotated validation dataset leads to results
    that are shown [[32](#bib.bib32)] to be over-optimistic and impossible to find
    in a real few-shot setting. Second, most of these studies have been mainly concentrated
    on the English language, and on GPT-based models [[33](#bib.bib33), [34](#bib.bib34),
    [35](#bib.bib35), [36](#bib.bib36)]. This can lead to over-fitting the prompts
    for this language and this language model. Hence, we identify a need for a systematic,
    model-independent, study of the prompt building in the clinical context, and for
    languages other than English. The contributions of this work are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We present and compare the most recent NER prompting techniques when applied
    to clinical NER in three languages : English, French and Spanish. To the best
    of our knowledge, this is the first work focused on NER prompts for languages
    other than English, and the first work comparing prompts for clinical NER.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We bring particular attention to tagging prompts, a novel NER prompting fashion,
    and measure the improvements brought by it.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We offer a fair comparison with the best-performing MLMs in a few-shot setting
    across languages, models, and prompt-structures when applied to few-shot clinical
    NER.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We conduct easily reproducible experiments, using easy-to-implement methods,
    exclusively on publicly available datasets and publicly available language models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 BACKGROUND AND SIGNIFICANCE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Few-shot NER with pre-trained Masked Language Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The standard fashion of using MLMs for NER is as encoders. Usually, an NER tagging
    layer is trained from scratch to map the encoding of text into NER tagging of
    its tokens [[12](#bib.bib12)]. Other approaches have been proposed to leverage
    MLMs for few-shot learning. Namely, metric learning [[37](#bib.bib37), [38](#bib.bib38),
    [39](#bib.bib39)] proposes to train systems to instead learn a metric over the
    output space. New instances can then be classified based on the distance separating
    them from other labeled instances. Label encoding [[40](#bib.bib40), [41](#bib.bib41),
    [42](#bib.bib42)] suggests, instead, to leverage label names or textual label
    descriptions and encode them along with the instances in order to better tag them.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Few-shot NER with Causal Language Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recently, prompt construction has gained interest in the community [[20](#bib.bib20),
    [43](#bib.bib43)]. While most related work focused on studying prompt formulation
    and exploring better-performing prompt structures [[24](#bib.bib24), [34](#bib.bib34),
    [44](#bib.bib44), [33](#bib.bib33)] also known as "prompt engineering", other
    work proposed continuous optimization of the prompt through prompt tuning [[45](#bib.bib45),
    [46](#bib.bib46), [47](#bib.bib47)], usually reporting marginal improvements over
    baselines.
  prefs: []
  type: TYPE_NORMAL
- en: There is no standard, widely adopted manner of building NER prompts [[43](#bib.bib43)].
    In fact, NER associates to each instance a set of spans, each of which having
    a type. This structured nature of the prediction make it hard to find an intuitive
    but efficient manner to prompt a language model for NER, that adapts well to all
    contexts.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, the main practice is to use separate prompts for different entity
    types [[48](#bib.bib48), [49](#bib.bib49), [50](#bib.bib50)]. This choice seems
    well-suited when the task is interested in a handful of types of entities (typically
    5-10). When interested in less entity types, a single prompt can be used for detecting
    all entities [[34](#bib.bib34)]. On the other hand, if there is more entity types,
    it could be interesting to enumerate every possible span in the input sentence
    and let the model predict the entity type of the span, if any [[51](#bib.bib51)].
    This method, on the inverse, is impractical for long inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We identify three families of manners to prompt LLMs :'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constrained prompting attempts to better formulate the NER task by constraining
    the generation to fill in specific hand-crafted templates, usually adapted to
    MLMs [[51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53), [54](#bib.bib54)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Listing prompts consist in simply making the language model predict the entities
    in a list [[34](#bib.bib34)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tagging prompts were studied more recently by [[33](#bib.bib33)]. They make
    the language model surround entity mentions with special tags.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2.3 Few-shot clinical NER
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: MLM-based few-shot NER has also been explored in the biomedical domain [[55](#bib.bib55)].
    Metric leaning [[38](#bib.bib38)] and label encoding [[40](#bib.bib40), [41](#bib.bib41)]
    have been explored, as well as other approaches such as active learning [[56](#bib.bib56)],
    supervised pretraining [[57](#bib.bib57)] and MLM-based in-context learning [[21](#bib.bib21)].
  prefs: []
  type: TYPE_NORMAL
- en: Few studies have focused on CLM-based few-shot clinical NER. In [[35](#bib.bib35)],
    GPT-3 and ChatGPT are evaluated on the 2010 i2b2/VA task [[58](#bib.bib58)] in
    a zero-shot context. In [[36](#bib.bib36)], GPT-3 is evaluated on a set of biomedical
    information extractions tasks including the NCBI-Disease [[18](#bib.bib18)]. Another
    interesting direction is partly fine-tuning [[59](#bib.bib59)] a general-domain
    CLM on clinical text [[60](#bib.bib60), [61](#bib.bib61)], and prompting the resulting
    CLM.
  prefs: []
  type: TYPE_NORMAL
- en: 3 MATERIALS AND METHODS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Evaluation tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to evaluate the models. We use 14 publicly-available NER datasets,
    described as follows. For each study language, we selected two out-domain datasets
    and two or three in-domain datasets, aiming to use comparable resources (same
    genre, tagset, annotation guidelines) across languages whenever possible.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1 General-domain evaluation datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: WikiNER [[62](#bib.bib62)] is a multilingual silver-standard annotated NER dataset.
    It consists in a late-2010 snapshot of Wikipedia in nine languages. Hyperlinks
    referring to persons, locations or organizations were automatically annotated.
    We use the English, French and Spanish versions of this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: CoNLL-2002 [[63](#bib.bib63)] and CoNLL-2003 [[64](#bib.bib64)] are two manually-annotated
    multilingual NER dataset released as a part of CoNLL shared tasks. Mentions of
    persons, locations, organizations and miscellaneous entities are annotated. We
    use the Spanish data of the 2002 version, which is a collection of news wire articles
    made available by the Spanish EFE News Agency, released in May 2000\. We use the
    English data of the 2003 version, which consists of Reuters news stories between
    1996 and 1997.
  prefs: []
  type: TYPE_NORMAL
- en: 'Quaero French Press [[65](#bib.bib65)] is a manually annotated corpus of about
    100 hours of speech transcribed from French speaking radio broadcast. This corpus
    was used in the 2011 Quaero named entity evaluation campaign. It comprises annotations
    for 5 entity types further divided into 32 subtypes. Our experiments relied on
    the five entity types: persons, locations, organizations, functions, and facilities.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 Clinical evaluation datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'E3C [[66](#bib.bib66)] is a European multilingual corpus (Italian, English,
    French, Spanish, and Basque) of semantically annotated clinical narratives. The
    texts are collected from multiple publicly-available sources such as abstracts
    extracted from CC-licensed journals. We use the gold standard material available
    from the English, French and Spanish versions of this dataset. The clinical narratives
    are annotated with 6 entity types : actors, body parts, events, RMLs (measurements
    and test results) and clinical entities.'
  prefs: []
  type: TYPE_NORMAL
- en: The n2c2-2019 [[8](#bib.bib8)] shared task focuses on medical concept normalization.
    It uses the MCN corpus developed by [[67](#bib.bib67)], often referred to as the
    n2c2-2019 dataset. It includes discharge summaries from the Partners HealthCare
    and Beth Israel Deaconess Medical Center. In order to convert the medical concept
    normalization task into an NER task, we use the annotated Concept Unique Identifiers
    (CUIs) to map each mention to the corresponding UMLS semantic group [[68](#bib.bib68),
    [69](#bib.bib69)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The NCBI-Disease [[18](#bib.bib18)] corpus gathers 793 PubMed abstracts where
    mentions of diseases are annotated in four types depending on their syntax : Specific
    Diseases (e.g. diastrophic dysplasia), Disease Classes (e.g. an autosomal recessive
    disease), Composite Mentions (e.g. colorectal, endometrial, and ovarian cancers),
    and Modifiers (e.g. C7-deficient) .'
  prefs: []
  type: TYPE_NORMAL
- en: QuaeroFrenchMed [[17](#bib.bib17)] consists of two text sources that we treat
    separately. The first part, EMEA is a collection of 13 patient information leaflets
    on marketed drugs from the European Medicines Agency (EMEA). The second part,
    MEDLINE, consist of 2,500 titles of research articles indexed in the MEDLINE database¹¹1[http://pubmed.ncbi.nlm.nih.gov/](http://pubmed.ncbi.nlm.nih.gov/).
    The two parts are annotated with 10 entity types corresponding to UMLS semantic
    groups.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Chilean Waiting List [[19](#bib.bib19)] corpus consists of 900 de-identified
    referrals for several specialty consultations in Spanish from the waiting list
    in Chilean public hospitals, manually annotated with 10 entity types : abbreviations,
    body parts, clinical findings, diagnostic procedure, diseases, family members,
    laboratory or test results, laboratory procedures, medications, procedures, signs
    or symptoms and therapeutic procedures. It can be noted that these types can be
    redundant (e.g. all diagnostic procedures are also annotated as procedures).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.3 Few-shot learning set-up
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In order to study our models in a few-shot context, we simulate the few-shot
    context by only providing the models with a few annotated examples. These are
    all the annotated examples models are allowed to use in training, in prompting
    and in validation. In this study, we choose to mainly focus on $k=100$.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we test the best-performing models with a full train dataset for
    a skyline comparison.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Language Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Table LABEL:tab:LM_features presents an overview of the language models used
    in our study. While French and Spanish are covered in many of the causal models,
    we can observe that English is ubiquitous. Except for mBERT and XLM-RoBERTa, masked
    language models cover only one of our study languages.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 NER with Masked Language Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As mentioned in section [2.1](#S2.SS1 "2.1 Few-shot NER with pre-trained Masked
    Language Models ‣ 2 BACKGROUND AND SIGNIFICANCE ‣ Few shot clinical entity recognition
    in three languages: Masked language models outperform LLM prompting"), Masked
    Language Models have been adapted to few-shot learning in architectures suited
    for low-ressource contexts [[37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39),
    [40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42)] However, in this work, we
    are interested in comparing the novel CLM approaches to the widespread, standard
    MLMs usage in without any further adaptation for few-shot learning. We use NLStruct
    [[95](#bib.bib95)], an open-source Python library that implements the standard
    approach described in section [2.1](#S2.SS1 "2.1 Few-shot NER with pre-trained
    Masked Language Models ‣ 2 BACKGROUND AND SIGNIFICANCE ‣ Few shot clinical entity
    recognition in three languages: Masked language models outperform LLM prompting").
    In addition, it processes nested entities, which are present in some or the study
    corpora. Instead of classifying the representation of every token separately into
    a BIO scheme, NLStruct classifies every representation span directly into entity
    types using a biLSTM-CRF tagger. We train the model for 20 epochs on 80% of the
    data and use the remaining held-out 20% for early stopping.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 NER with Causal Language Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our experiments prompt models to tag entities in the input sentence, instead
    of listing them. We discuss this choice in further detail in section [5.4.1](#S5.SS4.SSS1
    "5.4.1 Listing prompts ‣ 5.4 Ablation ‣ 5 DISCUSSION ‣ Few shot clinical entity
    recognition in three languages: Masked language models outperform LLM prompting").'
  prefs: []
  type: TYPE_NORMAL
- en: 'The upper part of figure [1](#S3.F1 "Figure 1 ‣ 3.4 NER with Causal Language
    Models ‣ 3 MATERIALS AND METHODS ‣ Few shot clinical entity recognition in three
    languages: Masked language models outperform LLM prompting") shows a sample tagging
    prompt, highlighting sections in the prompt that guided use to design features
    for prompt phrasing. The 9 prompt phrasing features we considered are described
    below.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c0b49f474f4d3dc870175fca6ae9b622.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Example of a tagging prompt, used in the main experiment (top) and
    a self-verification prompt (bottom) for detecting DISO mentions in n2c2-2019'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Prompt language: By default, we prompt all language models in English, as it
    is the most ubiquitous language in all of their training corpora. This feature
    allows the model to be prompted in French or Spanish, to align the prompt language
    with that of the test sentence.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Additional sentences: By default, we present 5 annotated sentences in the prompts.
    This feature presents 5 additional sentences (i.e., 10 sentences in total).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Self verification: By default, we follow [[33](#bib.bib33)] by selecting the
    5 closest sentences to the test sentence in terms of TF-IDF distance. The mentions
    tagged by the model are then considered to be the model’s final predictions. This
    feature selects instead the 5 sentences featuring the most entities of the target
    type and features them in an initial prompt. Intuitively, this prompt results
    in higher recall and lower precision. A second "self-verification" prompt is then
    used over the model’s initial predictions in order to filter out the false positives.
    A sample self-verification prompt is shown in the bottom part of figure [1](#S3.F1
    "Figure 1 ‣ 3.4 NER with Causal Language Models ‣ 3 MATERIALS AND METHODS ‣ Few
    shot clinical entity recognition in three languages: Masked language models outperform
    LLM prompting"). The number of demonstrations follows that of the main prompt.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Taggers: By default, we follow [[33](#bib.bib33)] prompting the model to surround
    mentions with @@ and ##. This feature prompts it to surround mentions with quotes
    << and >> instead.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Address a specialist in the prompt: By default, the first sentence is the task
    description shown in figure [1](#S3.F1 "Figure 1 ‣ 3.4 NER with Causal Language
    Models ‣ 3 MATERIALS AND METHODS ‣ Few shot clinical entity recognition in three
    languages: Masked language models outperform LLM prompting"). This feature starts
    the prompt with You are an excellent <specialist>. You can identify all the mentions
    of <entity-type> in a sentence, by putting them in a specific format. Here are
    some examples you can handle: instead. The <specialist> is a linguist or a clinician,
    following the task domain.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '6.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Include label definitions in the prompt: This feature adds a one-sentence description
    for each entity type. Full entity descriptions used can be found in appendix 1.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '7.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Introductory sentence for the test instance: By default, the demonstrations
    are immediately followed by the test instance. This feature separates them with
    Identify all the mentions of <entity-type> in the following sentence, by putting
    <begin-tag> in front and a <end-tag> behind each of them.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '8.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Require a long answer for the self-verification: By default, the self-verification
    prompt demonstrates Yes (respectively No) as answers. This feature demonstrates
    <mention> is a(n) <entity-type>, yes. (respectively <mention> is not a(n) <entity-type>,
    no.) instead.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '9.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Dialogue template: This feature replaces the Input: and Output: in the prompt
    by dashes to imitate a dialog template.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ICL learning performance is shown to vary greatly depending on the exact phrasing
    of the prompt [[30](#bib.bib30), [31](#bib.bib31)]. In addition, the optimal choice
    for each of these features can vary depending on the model used. For instance,
    intuitively, models that are heavily pretrained on the English language tend to
    perform better with an English template than one in the language of the corpus.
  prefs: []
  type: TYPE_NORMAL
- en: 'While our system aims to search for the best combination of parameters for
    each model, a grid search over them would require $2^{9}=512$ experiments for
    each model, for each dataset. In order to build a lighter system, we choose to
    perform a greedy search. We iterate over the features in this order, testing the
    non-default value, and keeping it if it performs better than the default. In section
    [5.4.3](#S5.SS4.SSS3 "5.4.3 Hyperparameter grid search ‣ 5.4 Ablation ‣ 5 DISCUSSION
    ‣ Few shot clinical entity recognition in three languages: Masked language models
    outperform LLM prompting"), we compare this approach to a grid search for one
    model over one dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Many efforts towards "few-shot learning" with CLMs optimize prompts on large
    held-out validation datasets [[20](#bib.bib20), [26](#bib.bib26), [27](#bib.bib27),
    [28](#bib.bib28)]. This leads to results that are shown [[32](#bib.bib32)] to
    be over-optimistic. A fair comparison between MLMs and CLMs should compare them
    with access to the same (small) number of annotated instances, which corresponds
    to our $k=100$. In this no-training context, we follow [[32](#bib.bib32)] optimizing
    these features through a leave-one-out cross-validation (LOOCV).
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Measures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We assess the performance of models using F-measure and grams of CO2 emissions.
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Micro-F1 for simplicity, we evaluate models over one global performance score.
    It is computed as the micro-average of F1-measures of the retrieval of each entity
    type.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carbon footprint we use GreenAlgorithms v2.2 [[96](#bib.bib96)] ²²2http://calculator.green-algorithms.org/
    to estimate the carbon footprint of each experiment, based on factors such as
    runtime, computing hardware and location where electricity used by our computer
    facility was produced.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4 RESULTS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table LABEL:tab:results and figure [2](#S4.F2 "Figure 2 ‣ 4.2 Environmental
    Impact ‣ 4 RESULTS ‣ Few shot clinical entity recognition in three languages:
    Masked language models outperform LLM prompting") describe the performance of
    the tested models.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Environmental Impact
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Appendix 2 details the carbon emission estimations for all of our experiments.
    In particular, we estimate the experiment using Mistral-7B over ConLL-2003 to
    have generated 41g of CO2 equivalent. (6g for prompt optimization and 35g for
    inference on the test set). LLaMA-2-70B, around 10 times larger, is estimated
    to have generated 191g of CO2 equivalent. (44g for prompt optimization and 147g
    for inference on the test set).
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the experiment on a the BERT-large MLM is estimated to have
    generated 6g of CO2 equivalent. (2g for fine-tuning and training and 4g for inference
    on the test set).
  prefs: []
  type: TYPE_NORMAL
- en: In total, the experiments described in this paper are estimated to have generated
    around 27kg of CO2 equivalent (25kg for the main experiments, and 2kg for ablation).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ce42b6332a8521a47f5a713037135559.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) English
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8b1d246aeb6fe02d3d9cc0307d639484.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) French
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/be9c312c16d2f11db1d9e66b14b12030.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Spanish
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: General vs. Clinical performance of studied models'
  prefs: []
  type: TYPE_NORMAL
- en: 5 DISCUSSION
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1 Comparison of model performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our experiments offer a comparative analysis of various masked and causal language
    models for named entity recognition. We further focus the scope to low resource
    settings commonly found in real-life biomedical applications. Our results show
    that, despite being smaller and theoretically requiring a larger amount of training
    data, masked, "BERT-like" models consistently outperform CLMs in this context.
    In addition, this performance comes at a much lower environmental impact (CO2
    emissions are 10-50 times lower for MLMs vs. CLMs).
  prefs: []
  type: TYPE_NORMAL
- en: Another important finding is that, in addition to their higher scores, the different
    MLMs achieve results that are relatively close to each other. For example, on
    the WikiNER generalist task in English, the 4 general-domain models tested achieved
    F1-scores of between 0.768 and 0.79.
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides, we show that the MLMs specialized in the biomedical field (ClinicalBERT,
    CamemBERT-bio, etc.), on the one hand, suffer a sharp drop in general domain tasks,
    illustrating the classical issue of "catastrophic forgetting"; and on the other
    hand, do not bring any significant improvement in specialized tasks, with the
    exception of Spanish tasks. This comment must, however, be balanced by the difference
    in size between the models : all the specialized models only have 110 million
    parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: Named entity recognition based on BERT-type representations has received a great
    deal of attention in recent years, and is undoubtedly more mature than the use
    of CLMs for this task. We have implemented the CLM-based NER techniques recently
    published in the literature, to the best of our knowledge. It is, of course, possible
    that new approaches will make it possible to increase performance in the future.
    However, this is arguably a difficult task for a generative model, as it is highly
    constrained in its syntax and its evaluation. These results are no indication
    of performance on other tasks such as classification.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Practical use of language models for low-resource NER
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Overall, our experiments suggest that the performance of language models for
    clinical named entity recognition is currently sub-optimal. In particular, even
    MLM-based models fail to approach the performance of fully supervised models.
    The three large models trained with the entirety of each training dataset (*skylines*
    Table LABEL:tab:results) systematically outperform the best few-shot results,
    by between 5% and 16% for the general domain, and between 8% and 48% for the biomedical
    domain. However, performance can be judged satisfactory enough for pre-annotation
    use, to complement or accelerate manual annotation, for example in an online or
    active learning context.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Limitations of our study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Random Noise
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In MLM experiments, the parameters of the NER tagging layer added on top of
    the pretrained language model are initialized randomly. Likewise, in CLM experiments,
    the demonstrations in the prompts are shuffled randomly, and the negative examples
    in the self verification prompts are selected randomly. These random decisions
    can introduce noise in our performance measurements. Replicating all the experiments
    would allow us to draw more solid conclusions [[97](#bib.bib97)], but would also
    come at a considerable cost (25kg of CO2 equivalent, and around 56 hours of computation
    for each replication). The large number of models tested and tasks addressed can
    however comfort the main observations of this article. For instance, we use Almost
    Stochastic Order (ASO) ³³3Given the performance scores of two algorithms A and
    B, each of which run several times with different settings, ASO computes a test-specific
    value ($\epsilon_{\_}{min}$=0) for all clinical datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Data contamination.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The size of the training corpora used for creating LLMs makes it increasingly
    difficult to control for data contamination, i.e. the presence of test corpora.
    The community is calling for efforts towards better documentation of training
    datasets [[99](#bib.bib99)]. While some datasets are by construction incompatible
    with some models (e.g., there is no Spanish training corpus in GPT-J or LLAMA-2)
    we are unable to affirm full exclusion of all datasets from all models studied.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Ablation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To better understand the contribution of each step of our approach, we carried
    out a series of complementary experiments.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.1 Listing prompts
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this section, we compare the adopted tagging prompts to listing prompts.
    In listing prompts, demonstrations simply list the tagged mentions. The list separator
    is optimized (in the same way as the taggers) between a comma and a newline character.
    Eventually, the introductory sentences asks to list entities. The results shown
    in table [1](#S5.T1 "Table 1 ‣ 5.4.1 Listing prompts ‣ 5.4 Ablation ‣ 5 DISCUSSION
    ‣ Few shot clinical entity recognition in three languages: Masked language models
    outperform LLM prompting") further corroborate our choice of only focusing on
    tagging prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: English French Spanish Model WikiNER CoNLL2003 E3C n2c2 NCBI WikiNER QFP E3C
    EMEA MEDLINE WikiNER CoNLL2002 E3C CWL Listing prompts Mistral-7B 0.659 0.533
    0.417 0.281 0.340 0.676 0.083 0.451 0.169 0.403 0.697 0.620 0.211 0.273 Tagging
    prompts Mistral-7B 0.754 0.646 0.488 0.291 0.395 0.727 0.428 0.590 0.229 0.333
    0.720 0.707 0.083 0.374
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: F1 scores obtained with the listing and tagging prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.2 Sample and sample size
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We tested our approach with different samples and different sample sizes for
    one MLM : XLM-RoBERTa-large, and one CLM : Mistral-7B. The results are reported
    in table [2](#S5.T2 "Table 2 ‣ 5.4.2 Sample and sample size ‣ 5.4 Ablation ‣ 5
    DISCUSSION ‣ Few shot clinical entity recognition in three languages: Masked language
    models outperform LLM prompting"). It can be noted that, whereas the standard
    deviation with respect to $p$ is rather high, a significant difference can still
    be consistently observed between the two models across samples of the same size.
    We also observe that, as the number of annotated instances decreases, the performance
    of the MLM drops faster than that of the CLM.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | CoNLL2003 | n2c2 |'
  prefs: []
  type: TYPE_TB
- en: '| 100 annotated instances |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | p=1 | p=2 | p=3 | p=1 | p=2 | p=3 |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-7B | 0.646 | 0.626 | 0.714 | 0.291 | 0.178 | 0.215 |'
  prefs: []
  type: TYPE_TB
- en: '| XLM-R-large | 0.826 | 0.814 | 0.786 | 0.462 | 0.478 | 0.526 |'
  prefs: []
  type: TYPE_TB
- en: '| 50 annotated instances |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | p=1 | p=2 | p=3 | p=1 | p=2 | p=3 |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-7B | 0.615 | 0.648 | 0.637 | 0.278 | 0.176 | 0.106 |'
  prefs: []
  type: TYPE_TB
- en: '| XLM-R-large | 0.697 | 0.77 | 0.714 | 0.431 | 0.476 | 0.35 |'
  prefs: []
  type: TYPE_TB
- en: '| 25 annotated instances |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | p=1 | p=2 | p=3 | p=1 | p=2 | p=3 |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-7B | 0.509 | 0.599 | 0.52 | 0.152 | 0.252 | 0.116 |'
  prefs: []
  type: TYPE_TB
- en: '| XLM-R-large | 0.487 | 0.588 | 0.637 | 0.393 | 0.361 | 0.283 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: F1 scores obtained over experiments with different training samples
    and different training sample sizes.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.3 Hyperparameter grid search
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In order to asses the quality of our adopted search method used to find the
    best feature combination to incorporate in the prompt, we compare this method
    to a naïve grid search over these features. We test all 512 combinations of our
    identified 9 features, for Mistral-7B over ConLL2003\. The scores found through
    LOOCV vary between 0.0 and 0.656 with a mean value of 0.387 and a median of 0.46\.
    The best-preforming combination is : Additional sentences, Self-verification,
    Introductory sentence for the test instance and Require a long answer for the
    self-verification, which is exactly the same combination we found initially through
    a greedy, tree search, that is around 20 times faster and less consuming.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 CONCLUSION
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This study assessed the performance of two types of large languages models,
    for few-shot entity recognition in three languages. Our experiments show that
    few-shot learning performance is significantly lower in the clinical vs. general
    domain. While masked language models perform better than causal language models
    (higher F1, lower CO2 emissions), few-shot use should be limited to assisting
    gold standard annotation rather than effective information extraction.
  prefs: []
  type: TYPE_NORMAL
- en: ACKNOWLEDGEMENT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work was performed using HPC resources from GENCI-IDRIS (Grant 2023-AD011014533).
    The authors thank Dr. Juan Manual Coria for his help phrasing prompts in Spanish.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] D. Demner-Fushman, W. W. Chapman, C. J. McDonald, [What can natural language
    processing do for clinical decision support?](https://www.sciencedirect.com/science/article/pii/S1532046409001087),
    Journal of Biomedical Informatics 42 (5) (2009) 760–772, biomedical Natural Language
    Processing. [doi:https://doi.org/10.1016/j.jbi.2009.08.007](https://doi.org/https://doi.org/10.1016/j.jbi.2009.08.007).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://www.sciencedirect.com/science/article/pii/S1532046409001087](https://www.sciencedirect.com/science/article/pii/S1532046409001087)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[2] J.-B. Escudié, B. Rance, G. Malamut, S. Khater, A. Burgun, C. Cellier,
    A.-S. Jannot, A novel data-driven workflow combining literature and electronic
    health records to estimate comorbidities burden for a specific disease: a case
    study on autoimmune comorbidities in patients with celiac disease, BMC medical
    informatics and decision making 17 (1) (2017) 1–10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Y. Wang, L. Wang, M. Rastegar-Mojarad, S. Moon, F. Shen, N. Afzal, S. Liu,
    Y. Zeng, S. Mehrabi, S. Sohn, H. Liu, [Clinical information extraction applications:
    A literature review](https://www.sciencedirect.com/science/article/pii/S1532046417302563),
    Journal of Biomedical Informatics 77 (2018) 34–49. [doi:https://doi.org/10.1016/j.jbi.2017.11.011](https://doi.org/https://doi.org/10.1016/j.jbi.2017.11.011).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://www.sciencedirect.com/science/article/pii/S1532046417302563](https://www.sciencedirect.com/science/article/pii/S1532046417302563)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[4] H. Cho, W. Choi, H. Lee, A method for named entity normalization in biomedical
    articles: Application to diseases and plants, BMC Bioinformatics 18 (10 2017).
    [doi:10.1186/s12859-017-1857-8](https://doi.org/10.1186/s12859-017-1857-8).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] P. Wajsbürt, A. Sarfati, X. Tannier, [Medical concept normalization in
    french using multilingual terminologies and contextual embeddings](https://www.sciencedirect.com/science/article/pii/S1532046421000137),
    Journal of Biomedical Informatics 114 (2021) 103684. [doi:https://doi.org/10.1016/j.jbi.2021.103684](https://doi.org/https://doi.org/10.1016/j.jbi.2021.103684).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://www.sciencedirect.com/science/article/pii/S1532046421000137](https://www.sciencedirect.com/science/article/pii/S1532046421000137)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[6] M. Sung, M. Jeong, Y. Choi, D. Kim, J. Lee, J. Kang, [BERN2: an advanced
    neural biomedical named entity recognition and normalization tool](https://doi.org/10.1093/bioinformatics/btac598),
    Bioinformatics 38 (20) (2022) 4837–4839. [arXiv:https://academic.oup.com/bioinformatics/article-pdf/38/20/4837/46535173/btac598.pdf](http://arxiv.org/abs/https://academic.oup.com/bioinformatics/article-pdf/38/20/4837/46535173/btac598.pdf),
    [doi:10.1093/bioinformatics/btac598](https://doi.org/10.1093/bioinformatics/btac598).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://doi.org/10.1093/bioinformatics/btac598](https://doi.org/10.1093/bioinformatics/btac598)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[7] C. Gérardin, P. Wajsbürt, P. Vaillant, A. Bellamine, F. Carrat, X. Tannier,
    [Multilabel classification of medical concepts for patient clinical profile identification](https://www.sciencedirect.com/science/article/pii/S0933365722000768),
    Artificial Intelligence in Medicine 128 (2022) 102311. [doi:https://doi.org/10.1016/j.artmed.2022.102311](https://doi.org/https://doi.org/10.1016/j.artmed.2022.102311).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://www.sciencedirect.com/science/article/pii/S0933365722000768](https://www.sciencedirect.com/science/article/pii/S0933365722000768)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[8] Y.-F. Luo, S. Henry, Y. Wang, F. Shen, O. Uzuner, A. Rumshisky, [The 2019
    n2c2/UMass Lowell shared task on clinical concept normalization](https://doi.org/10.1093/jamia/ocaa106),
    Journal of the American Medical Informatics Association 27 (10) (2020) 1529–e1.
    [arXiv:https://academic.oup.com/jamia/article-pdf/27/10/1529/39739985/ocaa106.pdf](http://arxiv.org/abs/https://academic.oup.com/jamia/article-pdf/27/10/1529/39739985/ocaa106.pdf),
    [doi:10.1093/jamia/ocaa106](https://doi.org/10.1093/jamia/ocaa106).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://doi.org/10.1093/jamia/ocaa106](https://doi.org/10.1093/jamia/ocaa106)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[9] R. Leaman, R. Khare, Z. Lu, [Challenges in clinical natural language processing
    for automated disorder normalization](https://www.sciencedirect.com/science/article/pii/S1532046415001501),
    Journal of Biomedical Informatics 57 (2015) 28–37. [doi:https://doi.org/10.1016/j.jbi.2015.07.010](https://doi.org/https://doi.org/10.1016/j.jbi.2015.07.010).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://www.sciencedirect.com/science/article/pii/S1532046415001501](https://www.sciencedirect.com/science/article/pii/S1532046415001501)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[10] J. Li, A. Sun, J. Han, C. Li, [A survey on deep learning for named entity
    recognition](https://doi.org/10.1109/TKDE.2020.2981314), IEEE Trans. on Knowl.
    and Data Eng. 34 (1) (2022) 50–70. [doi:10.1109/TKDE.2020.2981314](https://doi.org/10.1109/TKDE.2020.2981314).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://doi.org/10.1109/TKDE.2020.2981314](https://doi.org/10.1109/TKDE.2020.2981314)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[11] Y. Wang, H. Tong, Z. Zhu, Y. Li, [Nested named entity recognition: A survey](https://doi.org/10.1145/3522593),
    ACM Trans. Knowl. Discov. Data 16 (6) (jul 2022). [doi:10.1145/3522593](https://doi.org/10.1145/3522593).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://doi.org/10.1145/3522593](https://doi.org/10.1145/3522593)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[12] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, [Bert: Pre-training of deep
    bidirectional transformers for language understanding](https://api.semanticscholar.org/CorpusID:52967399),
    in: North American Chapter of the Association for Computational Linguistics, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://api.semanticscholar.org/CorpusID:52967399](https://api.semanticscholar.org/CorpusID:52967399)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[13] C. Sun, Z. Yang, L. Wang, Y. Zhang, H. Lin, J. Wang, [Biomedical named
    entity recognition using bert in the machine reading comprehension framework](https://www.sciencedirect.com/science/article/pii/S1532046421001283),
    Journal of Biomedical Informatics 118 (2021) 103799. [doi:https://doi.org/10.1016/j.jbi.2021.103799](https://doi.org/https://doi.org/10.1016/j.jbi.2021.103799).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://www.sciencedirect.com/science/article/pii/S1532046421001283](https://www.sciencedirect.com/science/article/pii/S1532046421001283)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[14] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, L. Zettlemoyer,
    [Deep contextualized word representations](https://aclanthology.org/N18-1202),
    in: M. Walker, H. Ji, A. Stent (Eds.), Proceedings of the 2018 Conference of the
    North American Chapter of the Association for Computational Linguistics: Human
    Language Technologies, Volume 1 (Long Papers), Association for Computational Linguistics,
    New Orleans, Louisiana, 2018, pp. 2227–2237. [doi:10.18653/v1/N18-1202](https://doi.org/10.18653/v1/N18-1202).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://aclanthology.org/N18-1202](https://aclanthology.org/N18-1202)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[15] C. Jia, X. Liang, Y. Zhang, [Cross-domain NER using cross-domain language
    modeling](https://aclanthology.org/P19-1236), in: A. Korhonen, D. Traum, L. Màrquez
    (Eds.), Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics, Association for Computational Linguistics, Florence, Italy, 2019,
    pp. 2464–2474. [doi:10.18653/v1/P19-1236](https://doi.org/10.18653/v1/P19-1236).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://aclanthology.org/P19-1236](https://aclanthology.org/P19-1236)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[16] Z. Liu, Y. Xu, T. Yu, W. Dai, Z. Ji, S. Cahyawijaya, A. Madotto, P. Fung,
    [Crossner: Evaluating cross-domain named entity recognition](https://ojs.aaai.org/index.php/AAAI/article/view/17587),
    Proceedings of the AAAI Conference on Artificial Intelligence 35 (15) (2021) 13452–13460.
    [doi:10.1609/aaai.v35i15.17587](https://doi.org/10.1609/aaai.v35i15.17587).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://ojs.aaai.org/index.php/AAAI/article/view/17587](https://ojs.aaai.org/index.php/AAAI/article/view/17587)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[17] A. Névéol, C. Grouin, J. Leixa, S. Rosset, P. Zweigenbaum, The quaero
    french medical corpus: A ressource for medical entity recognition and normalization,
    Proc of BioTextMining Work (2014) 24–30.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] R. I. Doğan, R. Leaman, Z. Lu, [Ncbi disease corpus: A resource for disease
    name recognition and concept normalization](https://www.sciencedirect.com/science/article/pii/S1532046413001974),
    Journal of Biomedical Informatics 47 (2014) 1–10. [doi:https://doi.org/10.1016/j.jbi.2013.12.006](https://doi.org/https://doi.org/10.1016/j.jbi.2013.12.006).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://www.sciencedirect.com/science/article/pii/S1532046413001974](https://www.sciencedirect.com/science/article/pii/S1532046413001974)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[19] P. Báez, F. Villena, M. Rojas, M. Durán, J. Dunstan, [The Chilean waiting
    list corpus: a new resource for clinical named entity recognition in Spanish](https://aclanthology.org/2020.clinicalnlp-1.32),
    in: Proceedings of the 3rd Clinical Natural Language Processing Workshop, Association
    for Computational Linguistics, Online, 2020, pp. 291–300. [doi:10.18653/v1/2020.clinicalnlp-1.32](https://doi.org/10.18653/v1/2020.clinicalnlp-1.32).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://aclanthology.org/2020.clinicalnlp-1.32](https://aclanthology.org/2020.clinicalnlp-1.32)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[20] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,
    P. Shyam, G. Sastry, A. Askell, et al., Language models are few-shot learners,
    Advances in neural information processing systems 33 (2020) 1877–1901.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] D.-H. Lee, A. Kadakia, K. Tan, M. Agarwal, X. Feng, T. Shibuya, R. Mitani,
    T. Sekiya, J. Pujara, X. Ren, [Good examples make a faster learner: Simple demonstration-based
    learning for low-resource NER](https://aclanthology.org/2022.acl-long.192), in:
    S. Muresan, P. Nakov, A. Villavicencio (Eds.), Proceedings of the 60th Annual
    Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
    Association for Computational Linguistics, Dublin, Ireland, 2022, pp. 2687–2700.
    [doi:10.18653/v1/2022.acl-long.192](https://doi.org/10.18653/v1/2022.acl-long.192).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://aclanthology.org/2022.acl-long.192](https://aclanthology.org/2022.acl-long.192)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[22] S. S. Du, W. Hu, S. M. Kakade, J. D. Lee, Q. Lei, Few-shot learning via
    learning the representation, provably, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] S. Shin, S.-W. Lee, H. Ahn, S. Kim, H. Kim, B. Kim, K. Cho, G. Lee, W. Park,
    J.-W. Ha, N. Sung, [On the effect of pretraining corpora on in-context learning
    by a large-scale language model](https://aclanthology.org/2022.naacl-main.380),
    in: M. Carpuat, M.-C. de Marneffe, I. V. Meza Ruiz (Eds.), Proceedings of the
    2022 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Association for Computational Linguistics,
    Seattle, United States, 2022, pp. 5168–5186. [doi:10.18653/v1/2022.naacl-main.380](https://doi.org/10.18653/v1/2022.naacl-main.380).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://aclanthology.org/2022.naacl-main.380](https://aclanthology.org/2022.naacl-main.380)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[24] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou,
    et al., Chain-of-thought prompting elicits reasoning in large language models,
    Advances in Neural Information Processing Systems 35 (2022) 24824–24837.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch,
    A. R. Brown, A. Santoro, A. Gupta, A. Garriga-Alonso, A. Kluska, A. Lewkowycz,
    A. Agarwal, A. Power, A. Ray, A. Warstadt, A. W. Kocurek, A. Safaya, A. Tazarv,
    A. Xiang, A. Parrish, A. Nie, A. Hussain, A. Askell, A. Dsouza, A. Slone, A. Rahane,
    A. S. Iyer, A. J. Andreassen, A. Madotto, A. Santilli, A. Stuhlmüller, A. M. Dai,
    A. La, A. Lampinen, A. Zou, A. Jiang, A. Chen, A. Vuong, A. Gupta, A. Gottardi,
    A. Norelli, A. Venkatesh, A. Gholamidavoodi, A. Tabassum, A. Menezes, A. Kirubarajan,
    A. Mullokandov, A. Sabharwal, A. Herrick, A. Efrat, A. Erdem, A. Karakaş, B. R.
    Roberts, B. S. Loe, B. Zoph, B. Bojanowski, B. Özyurt, B. Hedayatnia, B. Neyshabur,
    B. Inden, B. Stein, B. Ekmekci, B. Y. Lin, B. Howald, B. Orinion, C. Diao, C. Dour,
    C. Stinson, C. Argueta, C. Ferri, C. Singh, C. Rathkopf, C. Meng, C. Baral, C. Wu,
    C. Callison-Burch, C. Waites, C. Voigt, C. D. Manning, C. Potts, C. Ramirez, C. E.
    Rivera, C. Siro, C. Raffel, C. Ashcraft, C. Garbacea, D. Sileo, D. Garrette, D. Hendrycks,
    D. Kilman, D. Roth, C. D. Freeman, D. Khashabi, D. Levy, D. M. González, D. Perszyk,
    D. Hernandez, D. Chen, D. Ippolito, D. Gilboa, D. Dohan, D. Drakard, D. Jurgens,
    D. Datta, D. Ganguli, D. Emelin, D. Kleyko, D. Yuret, D. Chen, D. Tam, D. Hupkes,
    D. Misra, D. Buzan, D. C. Mollo, D. Yang, D.-H. Lee, D. Schrader, E. Shutova,
    E. D. Cubuk, E. Segal, E. Hagerman, E. Barnes, E. Donoway, E. Pavlick, E. Rodolà,
    E. Lam, E. Chu, E. Tang, E. Erdem, E. Chang, E. A. Chi, E. Dyer, E. Jerzak, E. Kim,
    E. E. Manyasi, E. Zheltonozhskii, F. Xia, F. Siar, F. Martínez-Plumed, F. Happé,
    F. Chollet, F. Rong, G. Mishra, G. I. Winata, G. de Melo, G. Kruszewski, G. Parascandolo,
    G. Mariani, G. X. Wang, G. Jaimovitch-Lopez, G. Betz, G. Gur-Ari, H. Galijasevic,
    H. Kim, H. Rashkin, H. Hajishirzi, H. Mehta, H. Bogar, H. F. A. Shevlin, H. Schuetze,
    H. Yakura, H. Zhang, H. M. Wong, I. Ng, I. Noble, J. Jumelet, J. Geissinger, J. Kernion,
    J. Hilton, J. Lee, J. F. Fisac, J. B. Simon, J. Koppel, J. Zheng, J. Zou, J. Kocon,
    J. Thompson, J. Wingfield, J. Kaplan, J. Radom, J. Sohl-Dickstein, J. Phang, J. Wei,
    J. Yosinski, J. Novikova, J. Bosscher, J. Marsh, J. Kim, J. Taal, J. Engel, J. Alabi,
    J. Xu, J. Song, J. Tang, J. Waweru, J. Burden, J. Miller, J. U. Balis, J. Batchelder,
    J. Berant, J. Frohberg, J. Rozen, J. Hernandez-Orallo, J. Boudeman, J. Guerr,
    J. Jones, J. B. Tenenbaum, J. S. Rule, J. Chua, K. Kanclerz, K. Livescu, K. Krauth,
    K. Gopalakrishnan, K. Ignatyeva, K. Markert, K. Dhole, K. Gimpel, K. Omondi, K. W.
    Mathewson, K. Chiafullo, K. Shkaruta, K. Shridhar, K. McDonell, K. Richardson,
    L. Reynolds, L. Gao, L. Zhang, L. Dugan, L. Qin, L. Contreras-Ochando, L.-P. Morency,
    L. Moschella, L. Lam, L. Noble, L. Schmidt, L. He, L. Oliveros-Colón, L. Metz,
    L. K. Senel, M. Bosma, M. Sap, M. T. Hoeve, M. Farooqi, M. Faruqui, M. Mazeika,
    M. Baturan, M. Marelli, M. Maru, M. J. Ramirez-Quintana, M. Tolkiehn, M. Giulianelli,
    M. Lewis, M. Potthast, M. L. Leavitt, M. Hagen, M. Schubert, M. O. Baitemirova,
    M. Arnaud, M. McElrath, M. A. Yee, M. Cohen, M. Gu, M. Ivanitskiy, M. Starritt,
    M. Strube, M. Swędrowski, M. Bevilacqua, M. Yasunaga, M. Kale, M. Cain, M. Xu,
    M. Suzgun, M. Walker, M. Tiwari, M. Bansal, M. Aminnaseri, M. Geva, M. Gheini,
    M. V. T, N. Peng, N. A. Chi, N. Lee, N. G.-A. Krakover, N. Cameron, N. Roberts,
    N. Doiron, N. Martinez, N. Nangia, N. Deckers, N. Muennighoff, N. S. Keskar, N. S.
    Iyer, N. Constant, N. Fiedel, N. Wen, O. Zhang, O. Agha, O. Elbaghdadi, O. Levy,
    O. Evans, P. A. M. Casares, P. Doshi, P. Fung, P. P. Liang, P. Vicol, P. Alipoormolabashi,
    P. Liao, P. Liang, P. W. Chang, P. Eckersley, P. M. Htut, P. Hwang, P. Miłkowski,
    P. Patil, P. Pezeshkpour, P. Oli, Q. Mei, Q. Lyu, Q. Chen, R. Banjade, R. E. Rudolph,
    R. Gabriel, R. Habacker, R. Risco, R. Millière, R. Garg, R. Barnes, R. A. Saurous,
    R. Arakawa, R. Raymaekers, R. Frank, R. Sikand, R. Novak, R. Sitelew, R. L. Bras,
    R. Liu, R. Jacobs, R. Zhang, R. Salakhutdinov, R. A. Chi, S. R. Lee, R. Stovall,
    R. Teehan, R. Yang, S. Singh, S. M. Mohammad, S. Anand, S. Dillavou, S. Shleifer,
    S. Wiseman, S. Gruetter, S. R. Bowman, S. S. Schoenholz, S. Han, S. Kwatra, S. A.
    Rous, S. Ghazarian, S. Ghosh, S. Casey, S. Bischoff, S. Gehrmann, S. Schuster,
    S. Sadeghi, S. Hamdan, S. Zhou, S. Srivastava, S. Shi, S. Singh, S. Asaadi, S. S.
    Gu, S. Pachchigar, S. Toshniwal, S. Upadhyay, S. S. Debnath, S. Shakeri, S. Thormeyer,
    S. Melzi, S. Reddy, S. P. Makini, S.-H. Lee, S. Torene, S. Hatwar, S. Dehaene,
    S. Divic, S. Ermon, S. Biderman, S. Lin, S. Prasad, S. Piantadosi, S. Shieber,
    S. Misherghi, S. Kiritchenko, S. Mishra, T. Linzen, T. Schuster, T. Li, T. Yu,
    T. Ali, T. Hashimoto, T.-L. Wu, T. Desbordes, T. Rothschild, T. Phan, T. Wang,
    T. Nkinyili, T. Schick, T. Kornev, T. Tunduny, T. Gerstenberg, T. Chang, T. Neeraj,
    T. Khot, T. Shultz, U. Shaham, V. Misra, V. Demberg, V. Nyamai, V. Raunak, V. V.
    Ramasesh, vinay uday prabhu, V. Padmakumar, V. Srikumar, W. Fedus, W. Saunders,
    W. Zhang, W. Vossen, X. Ren, X. Tong, X. Zhao, X. Wu, X. Shen, Y. Yaghoobzadeh,
    Y. Lakretz, Y. Song, Y. Bahri, Y. Choi, Y. Yang, Y. Hao, Y. Chen, Y. Belinkov,
    Y. Hou, Y. Hou, Y. Bai, Z. Seid, Z. Zhao, Z. Wang, Z. J. Wang, Z. Wang, Z. Wu,
    [Beyond the imitation game: Quantifying and extrapolating the capabilities of
    language models](https://openreview.net/forum?id=uyTL5Bvosj), Transactions on
    Machine Learning Research (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://openreview.net/forum?id=uyTL5Bvosj](https://openreview.net/forum?id=uyTL5Bvosj)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[26] D. Tam, R. R. Menon, M. Bansal, S. Srivastava, C. Raffel, [Improving and
    simplifying pattern exploiting training](https://aclanthology.org/2021.emnlp-main.407),
    in: M.-F. Moens, X. Huang, L. Specia, S. W.-t. Yih (Eds.), Proceedings of the
    2021 Conference on Empirical Methods in Natural Language Processing, Association
    for Computational Linguistics, Online and Punta Cana, Dominican Republic, 2021,
    pp. 4980–4991. [doi:10.18653/v1/2021.emnlp-main.407](https://doi.org/10.18653/v1/2021.emnlp-main.407).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://aclanthology.org/2021.emnlp-main.407](https://aclanthology.org/2021.emnlp-main.407)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[27] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
    A. Askell, P. Mishkin, J. Clark, G. Krueger, I. Sutskever, [Learning transferable
    visual models from natural language supervision](https://proceedings.mlr.press/v139/radford21a.html),
    in: M. Meila, T. Zhang (Eds.), Proceedings of the 38th International Conference
    on Machine Learning, Vol. 139 of Proceedings of Machine Learning Research, PMLR,
    2021, pp. 8748–8763.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://proceedings.mlr.press/v139/radford21a.html](https://proceedings.mlr.press/v139/radford21a.html)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[28] G. Qin, J. Eisner, [Learning how to ask: Querying LMs with mixtures of
    soft prompts](https://aclanthology.org/2021.naacl-main.410), in: K. Toutanova,
    A. Rumshisky, L. Zettlemoyer, D. Hakkani-Tur, I. Beltagy, S. Bethard, R. Cotterell,
    T. Chakraborty, Y. Zhou (Eds.), Proceedings of the 2021 Conference of the North
    American Chapter of the Association for Computational Linguistics: Human Language
    Technologies, Association for Computational Linguistics, Online, 2021, pp. 5203–5212.
    [doi:10.18653/v1/2021.naacl-main.410](https://doi.org/10.18653/v1/2021.naacl-main.410).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://aclanthology.org/2021.naacl-main.410](https://aclanthology.org/2021.naacl-main.410)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[29] Z. Zhao, E. Wallace, S. Feng, D. Klein, S. Singh, Calibrate before use:
    Improving few-shot performance of language models, in: International Conference
    on Machine Learning, PMLR, 2021, pp. 12697–12706.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Y. Lu, M. Bartolo, A. Moore, S. Riedel, P. Stenetorp, [Fantastically ordered
    prompts and where to find them: Overcoming few-shot prompt order sensitivity](https://aclanthology.org/2022.acl-long.556),
    in: S. Muresan, P. Nakov, A. Villavicencio (Eds.), Proceedings of the 60th Annual
    Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
    Association for Computational Linguistics, Dublin, Ireland, 2022, pp. 8086–8098.
    [doi:10.18653/v1/2022.acl-long.556](https://doi.org/10.18653/v1/2022.acl-long.556).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://aclanthology.org/2022.acl-long.556](https://aclanthology.org/2022.acl-long.556)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[31] S. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, L. Zettlemoyer,
    [Rethinking the role of demonstrations: What makes in-context learning work?](https://aclanthology.org/2022.emnlp-main.759),
    in: Y. Goldberg, Z. Kozareva, Y. Zhang (Eds.), Proceedings of the 2022 Conference
    on Empirical Methods in Natural Language Processing, Association for Computational
    Linguistics, Abu Dhabi, United Arab Emirates, 2022, pp. 11048–11064. [doi:10.18653/v1/2022.emnlp-main.759](https://doi.org/10.18653/v1/2022.emnlp-main.759).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://aclanthology.org/2022.emnlp-main.759](https://aclanthology.org/2022.emnlp-main.759)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[32] E. Perez, D. Kiela, K. Cho, [True few-shot learning with language models](https://openreview.net/forum?id=ShnM-rRh4T),
    in: A. Beygelzimer, Y. Dauphin, P. Liang, J. W. Vaughan (Eds.), Advances in Neural
    Information Processing Systems, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://openreview.net/forum?id=ShnM-rRh4T](https://openreview.net/forum?id=ShnM-rRh4T)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[33] S. Wang, X. Sun, X. Li, R. Ouyang, F. Wu, T. Zhang, J. Li, G. Wang, Gpt-ner:
    Named entity recognition via large language models (2023). [arXiv:2304.10428](http://arxiv.org/abs/2304.10428).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] D. Ashok, Z. Lipton, Promptner: Prompting for named entity recognition
    (May 2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Y. Hu, I. Ameer, X. Zuo, X. Peng, Y. Zhou, Z. Li, Y. Li, J. Li, X. Jiang,
    H. Xu, Zero-shot clinical entity recognition using chatgpt, arXiv preprint arXiv:2303.16416
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] B. Jimenez Gutierrez, N. McNeal, C. Washington, Y. Chen, L. Li, H. Sun,
    Y. Su, [Thinking about GPT-3 in-context learning for biomedical IE? think again](https://aclanthology.org/2022.findings-emnlp.329),
    in: Y. Goldberg, Z. Kozareva, Y. Zhang (Eds.), Findings of the Association for
    Computational Linguistics: EMNLP 2022, Association for Computational Linguistics,
    Abu Dhabi, United Arab Emirates, 2022, pp. 4497–4512. [doi:10.18653/v1/2022.findings-emnlp.329](https://doi.org/10.18653/v1/2022.findings-emnlp.329).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://aclanthology.org/2022.findings-emnlp.329](https://aclanthology.org/2022.findings-emnlp.329)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[37] A. Fritzler, V. Logacheva, M. Kretov, [Few-shot classification in named
    entity recognition task](https://doi.org/10.1145/3297280.3297378), in: Proceedings
    of the 34th ACM/SIGAPP Symposium on Applied Computing, SAC ’19, Association for
    Computing Machinery, New York, NY, USA, 2019, p. 993–1000. [doi:10.1145/3297280.3297378](https://doi.org/10.1145/3297280.3297378).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://doi.org/10.1145/3297280.3297378](https://doi.org/10.1145/3297280.3297378)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[38] Y. Yang, A. Katiyar, [Simple and effective few-shot named entity recognition
    with structured nearest neighbor learning](https://aclanthology.org/2020.emnlp-main.516),
    in: B. Webber, T. Cohn, Y. He, Y. Liu (Eds.), Proceedings of the 2020 Conference
    on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational
    Linguistics, Online, 2020, pp. 6365–6375. [doi:10.18653/v1/2020.emnlp-main.516](https://doi.org/10.18653/v1/2020.emnlp-main.516).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://aclanthology.org/2020.emnlp-main.516](https://aclanthology.org/2020.emnlp-main.516)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[39] J. Huang, C. Li, K. Subudhi, D. Jose, S. Balakrishnan, W. Chen, B. Peng,
    J. Gao, J. Han, [Few-shot named entity recognition: An empirical baseline study](https://aclanthology.org/2021.emnlp-main.813),
    in: M.-F. Moens, X. Huang, L. Specia, S. W.-t. Yih (Eds.), Proceedings of the
    2021 Conference on Empirical Methods in Natural Language Processing, Association
    for Computational Linguistics, Online and Punta Cana, Dominican Republic, 2021,
    pp. 10408–10423. [doi:10.18653/v1/2021.emnlp-main.813](https://doi.org/10.18653/v1/2021.emnlp-main.813).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://aclanthology.org/2021.emnlp-main.813](https://aclanthology.org/2021.emnlp-main.813)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[40] R. Aly, A. Vlachos, R. McDonald, [Leveraging type descriptions for zero-shot
    named entity recognition and classification](https://aclanthology.org/2021.acl-long.120),
    in: C. Zong, F. Xia, W. Li, R. Navigli (Eds.), Proceedings of the 59th Annual
    Meeting of the Association for Computational Linguistics and the 11th International
    Joint Conference on Natural Language Processing (Volume 1: Long Papers), Association
    for Computational Linguistics, Online, 2021, pp. 1516–1528. [doi:10.18653/v1/2021.acl-long.120](https://doi.org/10.18653/v1/2021.acl-long.120).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://aclanthology.org/2021.acl-long.120](https://aclanthology.org/2021.acl-long.120)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[41] J. Ma, M. Ballesteros, S. Doss, R. Anubhai, S. Mallya, Y. Al-Onaizan,
    D. Roth, [Label semantics for few shot named entity recognition](https://aclanthology.org/2022.findings-acl.155),
    in: S. Muresan, P. Nakov, A. Villavicencio (Eds.), Findings of the Association
    for Computational Linguistics: ACL 2022, Association for Computational Linguistics,
    Dublin, Ireland, 2022, pp. 1956–1971. [doi:10.18653/v1/2022.findings-acl.155](https://doi.org/10.18653/v1/2022.findings-acl.155).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://aclanthology.org/2022.findings-acl.155](https://aclanthology.org/2022.findings-acl.155)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[42] Y. Hou, W. Che, Y. Lai, Z. Zhou, Y. Liu, H. Liu, T. Liu, [Few-shot slot
    tagging with collapsed dependency transfer and label-enhanced task-adaptive projection
    network](https://aclanthology.org/2020.acl-main.128), in: D. Jurafsky, J. Chai,
    N. Schluter, J. Tetreault (Eds.), Proceedings of the 58th Annual Meeting of the
    Association for Computational Linguistics, Association for Computational Linguistics,
    Online, 2020, pp. 1381–1393. [doi:10.18653/v1/2020.acl-main.128](https://doi.org/10.18653/v1/2020.acl-main.128).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://aclanthology.org/2020.acl-main.128](https://aclanthology.org/2020.acl-main.128)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[43] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, G. Neubig, Pre-train, prompt,
    and predict: A systematic survey of prompting methods in natural language processing,
    ACM Computing Surveys 55 (9) (2023) 1–35.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] D. Vilar, M. Freitag, C. Cherry, J. Luo, V. Ratnakar, G. Foster, [Prompting
    PaLM for translation: Assessing strategies and performance](https://aclanthology.org/2023.acl-long.859),
    in: Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers), Association for Computational Linguistics,
    Toronto, Canada, 2023, pp. 15406–15427. [doi:10.18653/v1/2023.acl-long.859](https://doi.org/10.18653/v1/2023.acl-long.859).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://aclanthology.org/2023.acl-long.859](https://aclanthology.org/2023.acl-long.859)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[45] R. Ma, X. Zhou, T. Gui, Y. Tan, L. Li, Q. Zhang, X. Huang, [Template-free
    prompt tuning for few-shot NER](https://aclanthology.org/2022.naacl-main.420),
    in: M. Carpuat, M.-C. de Marneffe, I. V. Meza Ruiz (Eds.), Proceedings of the
    2022 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Association for Computational Linguistics,
    Seattle, United States, 2022, pp. 5721–5732. [doi:10.18653/v1/2022.naacl-main.420](https://doi.org/10.18653/v1/2022.naacl-main.420).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://aclanthology.org/2022.naacl-main.420](https://aclanthology.org/2022.naacl-main.420)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[46] A. Layegh, A. H. Payberah, A. Soylu, D. Roman, M. Matskin, Contrastner:
    Contrastive-based prompt tuning for few-shot ner, arXiv preprint arXiv:2305.17951
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] N. Hu, X. Zhou, B. Xu, H. Liu, X. Xie, H.-T. Zheng, [Vpn: Variation on
    prompt tuning for named-entity recognition](https://www.mdpi.com/2076-3417/13/14/8359),
    Applied Sciences 13 (14) (2023). [doi:10.3390/app13148359](https://doi.org/10.3390/app13148359).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://www.mdpi.com/2076-3417/13/14/8359](https://www.mdpi.com/2076-3417/13/14/8359)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[48] X. Li, J. Feng, Y. Meng, Q. Han, F. Wu, J. Li, [A unified MRC framework
    for named entity recognition](https://aclanthology.org/2020.acl-main.519), in:
    D. Jurafsky, J. Chai, N. Schluter, J. Tetreault (Eds.), Proceedings of the 58th
    Annual Meeting of the Association for Computational Linguistics, Association for
    Computational Linguistics, Online, 2020, pp. 5849–5859. [doi:10.18653/v1/2020.acl-main.519](https://doi.org/10.18653/v1/2020.acl-main.519).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://aclanthology.org/2020.acl-main.519](https://aclanthology.org/2020.acl-main.519)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[49] A. T. Liu, W. Xiao, H. Zhu, D. Zhang, S.-W. Li, A. O. Arnold, [Qaner:
    Prompting question answering models for few-shot named entity recognition](https://api.semanticscholar.org/CorpusID:247222693),
    ArXiv abs/2203.01543 (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://api.semanticscholar.org/CorpusID:247222693](https://api.semanticscholar.org/CorpusID:247222693)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[50] J. Chen, Y. Lu, H. Lin, J. Lou, W. Jia, D. Dai, H. Wu, B. Cao, X. Han,
    L. Sun, [Learning in-context learning for named entity recognition](https://aclanthology.org/2023.acl-long.764),
    in: A. Rogers, J. Boyd-Graber, N. Okazaki (Eds.), Proceedings of the 61st Annual
    Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
    Association for Computational Linguistics, Toronto, Canada, 2023, pp. 13661–13675.
    [doi:10.18653/v1/2023.acl-long.764](https://doi.org/10.18653/v1/2023.acl-long.764).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://aclanthology.org/2023.acl-long.764](https://aclanthology.org/2023.acl-long.764)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[51] L. Cui, Y. Wu, J. Liu, S. Yang, Y. Zhang, [Template-based named entity
    recognition using BART](https://aclanthology.org/2021.findings-acl.161), in: C. Zong,
    F. Xia, W. Li, R. Navigli (Eds.), Findings of the Association for Computational
    Linguistics: ACL-IJCNLP 2021, Association for Computational Linguistics, Online,
    2021, pp. 1835–1845. [doi:10.18653/v1/2021.findings-acl.161](https://doi.org/10.18653/v1/2021.findings-acl.161).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://aclanthology.org/2021.findings-acl.161](https://aclanthology.org/2021.findings-acl.161)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[52] Y. Shen, Z. Tan, S. Wu, W. Zhang, R. Zhang, Y. Xi, W. Lu, Y. Zhuang, [PromptNER:
    Prompt locating and typing for named entity recognition](https://aclanthology.org/2023.acl-long.698),
    in: A. Rogers, J. Boyd-Graber, N. Okazaki (Eds.), Proceedings of the 61st Annual
    Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
    Association for Computational Linguistics, Toronto, Canada, 2023, pp. 12492–12507.
    [doi:10.18653/v1/2023.acl-long.698](https://doi.org/10.18653/v1/2023.acl-long.698).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://aclanthology.org/2023.acl-long.698](https://aclanthology.org/2023.acl-long.698)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[53] F. Ye, L. Huang, S. Liang, K. Chi, [Decomposed two-stage prompt learning
    for few-shot named entity recognition](https://www.mdpi.com/2078-2489/14/5/262),
    Information 14 (5) (2023). [doi:10.3390/info14050262](https://doi.org/10.3390/info14050262).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://www.mdpi.com/2078-2489/14/5/262](https://www.mdpi.com/2078-2489/14/5/262)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[54] T. Schick, H. Schütze, [It’s not just size that matters: Small language
    models are also few-shot learners](https://aclanthology.org/2021.naacl-main.185),
    in: K. Toutanova, A. Rumshisky, L. Zettlemoyer, D. Hakkani-Tur, I. Beltagy, S. Bethard,
    R. Cotterell, T. Chakraborty, Y. Zhou (Eds.), Proceedings of the 2021 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies, Association for Computational Linguistics, Online,
    2021, pp. 2339–2352. [doi:10.18653/v1/2021.naacl-main.185](https://doi.org/10.18653/v1/2021.naacl-main.185).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://aclanthology.org/2021.naacl-main.185](https://aclanthology.org/2021.naacl-main.185)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[55] Y. Ge, Y. Guo, S. Das, M. A. Al-Garadi, A. Sarker, [Few-shot learning
    for medical text: A review of advances, trends, and opportunities](https://www.sciencedirect.com/science/article/pii/S153204642300179X),
    Journal of Biomedical Informatics 144 (2023) 104458. [doi:https://doi.org/10.1016/j.jbi.2023.104458](https://doi.org/https://doi.org/10.1016/j.jbi.2023.104458).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://www.sciencedirect.com/science/article/pii/S153204642300179X](https://www.sciencedirect.com/science/article/pii/S153204642300179X)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[56] A. Kormilitzin, N. Vaci, Q. Liu, A. Nevado-Holgado, [Med7: A transferable
    clinical natural language processing model for electronic health records](https://www.sciencedirect.com/science/article/pii/S0933365721000798),
    Artificial Intelligence in Medicine 118 (2021) 102086. [doi:https://doi.org/10.1016/j.artmed.2021.102086](https://doi.org/https://doi.org/10.1016/j.artmed.2021.102086).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://www.sciencedirect.com/science/article/pii/S0933365721000798](https://www.sciencedirect.com/science/article/pii/S0933365721000798)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[57] J. Huang, C. Li, K. Subudhi, D. Jose, S. Balakrishnan, W. Chen, B. Peng,
    J. Gao, J. Han, [Few-shot named entity recognition: An empirical baseline study](https://aclanthology.org/2021.emnlp-main.813),
    in: M.-F. Moens, X. Huang, L. Specia, S. W.-t. Yih (Eds.), Proceedings of the
    2021 Conference on Empirical Methods in Natural Language Processing, Association
    for Computational Linguistics, Online and Punta Cana, Dominican Republic, 2021,
    pp. 10408–10423. [doi:10.18653/v1/2021.emnlp-main.813](https://doi.org/10.18653/v1/2021.emnlp-main.813).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://aclanthology.org/2021.emnlp-main.813](https://aclanthology.org/2021.emnlp-main.813)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[58] Ö. Uzuner, B. R. South, S. Shen, S. L. DuVall, 2010 i2b2/va challenge
    on concepts, assertions, and relations in clinical text, Journal of the American
    Medical Informatics Association 18 (5) (2011) 552–556.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] B. Liao, Y. Meng, C. Monz, [Parameter-efficient fine-tuning without introducing
    new latency](https://aclanthology.org/2023.acl-long.233), in: A. Rogers, J. Boyd-Graber,
    N. Okazaki (Eds.), Proceedings of the 61st Annual Meeting of the Association for
    Computational Linguistics (Volume 1: Long Papers), Association for Computational
    Linguistics, Toronto, Canada, 2023, pp. 4242–4260. [doi:10.18653/v1/2023.acl-long.233](https://doi.org/10.18653/v1/2023.acl-long.233).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://aclanthology.org/2023.acl-long.233](https://aclanthology.org/2023.acl-long.233)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[60] T. Han, L. C. Adams, J.-M. Papaioannou, P. Grundmann, T. Oberhauser, A. Löser,
    D. Truhn, K. K. Bressem, Medalpaca–an open-source collection of medical conversational
    ai models and training data, arXiv preprint arXiv:2304.08247 (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] A. Toma, P. R. Lawler, J. Ba, R. G. Krishnan, B. B. Rubin, B. Wang, Clinical
    camel: An open-source expert-level medical language model with dialogue-based
    knowledge encoding, arXiv preprint arXiv:2305.12031 (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] J. Nothman, N. Ringland, W. Radford, T. Murphy, J. R. Curran, [Learning
    multilingual named entity recognition from wikipedia](https://www.sciencedirect.com/science/article/pii/S0004370212000276),
    Artificial Intelligence 194 (2013) 151–175, artificial Intelligence, Wikipedia
    and Semi-Structured Resources. [doi:https://doi.org/10.1016/j.artint.2012.03.006](https://doi.org/https://doi.org/10.1016/j.artint.2012.03.006).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://www.sciencedirect.com/science/article/pii/S0004370212000276](https://www.sciencedirect.com/science/article/pii/S0004370212000276)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[63] E. F. Tjong Kim Sang, [Introduction to the CoNLL-2002 shared task: Language-independent
    named entity recognition](https://aclanthology.org/W02-2024), in: COLING-02: The
    6th Conference on Natural Language Learning 2002 (CoNLL-2002), 2002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://aclanthology.org/W02-2024](https://aclanthology.org/W02-2024)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[64] E. F. Tjong Kim Sang, F. De Meulder, [Introduction to the CoNLL-2003 shared
    task: Language-independent named entity recognition](https://aclanthology.org/W03-0419),
    in: Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL
    2003, 2003, pp. 142–147.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://aclanthology.org/W03-0419](https://aclanthology.org/W03-0419)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[65] C. Grouin, S. Rosset, P. Zweigenbaum, K. Fort, O. Galibert, L. Quintard,
    [Proposal for an extension of traditional named entities: From guidelines to evaluation,
    an overview](https://aclanthology.org/W11-0411), in: N. Ide, A. Meyers, S. Pradhan,
    K. Tomanek (Eds.), Proceedings of the 5th Linguistic Annotation Workshop, Association
    for Computational Linguistics, Portland, Oregon, USA, 2011, pp. 92–100.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://aclanthology.org/W11-0411](https://aclanthology.org/W11-0411)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[66] B. Magnini, B. Altuna, A. Lavelli, M. Speranza, R. Zanoli, The e3c project:
    European clinical case corpus, Language 1 (L2) (2021) L3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Y.-F. Luo, W. Sun, A. Rumshisky, [Mcn: A comprehensive corpus for medical
    concept normalization](https://www.sciencedirect.com/science/article/pii/S1532046419300504),
    Journal of Biomedical Informatics 92 (2019) 103132. [doi:https://doi.org/10.1016/j.jbi.2019.103132](https://doi.org/https://doi.org/10.1016/j.jbi.2019.103132).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://www.sciencedirect.com/science/article/pii/S1532046419300504](https://www.sciencedirect.com/science/article/pii/S1532046419300504)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[68] D. A. Lindberg, B. L. Humphreys, A. T. McCray, The unified medical language
    system, Yearbook of medical informatics 2 (01) (1993) 41–51.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] A. McCray, A. Burgun, O. Bodenreider, Aggregating umls semantic types
    for reducing conceptual complexity, Studies in health technology and informatics
    84 (2001) 216–20. [doi:10.3233/978-1-60750-928-8-216](https://doi.org/10.3233/978-1-60750-928-8-216).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] L. Campillos, L. Deléger, C. Grouin, T. Hamon, A.-L. Ligozat, A. Névéol,
    A french clinical corpus with comprehensive semantic annotations: development
    of the medical entity and relation limsi annotated text corpus (merlot), Language
    Resources and Evaluation 52 (2018) 571–601.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov,
    S. Batra, P. Bhargava, S. Bhosale, et al., Llama 2: Open foundation and fine-tuned
    chat models, arXiv preprint arXiv:2307.09288 (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l.
    Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al., Mistral 7b, arXiv
    preprint arXiv:2310.06825 (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] B. Workshop, T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilić, D. Hesslow,
    R. Castagné, A. S. Luccioni, F. Yvon, et al., Bloom: A 176b-parameter open-access
    multilingual language model, arXiv preprint arXiv:2211.05100 (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] H. Laurençon, L. Saulnier, T. Wang, C. Akiki, A. Villanova del Moral,
    T. Le Scao, L. Von Werra, C. Mou, E. González Ponferrada, H. Nguyen, et al., The
    bigscience roots corpus: A 1.6 tb composite multilingual dataset, Advances in
    Neural Information Processing Systems 35 (2022) 31809–31826.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cappelli, H. Alobeidli,
    B. Pannier, E. Almazrouei, J. Launay, The refinedweb dataset for falcon llm: outperforming
    curated corpora with web data, and web data only, arXiv preprint arXiv:2306.01116
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] B. Wang, A. Komatsuzaki, GPT-J-6B: A 6 Billion Parameter Autoregressive
    Language Model, [https://github.com/kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax)
    (May 2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang,
    H. He, A. Thite, N. Nabeshima, et al., The pile: An 800gb dataset of diverse text
    for language modeling, arXiv preprint arXiv:2101.00027 (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan,
    M. Diab, X. Li, X. V. Lin, et al., Opt: Open pre-trained transformer language
    models, arXiv preprint arXiv:2205.01068 (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin,
    Z. Li, D. Li, E. Xing, et al., Judging llm-as-a-judge with mt-bench and chatbot
    arena, arXiv preprint arXiv:2306.05685 (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] A. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek, F. Guzmán,
    E. Grave, M. Ott, L. Zettlemoyer, V. Stoyanov, [Unsupervised cross-lingual representation
    learning at scale](https://aclanthology.org/2020.acl-main.747), in: D. Jurafsky,
    J. Chai, N. Schluter, J. Tetreault (Eds.), Proceedings of the 58th Annual Meeting
    of the Association for Computational Linguistics, Association for Computational
    Linguistics, Online, 2020, pp. 8440–8451. [doi:10.18653/v1/2020.acl-main.747](https://doi.org/10.18653/v1/2020.acl-main.747).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://aclanthology.org/2020.acl-main.747](https://aclanthology.org/2020.acl-main.747)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[81] Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba,
    S. Fidler, Aligning books and movies: Towards story-like visual explanations by
    watching movies and reading books, in: Proceedings of the IEEE international conference
    on computer vision, 2015, pp. 19–27.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,
    L. Zettlemoyer, V. Stoyanov, Roberta: A robustly optimized bert pretraining approach,
    arXiv preprint arXiv:1907.11692 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] E. Alsentzer, J. Murphy, W. Boag, W.-H. Weng, D. Jindi, T. Naumann, M. McDermott,
    [Publicly available clinical BERT embeddings](https://aclanthology.org/W19-1909),
    in: A. Rumshisky, K. Roberts, S. Bethard, T. Naumann (Eds.), Proceedings of the
    2nd Clinical Natural Language Processing Workshop, Association for Computational
    Linguistics, Minneapolis, Minnesota, USA, 2019, pp. 72–78. [doi:10.18653/v1/W19-1909](https://doi.org/10.18653/v1/W19-1909).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://aclanthology.org/W19-1909](https://aclanthology.org/W19-1909)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[84] A. E. Johnson, T. J. Pollard, L. Shen, L.-w. H. Lehman, M. Feng, M. Ghassemi,
    B. Moody, P. Szolovits, L. Anthony Celi, R. G. Mark, Mimic-iii, a freely accessible
    critical care database, Scientific data 3 (1) (2016) 1–9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] G. Wang, X. Liu, Z. Ying, G. Yang, Z. Chen, Z. Liu, M. Zhang, H. Yan,
    Y. Lu, Y. Gao, et al., Optimized glycemic control of type 2 diabetes with reinforcement
    learning: a proof-of-concept trial, Nature Medicine (2023) 1–10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] C. Vasantharajan, K. Z. Tun, H. Thi-Nga, S. Jain, T. Rong, C. E. Siong,
    Medbert: A pre-trained language model for biomedical named entity recognition,
    in: 2022 Asia-Pacific Signal and Information Processing Association Annual Summit
    and Conference (APSIPA ASC), 2022, pp. 1482–1488. [doi:10.23919/APSIPAASC55919.2022.9980157](https://doi.org/10.23919/APSIPAASC55919.2022.9980157).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] L. Martin, B. Muller, P. J. Ortiz Suárez, Y. Dupont, L. Romary, É. de la
    Clergerie, D. Seddah, B. Sagot, [CamemBERT: a tasty French language model](https://aclanthology.org/2020.acl-main.645),
    in: D. Jurafsky, J. Chai, N. Schluter, J. Tetreault (Eds.), Proceedings of the
    58th Annual Meeting of the Association for Computational Linguistics, Association
    for Computational Linguistics, Online, 2020, pp. 7203–7219. [doi:10.18653/v1/2020.acl-main.645](https://doi.org/10.18653/v1/2020.acl-main.645).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://aclanthology.org/2020.acl-main.645](https://aclanthology.org/2020.acl-main.645)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[88] P. J. O. Suárez, L. Romary, B. Sagot, A monolingual approach to contextualized
    word embeddings for mid-resource languages, arXiv preprint arXiv:2006.06202 (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] H. Le, L. Vial, J. Frej, V. Segonne, M. Coavoux, B. Lecouteux, A. Allauzen,
    B. Crabbé, L. Besacier, D. Schwab, [FlauBERT: Unsupervised language model pre-training
    for French](https://aclanthology.org/2020.lrec-1.302), in: N. Calzolari, F. Béchet,
    P. Blache, K. Choukri, C. Cieri, T. Declerck, S. Goggi, H. Isahara, B. Maegaard,
    J. Mariani, H. Mazo, A. Moreno, J. Odijk, S. Piperidis (Eds.), Proceedings of
    the Twelfth Language Resources and Evaluation Conference, European Language Resources
    Association, Marseille, France, 2020, pp. 2479–2490.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://aclanthology.org/2020.lrec-1.302](https://aclanthology.org/2020.lrec-1.302)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[90] Y. Labrak, A. Bazoge, R. Dufour, M. Rouvier, E. Morin, B. Daille, P.-A.
    Gourraud, [DrBERT: A robust pre-trained model in French for biomedical and clinical
    domains](https://aclanthology.org/2023.acl-long.896), in: A. Rogers, J. Boyd-Graber,
    N. Okazaki (Eds.), Proceedings of the 61st Annual Meeting of the Association for
    Computational Linguistics (Volume 1: Long Papers), Association for Computational
    Linguistics, Toronto, Canada, 2023, pp. 16207–16221. [doi:10.18653/v1/2023.acl-long.896](https://doi.org/10.18653/v1/2023.acl-long.896).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://aclanthology.org/2023.acl-long.896](https://aclanthology.org/2023.acl-long.896)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[91] R. Touchent, L. Romary, E. De La Clergerie, [CamemBERT-bio : Un modèle
    de langue français savoureux et meilleur pour la santé](https://hal.science/hal-04130187),
    in: C. Servan, A. Vilnat (Eds.), 18e Conférence en Recherche d’Information et
    Applications'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 16e Rencontres Jeunes Chercheurs en RI
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 30e Conférence sur le Traitement Automatique des Langues Naturelles
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 25e Rencontre des Étudiants Chercheurs en Informatique pour le Traitement Automatique
    des Langues, ATALA, Paris, France, 2023, pp. 323–334.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: URL [https://hal.science/hal-04130187](https://hal.science/hal-04130187)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[92] J. Cañete, G. Chaperon, R. Fuentes, J.-H. Ho, H. Kang, J. Pérez, Spanish
    pre-trained bert model and evaluation data, in: PML4DC at ICLR 2020, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] J. Tiedemann, [Parallel data, tools and interfaces in OPUS](http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf),
    in: N. Calzolari, K. Choukri, T. Declerck, M. U. Doğan, B. Maegaard, J. Mariani,
    A. Moreno, J. Odijk, S. Piperidis (Eds.), Proceedings of the Eighth International
    Conference on Language Resources and Evaluation (LREC’12), European Language Resources
    Association (ELRA), Istanbul, Turkey, 2012, pp. 2214–2218.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf](http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[94] C. P. Carrino, J. Llop, M. Pàmies, A. Gutiérrez-Fandiño, J. Armengol-Estapé,
    J. Silveira-Ocampo, A. Valencia, A. Gonzalez-Agirre, M. Villegas, [Pretrained
    biomedical language models for clinical NLP in Spanish](https://aclanthology.org/2022.bionlp-1.19),
    in: Proceedings of the 21st Workshop on Biomedical Language Processing, Association
    for Computational Linguistics, Dublin, Ireland, 2022, pp. 193–199. [doi:10.18653/v1/2022.bionlp-1.19](https://doi.org/10.18653/v1/2022.bionlp-1.19).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://aclanthology.org/2022.bionlp-1.19](https://aclanthology.org/2022.bionlp-1.19)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[95] P. Wajsbürt, [Extraction and normalization of simple and structured entities
    in medical documents](https://hal.archives-ouvertes.fr/tel-03624928), Theses,
    Sorbonne Université (Dec. 2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://hal.archives-ouvertes.fr/tel-03624928](https://hal.archives-ouvertes.fr/tel-03624928)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[96] L. Lannelongue, J. Grealey, M. Inouye, Green algorithms: quantifying the
    carbon footprint of computation, Advanced science 8 (12) (2021) 2100707.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] N. Reimers, I. Gurevych, [Reporting score distributions makes a difference:
    Performance study of LSTM-networks for sequence tagging](https://aclanthology.org/D17-1035),
    in: M. Palmer, R. Hwa, S. Riedel (Eds.), Proceedings of the 2017 Conference on
    Empirical Methods in Natural Language Processing, Association for Computational
    Linguistics, Copenhagen, Denmark, 2017, pp. 338–348. [doi:10.18653/v1/D17-1035](https://doi.org/10.18653/v1/D17-1035).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://aclanthology.org/D17-1035](https://aclanthology.org/D17-1035)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[98] R. Dror, S. Shlomov, R. Reichart, [Deep dominance - how to properly compare
    deep neural models](https://doi.org/10.18653/v1/p19-1266), in: A. Korhonen, D. R.
    Traum, L. Màrquez (Eds.), Proceedings of the 57th Conference of the Association
    for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019,
    Volume 1: Long Papers, Association for Computational Linguistics, 2019, pp. 2773–2785.
    [doi:10.18653/v1/p19-1266](https://doi.org/10.18653/v1/p19-1266).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://doi.org/10.18653/v1/p19-1266](https://doi.org/10.18653/v1/p19-1266)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[99] E. M. Bender, B. Friedman, [Data statements for natural language processing:
    Toward mitigating system bias and enabling better science](https://aclanthology.org/Q18-1041),
    Transactions of the Association for Computational Linguistics 6 (2018) 587–604.
    [doi:10.1162/tacl_a_00041](https://doi.org/10.1162/tacl_a_00041).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://aclanthology.org/Q18-1041](https://aclanthology.org/Q18-1041)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Appendix A NER labels descriptions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Appendix B Carbon footprint
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
