- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:43:07'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Wav2Prompt: End-to-End Speech Prompt Generation and Tuning For LLM in Zero
    and Few-shot Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.00522](https://ar5iv.labs.arxiv.org/html/2406.00522)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Keqi Deng, Guangzhi Sun, Philip C. Woodland
  prefs: []
  type: TYPE_NORMAL
- en: Department of Engineering, University of Cambridge
  prefs: []
  type: TYPE_NORMAL
- en: Trumpington St., Cambridge, UK
  prefs: []
  type: TYPE_NORMAL
- en: '{kd502, gs534, pw117}@cam.ac.uk'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Wav2Prompt is proposed which allows straightforward integration between spoken
    input and a text-based large language model (LLM). Wav2Prompt uses a simple training
    process with only the same data used to train an automatic speech recognition
    (ASR) model. After training, Wav2Prompt learns continuous representations from
    speech and uses them as LLM prompts. To avoid task over-fitting issues found in
    prior work and preserve the emergent abilities of LLMs, Wav2Prompt takes LLM token
    embeddings as the training targets and utilises a continuous integrate-and-fire
    mechanism for explicit speech-text alignment. Therefore, a Wav2Prompt-LLM combination
    can be applied to zero-shot spoken language tasks such as speech translation (ST),
    speech understanding (SLU), speech question answering (SQA) and spoken-query-based
    QA (SQQA). It is shown that for these zero-shot tasks, Wav2Prompt performs similarly
    to an ASR-LLM cascade and better than recent prior work. If relatively small amounts
    of task-specific paired data are available in few-shot scenarios, the Wav2Prompt-LLM
    combination can be end-to-end (E2E) fine-tuned. The Wav2Prompt-LLM combination
    then yields greatly improved results relative to an ASR-LLM cascade for the above
    tasks. For instance, for English-French ST with the BLOOMZ-7B1 LLM, a Wav2Prompt-LLM
    combination gave a 8.5 BLEU point increase over an ASR-LLM cascade.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Text-based large language models (LLMs) [[3](#bib.bib3), [37](#bib.bib37), [38](#bib.bib38),
    [28](#bib.bib28), [18](#bib.bib18)] have achieved remarkable performance in a
    wide range of natural language processing (NLP) tasks [[1](#bib.bib1)]. LLMs are
    trained on huge quantities of text and are highly flexible. They are able to be
    applied to a range of tasks that they have not been explicitly trained for, known
    as the LLM emergent abilities [[42](#bib.bib42), [36](#bib.bib36)]. To further
    expand the use-cases of LLMs, it is important to enable LLMs to handle other modalities
    including spoken input.
  prefs: []
  type: TYPE_NORMAL
- en: The conventional approach is to use an automatic speech recognition (ASR) model
    to transcribe speech into text, which is then used as the LLM input. However,
    this cascaded system suffers from error accumulation and loses valuable acoustic
    information [[9](#bib.bib9)]. Many studies have explored connecting LLMs directly
    to the speech acoustic encoder (Encoder-LLM) for various speech tasks such as
    ASR or speech translation (ST) [[8](#bib.bib8), [44](#bib.bib44), [48](#bib.bib48),
    [4](#bib.bib4)]. However, these approaches restrict the system to a specific task,
    thereby losing the ability of LLMs to handle a wide range of zero-shot spoken
    language tasks. Recent work has begun to explore ways to restore the zero-shot
    capabilities of LLMs, including efforts on audio or speech question-answering
    (QA) data [[12](#bib.bib12), [9](#bib.bib9)] and the addition of extra steps such
    as instruction tuning and activation tuning in the training pipeline [[36](#bib.bib36)].
    However, these approaches greatly complicate model training.
  prefs: []
  type: TYPE_NORMAL
- en: This paper proposes Wav2Prompt which allows straightforward integration between
    spoken input and an off-the-shelf text-based LLM. Wav2Prompt uses a simple training
    process with the same data as used to train an ASR model. After training, Wav2Prompt
    generates representations from speech and uses them as LLM prompts for downstream
    tasks. It allows the Wav2Prompt-LLM combination to work well in a range of zero-shot
    spoken language tasks. However, Wav2Prompt can also give much improved performance
    when task-specific spoken language data is available in few-shot scenarios via
    end-to-end (E2E) fine-tuning of Wav2Prompt, without updating the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Wav2Prompt takes the LLM token embeddings as the training targets to naturally
    maintain the zero-shot capability of text-based LLMs. This is a key difference
    to the conventional ASR task, which takes discrete text tokens as the only target
    and thus leads to the ASR-LLM cascade approach. However, learning LLM token embeddings
    is challenging for speech models given the difference in input sequence length
    between speech and text. Wav2Prompt addresses this issue using a continuous integrate-and-fire
    (CIF) [[7](#bib.bib7)] mechanism to generate a label-level speech representation
    and mean squared error (MSE) can be used to enforce consistency with the LLM token
    embeddings. Wav2Prompt can therefore be combined with LLM not only for zero-shot
    speech tasks, but also for E2E fine-tuning in few-shot scenarios, which is a key
    advantage compared to an ASR-LLM cascade.
  prefs: []
  type: TYPE_NORMAL
- en: Experiments were conducted to evaluate Wav2Prompt on diverse spoken language
    tasks including speech translation (ST), spoken language understanding (SLU),
    speech question answering (SQA), and spoken-query-based QA (SQQA), all of these
    are unseen during training as Wav2Prompt only uses ASR data for training. The
    results show that the Wav2Prompt-LLM combination could achieve similar performance
    to the ASR-LLM cascade in zero-shot cases and greatly surpasses the existing Encoder-LLM
    method [[8](#bib.bib8)]. In few-shot scenarios, after utilising the limited task-specific
    available data for E2E fine-tuning, Wav2Prompt showed improved performance over
    the ASR-LLM cascade for all of these tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main contributions of this paper can be summarised in three main parts:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wav2Prompt is proposed, which is, to the best of our knowledge, the first step
    towards extending LLMs to a range of zero-shot spoken language tasks using only
    ASR data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Task over-fitting to training data is a key issue addressed by Wav2Prompt which
    has previously limited the application of acoustic encoder enabled LLMs to other
    spoken language tasks in a zero-shot fashion [[36](#bib.bib36)]. This issue is
    thoroughly analysed and it is shown that the key step to unlock zero-shot capability
    is learning LLM token embeddings.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wav2Prompt achieves similar performance to an ASR-LLM cascade in a range of
    zero-shot speech tasks. In few-shot scenarios, Wav2Prompt greatly surpasses the
    performance of an ASR-LLM cascade by leveraging the advantage of E2E fine-tuning.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The rest of the paper is organised as follows. Section [2](#S2 "2 Related Work
    ‣ Wav2Prompt: End-to-End Speech Prompt Generation and Tuning For LLM in Zero and
    Few-shot Learning") reviews related work and discusses differences to Wav2Prompt.
    Section [3](#S3 "3 Analysis of task over-fitting ‣ Wav2Prompt: End-to-End Speech
    Prompt Generation and Tuning For LLM in Zero and Few-shot Learning") analyses
    the task over-fitting issue. Section [4](#S4 "4 Wav2Prompt ‣ Wav2Prompt: End-to-End
    Speech Prompt Generation and Tuning For LLM in Zero and Few-shot Learning") describes
    Wav2Prompt in detail. Section [5](#S5 "5 Experimental setup ‣ Wav2Prompt: End-to-End
    Speech Prompt Generation and Tuning For LLM in Zero and Few-shot Learning") and
     [6](#S6 "6 Experimental results ‣ Wav2Prompt: End-to-End Speech Prompt Generation
    and Tuning For LLM in Zero and Few-shot Learning") detail the experimental setup
    and results. Finally, Section [7](#S7 "7 Conclusions ‣ Wav2Prompt: End-to-End
    Speech Prompt Generation and Tuning For LLM in Zero and Few-shot Learning") concludes.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Text-based Large Language Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The evolution of text-based LLMs, exemplified by a large increase in model parameters
    and training data seen in GPT-3 [[3](#bib.bib3)] and PaLM [[5](#bib.bib5)], has
    revolutionised NLP tasks. This progress has facilitated the development of advanced
    models such as GPT-4 [[1](#bib.bib1)], showcasing the remarkable capabilities
    of LLMs in various domains. Alongside these advancements, "smaller" LLMs like
    LLaMa [[37](#bib.bib37), [38](#bib.bib38)] have been introduced, achieving a better
    balance between performance and computational resources. There are several variant
    models like Vicuna [[49](#bib.bib49)] developed from conversation-based fine-tuning
    and multi-lingual LLM, i.e. BLOOM [[18](#bib.bib18)]. One key aspect of LLMs is
    that they can exhibit remarkable performance in a range of tasks on which they
    have never been explicitly trained. Examples include zero-shot task transfer [[32](#bib.bib32)]
    and few-shot learning [[3](#bib.bib3)]. This is sometimes known as the emergent
    abilities of LLMs [[36](#bib.bib36), [24](#bib.bib24)].
  prefs: []
  type: TYPE_NORMAL
- en: Speech-Enabled Large Language Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While discrete self-supervised representations have been explored to build spoken
    generative LMs [[2](#bib.bib2), [40](#bib.bib40)], this paper focuses on utilising
    off-the-shelf text-based LLMs. Recently, several studies have worked on building
    speech-enabled LLMs to support direct speech input [[8](#bib.bib8), [44](#bib.bib44),
    [48](#bib.bib48), [4](#bib.bib4), [16](#bib.bib16)]. Since the speech input sequence
    is normally much longer than the corresponding text, different strategies have
    been studied for down-sampling. [[8](#bib.bib8), [48](#bib.bib48), [25](#bib.bib25)]
    stacks the acoustic encoder output to achieve fixed-rate reduction. [[4](#bib.bib4)]
    treats multiple modalities as foreign languages, in which CIF is used to obtain
    the speech representation. [[48](#bib.bib48)] explored the use of Q-Former [[20](#bib.bib20)]
    which transforms input sequences of varying lengths into fixed-length outputs.
    However, the emergent abilities of LLM are lost in this task-specific training.
    Unlike work such as [[4](#bib.bib4)], which focus on enabling LLMs to handle multi-modal
    inputs including speech, this paper focuses on maintaining the zero-shot abilities
    of LLMs. Some recent work explored regaining the zero-shot abilities of LLMs.
    In [[36](#bib.bib36)], an instruction tuning stage followed by an activation tuning
    stage was introduced after pre-training to alleviate task over-fitting. In addition,
    [[9](#bib.bib9)] simulated a speech QA dataset from ASR data, finding that a speech-enabled
    LLM trained on it could handle spoken QA tasks and potentially other tasks like
    ST. Prior work extended speech capabilities for LLM, but they come at the cost
    of increased complexity and extensive resources based on full-parameter or parameter-efficient
    training. Since text-based LLMs have a fundamental connection with speech, Wav2Prompt
    focuses on training a model that can be combined and E2E fine-tuned with a text-based
    LLM through a simple process while keeping the LLM fixed.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt Tuning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Fine-tuning LLMs can be expensive. As an alternative, the prompting technique
    fixes all LLM parameters and uses a prompt to query LLMs for diverse tasks [[21](#bib.bib21)].
    Early prompting uses simple keyword-based inputs or fill-in-the-blank style prompts
    [[11](#bib.bib11), [34](#bib.bib34)]. For generative LLM, natural language prompts
    can be used [[39](#bib.bib39), [3](#bib.bib3)]. However, these discrete prompts
    can result in sub-optimal performance in numerous cases [[35](#bib.bib35), [21](#bib.bib21)].
    Instead, prompt tuning adds trainable continuous embeddings, i.e. continuous prompts,
    to the original input token embedding [[22](#bib.bib22), [19](#bib.bib19)]. During
    training, only the parameters of the continuous prompts are updated [[21](#bib.bib21)].
    This paper follows the prompt tuning, updating only the parameters of Wav2Prompt
    while keeping LLM fixed in the few-shot scenario.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Analysis of task over-fitting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Task over-fitting [[36](#bib.bib36)] occurs when the speech-enabled LLM can
    only perform tasks that are seen during supervised training and shows limited
    performance on unseen tasks. This section provides a detailed analysis of this
    issue which leads to the proposed Wav2Prompt method.
  prefs: []
  type: TYPE_NORMAL
- en: 'To connect a decoder-only LLM with speech input, speech representations can
    be prepended to the original text token embedding sequence and the LLM will be
    conditioned on these speech representations when predicting the next token in
    order to perform speech tasks. To be more specific, in the normal case with a
    text-based user-input prompt supervision, the next token probabilities of LLM
    can be formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $p_{n}=p(y_{n}&#124;\bm{Y}_{0:n-1},\textbf{P},\Gamma)$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $\bm{Y}_{0:n-1}=([sos],y_{1},...,y_{n-1})$.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is counter-intuitive to let a fixed text-based LLM attend to speech representation
    S as it has never seen the speech input at pre-training. However, after E2E training
    on supervised data, prior work has shown connecting a fixed LLM with the acoustic
    encoder (referred to as Encoder-LLM) can perform ASR tasks [[8](#bib.bib8), [48](#bib.bib48)].
    Since the main building block of LLMs is the attention mechanism which is also
    the first module that interacts with inputs, the process is simplified to a single
    attention function ${\rm Att}(Q,K,V)$, Eq. [1](#S3.E1 "In 3 Analysis of task over-fitting
    ‣ Wav2Prompt: End-to-End Speech Prompt Generation and Tuning For LLM in Zero and
    Few-shot Learning") can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bm{l}_{n}={\rm Att}(\bm{z}_{n-1},[\textbf{Z};\textbf{P};\Gamma],[\textbf{Z};\textbf{P};\Gamma])$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where $\bm{l}_{n}$ is still possible to achieve. Since the attention mechanism
    is a weighted sum, as long as the weighted sum corresponding to the use of S and
    P are consistent, correct predictions can be obtained.
  prefs: []
  type: TYPE_NORMAL
- en: However, $z_{n-1}$ for other tasks, leading to so-called task over-fitting.
    For example, preliminary experiments found that the Encoder-LLM-based ASR system
    has trouble following new instructions to perform zero-shot ST.
  prefs: []
  type: TYPE_NORMAL
- en: Prior works relied on task-specific E2E training to implicitly optimise S [[8](#bib.bib8),
    [48](#bib.bib48), [4](#bib.bib4)], and through complex training pipelines to gradually
    enable S learning a correct alignment that can preserve the LLM zero-shot capability
    [[36](#bib.bib36)]. However, to learn a correct S, there is already a feasible
    and clear target, which is the LLM token embedding given that LLM can always flexibly
    handle text inputs for zero-shot tasks. Unlike prior work, this paper proposes
    using LLM embeddings as the target to explicitly guide the learning of the speech
    representation S, which greatly simplifies the process and only requires ASR data
    for training.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Wav2Prompt
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper proposes Wav2Prompt, a model that naturally enables text-based LLM
    to handle speech input while maintaining the zero-shot capabilities of the original
    LLM. Wav2Prompt provides a straightforward process that can be combined with LLM
    using only ASR data for training and does not require subsequent multi-stage tuning,
    it can serve as an E2E alternative approach that is superior to the conventional
    ASR-LLM cascade.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Wav2Prompt architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8538b7f173079fb3d97201aef29ed0b3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Illustration of the proposed Wav2Prompt architecture. $\bm{\oplus}$
    denotes addition. Prefix and postfix text are task-specific prompt templates that
    can contain instructions. Their embeddings are obtained through the LLM embedding
    layer, and the transcript token embeddings are the same.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The proposed Wav2Prompt is illustrated in Fig. [1](#S4.F1 "Figure 1 ‣ 4.1 Wav2Prompt
    architecture ‣ 4 Wav2Prompt ‣ Wav2Prompt: End-to-End Speech Prompt Generation
    and Tuning For LLM in Zero and Few-shot Learning"), which contains three main
    components: an acoustic encoder, a CIF module, and an LLM (including the LLM embedding
    layer). The acoustic encoder and the CIF module extract a label-level speech representation
    $\textbf{S}=(\bm{s}_{1},\cdots,\bm{s}_{M})$, which have the same length as the
    transcript text token embeddings so that mean squared error (MSE) loss can be
    used to enforce the representation consistency between them. This is one of the
    main differences from prior work [[8](#bib.bib8), [44](#bib.bib44), [48](#bib.bib48),
    [4](#bib.bib4), [36](#bib.bib36), [25](#bib.bib25)], which instead simply down-sampled
    the acoustic encoder output before feeding it into the LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, LLM refers to the text-based decoder-only LLM, such as LLaMA
    [[37](#bib.bib37), [38](#bib.bib38)]. The output of the CIF module is used as
    the prompt for LLM, while the parameters of LLM are always kept fixed (shown in
    grey in Fig. [1](#S4.F1 "Figure 1 ‣ 4.1 Wav2Prompt architecture ‣ 4 Wav2Prompt
    ‣ Wav2Prompt: End-to-End Speech Prompt Generation and Tuning For LLM in Zero and
    Few-shot Learning")) following prompt tuning [[21](#bib.bib21)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'The acoustic encoder employs a Conformer [[14](#bib.bib14)] structure. Denote
    the Conformer-based encoder output as $\textbf{E}=(\bm{e}_{1},\cdots,\bm{e}_{T})$
    is obtained via:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where FC represents a fully connected layer that maps $\bm{e}_{t,1:d-1}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'The label-level speech representation from CIF is then fed into the LLM as
    a prompt along with task-specific prompt templates that can contain instructions,
    denoted as prefix and postfix text in Fig. [1](#S4.F1 "Figure 1 ‣ 4.1 Wav2Prompt
    architecture ‣ 4 Wav2Prompt ‣ Wav2Prompt: End-to-End Speech Prompt Generation
    and Tuning For LLM in Zero and Few-shot Learning"). Suppose $\textbf{emb}^{\rm
    pre}$ is computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\textbf{L}={\rm LLM}({\rm Concat}(\textbf{emb}^{\rm pre},\textbf{S},\textbf{emb}^{\rm
    post},\textbf{Z}))$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where $\textbf{Z}=(\bm{z}_{0},\bm{z}_{1},\cdots,\bm{z}_{N})$.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Wav2Prompt is trained using only ASR data. First, the scaled weight $\hat{\alpha}_{t}$
    is used as the training target of S, and an MSE loss is computed:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\rm MSE}=\sum_{m=1}^{M}{\rm MSE}(\bm{s}_{m},\bm{p}_{m})$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: 'In addition, P is fed into the LLM and a cross-entropy (CE) loss $\mathcal{L}_{\rm
    CE}$ of Wav2Prompt is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\rm Train}=\mathcal{L}_{\rm CE}+\gamma\mathcal{L}_{\rm MSE}+\mu\mathcal{L}_{\rm
    qua}$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: where $\gamma$ are hyper-parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Application for zero-shot and few-shot tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Wav2Prompt is only trained on the ASR task, but it can be combined with the
    text-based LLM (denoted as Wav2Prompt-LLM) for other speech tasks in zero-shot
    and few-shot fashions. In this paper, zero-shot refers to not using any task-specific
    paired data for fine-tuning. For example, for the speech translation (ST) task,
    zero-shot refers to not fine-tuning the Wav2prompt-LLM system using paired source-language
    speech and target-language text data. Few-shot in this paper refers to using a
    very limited amount of paired data for fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-shot
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Wav2Prompt preserves the flexible zero-shot capabilities of LLMs. With ASR-data
    trained Wav2Prompt, the generated label-level speech representation S is fed into
    the LLM as a prompt, and only the instructions in the prefix and postfix text
    (as in Fig. [1](#S4.F1 "Figure 1 ‣ 4.1 Wav2Prompt architecture ‣ 4 Wav2Prompt
    ‣ Wav2Prompt: End-to-End Speech Prompt Generation and Tuning For LLM in Zero and
    Few-shot Learning")) need to be modified for unseen tasks. For example, to perform
    the ST task, the postfix text becomes “Translate the English text into French".
    The original weight ${\alpha}_{t}$ as the transcript length is unknown during
    inference. There is essentially no difference from a normal text-based LLM for
    different text tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Few-shot
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Compared to an ASR-LLM cascade, an important advantage of Wav2Prompt is that
    it can be E2E combined with an LLM so that paired data can be used to fine-tune
    in an E2E fashion. In the few-shot case, the prefix and postfix text are modified
    as in the zero-shot case. In addition, during E2E fine-tuning, in order to simplify
    the process, the MSE loss is not used as it requires high-quality ASR transcription
    and the goal here is to learn the downstream task rather than to keep the zero-shot
    ability. Another benefit is that the length of the speech representation S does
    not need to exactly match that of its corresponding transcript, allowing ${\alpha}_{t}$
    of Wav2Prompt is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\rm tune}=\mathcal{L}_{\rm CE}+\mu\mathcal{L}_{\rm qua}$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: 'where hyper-parameter $\mu$ keeps the same as Eq. [6](#S4.E6 "In 4.2 Training
    ‣ 4 Wav2Prompt ‣ Wav2Prompt: End-to-End Speech Prompt Generation and Tuning For
    LLM in Zero and Few-shot Learning") for simplicity.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experimental setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After training on ASR data, Wav2Prompt was evaluated on a range of unseen tasks,
    including speech translation (ST), spoken language understanding (SLU), speech
    question answering (SQA), and spoken-query-based QA (SQQA) tasks. For SQA, the
    system needs to answer text-based questions according to the content of the speech,
    while for SQQA, it needs to answer the spoken question.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ST experiments were conducted on Europarl-ST [[17](#bib.bib17)] English-Spanish
    (En-ES) and English-French (En-Fr) pairs. The corresponding English ASR data was
    used to train Wav2Prompt and the ASR models. For the few-shot scenarios, 10-hour
    paired data was randomly selected from the training data set as limited fine-tuning
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'For SLU, SQA, and SQQA tasks, the LibriSpeech corpus [[30](#bib.bib30)] was
    used as the ASR data. For SLU, the Fluent Speech Commands (FSC) corpus [[23](#bib.bib23)]
    was used to conduct the intent classification task. In the few-shot case, 2 hours
    of paired data were randomly selected from the training data set as limited fine-tuning
    data. For the SQA task, the question-answer pairs provided by [[36](#bib.bib36)]
    were used to augment the dev-clean set. For the SQQA task, the WikiQA [[46](#bib.bib46)]
    test set with synthesised speech queries provided by [[36](#bib.bib36)] was used.
    More details of the datasets used are listed in Appendix [A](#A1 "Appendix A Dat
    Set Statistics ‣ Wav2Prompt: End-to-End Speech Prompt Generation and Tuning For
    LLM in Zero and Few-shot Learning").'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Model specifications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Four different systems were built to compare with Wav2Prompt, and all these
    models used a 12-layer Conformer encoder. The LLMs used in this paper were always
    fixed following prompt tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Wav2Prompt-LLM
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Based on the Conformer encoder, Wav2Prompt only used an extra fully connected
    (FC) layer that mapped the speech representation to the LLM embedding dimension
    (i.e. 4096) as mentioned in Eq. [3](#S4.E3 "In 4.1 Wav2Prompt architecture ‣ 4
    Wav2Prompt ‣ Wav2Prompt: End-to-End Speech Prompt Generation and Tuning For LLM
    in Zero and Few-shot Learning"). $\gamma$ in Eq. [6](#S4.E6 "In 4.2 Training ‣
    4 Wav2Prompt ‣ Wav2Prompt: End-to-End Speech Prompt Generation and Tuning For
    LLM in Zero and Few-shot Learning") were set to 20 and 0.05, respectively. The
    LLM was fixed.'
  prefs: []
  type: TYPE_NORMAL
- en: ASR-LLM Cascade
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Based on the Conformer encoder, a connectionist temporal classification [[13](#bib.bib13)]
    (CTC)-based ASR model was built and only had an extra FC output layer that mapped
    the encoder output to the vocabulary size (1000) and BPE [[10](#bib.bib10)] modelling
    units were used. The recognised text from the ASR model was fed into the LLM,
    along with the prefix and postfix text, forming a cascaded system. In this paper,
    the proposed Wav2Prompt aims to achieve similar results to the ASR-LLM Cascade
    in the zero-shot scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Oracle-LLM
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An oracle system was built, in which the speech ground truth transcripts were
    fed into the LLM. The prefix and postfix text remained the same as the ASR-LLM
    Cascade system.
  prefs: []
  type: TYPE_NORMAL
- en: Encoder-LLM
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A prior speech-enabled LLM method [[8](#bib.bib8)] was implemented. Based on
    the Conformer encoder, every 8 consecutive encoder output frames were stacked
    to down-sample the sequence length. Then, an extra FC layer was used to map the
    stacked encoder output to the LLM embedding dimension (i.e. 4096) before being
    fed into the LLM just like Wav2Prompt. Following [[8](#bib.bib8)], the trained
    CTC-based ASR model was used to initialise the encoder parameters before the Encoder-LLM
    was E2E trained on ASR data. In this paper, the proposed Wav2Prompt aims to greatly
    surpass the Encoder-LLM in the zero-shot scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Flat-start Encoder-LLM
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This paper further explores directly training the Encoder-LLM on unseen tasks
    via few-shot learning in an E2E fashion without training on ASR data. This is
    to evaluate the importance of the alignment learned during ASR training to other
    tasks. This system is referred to as Flat-start Encoder-LLM in this paper. The
    encoder was still initialised by the CTC-based ASR model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the trainable component of all these built systems (except for the
    Oracle-LLM) consisted of the same encoder along with an additional FC layer. More
    details and the task-specific prompt templates used can be found in Appendix [B](#A2
    "Appendix B Training and hyper-parameter details ‣ Wav2Prompt: End-to-End Speech
    Prompt Generation and Tuning For LLM in Zero and Few-shot Learning") and [D](#A4
    "Appendix D Task-specific prompt templates ‣ Wav2Prompt: End-to-End Speech Prompt
    Generation and Tuning For LLM in Zero and Few-shot Learning").'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 LLMs and metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the ST task, LLaMA-2-7B was used. To evaluate the proposed Wav2Prompt on
    an LLM more proficient in translation, BLOOMZ-7B1 [[27](#bib.bib27)] was also
    employed. Case-sensitive detokenised BLEU [[31](#bib.bib31)] results are reported
    to evaluate translation quality.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the SLU, SQA, and SQQA tasks, Vicuna-7B-1.5 [[49](#bib.bib49)] was used,
    and ASR performance was also evaluated. Vicuna-7B-1.5 is a fine-tuned version
    of LLaMA-2 and adapted specifically to chat applications. Therefore, it shows
    good performance in following user instructions and is suitable for SLU and QA
    tasks that require an understanding capability. The word error rate (WER) was
    used to evaluate the ASR quality. For the SLU task, accuracy was used to measure
    the intent classification. For the SQA and SQQA task, following [[26](#bib.bib26)],
    Mistral-7B-Instruct-v0.2, which is an instruction fine-tuned LLM, was used to
    evaluate whether the answers predicted were correct based on the question and
    the right answer. Accuracy was used as the metric. The prompt template used to
    measure the accuracy was listed in Appendix [C](#A3 "Appendix C SQA and SQQA evaluation
    ‣ Wav2Prompt: End-to-End Speech Prompt Generation and Tuning For LLM in Zero and
    Few-shot Learning").'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Experimental results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 6.1 ST task results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 1: BLEU ($\uparrow$) results on the test sets of Europarl-ST En-Es and
    En-Fr pairs with different LLM. Zero-shot means no speech-translation paired data
    available for fine-tuning, while few-shot means 10-hour speech-translation paired
    data available'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Model | LLM | En-Es | En-Fr |'
  prefs: []
  type: TYPE_TB
- en: '| Zero-shot | Few-shot | Zero-shot | Few-shot |'
  prefs: []
  type: TYPE_TB
- en: '| Oracle-LLM | LLaMA-2-7B | 26.1 | 26.1 | 18.8 | 18.8 |'
  prefs: []
  type: TYPE_TB
- en: '| ASR-LLM Cascade | LLaMA-2-7B | 14.0 | 14.0 | 10.4 | 10.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Encoder-LLM | LLaMA-2-7B | 6.2 | 19.1 | 4.0 | 16.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Flat-start Encoder-LLM | LLaMA-2-7B | — | 1.8 | — | 2.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Proposed Wav2Prompt-LLM | LLaMA-2-7B | 13.8 | 19.9 | 9.2 | 16.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Oracle-LLM | BLOOMZ-7B1 | 32.9 | 32.9 | 24.4 | 24.4 |'
  prefs: []
  type: TYPE_TB
- en: '| ASR-LLM Cascade | BLOOMZ-7B1 | 17.7 | 17.7 | 15.1 | 15.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Encoder-LLM | BLOOMZ-7B1 | 7.9 | 25.8 | 5.5 | 23.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Flat-start Encoder-LLM | BLOOMZ-7B1 | — | 0.7 | — | 0.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Proposed Wav2Prompt-LLM | BLOOMZ-7B1 | 18.3 | 26.0 | 12.7 | 23.6 |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'The ST results are shown in Table [1](#S6.T1 "Table 1 ‣ 6.1 ST task results
    ‣ 6 Experimental results ‣ Wav2Prompt: End-to-End Speech Prompt Generation and
    Tuning For LLM in Zero and Few-shot Learning"). Since the Oracle-LLM and ASR-LLM
    Cascade systems cannot utilise speech-to-translation paired data for E2E fine-tuning
    in the few-shot scenario, their few-shot results are the same as their zero-shot
    results. Note this paper uses prompt tuning [[21](#bib.bib21)] and the LLM was
    fixed. As shown in Table [1](#S6.T1 "Table 1 ‣ 6.1 ST task results ‣ 6 Experimental
    results ‣ Wav2Prompt: End-to-End Speech Prompt Generation and Tuning For LLM in
    Zero and Few-shot Learning"), Wav2Prompt-LLM achieves competitive results in the
    zero-shot scenario compared to ASR-LLM Cascade, which shows that the proposed
    Wav2Prompt maintains the advantage of the LLM zero-shot capability. Moreover,
    Wav2Prompt-LLM greatly outperformed the Encoder-LLM in the zero-shot scenario,
    which is in line with the findings of the previous work [[36](#bib.bib36)] that
    Encoder-LLM overfits to the ASR task when trained on ASR data, and exhibited limited
    performance in unseen ST tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: In the few-shot scenario, after fine-tuning with limited paired data, Wav2Prompt-LLM
    exceeded the performance of the ASR-LLM Cascade, which is the main advantage of
    Wav2Prompt compared to standard ASR models combined with LLMs. In addition, after
    fine-tuning with limited data, the Encoder-LLM also shows strong results, only
    slightly poorer than Wav2Prompt. This indicates that although the Encoder-LLM
    overfits to the ASR tasks, this issue can be ameliorated via few-shot fine-tuning.
    The Flat-start Encoder-LLM results show that the limited data in the few-shot
    case was insufficient for an encoder, with no exposure to the LLM, to learn how
    to connect to the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: While BLOOMZ-7B1 gives higher BLEU scores than LLaMA-2-7B in translation tasks,
    the experimental conclusions are consistent across BLOOMZ-7B1 and LLaMA-2-7B,
    and it is also consistent across the En-Es and En-Fr tasks. Wav2Prompt can achieve
    similar results to the ASR-LLM Cascade in zero-shot scenarios and greatly outperforms
    the Encoder-LLM. In the few-shot scenarios, Wav2Prompt can surpasses the ASR-LLM
    Cascade. In conclusion, Wav2Prompt is an effective E2E alternative to conventional
    ASR when combined with an LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 SLU task results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 2: Cross-domain intent classification accuracy (%) ($\uparrow$) on FSC
    corpus for models trained on LibriSpeech corpus. In the few-shot, 2-hour paired
    data is available for fine-tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Model | LLM | Zero-shot | Few-shot |'
  prefs: []
  type: TYPE_TB
- en: '| Oracle-LLM | Vicuna-7B-1.5 | 97.86% | 97.86% |'
  prefs: []
  type: TYPE_TB
- en: '| ASR-LLM Cascade | Vicuna-7B-1.5 | 75.80% | 93.22% |'
  prefs: []
  type: TYPE_TB
- en: '| Encoder-LLM | Vicuna-7B-1.5 | 30.00% | 97.57% |'
  prefs: []
  type: TYPE_TB
- en: '| Proposed Wav2Prompt-LLM | Vicuna-7B-1.5 | 71.55% | 98.10% |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'The SLU task requires the LLM to understand the content of speech, and in this
    section, to be able to classify the intent of the speech. Furthermore, this experiment
    was conducted in a cross-domain scenario, i.e., the model trained on LibriSpeech
    ASR data was directly evaluated on FSC data. The results of intent classification
    are presented in Table [2](#S6.T2 "Table 2 ‣ 6.2 SLU task results ‣ 6 Experimental
    results ‣ Wav2Prompt: End-to-End Speech Prompt Generation and Tuning For LLM in
    Zero and Few-shot Learning"). In the few-shot scenario, although the ASR-LLM Cascade
    system cannot utilise SLU paired data for E2E fine-tuning, considering the cross-domain
    scenario, the limited ASR data corresponding to SLU paired data can be used by
    the ASR model for fine-tuning to achieve domain adaptation. Therefore, the few-shot
    results of the ASR-LLM Cascade showed a noticeable improvement compared to the
    zero-shot results. In the zero-shot scenario, Wav2Prompt-LLM achieved performance
    close to the ASR-LLM Cascade and greatly surpassed the Encoder-LLM, which again
    shows that the Encoder-LLM overfits to the ASR task that it was used in training,
    while Wav2Prompt retains the LLM zero-shot ability. In the few-shot scenario,
    even compared to the domain-adapted ASR-LLM Cascade, Wav2Prompt-LLM gave an improved
    performance, demonstrating the advantage of Wav2Prompt-LLM in E2E fine-tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 SQA and SQQA task results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 3: Accuracy (%) ($\uparrow$) of zero-shot SQA on LibriSpeech augmented
    with question-answer pairs and zero-shot SQQA on synthesised WikiQA.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Model | LLM | SQA | SQQA |'
  prefs: []
  type: TYPE_TB
- en: '| Oracle-LLM | Vicuna-7B-1.5 | 78.10% | 68.10% |'
  prefs: []
  type: TYPE_TB
- en: '| ASR-LLM Cascade | Vicuna-7B-1.5 | 73.82% | 51.36% |'
  prefs: []
  type: TYPE_TB
- en: '| Encoder-LLM | Vicuna-7B-1.5 | 64.83% | 19.54% |'
  prefs: []
  type: TYPE_TB
- en: '| Proposed Wav2Prompt-LLM | Vicuna-7B-1.5 | 74.21% | 51.58% |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'The zero-shot results of SQA and SQQA are shown in Table [3](#S6.T3 "Table
    3 ‣ 6.3 SQA and SQQA task results ‣ 6 Experimental results ‣ Wav2Prompt: End-to-End
    Speech Prompt Generation and Tuning For LLM in Zero and Few-shot Learning"), where
    the SQQA task was evaluated in a cross-domain scenario, making it more challenging.
    Overall, the ASR-LLM Cascade and Wav2Prompt-LLM achieved similar results, with
    Wav2Prompt-LLM being slightly better. Compared to these two systems, the Encoder-LLM
    showed a noticeable performance gap, especially on the SQQA task, again due to
    task over-fitting. Hence, the experimental conclusion is consistent with the above
    ST and SLU experiments that Wav2Prompt effectively retains the LLM zero-shot ability,
    making it an E2E alternative to conventional ASR when combined with an LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 ASR task results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 4: ASR WER ($\downarrow$) results on LibriSpeech test sets.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Model | Encoder | LLM | test | dev |'
  prefs: []
  type: TYPE_TB
- en: '| clean | other | clean | other |'
  prefs: []
  type: TYPE_TB
- en: '| SLAM-ASR [[25](#bib.bib25)] | Whisper-small | LLaMA-2-7B | 6.4 | 10.9 | —
    | — |'
  prefs: []
  type: TYPE_TB
- en: '| SLAM-ASR [[25](#bib.bib25)] | Whisper-small | LLaMA-2-7B-chat | 4.5 | 8.9
    | — | — |'
  prefs: []
  type: TYPE_TB
- en: '| SLAM-ASR [[25](#bib.bib25)] | Whisper-small | Vicuna-7B-1.5 | 4.2 | 9.5 |
    — | — |'
  prefs: []
  type: TYPE_TB
- en: '| Encoder-LLM | Conformer | Vicuna-7B-1.5 | 4.5 | 10.8 | 4.7 | 11.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Proposed Wav2Prompt-LLM | Conformer | Vicuna-7B-1.5 | 3.9 | 8.2 | 4.0 | 8.6
    |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Results with Whisper-small listed as Whisper-small (87M) has similar parameter
    size to our Conformer (83M). Direct comparison not feasible as Whisper-small trained
    on a very large dataset (680k hours).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'While this paper focuses on downstream tasks such as ST, SLU, SQA, and SQQA,
    the results of ASR are also given in Table [4](#S6.T4 "Table 4 ‣ 6.4 ASR task
    results ‣ 6 Experimental results ‣ Wav2Prompt: End-to-End Speech Prompt Generation
    and Tuning For LLM in Zero and Few-shot Learning"). Wav2Prompt-LLM gave competitive
    ASR performance on the LibriSpeech benchmark data and outperformed the Encoder-LLM,
    with this improvement being statistically significant ($p$<0.1%) according to
    the matched pair sentence segment test [[29](#bib.bib29)]. This shows that enforcing
    similarity between speech representations and LLM embeddings is beneficial for
    the LLM to transcribe speech input.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.5 Ablation studies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 5: Ablation studies on the MSE loss for Wav2Prompt. Zero-shot BLEU ($\uparrow$)
    results were shown.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Model | LLM | En-Es | En-Fr |'
  prefs: []
  type: TYPE_TB
- en: '| Oracle-LLM | LLaMA-2-7B | 26.1 | 18.8 |'
  prefs: []
  type: TYPE_TB
- en: '| ASR-LLM Cascade | LLaMA-2-7B | 14.0 | 10.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Encoder-LLM | LLaMA-2-7B | 6.2 | 4.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Proposed Wav2Prompt-LLM | LLaMA-2-7B | 13.8 | 9.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Proposed Wav2Prompt-LLM w/o MSE Loss | LLaMA-2-7B | 5.4 | 3.3 |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'In Wav2Prompt, the MSE loss is used to enforce the consistency between label-level
    speech representation and LLM embeddings, thereby preserving the zero-shot capability
    of text-based LLMs. In this section, ablation studies were conducted to validate
    the importance of the MSE loss. As shown in Table [5](#S6.T5 "Table 5 ‣ 6.5 Ablation
    studies ‣ 6 Experimental results ‣ Wav2Prompt: End-to-End Speech Prompt Generation
    and Tuning For LLM in Zero and Few-shot Learning"), without the MSE loss, the
    zero-shot ST performance of Wav2Prompt-LLM dropped noticeably, so that Wav2Prompt-LLM
    overfits to the ASR task, similar to how the Encoder-LLM struggled with unseen
    tasks. Therefore, learning to match the LLM embeddings is the key to unlocking
    the zero-shot capability of speech-enabled LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper describes Wav2Prompt, which proposes a method to connect spoken input
    with text-based LLMs using only ASR training data while retaining the zero-shot
    capability for other spoken language tasks. Wav2Prompt extracts label-level speech
    representations using the CIF mechanism and explicitly enforces the consistency
    between the speech representations and LLM embeddings using the MSE loss function,
    thus avoiding the issue of task over-fitting. Experiments on a range of tasks,
    including ST, SLU, SQA, and SQQA, showed that Wav2Prompt can achieve results close
    to the ASR-LLM cascade system in zero-shot scenarios and greatly outperforms the
    existing speech-enabled LLM method. It gives results that exceed those of the
    ASR-LLM cascade in few-shot scenarios. Wav2Prompt is an E2E trainable alternative
    to conventional ASR when combined with text LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments and Disclosure of Funding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Keqi Deng is funded by the Cambridge Trust. This work has been performed using
    resources provided by the Cambridge Tier-2 system operated by the University of
    Cambridge Research Computing Service (www.hpc.cam.ac.uk) funded by EPSRC Tier-2
    capital grant EP/T022159/1.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Achiam et al. [2024] Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya,
    I., Aleman, F.L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.,
    2024. GPT-4 technical report. [arXiv:2303.08774](http://arxiv.org/abs/2303.08774).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Borsos et al. [2023] Borsos, Z., Marinier, R., Vincent, D., Kharitonov, E.,
    Pietquin, O., Sharifi, M., Roblek, D., Teboul, O., Grangier, D., Tagliasacchi,
    M., et al., 2023. AudioLM: A language modeling approach to audio generation. IEEE/ACM
    Transactions on Audio, Speech, and Language Processing 31, 2523–2533. doi:[10.1109/TASLP.2023.3288409](http://dx.doi.org/10.1109/TASLP.2023.3288409).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Brown et al. [2020] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D.,
    Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al., 2020.
    Language models are few-shot learners, in: Proc. NeurIPS. URL: [https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2023] Chen, F., Han, M., Zhao, H., Zhang, Q., Shi, J., Xu, S.,
    Xu, B., 2023. X-LLM: Bootstrapping advanced large language models by treating
    multi-modalities as foreign languages [arXiv:2305.04160](http://arxiv.org/abs/2305.04160).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chowdhery et al. [2023] Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,
    G., Roberts, A., Barham, P., Chung, H.W., Sutton, C., Gehrmann, S., et al., 2023.
    PaLM: Scaling language modeling with pathways. Journal of Machine Learning Research
    24, 1–113. URL: [http://jmlr.org/papers/v24/22-1144.html](http://jmlr.org/papers/v24/22-1144.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deng and Woodland [2023] Deng, K., Woodland, P.C., 2023. Label-synchronous neural
    transducer for adaptable online E2E speech recognition. [arXiv:2311.11353](http://arxiv.org/abs/2311.11353).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dong and Xu [2020] Dong, L., Xu, B., 2020. CIF: Continuous integrate-and-fire
    for end-to-end speech recognition, in: Proc. ICASSP. doi:[10.1109/ICASSP40776.2020.9054250](http://dx.doi.org/10.1109/ICASSP40776.2020.9054250).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fathullah et al. [2024a] Fathullah, Y., Wu, C., Lakomkin, E., Jia, J., Shangguan,
    Y., Li, K., Guo, J., Xiong, W., Mahadeokar, J., Kalinli, O., Fuegen, C., Seltzer,
    M., 2024a. Prompting large language models with speech recognition abilities,
    in: Proc. ICASSP. doi:[10.1109/ICASSP48485.2024.10447605](http://dx.doi.org/10.1109/ICASSP48485.2024.10447605).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fathullah et al. [2024b] Fathullah, Y., Wu, C., Lakomkin, E., Li, K., Jia,
    J., Shangguan, Y., Mahadeokar, J., Kalinli, O., Fuegen, C., Seltzer, M., 2024b.
    AudioChatLlama: Towards general-purpose speech abilities for LLMs. [arXiv:2311.06753](http://arxiv.org/abs/2311.06753).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gage [1994] Gage, P., 1994. A new algorithm for data compression. The C Users
    Journal 12, 23–38. URL: [https://api.semanticscholar.org/CorpusID:59804030](https://api.semanticscholar.org/CorpusID:59804030).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. [2021] Gao, T., Fisch, A., Chen, D., 2021. Making pre-trained language
    models better few-shot learners, in: Zong, C., Xia, F., Li, W., Navigli, R. (Eds.),
    Proc. ACL/IJCNLP (1). URL: [https://aclanthology.org/2021.acl-long.295](https://aclanthology.org/2021.acl-long.295).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gong et al. [2024] Gong, Y., Luo, H., Liu, A.H., Karlinsky, L., Glass, J.R.,
    2024. Listen, think, and understand, in: Proc. ICLR. URL: [https://openreview.net/forum?id=nBZBPXdJlC](https://openreview.net/forum?id=nBZBPXdJlC).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Graves et al. [2006] Graves, A., Fernández, S., Gomez, F., Schmidhuber, J.,
    2006. Connectionist temporal classification: Labelling unsegmented sequence data
    with recurrent neural networks, in: Proc. ICML. URL: [https://api.semanticscholar.org/CorpusID:9901844](https://api.semanticscholar.org/CorpusID:9901844).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gulati et al. [2020] Gulati, A., Qin, J., Chiu, C.C., Parmar, N., Zhang, Y.,
    Yu, J., Han, W., Wang, S., Zhang, Z., Wu, Y., Pang, R., 2020. Conformer: Convolution-augmented
    Transformer for speech recognition, in: Proc. Interspeech. doi:[10.21437/Interspeech.2020-3015](http://dx.doi.org/10.21437/Interspeech.2020-3015).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He and Garner [2023] He, M., Garner, P.N., 2023. Can ChatGPT detect intent?
    evaluating large language models for spoken language understanding, in: Proc.
    Interspeech. doi:[10.21437/Interspeech.2023-1799](http://dx.doi.org/10.21437/Interspeech.2023-1799).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. [2024] Huang, C.y., Lu, K.H., Wang, S.H., Hsiao, C.Y., Kuan, C.Y.,
    Wu, H., Arora, S., Chang, K.W., Shi, J., Peng, Y., et al., 2024. Dynamic-Superb:
    Towards a dynamic, collaborative, and comprehensive instruction-tuning benchmark
    for speech, in: Proc. ICASSP. doi:[10.1109/ICASSP48485.2024.10448257](http://dx.doi.org/10.1109/ICASSP48485.2024.10448257).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Iranzo-Sánchez et al. [2020] Iranzo-Sánchez, J., Silvestre-Cerdà, J.A., Jorge,
    J., Roselló, N., Giménez, A., Sanchis, A., Civera, J., Juan, A., 2020. Europarl-ST:
    A multilingual corpus for speech translation of parliamentary debates, in: Proc.
    ICASSP. doi:[10.1109/ICASSP40776.2020.9054626](http://dx.doi.org/10.1109/ICASSP40776.2020.9054626).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Le Scao et al. [2023] Le Scao, T., Fan, A., Akiki, C., Pavlick, E., Ilić, S.,
    Hesslow, D., Castagné, R., Luccioni, A.S., Yvon, F., Gallé, M., et al., 2023.
    Bloom: A 176B-parameter open-access multilingual language model URL: [https://api.semanticscholar.org/CorpusID:253420279](https://api.semanticscholar.org/CorpusID:253420279).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lester et al. [2021] Lester, B., Al-Rfou, R., Constant, N., 2021. The power
    of scale for parameter-efficient prompt tuning, in: Proc. EMNLP. URL: [https://api.semanticscholar.org/CorpusID:233296808](https://api.semanticscholar.org/CorpusID:233296808).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2023] Li, J., Li, D., Savarese, S., Hoi, S., 2023. BLIP-2: Bootstrapping
    language-image pre-training with frozen image encoders and large language models,
    in: Proc. ICML. URL: [https://api.semanticscholar.org/CorpusID:256390509](https://api.semanticscholar.org/CorpusID:256390509).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2022] Liu, X., Ji, K., Fu, Y., Tam, W., Du, Z., Yang, Z., Tang,
    J., 2022. P-tuning: Prompt tuning can be comparable to fine-tuning across scales
    and tasks, in: Proc. ACL (2). URL: [https://aclanthology.org/2022.acl-short.8](https://aclanthology.org/2022.acl-short.8).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2023] Liu, X., Zheng, Y., Du, Z., Ding, M., Qian, Y., Yang, Z.,
    Tang, J., 2023. GPT understands, too. AI Open doi:[https://doi.org/10.1016/j.aiopen.2023.08.012](http://dx.doi.org/https://doi.org/10.1016/j.aiopen.2023.08.012).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lugosch et al. [2019] Lugosch, L., Ravanelli, M., Ignoto, P., Tomar, V.S.,
    Bengio, Y., 2019. Speech Model Pre-Training for End-to-End Spoken Language Understanding,
    in: Proc. Interspeech, pp. 814–818. doi:[10.21437/Interspeech.2019-2396](http://dx.doi.org/10.21437/Interspeech.2019-2396).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. [2024a] Ma, R., Liusie, A., Gales, M.J.F., Knill, K.M., 2024a. Investigating
    the emergent audio classification ability of ASR foundation models. [arXiv:2311.09363](http://arxiv.org/abs/2311.09363).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. [2024b] Ma, Z., Yang, G., Yang, Y., Gao, Z., Wang, J., Du, Z., Yu,
    F., Chen, Q., Zheng, S., Zhang, S., Chen, X., 2024b. An embarrassingly simple
    approach for LLM with strong ASR capacity. [arXiv:2402.08846](http://arxiv.org/abs/2402.08846).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Maaz et al. [2023] Maaz, M., Rasheed, H., Khan, S., Khan, F.S., 2023. Video-ChatGPT:
    Towards detailed video understanding via large vision and language models. [arXiv:2306.05424](http://arxiv.org/abs/2306.05424).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Muennighoff et al. [2023] Muennighoff, N., Wang, T., Sutawika, L., Roberts,
    A., Biderman, S., Scao, T.L., Bari, M.S., Shen, S., Yong, Z.X., Schoelkopf, H.,
    Tang, X., Radev, D., Aji, A.F., Almubarak, K., Albanie, S., Alyafeai, Z., Webson,
    A., Raff, E., Raffel, C., 2023. Crosslingual generalization through multitask
    finetuning. [arXiv:2211.01786](http://arxiv.org/abs/2211.01786).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ouyang et al. [2022] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,
    C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al., 2022. Training
    language models to follow instructions with human feedback. Proc. NeurIPS URL:
    [https://openreview.net/forum?id=TG8KACxEON](https://openreview.net/forum?id=TG8KACxEON).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pallet et al. [1990] Pallet, D., Fisher, W., Fiscus, J., 1990. Tools for the
    analysis of benchmark speech recognition tests, in: Proc. ICASSP. doi:[10.1109/ICASSP.1990.115546](http://dx.doi.org/10.1109/ICASSP.1990.115546).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Panayotov et al. [2015] Panayotov, V., Chen, G., Povey, D., Khudanpur, S.,
    2015. LibriSpeech: an ASR corpus based on public domain audio books, in: Proc.
    ICASSP. doi:[10.1109/ICASSP.2015.7178964](http://dx.doi.org/10.1109/ICASSP.2015.7178964).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Papineni et al. [2002] Papineni, K., Roukos, S., Ward, T., Zhu, W.J., 2002.
    BLEU: a method for automatic evaluation of machine translation, in: Proc. ACL.
    URL: [https://aclanthology.org/P02-1040/](https://aclanthology.org/P02-1040/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Radford et al. [2021] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh,
    G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al., 2021.
    Learning transferable visual models from natural language supervision, in: Proc.
    ICML, pp. 8748–8763. URL: [https://api.semanticscholar.org/CorpusID:231591445](https://api.semanticscholar.org/CorpusID:231591445).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Radford et al. [2023] Radford, A., Kim, J.W., Xu, T., Brockman, G., Mcleavey,
    C., Sutskever, I., 2023. Robust speech recognition via large-scale weak supervision,
    in: Proc. ICML. URL: [https://proceedings.mlr.press/v202/radford23a.html](https://proceedings.mlr.press/v202/radford23a.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schick and Schütze [2020] Schick, T., Schütze, H., 2020. Exploiting cloze-questions
    for few-shot text classification and natural language inference, in: Proc. EACL.
    URL: [https://api.semanticscholar.org/CorpusID:210838924](https://api.semanticscholar.org/CorpusID:210838924).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shin et al. [2020] Shin, T., Razeghi, Y., IV, R.L.L., Wallace, E., Singh, S.,
    2020. AutoPrompt: Eliciting knowledge from language models with automatically
    generated prompts, in: Proc. EMNLP. URL: [https://api.semanticscholar.org/CorpusID:226222232](https://api.semanticscholar.org/CorpusID:226222232).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tang et al. [2024] Tang, C., Yu, W., Sun, G., Chen, X., Tan, T., Li, W., Lu,
    L., MA, Z., Zhang, C., 2024. SALMONN: Towards generic hearing abilities for large
    language models, in: Proc. ICLR. URL: [https://openreview.net/forum?id=14rn7HpKVk](https://openreview.net/forum?id=14rn7HpKVk).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. [2023a] Touvron, H., Lavril, T., Izacard, G., Martinet, X.,
    Lachaux, M.A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez,
    A., Joulin, A., Grave, E., Lample, G., 2023a. LLaMA: Open and efficient foundation
    language models. [arXiv:2302.13971](http://arxiv.org/abs/2302.13971).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. [2023b] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
    A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al., 2023b.
    LLaMa 2: Open foundation and fine-tuned chat models. [arXiv:2307.09288](http://arxiv.org/abs/2307.09288).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Victor et al. [2022] Victor, S., Albert, W., Colin, R., Stephen, B., Lintang,
    S., Zaid, A., Antoine, C., Arnaud, S., Arun, R., Manan, D., et al., 2022. Multitask
    prompted training enables zero-shot task generalization, in: Proc. ICLR. URL:
    [https://openreview.net/forum?id=9Vrb9D0WI4](https://openreview.net/forum?id=9Vrb9D0WI4).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2023] Wang, C., Chen, S., Wu, Y., Zhang, Z., Zhou, L., Liu, S.,
    Chen, Z., Liu, Y., Wang, H., Li, J., He, L., Zhao, S., Wei, F., 2023. Neural codec
    language models are zero-shot text to speech synthesizers. [arXiv:2301.02111](http://arxiv.org/abs/2301.02111).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Watanabe et al. [2018] Watanabe, S., Hori, T., Karita, S., Hayashi, T., Nishitoba,
    J., Unno, Y., Enrique Yalta Soplin, N., Heymann, J., Wiesner, M., Chen, N., Renduchintala,
    A., Ochiai, T., 2018. ESPnet: End-to-end speech processing toolkit, in: Proc.
    Interspeech. doi:[10.21437/Interspeech.2018-1456](http://dx.doi.org/10.21437/Interspeech.2018-1456).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. [2022] Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud,
    S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., Chi, E.H., Hashimoto, T.,
    Vinyals, O., Liang, P., Dean, J., Fedus, W., 2022. Emergent abilities of large
    language models. Trans. Mach. Learn. Res. 2022. URL: [https://openreview.net/forum?id=yzkSU5zdwD](https://openreview.net/forum?id=yzkSU5zdwD).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wolf et al. [2020] Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C.,
    Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., et al., 2020. Transformers:
    State-of-the-art natural language processing, in: Proc. EMNLP (Demos), pp. 38–45.
    doi:[10.18653/v1/2020.emnlp-demos.6](http://dx.doi.org/10.18653/v1/2020.emnlp-demos.6).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. [2023] Wu, J., Gaur, Y., Chen, Z., Zhou, L., Zhu, Y., Wang, T., Li,
    J., Liu, S., Ren, B., Liu, L., Wu, Y., 2023. On decoder-only architecture for
    speech-to-text and large language model integration, in: Proc. ASRU. doi:[10.1109/ASRU57964.2023.10389705](http://dx.doi.org/10.1109/ASRU57964.2023.10389705).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. [2024] Xu, H., Kim, Y.J., Sharaf, A., Awadalla, H.H., 2024. A paradigm
    shift in machine translation: Boosting translation performance of large language
    models, in: Proc. ICLR. URL: [https://openreview.net/forum?id=farT6XXntP](https://openreview.net/forum?id=farT6XXntP).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. [2015] Yang, Y., Yih, W.t., Meek, C., 2015. WikiQA: A challenge
    dataset for open-domain question answering, in: Proc. EMNLP. doi:[10.18653/v1/D15-1237](http://dx.doi.org/10.18653/v1/D15-1237).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yi et al. [2021] Yi, C., Zhou, S., Xu, B., 2021. Efficiently fusing pretrained
    acoustic and linguistic encoders for low-resource speech recognition. IEEE Signal
    Process. Lett. 28, 788–792. doi:[10.1109/LSP.2021.3071668](http://dx.doi.org/10.1109/LSP.2021.3071668).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. [2024] Yu, W., Tang, C., Sun, G., Chen, X., Tan, T., Li, W., Lu,
    L., Ma, Z., Zhang, C., 2024. Connecting speech encoder and large language model
    for ASR, in: Proc. ICASSP. doi:[10.1109/ICASSP48485.2024.10445874](http://dx.doi.org/10.1109/ICASSP48485.2024.10445874).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2023] Zhang, X., Peng, B., Li, K., Zhou, J., Meng, H., 2023.
    SGP-TOD: Building task bots effortlessly via schema-guided LLM prompting, in:
    Proc. EMNLP (Findings). doi:[10.18653/v1/2023.findings-emnlp.891](http://dx.doi.org/10.18653/v1/2023.findings-emnlp.891).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Dat Set Statistics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The data set training and test statistics for the corpora used in the experiments
    are shown in Table [6](#A1.T6 "Table 6 ‣ Appendix A Dat Set Statistics ‣ Wav2Prompt:
    End-to-End Speech Prompt Generation and Tuning For LLM in Zero and Few-shot Learning").
    The Europarl-ST data was collected from the European Parliament debate [[17](#bib.bib17)].
    LibriSpeech is an audiobook reading corpus [[30](#bib.bib30)]. Question-answer
    pairs provided by [[36](#bib.bib36)] were used to augment the dev-clean set for
    the SQA task, where the questions and answers were generated based on transcript
    text using GPT3.5\. The WikiQA [[46](#bib.bib46)] test set with synthesised speech
    queries provided by [[36](#bib.bib36)] was used for the SQQA task, in which the
    answers generated from GPT4 were used as the reference answers. The FSC data [[23](#bib.bib23)]
    was collected from English commands commonly used for a smart home or virtual
    assistant, which has 31 distinct intents.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Statistics of datasets used in this paper'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | Europarl-ST |'
  prefs: []
  type: TYPE_TB
- en: '| ASR Data Train Set | train |'
  prefs: []
  type: TYPE_TB
- en: '|     -Hours | 81 hours |'
  prefs: []
  type: TYPE_TB
- en: '|     -Samples | 34K |'
  prefs: []
  type: TYPE_TB
- en: '| Few-shot ST Data Train sets | train-en-es-10h | train-en-fr-10h |'
  prefs: []
  type: TYPE_TB
- en: '|     -Hours | 10 hours | 10 hours |'
  prefs: []
  type: TYPE_TB
- en: '|     -Samples | 4.2K | 4.2K |'
  prefs: []
  type: TYPE_TB
- en: '| ST Data Test sets | test-en-es | test-en-fr |'
  prefs: []
  type: TYPE_TB
- en: '|     -Hours | 2.9 hours | 2.8 hours |'
  prefs: []
  type: TYPE_TB
- en: '|     -Samples | 1.3K | 1.2K |'
  prefs: []
  type: TYPE_TB
- en: '|  | LibriSpeech |'
  prefs: []
  type: TYPE_TB
- en: '| ASR Data Train set | train-960 |'
  prefs: []
  type: TYPE_TB
- en: '|     -Hours | 960 hours |'
  prefs: []
  type: TYPE_TB
- en: '|     -Samples | 281K |'
  prefs: []
  type: TYPE_TB
- en: '| Test sets | test-clean / other | dev-clean / other |'
  prefs: []
  type: TYPE_TB
- en: '|     -Hours | 5.4 / 5.3 hours | 5.4 / 5.1 hours |'
  prefs: []
  type: TYPE_TB
- en: '|     -Samples | 2.6 / 2.9K | 2.7 / 2.9K |'
  prefs: []
  type: TYPE_TB
- en: '|  | Synthesised WikiQA |'
  prefs: []
  type: TYPE_TB
- en: '| SQQA Data test set | test |'
  prefs: []
  type: TYPE_TB
- en: '|     -Hours | 0.5 hours |'
  prefs: []
  type: TYPE_TB
- en: '|     -Samples | 0.6K |'
  prefs: []
  type: TYPE_TB
- en: '|  | Fluent Speech Commands (FSC) |'
  prefs: []
  type: TYPE_TB
- en: '| Few-shot SLU Data train set | train-2h |'
  prefs: []
  type: TYPE_TB
- en: '|     -Hours | 2 hours |'
  prefs: []
  type: TYPE_TB
- en: '|     -Samples | 3.2K |'
  prefs: []
  type: TYPE_TB
- en: '| SLU Data test set | test |'
  prefs: []
  type: TYPE_TB
- en: '|     -Hours | 2.4 hours |'
  prefs: []
  type: TYPE_TB
- en: '|     -Samples | 3.8K |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |'
  prefs: []
  type: TYPE_TB
- en: Appendix B Training and hyper-parameter details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the Conformer encoder, the kernel size of the convolution module was set
    to 31. For models trained on the ASR data of Europarl-ST, the attention dimension,
    feed-forward dimension, and attention heads of the Conformer encoder were set
    to 256, 2048, and 4\. For models trained on LibriSpeech data, the attention dimension
    was set to 512 and 8 attention heads were used. The data was pre-processed following
    ESPnet [[41](#bib.bib41)] recipes. Following the ESPnet recipe, 80-dimensional
    logarithmic Mel filter bank (Mel-fbank) coefficients was used as the speech input
    feature. Before the Mel-fbank features were fed into the encoder, convolutional
    layers were used to down-sample in time by a factor of 4\. For models trained
    on the ASR data of Europarl-ST, the Mel-fbank features were extracted every 8 ms
    with a window size of 32 ms, the CTC ASR model was trained for 25 epochs using
    a learning rate $3\cdot 10^{-3}$ with 40k warmup steps, and the Wav2Prompt-LLM
    and Encoder-LLM converged after 15 epochs of training. In the few-shot scenarios
    of FSC data, the models were fine-tuned for 20 epochs. The number of trainable
    parameters of the Wav2Prompt-LLM, ASR-LLM Cascade, and Encoder-LLM systems were
    85.33 M, 83.74 M, and 100.01 M, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Model training was performed on 2 NVIDIA A100 GPUs each with 80GB GPU memory.
    For the Europarl-ST, 40 M batch bins (as implemented by ESPnet) were used, and
    each epoch took about 45 minutes. For the LibriSpeech, 64 M batch bins (as implemented
    by ESPnet) were used, and each epoch took about 2 hours. During decoding, the
    beam size was set to 5\. For ASR decoding, the repetition penalty as implemented
    by Huggingface [[43](#bib.bib43)] was set to 1.5.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C SQA and SQQA evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Mistral-7B-Instruct-v0.2 was used to evaluate the accuracy of the model prediction
    in the SQA and SQQA tasks. The prompt used in this paper is listed in Table [7](#A3.T7
    "Table 7 ‣ Appendix C SQA and SQQA evaluation ‣ Wav2Prompt: End-to-End Speech
    Prompt Generation and Tuning For LLM in Zero and Few-shot Learning"), which follows
    [[26](#bib.bib26)]. The accuracy was computed by counting the frequency with which
    the Mistral LLM outputs ‘yes’.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: Prompt used in this paper to evaluate speech QA task.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Prompt | Please evaluate the following question-answer pair: |'
  prefs: []
  type: TYPE_TB
- en: '|  | Question: [question] |'
  prefs: []
  type: TYPE_TB
- en: '|  | Correct Answer: [answer] |'
  prefs: []
  type: TYPE_TB
- en: '|  | Predicted Answer: [prediction] |'
  prefs: []
  type: TYPE_TB
- en: '|  | Provide your evaluation only as a yes/no and score where the score is
    an integer value |'
  prefs: []
  type: TYPE_TB
- en: '|  | between 0 and 5, with 5 indicating the highest meaningful match. Please
    generate the |'
  prefs: []
  type: TYPE_TB
- en: '|  | response in the form of a Python dictionary string with keys ‘pred’ and
    ‘score’, where |'
  prefs: []
  type: TYPE_TB
- en: '|  | value of ‘pred’ is a string of ‘yes’ or ‘no’ and value of ‘score’ is in
    INTEGER, not |'
  prefs: []
  type: TYPE_TB
- en: '|  | STRING. DO NOT PROVIDE ANY OTHER OUTPUT TEXT OR EXPLANATION. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Only provide the Python dictionary string. For example, your response
    should look like |'
  prefs: []
  type: TYPE_TB
- en: '|  | this:{‘pred’: ‘yes’, ‘score’: 4}. |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |'
  prefs: []
  type: TYPE_TB
- en: Appendix D Task-specific prompt templates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The prefix and postfix text used in this paper as task-specific prompt templates
    are listed in Table [8](#A5.T8 "Table 8 ‣ Appendix E Limitations ‣ Wav2Prompt:
    End-to-End Speech Prompt Generation and Tuning For LLM in Zero and Few-shot Learning"),
    which were designed based on the intended use of different LLM [[49](#bib.bib49),
    [38](#bib.bib38), [18](#bib.bib18)] or related work [[45](#bib.bib45), [15](#bib.bib15)].'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This paper is limited in the following aspects: First, this paper explores
    the use of off-the-shelf text-based LLMs, so the upper bound of performance is
    determined by the accessible text-based LLMs. However, due to the limitations
    of computing resources, this paper explored various 7B LLMs. Further larger LLMs
    are challenging given our current computing resources. Moreover, Wav2Prompt relies
    on open-source LLMs and cannot use closed-source LLMs, such as GPT4.'
  prefs: []
  type: TYPE_NORMAL
- en: Second, this paper follows the prompt tuning approach without updating the LLM
    parameters. Considering many systems have been built to compare with our proposed
    Wav2Prompt, fine-tuning the LLM parameters would greatly increase the resources
    required for training, which is challenging given our limited computing resources.
    Future work may explore the performance of Wav2Prompt when updating the LLM parameters.
    However, the prompt tuning approach also has the advantage that only one LLM needs
    to be maintained for a series of tasks, which is very memory efficient in real-world
    deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8: Prefix and postfix text used in this paper'
  prefs: []
  type: TYPE_NORMAL
- en: '|   ASR Train: Prefix | "" |'
  prefs: []
  type: TYPE_TB
- en: '| ASR Train: Postfix | Repeat the above English text: |'
  prefs: []
  type: TYPE_TB
- en: '| ST Test Llama: Prefix | Translate this from English to [target language]:
    |'
  prefs: []
  type: TYPE_TB
- en: '| English: |'
  prefs: []
  type: TYPE_TB
- en: '| ST Test Llama: Postfix | [target language]: |'
  prefs: []
  type: TYPE_TB
- en: '| ST Test Bloomz: Prefix | "" |'
  prefs: []
  type: TYPE_TB
- en: '| ST Test Bloomz: Postfix | Translate the above English text into [target language]:
    |'
  prefs: []
  type: TYPE_TB
- en: '| SLU Test: Prefix | We will show you some commands given by a user to a voice
    assistant |'
  prefs: []
  type: TYPE_TB
- en: '| like Siri or Olly. Please classify the intent of the command. |'
  prefs: []
  type: TYPE_TB
- en: '| There are 31 unique intents in total, which are divided into three slots:
    |'
  prefs: []
  type: TYPE_TB
- en: '| "action", "object", and "location". A slot takes on one of multiple values:
    |'
  prefs: []
  type: TYPE_TB
- en: '| the "action" slot can take on the values: "change language", […]; |'
  prefs: []
  type: TYPE_TB
- en: '| the "object" slot can take on the values: "none", "music", […]; |'
  prefs: []
  type: TYPE_TB
- en: '| the "location" slot can take on the values: "none", "kitchen", […]. |'
  prefs: []
  type: TYPE_TB
- en: '| The format of intent is: "action_object_location". The list of all the |'
  prefs: []
  type: TYPE_TB
- en: '| intents are: "increase_volume_none", […]. |'
  prefs: []
  type: TYPE_TB
- en: '| You can first repeat the command and then think about the intent. |'
  prefs: []
  type: TYPE_TB
- en: '| Please give answers like: {"Command": <your_repeated_command>, |'
  prefs: []
  type: TYPE_TB
- en: '| "Intent": <your_intent_prediction>}. For example: […]. The intent in |'
  prefs: []
  type: TYPE_TB
- en: '| your answer must match one of the intents given above. If you are |'
  prefs: []
  type: TYPE_TB
- en: '| uncertain, choose the one that you think is the most likely. |'
  prefs: []
  type: TYPE_TB
- en: '| Here are the commands: |'
  prefs: []
  type: TYPE_TB
- en: '| USER: |'
  prefs: []
  type: TYPE_TB
- en: '| SLU Test: Postfix | Repeat the above English text and classify the intent:
    |'
  prefs: []
  type: TYPE_TB
- en: '| ASSISTANT: |'
  prefs: []
  type: TYPE_TB
- en: '| SQA Test: Prefix | Give a precise and clear answer to the question based
    on the context. A |'
  prefs: []
  type: TYPE_TB
- en: '| good answer needs to contain the right information and be as short and |'
  prefs: []
  type: TYPE_TB
- en: '| clear as possible. Don’t be verbose. |'
  prefs: []
  type: TYPE_TB
- en: '|  | CONTEXT: |'
  prefs: []
  type: TYPE_TB
- en: '| SQA Test: Postfix | QUESTION: [question] |'
  prefs: []
  type: TYPE_TB
- en: '| ANSWER: |'
  prefs: []
  type: TYPE_TB
- en: '| SQQA Test: Prefix | Give a precise and clear answer to the question. Don’t
    be verbose. You |'
  prefs: []
  type: TYPE_TB
- en: '| can first repeat the question and then think about the answer. Please give
    |'
  prefs: []
  type: TYPE_TB
- en: '| answers like: {"Question": <your_repeated_question>, "Answer": <your |'
  prefs: []
  type: TYPE_TB
- en: '|  | _answer>}. If you are not sure, leave the answer blank, like {"Question":
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | <your_repeated_question>, "Answer": ""}. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Here are the questions: |'
  prefs: []
  type: TYPE_TB
- en: '|  | USER: |'
  prefs: []
  type: TYPE_TB
- en: '| SQQA Test: Postfix | Repeat the above English text and answer the question:
    |'
  prefs: []
  type: TYPE_TB
- en: '| ASSISTANT: |'
  prefs: []
  type: TYPE_TB
- en: '| ASR Test: Prefix | "" |'
  prefs: []
  type: TYPE_TB
- en: '| ASR Test: Postfix | Resume the above English text: |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |'
  prefs: []
  type: TYPE_TB
- en: Third, for tasks like SQQA, this paper only compared performance under a zero-shot
    scenario because the synthesised test set was provided by previous work of [[36](#bib.bib36)],
    and we do not have a powerful speech synthesis system available. Fourth, limited
    by training data and computing resources, we were not able to train our Wav2Prompt
    as extensively as some pre-trained speech models like Whisper [[33](#bib.bib33)],
    but we have conducted extensive experiments on many corpora, including LibriSpeech,
    the most widely used dataset used in ASR research. This paper mainly evaluates
    Wav2Prompt on the English task, including ST between two European language pairs.
    While we believe Wav2Prompt can also be applied to other languages, including
    multi-lingual tasks, the performance has not been verified and is left as future
    work.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, this paper has validated Wav2Prompt on five unseen tasks (including
    two ST tasks) and also ASR task. However, due to limited resources, there are
    still potential applications that have not been evaluated, which is left as future
    work.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix F Broader impact
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Wav2Prompt allows easy integration of spoken input with text-based LLMs and
    provides more use-cases for off-the-shelf text-based LLMs. Wav2Prompt provides
    similar performance to a conventional cascade of ASR followed by a text based
    LLM for a zero shot performance on a range of tasks. However since it allows E2E
    fine-tuning, Wav2Prompt provides much improved performance in few-shot scenarios
    on tasks including speech translation, spoken intent classification and spoken
    question answering. This ability to perform very well on a range of spoken language
    tasks when Wav2Prompt is initially trained only on ASR training data is beneficial
    in many circumstances where task-specific training data is limited. This is true
    for many tasks, and the issue of limited spoken training data is even more severe
    for under-resourced languages and hence this is a significant benefit of Wav2Prompt.
    Wav2Prompt does not give rise to any additional potential biases beyond the ones
    directly inherited from the pre-trained LLM checkpoints and the speech training
    data used.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix G Assets and licenses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following licenses apply to the models used in this paper:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LLaMA2: [https://huggingface.co/meta-llama/Llama-2-7b-hf/blob/main/LICENSE.txt](https://huggingface.co/meta-llama/Llama-2-7b-hf/blob/main/LICENSE.txt)
    applies to LLaMA-2-7B and Vicuna-7B-1.5.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Apache-2.0: [https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0)
    applies to Mistral-7B-Instruct-v0.2.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BigScience RAIL License v1.0: [https://huggingface.co/spaces/bigscience/license](https://huggingface.co/spaces/bigscience/license)
    applies to BLOOMZ-7B1.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following licenses apply to the datasets used in this paper:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CC BY-NC 4.0: [https://spdx.org/licenses/CC-BY-NC-4.0](https://spdx.org/licenses/CC-BY-NC-4.0)
    applies to Europarl-ST data.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CC BY 4.0: [https://spdx.org/licenses/CC-BY-4.0](https://spdx.org/licenses/CC-BY-4.0)
    applies to LibriSpeech data.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CC-BY-NC-ND-4.0: [https://spdx.org/licenses/CC-BY-NC-ND-4.0](https://spdx.org/licenses/CC-BY-NC-ND-4.0)
    applies to Fluent Speech Commands data.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following license applies to the code and Python package used in this paper:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Apache-2.0: applies to Huggingface Transformers ([https://github.com/huggingface/transformers/blob/main/LICENSE](https://github.com/huggingface/transformers/blob/main/LICENSE))
    and ESPnet ([https://github.com/espnet/espnet/blob/master/LICENSE](https://github.com/espnet/espnet/blob/master/LICENSE)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
