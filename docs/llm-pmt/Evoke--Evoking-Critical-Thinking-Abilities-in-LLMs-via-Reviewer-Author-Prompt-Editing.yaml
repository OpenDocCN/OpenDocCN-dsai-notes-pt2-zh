- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:48:32'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Evoke: Evoking Critical Thinking Abilities in LLMs via Reviewer-Author Prompt
    Editing'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.13855](https://ar5iv.labs.arxiv.org/html/2310.13855)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Xinyu Hu¹, Pengfei Tang¹, Simiao Zuo¹, Zihan Wang², Bowen Song³, Qiang Lou¹,
  prefs: []
  type: TYPE_NORMAL
- en: Jian Jiao¹, Denis Charles¹
  prefs: []
  type: TYPE_NORMAL
- en: ¹Microsoft
  prefs: []
  type: TYPE_NORMAL
- en: ²University of Washington
  prefs: []
  type: TYPE_NORMAL
- en: ³University of Michigan Corresponding author.
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Large language models (LLMs) have made impressive progress in natural language
    processing. These models rely on proper human instructions (or prompts) to generate
    suitable responses. However, the potential of LLMs are not fully harnessed by
    commonly-used prompting methods: many human-in-the-loop algorithms employ ad-hoc
    procedures for prompt selection; while auto prompt generation approaches are essentially
    searching all possible prompts randomly and inefficiently. We propose Evoke, an
    automatic prompt refinement framework. In Evoke, there are two instances of a
    same LLM: one as a reviewer (LLM-Reviewer), it scores the current prompt; the
    other as an author (LLM-Author), it edits the prompt by considering the edit history
    and the reviewer’s feedback. Such an author-reviewer feedback loop ensures that
    the prompt is refined in each iteration. We further aggregate a data selection
    approach to Evoke, where only the hard samples are exposed to the LLM. The hard
    samples are more important because the LLM can develop deeper understanding of
    the tasks out of them, while the model may already know how to solve the easier
    cases. Experimental results show that Evoke significantly outperforms existing
    methods. For instance, in the challenging task of logical fallacy detection, Evoke
    scores above 80, while all other baseline methods struggle to reach 20.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Consider an intriguing trio that at first glance seems unrelated: bumble bees,
    cell phones, and exciting news. At a superficial level, their commonality might
    note their plural forms; however, a more profound analysis reveals a shared essence:
    they all “create a buzz.” This comparison sheds light on the depth and intricacy
    of human cognitive processes. At the heart of such processes is critical thinking,
    the ability to conceptualize, analyze, question, and evaluate ideas and beliefs.
    As we transition to the domain of artificial intelligence, it is observed that
    large language models (LLMs) have remarkably evolved as general problem solvers,
    urging us to ponder:'
  prefs: []
  type: TYPE_NORMAL
- en: Can LLMs think on their own?
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, we observe that existing prompting methods are inadequate in evoking
    the critical thinking abilities of LLMs. For example, in Figure [2](#S1.F2 "Figure
    2 ‣ 1 Introduction ‣ Evoke: Evoking Critical Thinking Abilities in LLMs via Reviewer-Author
    Prompt Editing"), we show two prompts for solving a common concept task. From
    Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Evoke: Evoking Critical Thinking
    Abilities in LLMs via Reviewer-Author Prompt Editing") (left), we see that for
    the input trio “bumble bees, cell phones, and exciting news”, the LLM outputs
    a superficial common concept “plural form” using the hand-crafted prompt. On the
    other hand, with the prompt generated by the proposed method, the LLM demonstrates
    much deeper understanding about the task , i.e., it generates the correct answer
    “can cause a buzz” (see Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Evoke:
    Evoking Critical Thinking Abilities in LLMs via Reviewer-Author Prompt Editing"),
    right). These results indicate that the quality of prompts are directly related
    to the performance of LLMs. In this work, we focus on prompting methods that enables
    LLMs to think on their own.'
  prefs: []
  type: TYPE_NORMAL
- en: The current prompting methodologies exhibit significant drawbacks. Many prompting
    methods are ad hoc because of their human-in-the-loop development paradigm. In
    such a process, given a target task, we first draft an initial prompt. Then, we
    refine the prompt using techniques such as chain-of-thought, few-shot demonstrations,
    and coding-style problem descriptions (Wei et al., [2022c](#bib.bib27), [a](#bib.bib25);
    Gao et al., [2023](#bib.bib5)) based on the model’s performance on the target
    task. We note that in practice, a hand-crafted prompt optimized for one task rarely
    translates to satisfactory performance in another task (Zhang et al., [2023](#bib.bib31)).
    Therefore, each task becomes a new expedition, with its own set of trials, errors,
    and validations. Such an ad hoc human-in-the-loop development procedure introduces
    extensive human labor requirements, which significantly hinder the applicability
    of LLMs in real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: Existing works develop algorithms to automatically generate prompts instead
    of relying on ad hoc human optimization (Shin et al., [2020](#bib.bib17); Honovich
    et al., [2022](#bib.bib6); Zhou et al., [2022](#bib.bib32)). However, these methods
    often lack feedback loops, such that the refinement procedure essentially performs
    a random search. For example, in each refinement iteration, Zhou et al. ([2022](#bib.bib32))
    simply rephrases the prompt into multiple candidates, and then select the candidate
    that yields the best performance as the refined prompt. Note that such a procedure
    fails to learn from past successes and failures, such that refined prompt does
    not enrich the original prompt with additional context.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a4546b9ce09c5fccf150b8dcdc99749a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Comparison between hand-crafted and Evoke prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3beb54ed185848fb921290780728ee11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Simplified workflow of Evoke.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We propose Evoke, which addresses the aforementioned drawbacks by leveraging
    an author-reviewer paradigm. In this paradigm, there are two distinct purposes
    an LLM can serve: one instance as an author (LLM-Author) tasked with editing prompts,
    and another instance as a reviewer (LLM-Reviewer) tasked with evaluating the quality
    of the prompts generated by the LLM-Author. Each role is played independently
    by separate instances of the same LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: Critical thinking is not something you do once with an issue and then drop it.
    It requires that we update our knowledge as new information comes in.
  prefs: []
  type: TYPE_NORMAL
- en: Daniel Levitin
  prefs: []
  type: TYPE_NORMAL
- en: 'The essence of this quote resonates with the feedback loop in the workflow
    of Evoke, as depicted in Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Evoke:
    Evoking Critical Thinking Abilities in LLMs via Reviewer-Author Prompt Editing").
    The workflow comprises three steps: First, the LLM-Author edits prompts from previous
    iterations, taking into account the past edits and the feedback from the LLM-Reviewer.
    Second, the LLM-Reviewer scores the revised prompts from the LLM-Author, and the
    top-n candidates with the highest scores are selected for subsequent procedures.
    The LLM-Reviewer employs a memory module that stores history edits, prompts and
    task accuracy of history prompts. Finally, the task accuracy for each instruction
    is computed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To further enhance the efficacy of Evoke, we propose a data selection strategy.
    In this strategy, only the hard samples selected by a selector are exposed to
    the LLM. The intuition is that the LLM can develop deeper understanding of the
    tasks out of the hard samples, while it already knows how to solve the easier
    cases. Through extensive experiments (see Figure [10](#S4.F10 "Figure 10 ‣ 4.4
    Analysis ‣ 4 Experiments ‣ Evoke: Evoking Critical Thinking Abilities in LLMs
    via Reviewer-Author Prompt Editing") in the experiments), we see that retaining
    the hard samples indeed improves efficacy of Evoke.'
  prefs: []
  type: TYPE_NORMAL
- en: We conduct extensive experiments to demonstrate the effectiveness of Evoke.
    Specifically, on eight tasks from the Instruction Induction (Honovich et al.,
    [2022](#bib.bib6)) dataset and the Big Bench Instruction Induction (Zhou et al.,
    [2022](#bib.bib32)) dataset, we show that Evoke significantly outperforms existing
    automatic prompt engineering approaches. For example, on the challenging logical
    fallacy detection task, Evoke achieves a score of over 80, while all the baseline
    methods struggle to reach 20\. We also show that Evoke can improve LLMs’ robustness
    against adversarial attacks, and can also handle fine-grained named entity recognition
    tasks with exceptional performance. As an example, Evoke achieves significant
    performance gain on an adversarially constructed dataset, indicating that the
    proposed method can improve robustness of LLMs. Additionally, we provide detailed
    analysis on the effectiveness of each component of Evoke.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Large Language Models. Recently, LLMs have shown emergent abilities—capabilities
    to perform tasks they weren’t explicitly trained for (Wei et al., [2022a](#bib.bib25),
    [b](#bib.bib26); Bubeck et al., [2023](#bib.bib1)). This includes common sense
    question answering, code generation, and cross-domain problem solving, enriching
    their utility across unforeseen domains (Chen et al., [2021](#bib.bib2); Sarsa
    et al., [2022](#bib.bib15); Thirunavukarasu et al., [2023](#bib.bib21); Huang
    and Chang, [2022](#bib.bib8); Du et al., [2023](#bib.bib4)). Subsequently, adapting
    LLMs to specific problems has drawn attention, and several methods have been proposed:
    Reinforcement Learning from Human Feedback (RLHF Ouyang et al. [2022](#bib.bib13)),
    efficient fine-tuning (Hu et al., [2022](#bib.bib7); Dettmers et al., [2023](#bib.bib3)),
    and prompt engineering (White et al., [2023](#bib.bib28)), among others. Each
    method has its pros and cons. For instance, RLHF can significantly improve performance
    but may require extensive human annotations. Efficient fine-tuning, on the other
    hand, can be less resource-intensive but might fall short in achieving the desired
    level of task-specific optimization. Prompt engineering, while innovative, may
    require a well-crafted prompt to effectively guide the model towards accurate
    outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: In-Context Learning and Prompt Engineering. In-Context Learning (ICL) refers
    to the ability of LLMs to learn a new task from a small set of examples presented
    within the context (the prompt) at inference time, without updating any parameters
    (Wei et al., [2022a](#bib.bib25)). This paradigm has significantly improved the
    capabilities of LLMs across various tasks. Many studies have explored the reasons
    behind such improvements, examining aspects like Bayesian optimization and the
    difficulty of demonstrations (Xie et al., [2022](#bib.bib29); Min et al., [2022](#bib.bib12);
    Liu et al., [2022](#bib.bib11); Yoo et al., [2022](#bib.bib30)).
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering plays a pivotal role in facilitating ICL. It entails the
    design of prompts that arm the LLM with the essential information needed to learn
    and adeptly perform the new task. Each prompt essentially sets the stage for the
    LLM, enclosing the task’s requirements and guiding the model towards producing
    the desired output. By carefully crafting prompts, it is possible to leverage
    the inherent capability of LLMs, enabling them to tackle a wide range of tasks
    even with limited or no prior explicit training on those tasks. Recently, methods
    such as Chain-of-Thought (CoT), Zero-CoT, Self-Consistency, Program-Aided, and
    Few-Shot Prompting have been demonstrated to be effective (Wei et al., [2022c](#bib.bib27);
    Kojima et al., [2022](#bib.bib10); Wang et al., [2022](#bib.bib24); Gao et al.,
    [2023](#bib.bib5); Reynolds and McDonell, [2021](#bib.bib14)).
  prefs: []
  type: TYPE_NORMAL
- en: Automatic Prompt Engineering. The existing methodologies for automating discrete
    prompt optimization have their roots in instruction induction, as discussed by
    Honovich et al. [2022](#bib.bib6). It was discovered that LLMs can generate natural
    language instructions based on a small number of input-output pair examples. Building
    on this, Zhou et al. ([2022](#bib.bib32)) proposed a new algorithm for the automatic
    generation and selection of instructions for LLMs. The algorithm, named Automatic
    Prompt Engineer (APE), is capable of generating prompts that achieve human-level
    performance across a diverse range of NLP tasks. Work has also been done on automating
    prompt generation for specific domains like code generation, as discussed in Shrivastava
    et al. [2023](#bib.bib18).
  prefs: []
  type: TYPE_NORMAL
- en: 3 Iterative Reviewer-Author Prompt Editing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Evoke, the same LLM plays two different roles: an author (LLM-Author) that
    is in charge of editing and refine prompts, and a reviewer (LLM-Reviewer) that
    is in charge of scoring the refined prompts. We use two different prompts for
    the author’s and the reviewer’s task.'
  prefs: []
  type: TYPE_NORMAL
- en: '$\diamond$ LLM-Author edits and generates new prompts based on feedback from
    LLM-Reviewer. The prompt for LLM-Author consists of several components:'
  prefs: []
  type: TYPE_NORMAL
- en: a
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Input for editing: Current task instruction to be refined and training data;'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Instruction for editing: “We’ve provided pairs consisting of inputs, the teacher’s
    correct answers, and the students’ responses. Please review the incorrect responses
    from the students and summarize key points that could be adjusted in the instruction
    to enhance student accuracy. Highlight major edits and present the updated task
    instruction.”;'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Memory: prior history (edits, scores).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: LLM-Author refines the instructions (prompts for the given task) by utilizing
    the training data and a memory component. We note that the memory consists of
    all prior (edit, score) pairs, where the score comes from LLM-Reviewer. This memory
    component enables LLM-Author to execute increasingly effective edits, drawing
    upon feedback from previous edits.
  prefs: []
  type: TYPE_NORMAL
- en: 'Require: Training set; Initial prompt for the target task (i.e., the one we
    want to refine).// InitializationLLM-Selector: Initialize data scoring instruction.LLM-Author:
    Initialize prompt editing instruction.LLM-Review: Initialize prompt reviewing
    instruction.while *$t\leq T$* do        // LLM-Selector       Assign difficulty
    scores for each data point in the training set.       Select a training subset
    based on the difficulty level.        // LLM-Author       LLM-Author generates
    multiple prompts based on the training data and its own memory.        // LLM-Reviewer      
    LLM-Reviewer scores the quality of each generated prompt from LLM-Author based
    on its own memory.       Select top-n prompts based on the generated scores from
    LLM-Reviewer.       Get task accuracy for all prompts.        // Memory update      
    Memory of LLM-Author appends (edits, scores).       Memory of LLM-Reviewer appends
    (edits, prompts, task accuracy).Return: The prompt with the highest task accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Evoke
  prefs: []
  type: TYPE_NORMAL
- en: '$\diamond$ LLM-Reviewer scores the quality of prompts generated by LLM-Author.
    The input prompt for LLM-Reviewer consists of several components:'
  prefs: []
  type: TYPE_NORMAL
- en: a
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Input for scoring: problem description and current instruction from LLM-Author;'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Instruction for scoring: “Please rate the following instruction on a scale
    of 1 to 10, where 10 represents the highest level of clarity in problem description,
    execution steps, and a comprehensive explanation of the problem.”;'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Memory: prior (edits, instructions, task accuracy).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The instructions generated by LLM-Author are forwarded to LLM-Reviewer for evaluation.
    Based on the scores generated by LLM-Reviewer, only a subset of high-scoring candidates
    is selected to move on to the subsequent iteration. Through this iterative editing
    process between LLM-Author and LLM-Reviewer, LLM-Author can refine instructions
    in each iteration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Details of the algorithm can be found in Algorithm [1](#alg1 "In 3.1 Overview
    ‣ 3 Iterative Reviewer-Author Prompt Editing ‣ Evoke: Evoking Critical Thinking
    Abilities in LLMs via Reviewer-Author Prompt Editing").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e1de64953a4cd830f2a2dabd1e5ff7c0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Illustration of Prompt Editing for the first three steps in the Task
    of Movie Recommendation within Big Bench.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate the effectiveness of Evoke, first three edits from the Movie
    Recommendation task in Big Bench are presented in Figure [3](#S3.F3 "Figure 3
    ‣ 3.1 Overview ‣ 3 Iterative Reviewer-Author Prompt Editing ‣ Evoke: Evoking Critical
    Thinking Abilities in LLMs via Reviewer-Author Prompt Editing"). To start with,
    the prompt contains the basic task instruction. Next, it extracts key factors
    considered in movie recommendation, such as the genre of each movie, the distance
    between the given movies and the movies the user has watched before, and the popularity
    of the movies. In the final step, a well-explained example is presented with a
    detailed explanation following aforementioned factors. In summary, Evoke successfully
    concludes the key components of movie recommendation, and curates a demonstration
    with detailed explanation.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Data Selection via LLM-Selector
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In practice, we find that not all samples are equally important to model performance
    (see Figure [10](#S4.F10 "Figure 10 ‣ 4.4 Analysis ‣ 4 Experiments ‣ Evoke: Evoking
    Critical Thinking Abilities in LLMs via Reviewer-Author Prompt Editing")). In
    particular, we find that even without prompt refinement, the LLM already knows
    how to solve some “easier” cases. Therefore, we only use “hard” samples in each
    refinement iteration. Specifically, we assign a third role besides an author a
    reviewer to the LLM: a data selector. The LLM-Selector evaluates the difficulty
    level (on a scale of 1 to 10) of each data point by assessing, based on the current
    task instruction, how challenging it is to derive the correct answer from the
    input. The input prompt for LLM-Selector consists of several components:'
  prefs: []
  type: TYPE_NORMAL
- en: a
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Input for evaluating difficulty level: current instruction and input-output
    pair;'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Instruction for evaluating difficulty level: “As an experienced teacher with
    insight into the various levels of difficulty of exam questions, please rate the
    following question on a scale of 1 to 10, considering factors such as conceptual
    understanding, application of knowledge, problem-solving skills, time required,
    clarity of language, and accessibility, where 1 denotes extremely easy and 10
    denotes extremely difficult.”.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Empirically, we can further improve effectiveness of Evoke by using such a data
    selection strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We conduct extensive experiments to demonstrate the effectiveness of Evoke.
    We show that for any given task, the prompts generated by Evoke include clear
    definitions and well-structured task execution steps. Moreover, these prompts
    feature demonstrations accompanied by detailed explanations. In all experiments,
    we utilize the Azure OpenAI API service (GPT-4) for the involved LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Main Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Datasets. We perform a comprehensive evaluation on eight tasks from Instruction
    Induction (Honovich et al., [2022](#bib.bib6)) and Big Bench Instruction Induction
    (BBII) (Zhou et al., [2022](#bib.bib32)), including
  prefs: []
  type: TYPE_NORMAL
- en: 'orthography starts with: Extract the words starting with a given letter from
    the input sentence.'
  prefs: []
  type: TYPE_NORMAL
- en: 'common concept: Find a common characteristic for the given objects.'
  prefs: []
  type: TYPE_NORMAL
- en: 'rhymes: Write a word that rhymes with the input word.'
  prefs: []
  type: TYPE_NORMAL
- en: 'movie recommendation: Recommend movies similar to the given list of movies.'
  prefs: []
  type: TYPE_NORMAL
- en: 'logical fallacy detection: Detect informal and formal logical fallacies.'
  prefs: []
  type: TYPE_NORMAL
- en: 'presuppositions as nli: Determine whether the first sentence entails or contradicts
    the second.'
  prefs: []
  type: TYPE_NORMAL
- en: 'winowhy: Evaluate the reasoning in answering Winograd Schema Challenge questions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'epistemic reasoning: Determine whether one sentence entails the next.'
  prefs: []
  type: TYPE_NORMAL
- en: These tasks covers a wide range of natural language understanding, reasoning
    and inference tasks. For each task, we divide the dataset randomly into two sets,
    60% of the data is allocated for training (prompt refinement) and the remaining
    40% is for testing (prompt evaluation).
  prefs: []
  type: TYPE_NORMAL
- en: 'Baselines. We compare our methods against two baselines: human curated prompts
    (Human) from Honovich et al. ([2022](#bib.bib6)); Suzgun et al. ([2022](#bib.bib20))
    and automatic prompt engineer (APE) proposed in (Zhou et al., [2022](#bib.bib32)).
    APE first deduces an initial prompt from input-output pairs, and subsequently
    employs LLMs to refine and generate new prompt candidates. However, prompts are
    simply paraphrased during the refinement process of APE, which largely resembles
    random searching in the space of all possible prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Main Results. Figure [4](#S4.F4 "Figure 4 ‣ 4.1 Main Results ‣ 4 Experiments
    ‣ Evoke: Evoking Critical Thinking Abilities in LLMs via Reviewer-Author Prompt
    Editing") demonstrates experimental results. We observe that Evoke outperforms
    all the baselines in all eight tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, on the challenging logical fallacy detection task from BBII, performance
    of Evoke is more than 80, while performance of both APE and Human are below 20.
    This is because Evoke is adept at conceptualizing the core definition of a task,
    decomposing a complex task into smaller subtasks, and curating relevant demonstrations
    accompanied by detailed explanations. To demonstrate the power of Evoke, we show
    the generated prompt for logical fallacy detection in Table [1](#S4.T1 "Table
    1 ‣ 4.1 Main Results ‣ 4 Experiments ‣ Evoke: Evoking Critical Thinking Abilities
    in LLMs via Reviewer-Author Prompt Editing"). We see that the prompt begins with
    a clear task introduction and objective, followed by a fine-grained definition
    of logical fallacy. It then articulates the criteria for evaluation and the task
    steps to follow. Lastly, it provides a list of common logical fallacies, each
    accompanied by a detailed description. Additionally, a well-structured prompt
    for epistemic reasoning is presented in Table [2](#S4.T2 "Table 2 ‣ 4.1 Main Results
    ‣ 4 Experiments ‣ Evoke: Evoking Critical Thinking Abilities in LLMs via Reviewer-Author
    Prompt Editing").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/af5826c7e2a6fa6efd85b39591ee56b8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Results on eight tasks from the Instruction Induction and the Big
    Bench Instruction Induction datasets. We report the average accuracy over three
    runs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Prompt generated by Evoke for logical fallacy detection.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt from Evoke | Breakdown |'
  prefs: []
  type: TYPE_TB
- en: '| In this task, you are required to identify both informal and formal logical
    fallacies in the provided input statements. Your response should be a binary value:
    return 1 if the query is logically valid (i.e., free from any logical fallacies),
    and return 0 if the query is logically invalid (i.e., contains at least one logical
    fallacy). | Introduction and objective |'
  prefs: []
  type: TYPE_TB
- en: '| A logical fallacy refers to an error in reasoning. Informal fallacies are
    often content-dependent, such as appealing to irrelevant authority or making hasty
    generalizations. Formal fallacies, on the other hand, are structural errors in
    reasoning that occur regardless of the content. | Definition of logical fallacy
    |'
  prefs: []
  type: TYPE_TB
- en: '| It is crucial to consider the structure and the substance of the argument,
    evaluating whether the conclusions follow logically from the premises, and whether
    the premises and assumptions are factual and valid. Be cautious not to let personal
    beliefs interfere with your analysis. | Evaluation criteria |'
  prefs: []
  type: TYPE_TB
- en: '| For each given pair, compare the input statement against the principles of
    logical reasoning, to determine whether it contains a logical fallacy or not.
    Ensure your answer reflects the presence or absence of logical fallacies, thus
    determining the logical validity or invalidity of the statement. | Task steps
    |'
  prefs: []
  type: TYPE_TB
- en: '| Here are some common examples of logical fallacies: - Ad Hominem: {details}
    - Appeal to Nature: {details} - Hasty Generalization: {details} - Post Hoc: {details}
    - False Cause: {details} | Common examples of logical fallacy |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Prompt generated by Evoke for epistemic reasoning.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt from Evoke | Breakdown |'
  prefs: []
  type: TYPE_TB
- en: '| In this task, your goal is to determine whether the statement in the “Hypothesis”
    logically follows from the statement in the “Premise.” This is known as entailment.
    If the “Hypothesis” statement is a logical consequence of the “Premise” statement,
    then it is an entailment. If it is not, then it is a non-entailment. | Introduction
    and objective |'
  prefs: []
  type: TYPE_TB
- en: '| -Make sure to carefully consider the relations and assumptions mentioned
    in both the “Premise” and the “Hypothesis” statements. -The entailment does not
    depend on the truth of the statements, but rather whether the logic in the “Hypothesis”
    follows from the “Premise”. -Pay close attention to the wording and structure
    of the sentences to analyze whether one entails the other. | Guidelines |'
  prefs: []
  type: TYPE_TB
- en: '| Examples: Entailment Premise: The sun rises in the east. Hypothesis: The
    sun rises. Explanation: The Hypothesis is a simplified version of the Premise
    and does not introduce any new information or contradictions, hence it’s an entailment.
    Non-entailment Premise: Sarah believes that all cats are black. Hypothesis: All
    cats are black. Explanation: Even though the Hypothesis is expressed in the Premise,
    it’s tied to Sarah’s belief and not presented as a fact, hence it’s a non-entailment.
    | Examples |'
  prefs: []
  type: TYPE_TB
- en: '| Now, review the provided pairs of statements. Determine if the Hypothesis
    logically follows from the Premise and respond with either entailment or non-entailment.
    | Task Execution |'
  prefs: []
  type: TYPE_TB
- en: 4.2 Towards Adversarial Robustness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/35ff2fc8d5b049a05cbf2edd23197c45.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Results on clean and adversarially attacked SST2 and QQP datasets.
    We report the average accuracy over three runs. We note that RobEnc is only applied
    to the attacked data.'
  prefs: []
  type: TYPE_NORMAL
- en: Despite their superior performance, LLMs are not robust to adversarial attacks
    (Wang et al., [2023](#bib.bib23)). For example, when asking GPT-4 whether “pretty”
    is a positive word, the model can output the correct answer. However, if we ask
    whether “prettye”, a clear typo of “pretty”, is a positive word, the LLM outputs
    an opposite answer. We show that Evoke can generate prompts which alert the LLM
    in paying attention to potential typos, and thus can improve model robustness.
  prefs: []
  type: TYPE_NORMAL
- en: 'Datasets. We adopt two datasets: SST-2 (Socher et al., [2013](#bib.bib19))
    is a sentiment classification task, where we need to decide whether a movie review
    is positive or negative; and QQP (Wang et al., [2019](#bib.bib22)) is a task where
    we need to determine whether two sentences are paraphrases of each other.'
  prefs: []
  type: TYPE_NORMAL
- en: To evaluate whether Evoke can improve LLMs’ robustness, we add typos to the
    datasets. Specifically, we perform character-level adversarial attacks for each
    sample. In the attack, we change at most one character in each word, and we change
    at most 4 words in each sentence (Jones et al., [2020](#bib.bib9)). In this way,
    the constructed adversarial texts are human-interpretable and simulate real typos.
    As an example, one sample from SST-2 is “that’s pure pr hype”, and its corresponding
    adversarial (corrupted) sample after the attack is “tha’cs pure pr hyp”. We evaluate
    performance of different prompting methods on the corrupted samples.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/169e82487f5f367bd520b7e0f998eb16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Prompts from APE and Evoke on adversarial attacked SST-2 task'
  prefs: []
  type: TYPE_NORMAL
- en: 'Baselines. Besides APE and Evoke, we evaluate another model: RobEnc (Jones
    et al., [2020](#bib.bib9)), which is a widely-used rule-based defense approach.
    RobEnc works as a clustering denoiser to cluster and denoise potentially corrupted
    inputs into an encoding, and then the denoised encoding is fed to the subsequent
    model (e.g., GPT-4) for inference. RobEnc learns rule-based word cluster for denoising:
    for example, if the word “hallo” is clustered around the word “hello”, then all
    the “hallo” in the input will be converted to “hello”.'
  prefs: []
  type: TYPE_NORMAL
- en: Results.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Figure [5](#S4.F5 "Figure 5 ‣ 4.2 Towards Adversarial Robustness ‣ 4 Experiments
    ‣ Evoke: Evoking Critical Thinking Abilities in LLMs via Reviewer-Author Prompt
    Editing") summarizes experimental results. We observe that Evoke significantly
    outperforms all the baselines in all the tasks. The performance gain is more significant
    for adversarially constructed datasets, e.g., Adversarial-SST2 and Adversarial-QQP.
    To understand this, we show the prompts generated by APE and Evoke in Figure [6](#S4.F6
    "Figure 6 ‣ 4.2 Towards Adversarial Robustness ‣ 4 Experiments ‣ Evoke: Evoking
    Critical Thinking Abilities in LLMs via Reviewer-Author Prompt Editing"). We see
    that although the prompt from APE provides a clear instruction regarding the given
    task and acknowledges the existence of typos, it does not provide clear guidelines
    on how to address the typo. On the other hand, the prompt from Evoke provides
    detailed explanations and actionable suggestions about defending against typos.'
  prefs: []
  type: TYPE_NORMAL
- en: '4.3 Towards Fine-Grained Tasks: Named Entity Recognition'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Tasks in Figure [4](#S4.F4 "Figure 4 ‣ 4.1 Main Results ‣ 4 Experiments ‣ Evoke:
    Evoking Critical Thinking Abilities in LLMs via Reviewer-Author Prompt Editing")
    and Figure [5](#S4.F5 "Figure 5 ‣ 4.2 Towards Adversarial Robustness ‣ 4 Experiments
    ‣ Evoke: Evoking Critical Thinking Abilities in LLMs via Reviewer-Author Prompt
    Editing") are all sentence-level classification tasks, e.g., deciding whether
    a sentence is of positive or negative sentiment. In this section, we investigate
    whether Evoke can handle more fine-grained tasks, such as token-level named entity
    recognition (Schneider et al., [2020](#bib.bib16); Zuo et al., [2023](#bib.bib33)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/22b451a8a4275f7f73a0d79aa0b94eac.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Results of APE and Evoke on an in-house multi-lingual NER dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bdd7cdc0259315fdff04ab7e1fb103e4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Prompt from Evoke on the NER task.'
  prefs: []
  type: TYPE_NORMAL
- en: We collect multi-lingual in-house query data from a search engine, and for each
    token in the query, our goal is to assign the token to a pre-defined class (e.g.,
    brand, location).
  prefs: []
  type: TYPE_NORMAL
- en: 'We illustrate results of Evoke on the fine-grained NER task in Figure [8](#S4.F8
    "Figure 8 ‣ 4.3 Towards Fine-Grained Tasks: Named Entity Recognition ‣ 4 Experiments
    ‣ Evoke: Evoking Critical Thinking Abilities in LLMs via Reviewer-Author Prompt
    Editing"). We see that Evoke significantly outperforms APE on all the languages.
    We further show the prompt generated by Evoke in Figure [8](#S4.F8 "Figure 8 ‣
    4.3 Towards Fine-Grained Tasks: Named Entity Recognition ‣ 4 Experiments ‣ Evoke:
    Evoking Critical Thinking Abilities in LLMs via Reviewer-Author Prompt Editing").
    From the prompt, we see that Evoke is able to automatically generate examples
    and explanations about the task.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'LLM-Reviewer can judge the quality of prompts. Recall that in Evoke, LLM-Reviewer
    scores all the prompts generated by LLM-Author. We empirically show that the scores
    can reflect the quality of the generated prompts. To examine the effectiveness
    of these scores, we illustrate the relationship between the scores and the task
    accuracy in Figure [10](#S4.F10 "Figure 10 ‣ 4.4 Analysis ‣ 4 Experiments ‣ Evoke:
    Evoking Critical Thinking Abilities in LLMs via Reviewer-Author Prompt Editing").
    In the experiments, we consider two tasks: Adversarial-SST2 and Common-Concept.
    From the results, we see that the scores can indeed reflect the final task accuracy.
    For example, for Common-Concept, we see that the task accuracy is about 5% when
    the prompt score is 6, and the task accuracy increases to about 17% when the prompt
    score increases to 7\. A similar trend is also revealed on the Adversarial-SST2
    task. We see that when the score is 7.5, the final task accuracy barely reaches
    75%. And when the score increases to 8, the task accuracy increases to 85%.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d95059d76d8d3878ebad19456565e439.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Correlation between scores generated by LLM-Reviewer and task accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5431bb46f38a3063bcb8b5fad0245c88.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Task accuracy over the number of iteration steps.'
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM-Author iteratively improves prompt generation. In Evoke, because LLM-Author
    takes the feedback from LLM-Reviewer into consideration, it can iteratively improve
    the generated prompt. We demonstrate this in Figure [10](#S4.F10 "Figure 10 ‣
    4.4 Analysis ‣ 4 Experiments ‣ Evoke: Evoking Critical Thinking Abilities in LLMs
    via Reviewer-Author Prompt Editing") (the left-most orange bars). From the results,
    we see that indeed the final task accuracy continues to increases when we increase
    the number of iteration steps. For example, on Adversarial-SST2, with one refinement
    iteration, the final task accuracy is about 75%. When we increase the number of
    refinement iterations to 3, we see that task accuracy significantly increases
    to above 90%.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Effectiveness of LLM-Selector. Recall that in Evoke, we only consider the “hard”
    samples in each iteration. We demonstrate the effectiveness of such a strategy
    in Figure [10](#S4.F10 "Figure 10 ‣ 4.4 Analysis ‣ 4 Experiments ‣ Evoke: Evoking
    Critical Thinking Abilities in LLMs via Reviewer-Author Prompt Editing"). We consider
    three settings: Hard is the strategy that we adopt in Evoke; Random is when we
    randomly select samples instead of selecting based on a score; and Easy is when
    we select the easy samples instead of the hard ones. From the results, we see
    that on both Common-Concept and Adversarial SST-2, Easy yields the worst performance,
    indicating that the hard samples are more helpful than the easy ones. Moreover,
    we observe that performance of Random is worse than Hard (i.e., Evoke), further
    implying the effectiveness of the proposed data selection strategy.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We propose Evoke, an author-reviewer framework for automatic prompt engineering.
    In Evoke, the same LLM serves two roles: as a reviewer it scores the quality of
    the prompt; and as an author it refines the prompt, taking the feedback of the
    reviewer into account. We further propose a data selection strategy, where we
    only expose the hard samples to the model. Extensive experiments show that Evoke outperforms
    existing automatic prompt engineering approaches.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Bubeck et al. (2023) Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J.,
    Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S. et al. (2023).
    Sparks of artificial general intelligence: Early experiments with gpt-4. ArXiv
    preprint, abs/2303.12712.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021) Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,
    Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G. et al. (2021). Evaluating
    large language models trained on code. ArXiv preprint, abs/2107.03374.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. (2023) Dettmers, T., Pagnoni, A., Holtzman, A. and Zettlemoyer,
    L. (2023). Qlora: Efficient finetuning of quantized llms. ArXiv preprint, abs/2305.14314.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Du et al. (2023) Du, Y., Watkins, O., Wang, Z., Colas, C., Darrell, T., Abbeel,
    P., Gupta, A. and Andreas, J. (2023). Guiding pretraining in reinforcement learning
    with large language models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2023) Gao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y.,
    Callan, J. and Neubig, G. (2023). Pal: Program-aided language models. In International
    Conference on Machine Learning. PMLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Honovich et al. (2022) Honovich, O., Shaham, U., Bowman, S. R. and Levy, O.
    (2022). Instruction induction: From few examples to natural language task descriptions.
    ArXiv preprint, abs/2205.10782.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2022) Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,
    S., Wang, L. and Chen, W. (2022). Lora: Low-rank adaptation of large language
    models. In The Tenth International Conference on Learning Representations, ICLR
    2022, Virtual Event, April 25-29, 2022. OpenReview.net.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang and Chang (2022) Huang, J. and Chang, K. C.-C. (2022). Towards reasoning
    in large language models: A survey. ArXiv preprint, abs/2212.10403.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jones et al. (2020) Jones, E., Jia, R., Raghunathan, A. and Liang, P. (2020).
    Robust encodings: A framework for combating adversarial typos. In Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics. Association
    for Computational Linguistics, Online.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kojima et al. (2022) Kojima, T., Gu, S. S., Reid, M., Matsuo, Y. and Iwasawa,
    Y. (2022). Large language models are zero-shot reasoners. Advances in neural information
    processing systems, 35 22199–22213.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2022) Liu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L. and Chen,
    W. (2022). What makes good in-context examples for GPT-3? In Proceedings of Deep
    Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and
    Integration for Deep Learning Architectures. Association for Computational Linguistics,
    Dublin, Ireland and Online.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Min et al. (2022) Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi,
    H. and Zettlemoyer, L. (2022). Rethinking the role of demonstrations: What makes
    in-context learning work? In Proceedings of the 2022 Conference on Empirical Methods
    in Natural Language Processing. Association for Computational Linguistics, Abu
    Dhabi, United Arab Emirates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. (2022) Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,
    C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A. et al. (2022). Training
    language models to follow instructions with human feedback. Advances in Neural
    Information Processing Systems, 35 27730–27744.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reynolds and McDonell (2021) Reynolds, L. and McDonell, K. (2021). Prompt programming
    for large language models: Beyond the few-shot paradigm. In Extended Abstracts
    of the 2021 CHI Conference on Human Factors in Computing Systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sarsa et al. (2022) Sarsa, S., Denny, P., Hellas, A. and Leinonen, J. (2022).
    Automatic generation of programming exercises and code explanations using large
    language models. In Proceedings of the 2022 ACM Conference on International Computing
    Education Research-Volume 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schneider et al. (2020) Schneider, E. T. R., de Souza, J. V. A., Knafou, J.,
    Oliveira, L. E. S. e., Copara, J., Gumiel, Y. B., Oliveira, L. F. A. d., Paraiso,
    E. C., Teodoro, D. and Barra, C. M. C. M. (2020). BioBERTpt - a Portuguese neural
    language model for clinical named entity recognition. In Proceedings of the 3rd
    Clinical Natural Language Processing Workshop. Association for Computational Linguistics,
    Online.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shin et al. (2020) Shin, T., Razeghi, Y., Logan IV, R. L., Wallace, E. and
    Singh, S. (2020). AutoPrompt: Eliciting Knowledge from Language Models with Automatically
    Generated Prompts. In Proceedings of the 2020 Conference on Empirical Methods
    in Natural Language Processing (EMNLP). Association for Computational Linguistics,
    Online.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shrivastava et al. (2023) Shrivastava, D., Larochelle, H. and Tarlow, D. (2023).
    Repository-level prompt generation for large language models of code. In International
    Conference on Machine Learning. PMLR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Socher et al. (2013) Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning,
    C. D., Ng, A. and Potts, C. (2013). Recursive deep models for semantic compositionality
    over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical
    Methods in Natural Language Processing. Association for Computational Linguistics,
    Seattle, Washington, USA.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suzgun et al. (2022) Suzgun, M., Scales, N., Schärli, N., Gehrmann, S., Tay,
    Y., Chung, H. W., Chowdhery, A., Le, Q. V., Chi, E. H., Zhou, D. et al. (2022).
    Challenging big-bench tasks and whether chain-of-thought can solve them. ArXiv
    preprint, abs/2210.09261.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thirunavukarasu et al. (2023) Thirunavukarasu, A. J., Ting, D. S. J., Elangovan,
    K., Gutierrez, L., Tan, T. F. and Ting, D. S. W. (2023). Large language models
    in medicine. Nature medicine 1–11.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2019) Wang, A., Singh, A., Michael, J., Hill, F., Levy, O. and
    Bowman, S. R. (2019). GLUE: A multi-task benchmark and analysis platform for natural
    language understanding. In 7th International Conference on Learning Representations,
    ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023) Wang, J., Hu, X., Hou, W., Chen, H., Zheng, R., Wang, Y.,
    Yang, L., Huang, H., Ye, W., Geng, X. et al. (2023). On the robustness of chatgpt:
    An adversarial and out-of-distribution perspective. ArXiv preprint, abs/2302.12095.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022) Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang,
    S., Chowdhery, A. and Zhou, D. (2022). Self-consistency improves chain of thought
    reasoning in language models. ArXiv preprint, abs/2203.11171.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022a) Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester,
    B., Du, N., Dai, A. M. and Le, Q. V. (2022a). Finetuned language models are zero-shot
    learners. In The Tenth International Conference on Learning Representations, ICLR
    2022, Virtual Event, April 25-29, 2022. OpenReview.net.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022b) Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud,
    S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D. et al. (2022b). Emergent abilities
    of large language models. ArXiv preprint, abs/2206.07682.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022c) Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi,
    E., Le, Q. V., Zhou, D. et al. (2022c). Chain-of-thought prompting elicits reasoning
    in large language models. Advances in Neural Information Processing Systems, 35
    24824–24837.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: White et al. (2023) White, J., Fu, Q., Hays, S., Sandborn, M., Olea, C., Gilbert,
    H., Elnashar, A., Spencer-Smith, J. and Schmidt, D. C. (2023). A prompt pattern
    catalog to enhance prompt engineering with chatgpt. ArXiv preprint, abs/2302.11382.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie et al. (2022) Xie, S. M., Raghunathan, A., Liang, P. and Ma, T. (2022).
    An explanation of in-context learning as implicit bayesian inference. In The Tenth
    International Conference on Learning Representations, ICLR 2022, Virtual Event,
    April 25-29, 2022. OpenReview.net.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yoo et al. (2022) Yoo, K. M., Kim, J., Kim, H. J., Cho, H., Jo, H., Lee, S.-W.,
    Lee, S.-g. and Kim, T. (2022). Ground-truth labels matter: A deeper look into
    input-label demonstrations. In Proceedings of the 2022 Conference on Empirical
    Methods in Natural Language Processing. Association for Computational Linguistics,
    Abu Dhabi, United Arab Emirates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023) Zhang, B., Haddow, B. and Birch, A. (2023). Prompting large
    language model for machine translation: A case study. ArXiv preprint, abs/2301.07069.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2022) Zhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S.,
    Chan, H. and Ba, J. (2022). Large language models are human-level prompt engineers.
    ArXiv preprint, abs/2211.01910.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zuo et al. (2023) Zuo, S., Tang, P., Hu, X., Lou, Q., Jiao, J. and Charles,
    D. (2023). Deeptagger: Knowledge enhanced named entity recognition for web-based
    ads queries. ArXiv preprint, abs/2306.17413.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Instruction Induction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Evoke, we use the author-reviewer framework to modify a task-specific prompt.
    In the experiments, we use an off-the-shelf algorithm to generate the initial
    task-specific prompt. Table [3](#A1.T3 "Table 3 ‣ Appendix A Instruction Induction
    ‣ Evoke: Evoking Critical Thinking Abilities in LLMs via Reviewer-Author Prompt
    Editing") demonstrates examples of using instruction induction (Honovich et al.,
    [2022](#bib.bib6)) for prompt initialization.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Three examples of instruction inferred from input-output pairs'
  prefs: []
  type: TYPE_NORMAL
- en: '| Input | Output | Inferred Instruction |'
  prefs: []
  type: TYPE_TB
- en: '| Departure | Arrival | Get antonym |'
  prefs: []
  type: TYPE_TB
- en: '| I am Mike | Ich ben Mike | Translate to German |'
  prefs: []
  type: TYPE_TB
- en: '| Build | Built | Get passive voice of the given verb |'
  prefs: []
  type: TYPE_TB
- en: Appendix B Prompts of LLM roles in Evoke
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: B.1 LLM-Reviewer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Prompt for LLM-Reviewer |'
  prefs: []
  type: TYPE_TB
- en: '| As an experienced teacher, you are well-versed in discerning effective instruction
    that guides students toward correct answers. Please rate the following instruction
    on a scale of 1 to 10, where 10 represents the highest level of clarity in problem
    description, execution steps, and a comprehensive explanation of the problem.
    |'
  prefs: []
  type: TYPE_TB
- en: '| The task at hand is titled: {description} |'
  prefs: []
  type: TYPE_TB
- en: '| History that may help you: {memory} |'
  prefs: []
  type: TYPE_TB
- en: '| The instruction to be rated is as follows: {instruction} |'
  prefs: []
  type: TYPE_TB
- en: '| Kindly provide your rating below. |'
  prefs: []
  type: TYPE_TB
- en: B.2 LLM-Author
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Prompt for LLM-Author |'
  prefs: []
  type: TYPE_TB
- en: '| Task Instruction: {instruction} |'
  prefs: []
  type: TYPE_TB
- en: '| We’ve provided pairs consisting of inputs, the teacher’s correct answers,
    and the students’ responses. Please review the incorrect responses from the students
    and summarize key points that could be adjusted in the instruction to enhance
    student accuracy. |'
  prefs: []
  type: TYPE_TB
- en: '| Pairs: {pairs} |'
  prefs: []
  type: TYPE_TB
- en: '| History that may help you: {memory} |'
  prefs: []
  type: TYPE_TB
- en: '| To improve the outcome, please revise the task instruction. Highlight major
    edits and present the updated task instruction. |'
  prefs: []
  type: TYPE_TB
- en: B.3 LLM-Selector
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Prompt for LLM-Selector |'
  prefs: []
  type: TYPE_TB
- en: '| As an experienced teacher with insight into the various levels of difficulty
    of exam questions, please rate the following question on a scale of 1 to 10, considering
    factors such as conceptual understanding, application of knowledge, problem-solving
    skills, time required, clarity of language, and accessibility, where 1 denotes
    extremely easy and 10 denotes extremely difficult. |'
  prefs: []
  type: TYPE_TB
- en: '| Task instruction: {instruction} |'
  prefs: []
  type: TYPE_TB
- en: '| Input: {input} |'
  prefs: []
  type: TYPE_TB
- en: '| Correct answer: {answer} |'
  prefs: []
  type: TYPE_TB
- en: Appendix C Generated Instructions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We include generated instructions from all tasks below.
  prefs: []
  type: TYPE_NORMAL
- en: C.1 Orthography Starts With
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Prompt from Evoke |'
  prefs: []
  type: TYPE_TB
- en: '| Given an input sentence and a specified letter, identify the word or words
    starting with the given letter. If there are two or more words in a sequence starting
    with the specified letter, include all of them as a single answer. Ensure to present
    the word or group of words. Here are the steps to follow: -Read the provided input
    sentence carefully. |'
  prefs: []
  type: TYPE_TB
- en: '| -Identify the word or words that start with the specified letter. |'
  prefs: []
  type: TYPE_TB
- en: '| -If there are consecutive words starting with the specified letter, group
    them together as one entity. |'
  prefs: []
  type: TYPE_TB
- en: '| -For example, if the input is ”I prefer eating apples.” and the specified
    letter is [e], your answer should be eating. |'
  prefs: []
  type: TYPE_TB
- en: C.2 Common Concept
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Prompt from Evoke |'
  prefs: []
  type: TYPE_TB
- en: '| Given a list, find the commonality between the inputs. The commonality should
    be a meaningful characteristic, property, or relation that applies to all the
    inputs, not just a superficial or coincidental feature. For example, can be used
    for repairs is a valid commonality for [’sewing’, ’wrenches’, ’glue’, ’surgery’],
    but tools or skills for joining is too broad and vague, and contain the letter
    e is too trivial and irrelevant. |'
  prefs: []
  type: TYPE_TB
- en: C.3 Rhymes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Prompt from Evoke |'
  prefs: []
  type: TYPE_TB
- en: '| For this task, you are required to find a word that rhymes with the given
    word. The word you provide should not be the same as the given word, and should
    be a real, correctly spelled word from the English language. A rhyming word is
    defined as a word that has the last syllable sounding identical to the last syllable
    of the given word. For example, if the given word is ”hat”, a word that rhymes
    with it is ”cat”. Here are the steps to complete this task: |'
  prefs: []
  type: TYPE_TB
- en: '| 1\. Read the given word carefully. |'
  prefs: []
  type: TYPE_TB
- en: '| 2\. Think of a word that has the same ending sound as the given word. |'
  prefs: []
  type: TYPE_TB
- en: '| 3\. Ensure that the word you thought of is a real word, is spelled correctly,
    and is not the same as the given word. |'
  prefs: []
  type: TYPE_TB
- en: '| 4\. Write down the rhyming word next to the given word. |'
  prefs: []
  type: TYPE_TB
- en: '| Now, please proceed with finding a word that rhymes with each of the following
    words. |'
  prefs: []
  type: TYPE_TB
- en: C.4 Movie Recommendation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Prompt from Evoke |'
  prefs: []
  type: TYPE_TB
- en: '| Given user’s interest in movies he watched previously: ‘watched‘. Now given
    four different movies from A to D, please recommend one that might be the most
    interest of the user. To help you make a good recommendation, consider the following
    factors: - The genre, theme, and tone of the movies. For example, if the user
    likes comedy, action, or drama. - The similarity or difference between the movies
    and the ones the user watched before. For example, if the movies are part of a
    series, a remake, or a spin-off. - The popularity, ratings, and reviews of the
    movies. For example, if the movies are critically acclaimed, award-winning, or
    have a large fan base. Use these factors to compare and contrast the movies and
    explain why you think one of them is the best choice for the user. Do not just
    pick a movie based on your personal preference or guesswork. Example: If the user
    watched The Godfather, The Godfather Part II, and Goodfellas, and the options
    are A) The Departed, B) Scarface, C) The Irishman, and D) Casino, a possible answer
    is: A The Departed is a crime thriller that has a similar genre, theme, and tone
    to the movies the user watched before. It is also a remake of a Hong Kong film
    called Infernal Affairs, which adds a twist to the familiar story of undercover
    agents and mobsters. The Departed is a highly popular and acclaimed movie that
    won four Oscars, including Best Picture and Best Director. It has a star-studded
    cast that includes Leonardo DiCaprio, Matt Damon, Jack Nicholson, and Mark Wahlberg.
    The user might enjoy the suspense, the plot twists, and the performances of the
    actors in this movie. Therefore, I recommend The Departed as the best option for
    the user. |'
  prefs: []
  type: TYPE_TB
- en: C.5 Logical Fallacy Detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Prompt from Evoke |'
  prefs: []
  type: TYPE_TB
- en: '| In this task, you are required to identify both informal and formal logical
    fallacies in the provided input statements. Your response should be a binary value:
    return 1 if the query is logically valid (i.e., free from any logical fallacies),
    and return 0 if the query is logically invalid (i.e., contains at least one logical
    fallacy). A logical fallacy refers to an error in reasoning. Informal fallacies
    are often content-dependent, such as appealing to irrelevant authority or making
    hasty generalizations. Formal fallacies, on the other hand, are structural errors
    in reasoning that occur regardless of the content. It is crucial to consider the
    structure and the substance of the argument, evaluating whether the conclusions
    follow logically from the premises, and whether the premises and assumptions are
    factual and valid. Be cautious not to let personal beliefs interfere with your
    analysis. For each given pair, compare the input statement against the principles
    of logical reasoning, to determine whether it contains a logical fallacy or not.
    Ensure your answer reflects the presence or absence of logical fallacies, thus
    determining the logical validity or invalidity of the statement. Here are some
    common examples of logical fallacies: - Ad Hominem: Attacking the character of
    a person making an argument rather than the argument itself. - Appeal to Nature:
    Claiming something is good because it’s natural, or bad because it’s unnatural.
    - Hasty Generalization: Making a broad claim based on a small or unrepresentative
    sample size. - Post Hoc: Assuming that because one event followed another, the
    first event caused the second event. - False Cause: Assuming a false or misleading
    cause-and-effect relationship. |'
  prefs: []
  type: TYPE_TB
- en: C.6 Presuppositions as NLI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Prompt from Evoke |'
  prefs: []
  type: TYPE_TB
- en: '| Determine whether the first sentence entails, contradicts, or is neutral
    to the second sentence. The term ”entailment” means that the information in the
    first sentence logically supports or leads to the conclusion presented in the
    second sentence. The term ”contradiction” means that the information in the first
    sentence logically opposes or disproves the information in the second sentence.
    The term ”neutral” implies that the information in the first sentence neither
    supports nor opposes the information in the second sentence; they are unrelated
    or the relation between them is ambiguous. |'
  prefs: []
  type: TYPE_TB
- en: '| It’s important to focus on the factual information provided rather than assumptions
    or external knowledge. Make sure to carefully read both sentences and analyze
    their logical relation based only on the given text. Entailment: The information
    in the first sentence supports the conclusion in the second sentence. Contradiction:
    The information in the first sentence opposes or disproves the information in
    the second sentence. |'
  prefs: []
  type: TYPE_TB
- en: '| Neutral: The information in the first sentence neither supports nor opposes
    the information in the second sentence, or the relation between them is ambiguous.
    |'
  prefs: []
  type: TYPE_TB
- en: '| For each pair, please provide the correct judgment between entailment, contradiction,
    and neutral, based only on the provided text. Please avoid assumptions and focus
    solely on the text provided. |'
  prefs: []
  type: TYPE_TB
- en: C.7 Winowhy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Prompt from Evoke |'
  prefs: []
  type: TYPE_TB
- en: '| In the given text, you are required to evaluate the reasoning provided concerning
    the identification of the antecedent of a pronoun in a sentence. The antecedent
    is the noun that the pronoun is referring to. Carefully examine the reasoning
    to determine if it accurately identifies the antecedent based solely on the information
    presented within the sentence itself. Here are the steps you should follow: Read
    the sentence and the reasoning provided thoroughly. |'
  prefs: []
  type: TYPE_TB
- en: '| -Assess whether the reasoning accurately identifies the antecedent of the
    pronoun based solely on the provided text. Avoid making assumptions or using external
    knowledge. |'
  prefs: []
  type: TYPE_TB
- en: '| -If the reasoning correctly identifies the antecedent of the pronoun, based
    on the information given in the sentence. |'
  prefs: []
  type: TYPE_TB
- en: '| -If the reasoning fails to accurately identify the antecedent of the pronoun
    or relies on assumptions or external information. |'
  prefs: []
  type: TYPE_TB
- en: '| Remember, |'
  prefs: []
  type: TYPE_TB
- en: '| Your evaluation should strictly be based on the information provided in the
    text. |'
  prefs: []
  type: TYPE_TB
- en: '| Your goal is to assess the accuracy of the reasoning in identifying the antecedent
    of the pronoun. |'
  prefs: []
  type: TYPE_TB
- en: C.8 Epistemic Reasoning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Prompt from Evoke |'
  prefs: []
  type: TYPE_TB
- en: '| In this task, your goal is to determine whether the statement in the ”Hypothesis”
    logically follows from the statement in the ”Premise.” This is known as entailment.
    If the ”Hypothesis” statement is a logical consequence of the ”Premise” statement,
    then it is an entailment. If it is not, then it is a non-entailment. |'
  prefs: []
  type: TYPE_TB
- en: '| -Make sure to carefully consider the relations and assumptions mentioned
    in both the ”Premise” and the ”Hypothesis” statements. |'
  prefs: []
  type: TYPE_TB
- en: '| -The entailment does not depend on the truth of the statements, but rather
    whether the logic in the ”Hypothesis” follows from the ”Premise”. |'
  prefs: []
  type: TYPE_TB
- en: '| -Pay close attention to the wording and structure of the sentences to analyze
    whether one entails the other. |'
  prefs: []
  type: TYPE_TB
- en: '| Examples: Entailment Premise: The sun rises in the east. Hypothesis: The
    sun rises. |'
  prefs: []
  type: TYPE_TB
- en: '| Explanation: The Hypothesis is a simplified version of the Premise and does
    not introduce any new information or contradictions, hence it’s an entailment.
    Non-entailment Premise: Sarah believes that all cats are black. |'
  prefs: []
  type: TYPE_TB
- en: '| Hypothesis: All cats are black. Explanation: Even though the Hypothesis is
    expressed in the Premise, it’s tied to Sarah’s belief and not presented as a fact,
    hence it’s a non-entailment. Now, review the provided pairs of statements. Determine
    if the Hypothesis logically follows from the Premise and respond with either entailment
    or non-entailment. |'
  prefs: []
  type: TYPE_TB
- en: C.9 Adversarial SST2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Prompt from Evoke |'
  prefs: []
  type: TYPE_TB
- en: '| Now given a user query, please do a sentiment analysis where 0 represents
    negative sentiment and 1 represents positive sentiment. Please note that the input
    may contain typos. These are not intentional and do not reflect the user’s actual
    sentiment. You should try to correct the typos or infer the intended meaning from
    the context before assigning a sentiment score. For example, ‘to merely badv trather
    than painfzully awfl‘ could be corrected to ‘to merely bad rather than painfully
    awful‘ and scored as 0\. Similarly, ‘l comelling mobion‘ could be corrected to
    ‘a compelling motion‘ and scored as 1\. Sentiment analysis. |'
  prefs: []
  type: TYPE_TB
- en: C.10 Adversarial QQP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Prompt from Evoke |'
  prefs: []
  type: TYPE_TB
- en: '| You will be given a pair of questions and asked to determine whether they
    are paraphrases of each other. Paraphrases are questions that have the same meaning
    or ask about the same information, even if they use different words or structures.
    Please answer with a binary value of 1 if the questions are paraphrases, or 0
    if they are not. Please pay close attention to typos, spelling, grammar, and punctuation
    before answering, as they may affect the meaning of the questions. If you are
    not sure whether the questions are paraphrases or not, you can use some strategies
    to help you decide, such as: - Compare the keywords and topics of the questions.
    Do they match or relate to each other? - Rewrite one question in a different way
    and see if it still conveys the same message as the other question. - Think about
    the context and purpose of the questions. Are they asking for the same type of
    information or response? For example, the questions What is the capital of France?
    and Which city is the seat of the French government? are paraphrases, because
    they both ask about the same fact and can be answered with the same word (Paris).
    However, the questions How do you play the guitar? and What are some guitar chords?
    are not paraphrases, because they ask for different kinds of information and have
    different levels of specificity. |'
  prefs: []
  type: TYPE_TB
