- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:47:32'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Do LLMs Work on Charts? Designing Few-Shot Prompts for Chart Question Answering
    and Summarization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2312.10610](https://ar5iv.labs.arxiv.org/html/2312.10610)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Xuan Long Do¹, Mohammad Hassanpour², Ahmed Masry², Parsa Kavehzadeh²,
  prefs: []
  type: TYPE_NORMAL
- en: Enamul Hoque², Shafiq Joty^(3,4)
  prefs: []
  type: TYPE_NORMAL
- en: ¹National University of Singapore, ²York University, Canada,
  prefs: []
  type: TYPE_NORMAL
- en: ³Nanyang Technological University, Singapore, ⁴Salesforce Research
  prefs: []
  type: TYPE_NORMAL
- en: xuanlong.do@u.nus.edu, {mhpour, parsaka, enamulh}@yorku.ca,
  prefs: []
  type: TYPE_NORMAL
- en: srjoty@ntu.edu.sg, ahmed.elmasry24653@gmail.com Work done when the author was
    on leave from NTU
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A variety of tasks have been proposed recently to facilitate exploration and
    analysis of charts such as chart QA and summarization. The dominant paradigm to
    solve these tasks has been to fine-tune a pretrained model on the task data. However,
    this approach is not only expensive but also not generalizable to unseen tasks.
    On the other hand, large language models (LLMs) have shown impressive generalization
    capabilities to unseen tasks with zero- or few-shot prompting. However, their
    application to chart-related tasks is not trivial as these tasks typically involve
    considering not only the underlying data but also the visual features in the chart
    image. We propose PromptChart, a multimodal few-shot prompting framework with
    LLMs for chart-related applications. By analyzing the tasks carefully, we have
    come up with a set of prompting guidelines for each task to elicit the best few-shot
    performance from LLMs. We further propose a strategy to inject visual information
    into the prompts. Our experiments on three different chart-related information
    consumption tasks show that with properly designed prompts LLMs can excel on the
    benchmarks, achieving state-of-the-art.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data visualizations such as bar charts and line charts are frequently used for
    analyzing data, to derive important insights and make informed decisions Hoque
    et al. ([2022](#bib.bib18)). However, comprehending important patterns and trends
    from charts and answering complex questions about them can be cognitively demanding
    Whitaker and Jacobbe ([2017](#bib.bib49)); del Puy Pérez-Echeverría et al. ([2018](#bib.bib41)).
    To support users in analyzing charts, various downstream tasks have been proposed
    such as chart question answering or ChartQA Masry et al. ([2022](#bib.bib34));
    Kantharaj et al. ([2022a](#bib.bib21)); Lee et al. ([2022](#bib.bib25)) and chart
    summarization Kantharaj et al. ([2022b](#bib.bib22)).
  prefs: []
  type: TYPE_NORMAL
- en: To date, the dominant strategy to tackle these downstream tasks is to fine-tune
    a pre-trained language model or vision-language model on each task. While this
    strategy generally yields good performance, it requires a large amount of labeled
    data for each downstream task as well as computational resources to train. Furthermore,
    recent studies have shown that even after fine-tuning, these models still struggle
    with queries that involve logical and arithmetic reasoning Liu et al. ([2022b](#bib.bib28));
    Masry et al. ([2022](#bib.bib34)); Kantharaj et al. ([2022a](#bib.bib21)); Cheng
    et al. ([2023](#bib.bib7)).
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, large language models (LLMs) have demonstrated impressive generalization
    capabilities to unseen tasks through *in-context learning* and/or *instructional
    tuning*. With in-context learning, LLMs can perform a new task only by looking
    at few examples without making any gradient update on the task data Brown et al.
    ([2020](#bib.bib6)); Liu et al. ([2023a](#bib.bib29)), while instructional tuning
    aims to enhance the model’s instruction following capability by explicitly finetuning
    an LLM on large amount of multi-task instructional data Wei et al. ([2022a](#bib.bib47));
    Ouyang et al. ([2022](#bib.bib38)). Such generalization ability of LLMs is game
    changing compared to the predominant finetuning paradigm with two key advantages.
    First, it eliminates the need to train the model on each downstream task, which
    is highly resource consuming for LLMs. Second, such ability makes LM-as-a-service
    possible Sun et al. ([2022](#bib.bib45)), powering wide range of real world applications.
    Thus, there are emergent studies focusing on few-shot prompting for vision-language
    tasks Alayrac et al. ([2022](#bib.bib1)); Zhou et al. ([2022](#bib.bib50)); Najdenkoska
    et al. ([2023](#bib.bib37)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/06f65be8156b1556bcce7207100b3207.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: An overview of our proposed PromptChart prompting framework. The
    *Visual Data Table Generator* module is omitted for the tasks of Long-form Chart
    Question Answering and Chart Summarization.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite these recent advances, there has not been any comprehensive work on
    how few-shot prompting can be effectively applied to chart comprehension and reasoning
    tasks. Liu et al. ([2022a](#bib.bib27)) present a one-shot method for ChartQA,
    which consists of two main steps: *(i)* extracting the data table from an input
    chart; *(ii)* constructing one-shot prompt to infer the output from an LLM. Nonetheless,
    this work only focuses on the one-shot setting and only one type of downstream
    task (i.e., factoid chart question answering). Moreover, it only uses the extracted
    data table to answer tasks without using visual features (e.g., colors, positions,
    and shapes of graphical marks) from the chart, which limits the model’s ability
    to perform visual reasoning. To our knowledge, there has not been any work that
    explores few-shot prompting for a diverse range of chart comprehension and reasoning
    tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: To bridge this gap, we take the first step to study few-shot prompting for chart-related
    tasks. We design PromptChart, a multimodal prompting framework consisting of three
    modules as shown in [Fig. 1](#S1.F1 "In 1 Introduction ‣ Do LLMs Work on Charts?
    Designing Few-Shot Prompts for Chart Question Answering and Summarization"). Our
    main contribution is the *Prompt Constructor* module, which aims to construct
    effective few-shot prompts. For each task, we focus on analyzing carefully its
    subcategories and experiment with different few-shot prompting setups to verify
    the necessity and effectiveness of each demonstration. We then define a set of
    attributes for each task to guide the prompt constructions. To effectively encode
    the visual information into the few-shot prompts, we propose *Visual Data Table
    Generator (VDTG)* module to generate the *visual data table* - a data table that
    includes visual information such as colors and positions of the chart’s labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'We evaluate PromptChart on three different benchmarks: (1) Factoid QA with
    charts where the answer to a question is a short phrase; (2) Long-form Chart QA
    or LCQA where the answer is an explanatory text; (3) Chart summarization where
    the output is a summary of the chart. Compared with existing strong baselines,
    our framework achieves significant improvements in automatic metrics and human
    preferences across all downstream tasks, achieving state-of-the-art performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our work is mainly related to two lines of prior studies that we briefly describe
    here. An extended related work can be found in [Section A.1](#A1.SS1 "A.1 Extended
    Related Work ‣ Appendix A Appendices ‣ Do LLMs Work on Charts? Designing Few-Shot
    Prompts for Chart Question Answering and Summarization").
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Chart QA and Summarization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recently, there has been growing interest in solving various chart-related downstream
    tasks. For example, the chart question answering (CQA) problem takes a question
    about a chart as input and produces the answer as output Hoque et al. ([2022](#bib.bib18)).
    Methani et al. ([2020](#bib.bib36)) and Masry et al. ([2022](#bib.bib34)) propose
    chart question answering benchmarks targeting factoid questions that require visual
    and arithmetic reasoning. Meanwhile, long-form chart question answering task (or
    open-ended CQA) Kantharaj et al. ([2022a](#bib.bib21)) requires an explanatory
    answer for each question by reasoning over the chart image. In addition, chart
    summarization task Kantharaj et al. ([2022b](#bib.bib22)) focuses on having a
    chart image as input and generating a natural language summary covering key insights
    from the chart. In this work, we verify the effectiveness of our proposed framework
    on the above three chart question answering and summarization tasks. We study
    these tasks because they require interactions between textual queries and chart
    images and also because there are adequate publicly available datasets for them.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Prompting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the scaling of model sizes Devlin et al. ([2019](#bib.bib14)); Radford
    et al. ([2019](#bib.bib42)); Brown et al. ([2020](#bib.bib6)); Chowdhery et al.
    ([2022](#bib.bib10)), large language models (LLMs) have demonstrated strong capabilities
    in solving multiple NLP downstream tasks only by conditioning on the input prompt
    which contains a few demonstrations (a.k.a., few-shot prompting). Such paradigms
    of prompting LLMs are called prompt-based learning or in-context learning Liu
    et al. ([2023a](#bib.bib29)); Beltagy et al. ([2022](#bib.bib3)). This line of
    research has attracted great attention from the research community to solve downstream
    tasks including vision-language tasks Alayrac et al. ([2022](#bib.bib1)); Zhou
    et al. ([2022](#bib.bib50)); Najdenkoska et al. ([2023](#bib.bib37)). However,
    exploring how chart-related downstream tasks could benefit from the above techniques
    has received limited attention. Recently, Liu et al. ([2022a](#bib.bib27)) propose
    the first one-shot reasoning framework for chart question answering based on PaLM
    Chowdhery et al. ([2022](#bib.bib10)). Nonetheless, this framework suffers from
    several limitations as discussed in [Section 1](#S1 "1 Introduction ‣ Do LLMs
    Work on Charts? Designing Few-Shot Prompts for Chart Question Answering and Summarization").
    On the contrary, our study framework PromptChart overcomes these limitations.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Problem Formulation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: $\bullet$ Problem Definition
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We focus on three chart-related tasks: factoid chart QA (FCQA), long-from chart
    QA (LCQA) and chart summarization (CS). A sample in FCQA can be expressed as $(C,D,Q,A)$
    as output.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Datasets | Type | #Charts/#QA pairs |'
  prefs: []
  type: TYPE_TB
- en: '| ChartQA | Factoid CQA | 21.1K/32.7K |'
  prefs: []
  type: TYPE_TB
- en: '| OpenCQA | Long-form CQA | 7.7K/7.7K |'
  prefs: []
  type: TYPE_TB
- en: '| Chart-to-Text | Chart Summarization | 44K/44K |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Statistics of the downstream benchmarks.'
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Benchmarks
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We choose ChartQA Masry et al. ([2022](#bib.bib34)) – a benchmark which contains
    factoid chart question-answer pairs with visual and logical reasoning, OpenCQA
    Kantharaj et al. ([2022a](#bib.bib21)) – another QA benchmark where the answers
    are explanatory descriptions, Chart-to-Text Kantharaj et al. ([2022b](#bib.bib22))
    – a benchmark for chart summarization, as the FCQA, LCQA, CS benchmarks respectively
    to study. For Chart-to-Text, it consists of two datasets, referred to as Pew and
    Statista. The statistics of the benchmarks are presented in [Table 1](#S3.T1 "In
    ∙ Problem Definition ‣ 3 Problem Formulation ‣ Do LLMs Work on Charts? Designing
    Few-Shot Prompts for Chart Question Answering and Summarization").
  prefs: []
  type: TYPE_NORMAL
- en: 4 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Fig. 1](#S1.F1 "In 1 Introduction ‣ Do LLMs Work on Charts? Designing Few-Shot
    Prompts for Chart Question Answering and Summarization") shows our prompting framework
    PromptChart, consisting of three modules: *(i) Prompt Constructor (PC)*; *(ii)
    Visual Data Table Generator (VDTG)*; *(iii) InstructGPT* Ouyang et al. ([2022](#bib.bib38)).
    In the case of FCQA, the chart image $C$ are input to the PC module to construct
    the prompt without going through the VDTG module. We present the details of each
    module one by one below.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Prompt Constructor (PC)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For each task, the prompt constructor module constructs the prompts to query
    the LLM. Liu et al. ([2022a](#bib.bib27)) only use a one-shot prompt without conducting
    a detailed task-prompt analysis. We argue that task-prompt analysis is critical
    because it helps to identify the necessary demonstrations for each prompt to maximize
    the LLM’s performance. To achieve this, we thoroughly analyze the task and propose
    a set of guidelines for prompt construction.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Factoid Chart Question Answering (FCQA) Inspired by Chain-of-Thought Wei et al.
    ([2022b](#bib.bib48)), we propose *chain of chart reasoning (CCR)*, a prompting
    strategy for FCQA. We categorize the task in FCQA into $9$ subcategories outlined
    in [Table 2](#S4.T2 "In ∙ ‣ 4.1 Prompt Constructor (PC) ‣ 4 Methodology ‣ Do LLMs
    Work on Charts? Designing Few-Shot Prompts for Chart Question Answering and Summarization")
    and design a CCR prompt format specifically for each. Our CCR formats satisfy
    two main properties. Firstly, the formats describe clearly the reasoning process
    step by step, leading to the final answer for each query. Secondly, the elements
    in the CCR (operands, operators, visual attributes) need to be well-specified
    with clear relationships. Here, visual attributes include colors (e.g., ‘red’,
    ‘green’) and positions (e.g., ‘left’, ‘right’) of marks (e.g., bars and lines).
    We follow these guidelines to carefully handcraft our prompt consisting of 6 demonstrations
    which are samples from ChartQA Masry et al. ([2022](#bib.bib34)). We show an example
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Question: Is the average value of Andean Latin America and Cambodia more than
    the value of Thailand?'
  prefs: []
  type: TYPE_NORMAL
- en: CCR:*"The value of Andean Latin America is 1.47 and the value of Cambodia is
    0.77. So the average value of Andean Latin America and Cambodia is ( 1.47 + 0.77
    ) / 2 = 1.12. The value of Thailand is 0.39. Since 1.12 > 0.39, the average value
    of Andean Latin America and Cambodia is more than the value of Thailand. The answer
    is Yes"*
  prefs: []
  type: TYPE_NORMAL
- en: We present all $6$ demonstrations in [Table 6](#A1.T6 "In A.3 FCQA Prompt ‣
    Appendix A Appendices ‣ Do LLMs Work on Charts? Designing Few-Shot Prompts for
    Chart Question Answering and Summarization"), and discuss their details in [Section 6.1](#S6.SS1
    "6.1 Demonstration Selection for FCQA ‣ 6 Discussion ‣ Do LLMs Work on Charts?
    Designing Few-Shot Prompts for Chart Question Answering and Summarization").
  prefs: []
  type: TYPE_NORMAL
- en: '| Tasks | Specifics - Supportive Case | Chain of Chart Reasoning (CCR) Prompt
    Format |'
  prefs: []
  type: TYPE_TB
- en: '|  | Visual Retrieval (Color & Position) - Case 1 | *No demonstration needed*
    |'
  prefs: []
  type: TYPE_TB
- en: '| Retrieval | Numerical Retrieval - Case 2 | *No demonstration needed* |'
  prefs: []
  type: TYPE_TB
- en: '|  | Compositional Retrieval - Case 3 | *No demonstration needed* |'
  prefs: []
  type: TYPE_TB
- en: '|  | Complex Retrieval - Case 9 | operands with visual attributes, operands
    without visual attributes, reasoning, result. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Add & Subtraction - Case 4 | operands without visual attributes, operators,
    reasoning, result. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Division & Multiplication - Case 5 | operands without visual attributes,
    operators, reasoning, result. |'
  prefs: []
  type: TYPE_TB
- en: '| Reasoning | Visual Reasoning - Case 6 | operands with visual attributes,
    operators, reasoning, result. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Compositional Reasoning - Case 7 | operands with visual attributes, operands
    without visual attributes, operators, reasoning, result. |'
  prefs: []
  type: TYPE_TB
- en: '| Boolean | All types of reasoning - Case 8 | operands with visual attributes,
    operands without visual attributes, operators, reasoning, result. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Task types and prompt format for factoid chart question answering.
    Supportive cases are illustrated in [Fig. 2](#S6.F2 "In 6.1 Demonstration Selection
    for FCQA ‣ 6 Discussion ‣ Do LLMs Work on Charts? Designing Few-Shot Prompts for
    Chart Question Answering and Summarization"). Each element in the CCR is colored
    uniquely.'
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Long-form Chart Question Answering (LCQA) Similar to FCQA, our goal is to build
    a prompt consisting of demonstrations that represent most of the available long-form
    chart QA pairs. We adhere to the following three principles while designing the
    prompt for the LCQA task: (i) Descriptive answer to each question in the prompt
    should be focused and informative enough, i.e., should not be too long and beyond
    the scope of the question or too short and unable to capture the full extent of
    the answer. (ii) the prompt should cover the different QA pair types. (iii) If
    the LLM is inferred to generate an answer for a question that is in the prompt,
    that generated answer must be as close as the answer given in the prompt. This
    property ensures that the model does not hallucinate when answering the questions
    that it is instructed from the demonstrations, leading towards more factually-correct
    generations.'
  prefs: []
  type: TYPE_NORMAL
- en: We carefully select our prompt having a total of 6 demonstrations from OpenCQA
    Kantharaj et al. ([2022a](#bib.bib21)). Among them, 2 questions are *Describe
    and Summary*, i.e., questions asking to describe or summarize the chart based
    on statistical properties such as describing data distribution, outliers, and
    trends, 2 are *Comparative* i.e., questions asking to compare the specified items
    in the chart, and 2 are *Discover*, questions asking to derive key insights or
    findings from the chart. All the demonstrations are shown in [Table 7](#A1.T7
    "In A.4 LCQA Prompt ‣ Appendix A Appendices ‣ Do LLMs Work on Charts? Designing
    Few-Shot Prompts for Chart Question Answering and Summarization") with details
    in [Section 6.2](#S6.SS2 "6.2 Best Number of Demonstrations for LCQA and CS Task
    ‣ 6 Discussion ‣ Do LLMs Work on Charts? Designing Few-Shot Prompts for Chart
    Question Answering and Summarization").
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Chart Summarization (CS) Except design principle (i), the other two principles
    of LCQA are applicable to chart summarization. Specifically, the prompt should
    cover different summarization types (e.g., comparative, descriptive, reasoning)
    and the LLM should not hallucinate on demonstrations. We select 3 demonstrations
    from the Pew dataset and 3 demonstrations for the Statista dataset of Chart-to-Text
    Kantharaj et al. ([2022b](#bib.bib22)). [Table 8](#A1.T8 "In A.5 Chart Summarization
    Prompt ‣ Appendix A Appendices ‣ Do LLMs Work on Charts? Designing Few-Shot Prompts
    for Chart Question Answering and Summarization") presents all the demonstrations,
    which contains examples of three summary types: *Perceptual and Cognitive* (e.g.,
    cause and effect, trends), *Statistical and Comparative* (e.g., min, max, higher,
    smaller), and *Visual Encoding* (relating to visual information such as colors
    and positions).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Visual Data Table Generator (VDTG)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Prior work Liu et al. ([2022a](#bib.bib27)) only considers the data table without
    any visual information from the chart, which makes the task of chart QA lose its
    visual nature. To study how the visual information affects the model performance
    in few-shot prompting, we introduce the VDTG module to construct *visual data
    table*, a representation integrating the usual data table with visual attributes
    of the marks ([Fig. 4](#A1.F4 "In A.6 Visual Data Table Generator (VDTG) ‣ Appendix
    A Appendices ‣ Do LLMs Work on Charts? Designing Few-Shot Prompts for Chart Question
    Answering and Summarization")). Given a chart image, the VDTG module is trained
    to generate a visual data table with two properties: (i) it has the associated
    colors (in natural language) of the rows/columns’ labels; (ii) the rows and columns
    of the chart are sorted in their appearing positions in the chart.'
  prefs: []
  type: TYPE_NORMAL
- en: We construct the training dataset for VDTG module from the *full version* of
    ChartQA at Github¹¹1https://github.com/vis-nlp/ChartQA. To create the ground-truth
    visual data table for each chart image, we first create *sorted data table* by
    collecting all the texts and coordinators of the bounding boxes of the chart and
    sorting them by the x-axis values and y-axis values. We also collect all the colors
    of the charts’ data marks (e.g., bars and lines) by parsing their original SVG
    files. Since those colors are in hexadecimal format, we convert them into natural
    language colors by mapping them with the name of its closest color in webcolors²²2https://pypi.org/project/webcolors/
    using a KDTree Bentley ([1975](#bib.bib5)). Still, we have noticed some hexadecimal
    codes are not present in the webcolors set, so we decided to manually check 300
    random visual data tables and added 80 new mappings {hex, color name} to the set.
    Moreover, we remove the charts whose bounding boxes information is not fully provided.
    Overall, we obtain a total of 18,317 training samples and 1,056 validation samples.
    We fine-tune UniChart Masry et al. ([2023](#bib.bib33)) on our constructed dataset,
    a SoTA Chart-to-Table model, to generate the visual data tables. It is worth noting
    that this module is chart-type agnostic. Naturally, since it is trained on ChartQA,
    it supports the available chart types in ChartQA, including a variety of bar charts,
    line charts, and pie charts. The details of our training and inference for this
    module are in [Section A.2](#A1.SS2 "A.2 VDTG Training Details ‣ Appendix A Appendices
    ‣ Do LLMs Work on Charts? Designing Few-Shot Prompts for Chart Question Answering
    and Summarization").
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 InstructGPT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We use the frozen InstructGPT Ouyang et al. ([2022](#bib.bib38)) as our LLM,
    which is based on GPT-3 Brown et al. ([2020](#bib.bib6)) and has been fine-tuned
    on natural human written instructions. Note that other LLMs such as ChatGPT can
    also be directly used.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1 QA and Summarization Experiments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '$\bullet$ streams of baselines. The *first stream* consists of *Models with
    fine-tuning strategies*, which can be further categorized into two subcategories:
    (a) models that take the chart image as input such as Pix2Struct Lee et al. ([2022](#bib.bib25))
    - the state-of-the-art visually-situated language model, MatCha Liu et al. ([2022b](#bib.bib28)),
    and UniChart Masry et al. ([2023](#bib.bib33)) - the state-of-the-art chart pretraining
    model; and (b) models that take the data table as input such as VisionTapas Masry
    et al. ([2022](#bib.bib34)) - the original state-of-the-art vision-language model
    on ChartQA, VL-T5 Cho et al. ([2021](#bib.bib8)) - the state-of-the-art model
    on OpenCQA, T5 Raffel et al. ([2020](#bib.bib43)) a competitive text generation
    model on the chart summarization task as reported by Kantharaj et al. ([2022b](#bib.bib22)).
    The *second stream* consists of *Large language models with data tables provided*:
    FlanT5-xxl Chung et al. ([2022](#bib.bib11)) - an instructions-fine-tuned large
    language model achieving impressive few-shot results on many NLP tasks, PAL Gao
    et al. ([2022](#bib.bib16)) - the state-of-the-art few-shot code-based reasoning
    model on the GSM8K benchmark Cobbe et al. ([2021](#bib.bib12)). The *third stream*
    comprises *Large language models without data tables provided*: DePlot + FlanPaLM
    Liu et al. ([2022a](#bib.bib27)) - the state-of-the-art factoid chart question
    answering model.'
  prefs: []
  type: TYPE_NORMAL
- en: For the FCQA task, we select Pix2Struct, MatCha, DePlot, PAL, FlanT5-xxl as
    the five baselines. We adopt FlanT5-xxl and VL-T5 as the two baselines for the
    LCQA task. For the CS task, we compare our model with FlanT5-xxl and T5.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Inference InstructGPT model is called for the inference through OpenAI API being
    *text-davinci-003* in April 2023\. We utilize Nucleus Sampling Holtzman et al.
    ([2020](#bib.bib17)) as our decoding strategy with a temperature value of 0.7,
    a max_tokens value of 256, and a top_p value of 0.9 as our decoding hyper-parameters.
    The frequency_penalty and presence_penalty are set to 0.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Automatic Evaluation & Human Preferences For text generation tasks, prior works
    Belz and Reiter ([2006](#bib.bib4)); Tan et al. ([2015](#bib.bib46)); Liu et al.
    ([2023b](#bib.bib31)) discuss that n-gram reference-based evaluation metrics for
    text generation tasks such as BLEU Papineni et al. ([2002](#bib.bib39)) and ROUGE
    Lin ([2004](#bib.bib26)) may have a relatively low correlation with human judgments
    and generated texts with very high scores can have a poor quality Smith et al.
    ([2016](#bib.bib44)); Kann et al. ([2018](#bib.bib20)); Mathur et al. ([2020](#bib.bib35))
    . Therefore, we focus on evaluating the factual correctness of the models for
    LCQA and CS tasks. In particular, we use *QAFactEval* Fabbri et al. ([2022](#bib.bib15))
    (by averaging all its criteria), a QA-based metric that computes a factual score
    based on the ability of a QA model to answer questions generated from the input,
    given the generated text. We further follow Ouyang et al. ([2022](#bib.bib38))
    to conduct *human preferences* to evaluate the performance of models. Specifically,
    for each dataset, we randomly select $300$ samples. We hire three annotators who
    are English native speakers and collect the generated outputs of these samples
    from the best-performing baseline (which is not a variant of our model) and PromptChart.
    We then ask the annotators to rate which one they prefer, or it is a tier. The
    annotators’ agreements are measured by Kripp.’s alpha Krippendorff ([2011](#bib.bib24)).
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Zero- and Few-shot Settings
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For zero-shot, we follow Kojima et al. ([2022](#bib.bib23)) to prompt the InstructGPT
    model to generate the textual outputs step by step. Specifically, we use "Answer
    the following question step by step." as the FCQA instruction and "Answer the
    following question step by step by a single paragraph." as the LCQA instruction.
    For the CS task, the instruction is "Summarize the trends in the chart step by
    step and write the summary.". The full sets of few-shot demonstrations for FCQA,
    LCQA, CS tasks are provided in [Table 6](#A1.T6 "In A.3 FCQA Prompt ‣ Appendix
    A Appendices ‣ Do LLMs Work on Charts? Designing Few-Shot Prompts for Chart Question
    Answering and Summarization"), [Table 7](#A1.T7 "In A.4 LCQA Prompt ‣ Appendix
    A Appendices ‣ Do LLMs Work on Charts? Designing Few-Shot Prompts for Chart Question
    Answering and Summarization"), [Table 8](#A1.T8 "In A.5 Chart Summarization Prompt
    ‣ Appendix A Appendices ‣ Do LLMs Work on Charts? Designing Few-Shot Prompts for
    Chart Question Answering and Summarization") respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Gold table? | Mode | ChartQA |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | Aug. | Human | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| T5 | Yes | fine-tuned | 56.96 | 25.12 | 41.04 |'
  prefs: []
  type: TYPE_TB
- en: '| VLT5 | Yes | fine-tuned | 56.88 | 26.24 | 41.56 |'
  prefs: []
  type: TYPE_TB
- en: '| VisionTapas | Yes | fine-tuned | 61.44 | 29.60 | 45.52 |'
  prefs: []
  type: TYPE_TB
- en: '| Pix2Struct | No | fine-tuned | 81.60 | 30.5 | 56.0 |'
  prefs: []
  type: TYPE_TB
- en: '| MatCha | No | fine-tuned | 90.2 | 38.02 | 64.2 |'
  prefs: []
  type: TYPE_TB
- en: '| UniChart | No | fine-tuned | 88.56 | 43.92 | 66.24 |'
  prefs: []
  type: TYPE_TB
- en: '| FlanT5-xxl | Yes | zero-shot | 63.84 | 27.84 | 45.84 |'
  prefs: []
  type: TYPE_TB
- en: '| PAL | Yes | zero-shot | 80.88 | 42.77 | 61.83 |'
  prefs: []
  type: TYPE_TB
- en: '| InstructGPT | Yes | zero-shot | 62.24 | 40.96 | 51.60 |'
  prefs: []
  type: TYPE_TB
- en: '| DePlot + Inst. + CCR | No | zero-shot | 78.64 | 45.78 | 62.21 |'
  prefs: []
  type: TYPE_TB
- en: '| UniChart + Inst. + CCR | No | zero-shot | 79.76 | 46.67 | 63.28 |'
  prefs: []
  type: TYPE_TB
- en: '| PromptChart | No | zero-shot | 79.44 | 47.11 | 63.1 |'
  prefs: []
  type: TYPE_TB
- en: '| DePlot + GPT-3 + CoT | No | one-shot | 37.3 | 36.5 | 36.9 |'
  prefs: []
  type: TYPE_TB
- en: '| DePlot + FlanPaLM + CoT | No | one-shot | 76.7 | 57.8 | 67.3 |'
  prefs: []
  type: TYPE_TB
- en: '| FlanT5-xxl | Yes | few-shot | 64.56 | 28.56 | 46.56 |'
  prefs: []
  type: TYPE_TB
- en: '| PAL | Yes | few-shot | 0.00 | 0.16 | 0.08 |'
  prefs: []
  type: TYPE_TB
- en: '| InstructGPT | Yes | few-shot | 75.12 | 49.52 | 62.32 |'
  prefs: []
  type: TYPE_TB
- en: '| DePlot + Inst. + CCR | No | few-shot | 80.28 | 60.49 | 70.39 |'
  prefs: []
  type: TYPE_TB
- en: '| UniChart + Inst. + CCR | No | few-shot | 81.68 | 62.24 | 71.96 |'
  prefs: []
  type: TYPE_TB
- en: '| PromptChart | No | few-shot | 81.44 | 63.2 | 72.32 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Factoid Chart QA experimental results. PromptChart is equivalent to
    InstructGPT + CCR + Visual Information. All the metrics are measured in percentage.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Main Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: $\bullet$
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: VDTG Results To evaluate our visual data table generation model, we follow recent
    relevant works in the Chart-to-Table domain and evaluate our model using the Relative
    Number Set Similarity (RNSS) Masry et al. ([2022](#bib.bib34)) and Relative Mapping
    Similarity (RMS) Liu et al. ([2022a](#bib.bib27)) metrics. Our model exhibits
    an impressive RNSS of 94.29% and RMS of 89.55%, highlighting its strong capability
    to extract data from images. This successful visual data table extraction process
    has significantly contributed to the overall performance of PromptChart.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: FCQA, LCQA, CS Results Our main experimental results for FCQA task on ChartQA
    are outlined in [Table 3](#S5.T3 "In ∙ Zero- and Few-shot Settings ‣ 5.1 QA and
    Summarization Experiments ‣ 5 Experiments ‣ Do LLMs Work on Charts? Designing
    Few-Shot Prompts for Chart Question Answering and Summarization"). We derive three
    main observations. Firstly, compared to zero-shot, one-shot, and fine-tuned baselines,
    PromptChart (few-shot) outperforms them by large margins, especially on the *human*
    test set. These improvements verify the effectiveness of our proposed FCQA prompt.
    Secondly, our experiments on *augmented* test set reveal that the PromptChart
    gains minor improvements when conditioning on the few-shot prompt instead of the
    instruction only (i.e., the zero-shot setting). We attribute this to the nature
    of the questions from *augmented* test set since they are mostly retrieval and
    not reasoning questions, which makes them less challenging for LLMs such as InstructGPT.
    Thirdly, the results of PromptChart and its variant *UniChart + InstructGPT +
    CCR* indicate that although the visual data tables slightly worsen the performance
    of the model on visually-irrelevant questions, they boost the performance of the
    models on visually-related questions, leading to overall improvements. We further
    provide additional analysis for the visual data table representation in [Section A.8](#A1.SS8
    "A.8 Analysis of Visual Data Table Representation ‣ Appendix A Appendices ‣ Do
    LLMs Work on Charts? Designing Few-Shot Prompts for Chart Question Answering and
    Summarization"). Exemplary cases are in [Table 15](#A1.T15 "In A.10 Error Analysis
    Case Studies ‣ Appendix A Appendices ‣ Do LLMs Work on Charts? Designing Few-Shot
    Prompts for Chart Question Answering and Summarization").
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Mode | OpenCQA | Chart-to-Text |'
  prefs: []
  type: TYPE_TB
- en: '|  | QAFactEval | HumP | QAFactEval-P | HumP-P | QAFactEval-S | HumP-S |'
  prefs: []
  type: TYPE_TB
- en: '| T5 | fine-tuned | 21.39 | - | 18.57 | - | 53.69 | 2.34 |'
  prefs: []
  type: TYPE_TB
- en: '| VLT5 | fine-tuned | 22.65 | 10.34 | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| MatCha | fine-tuned | - | - | 13.03 | - | 35.50 | - |'
  prefs: []
  type: TYPE_TB
- en: '| FlanT5-xxl | zero-shot | 13.97 | - | 19.36 | 4 | 23.98 | - |'
  prefs: []
  type: TYPE_TB
- en: '| InstructGPT | zero-shot | 23.16 | - | 18.13 | - | 24.72 | - |'
  prefs: []
  type: TYPE_TB
- en: '| PromptChart | zero-shot | 25.85 | - | 18.44 | - | 25.31 | - |'
  prefs: []
  type: TYPE_TB
- en: '| FlanT5-xxl | few-shot | 5.61 | - | 18.29 | - | 34.05 | - |'
  prefs: []
  type: TYPE_TB
- en: '| InstructGPT | few-shot | 24.23 | - | 29.17 | - | 38.02 | - |'
  prefs: []
  type: TYPE_TB
- en: '| PromptChart | few-shot | 27.19 | 81.67 | 31.17 | 94.34 | 49.88 | 94.23 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Long-form chart question answering and chart summarization experimental
    results. Metrics are measured in percentage.'
  prefs: []
  type: TYPE_NORMAL
- en: Our evaluations in [Table 4](#S5.T4 "In ∙ ‣ 5.2 Main Results ‣ 5 Experiments
    ‣ Do LLMs Work on Charts? Designing Few-Shot Prompts for Chart Question Answering
    and Summarization") for LCQA and CS tasks on OpenCQA and Chart-to-Text benchmarks
    illustrate a number of noteworthy findings. Firstly, with the carefully-designed
    prompts that we propose, PromptChart significantly improves the factuality scores
    on OpenCQA and Chart-to-Text Pew datasets, surpassing the performance of all fine-tuned
    baselines. Secondly, the model also exhibits superior human preferences in comparison
    to these fine-tuned methods. Finally, we observe that PromptChart generally performs
    better in the few-shot setting compared to the zero-shot setting. This highlights
    the importance of good demonstrations in determining its performance. Our annotators
    achieve strong agreements with an average Kripp.’s alpha values of 0.86 (0.79
    for OpenCQA, 0.91 for Chart-to-Text-Pew, and 0.88 for Chart-to-Text-Statista).
  prefs: []
  type: TYPE_NORMAL
- en: 6 Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 6.1 Demonstration Selection for FCQA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/937c1416cfe8870fe2a9017f6db6283e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Supportive cases for our ChartQA prompt construction. Green answers
    are correct answers, and red ones are errors.'
  prefs: []
  type: TYPE_NORMAL
- en: We discuss the necessity of each demonstration in our proposed six-shot prompt
    for FCQA task. [Fig. 2](#S6.F2 "In 6.1 Demonstration Selection for FCQA ‣ 6 Discussion
    ‣ Do LLMs Work on Charts? Designing Few-Shot Prompts for Chart Question Answering
    and Summarization") presents our supportive cases and the InstructGPT’s answers
    in the *zero-shot* setting. By examining these cases, we derive several observations.
    Firstly, InstructGPT model can do *Visual Retrieval*, *Numerical Retrieval*, and
    *Compositional Retrieval* without any demonstration (i.e., in zero-shot setting),
    which are proven by Case 1, 2, 3 respectively. Secondly, without any demonstration,
    the model seems to fluctuate in getting correct answers for arithmetic reasoning
    types including *Add & Subtraction* and *Division* (Case 4, 5, 6, 7). Thirdly,
    for more complex reasoning types such as *Compositional Reasoning*, the model
    performs poorly (Case 9) even in the reasoning steps (Case 8). Based on these
    observations, we propose the prompt consisting of $6$% (56.95% in the human set,
    and 80.88% in the augmented set), which is far lower from our proposed prompt,
    suggesting that our prompt is generalizable to other variations of questions within
    the benchmark. Note that, for a new benchmark, our current selection of demonstrations
    may not be sufficient, however, our methodology of selecting demonstrations can
    be adapted to improve generalizability.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Best Number of Demonstrations for LCQA and CS Task
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3269b529a7d02f26fa21c34925d5baf4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Performance of our PromptChart with different numbers of demonstrations
    in the prompt on 300 random samples of each benchmark.'
  prefs: []
  type: TYPE_NORMAL
- en: We conduct experiments to select the best number of demonstrations in the prompts
    fed to the model for the tasks of LCQA and CS. Specifically, we select a subset
    containing 300 random samples from the validation splits of Chart-to-Text (Pew
    dataset and Statista dataset) and OpenCQA as our mini-test sets. We then compare
    the performance of our model at different numbers of carefully-selected demonstrations
    (1, 3, 6, 9) and report the corresponding QAFactEval scores Fabbri et al. ([2022](#bib.bib15)).
    We select the demonstrations thoroughly, following the proposed properties in
    [Section 4](#S4 "4 Methodology ‣ Do LLMs Work on Charts? Designing Few-Shot Prompts
    for Chart Question Answering and Summarization"). After obtaining the best number
    of demonstrations on each mini-test set, we randomly select the same number of
    demonstrations to test the effectiveness of our proposed properties. The results
    are presented in [Fig. 3](#S6.F3 "In 6.2 Best Number of Demonstrations for LCQA
    and CS Task ‣ 6 Discussion ‣ Do LLMs Work on Charts? Designing Few-Shot Prompts
    for Chart Question Answering and Summarization"). We derive two main observations.Firstly,
    as we add more demonstrations to the prompt, the model may tend to generate more
    factually incorrect texts compared to using the previous number of demonstrations.
    Secondly, our careful sets of demonstrations boost the performance of the model
    significantly compared to random selections, which illustrates the effectiveness
    of our proposed properties.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Dive Deeper into the Proposed Properties
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Model | OpenCQA | Pew | Statista |'
  prefs: []
  type: TYPE_TB
- en: '| LCQA w/o *(ii)* | 25.87 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| LCQA w/o *(iii)* | 27.01 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| PromptChart | 27.19 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| CS w/o *(ii)* | - | 29.11 | 47.61 |'
  prefs: []
  type: TYPE_TB
- en: '| CS w/o *(iii)* | - | 30.79 | 48.16 |'
  prefs: []
  type: TYPE_TB
- en: '| PromptChart | - | 31.17 | 49.88 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Effects of proposed properties. We select the same number of demonstrations
    with our proposed number for each task as in [Section 4](#S4 "4 Methodology ‣
    Do LLMs Work on Charts? Designing Few-Shot Prompts for Chart Question Answering
    and Summarization").'
  prefs: []
  type: TYPE_NORMAL
- en: 'To verify the necessity of our proposed properties for guiding the prompt construction
    in [Section 4](#S4 "4 Methodology ‣ Do LLMs Work on Charts? Designing Few-Shot
    Prompts for Chart Question Answering and Summarization"), we conduct experiments
    by removing the properties one by one, resulting in $4$ experiments. Our illustrative
    cases are presented in [Section A.9](#A1.SS9 "A.9 Illustrative Cases for Proposed
    Properties of LCQA and CS Prompts ‣ Appendix A Appendices ‣ Do LLMs Work on Charts?
    Designing Few-Shot Prompts for Chart Question Answering and Summarization"):'
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ LCQA - w/o prop. *(ii)* Our prompt for LCQA task only contains 6 *Describe
    and Summary* question-answer pairs. This experiment is to show that without *Comparative*
    and *Discoverer* QA pairs in the prompt, the model achieved lower performance.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ other samples in OpenCQA dataset as demonstrations such that the model
    hallucinates on demonstrations, showing that it is better to ensure this property.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ *Statistical and Comparative* summaries. This experiment is to verify
    that without any *Perceptual and Cognitive* summary in the demonstrations, the
    model might output numbers or texts that do not exist in the input.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ other summaries in the dataset as demonstrations such that there exists
    at least one of them which the model hallucinates on, showing that it is better
    to ensure property *(iii)*.
  prefs: []
  type: TYPE_NORMAL
- en: Our experimental results are outlined in [Table 5](#S6.T5 "In 6.3 Dive Deeper
    into the Proposed Properties ‣ 6 Discussion ‣ Do LLMs Work on Charts? Designing
    Few-Shot Prompts for Chart Question Answering and Summarization"). We observe
    that removing any proposed property results in decreased performance of the model,
    which enforces our proposed properties.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Error Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Despite achieving promising performance, our proposed framework still falls
    into several categories of errors. We discuss them below and all the case studies
    are introduced in [Table 16](#A1.T16 "In A.10 Error Analysis Case Studies ‣ Appendix
    A Appendices ‣ Do LLMs Work on Charts? Designing Few-Shot Prompts for Chart Question
    Answering and Summarization").
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Factoid Chart Question Answering To understand the challenges of FCQA task
    and promising future directions, we manually investigate 100 FCQA questions that
    the model failed to output the correct answers and realize the following issues:'
  prefs: []
  type: TYPE_NORMAL
- en: (1) Logical and mathematical reasoning is a common error that LLMs such as InstructGPT
    often make Gao et al. ([2022](#bib.bib16)). Case 1 provides an illustration of
    this phenomenon, where the model correctly identifies the required operands and
    operators, but still produces incorrect computational output. The model also frequently
    makes logical errors, as demonstrated in Case 2, where despite correctly calculating
    the average percentage of three bars as 6.75%, which is greater than 6%, the model
    fails to conclude the correct answer.
  prefs: []
  type: TYPE_NORMAL
- en: (2) Visual data table generation challenge Although directly fine-tuning UniChart
    Masry et al. ([2023](#bib.bib33)) to generate visual data tables with correct
    positions gives us some minor improvements, we observe that UniChart sometimes
    generates wrong names of the colors. [Table 9](#A1.T9 "In A.6 Visual Data Table
    Generator (VDTG) ‣ Appendix A Appendices ‣ Do LLMs Work on Charts? Designing Few-Shot
    Prompts for Chart Question Answering and Summarization") illustrates some of such
    cases. Given a huge room for future research as discussed in [Section A.8](#A1.SS8
    "A.8 Analysis of Visual Data Table Representation ‣ Appendix A Appendices ‣ Do
    LLMs Work on Charts? Designing Few-Shot Prompts for Chart Question Answering and
    Summarization"), future works can focus on improving this module.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Long-form Chart Question Answering & Chart Summarization Our human evaluation
    on PromptChart reveal several key challanges:'
  prefs: []
  type: TYPE_NORMAL
- en: (1) Hallucinations & factual errors are observed from the generated answers
    of our PromptChart. These errors are well-known LLMs issues, and have been studied
    extensively by prior works Fabbri et al. ([2022](#bib.bib15)); Ji et al. ([2023](#bib.bib19)).
    For LCQA, one example of such errors is Case 5 in which the model generates unwanted
    numbers (highlighted in red color) that are not in the chat image. Similarly for
    CS, in Case 6, the red passage could not be found in the chart image and input
    information, but the model generates it confidently.
  prefs: []
  type: TYPE_NORMAL
- en: (2) Logical & mathematical reasoning errors also appear during generating long-form
    answers and summaries. Case 4 is an example of LCQA task, in which "decreased
    by 10 percentage points" should be from "88 % to 78 %" instead of "78 % to 88
    %" as generated. Additionally, Case 3 illustrates a CS example, in which the model,
    despite correctly retrieving 8269.54 kg in 2019, a number being much bigger than
    6348.43 in 2011, still concludes that 6348.43 in 2011 is the largest amount.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we take the first step to study few-shot prompting with large
    language models for chart question answering and summarization tasks. We propose
    PromptChart, a multimodal few-shot prompting framework with LLMs. Through meticulous
    task analysis, we have developed a comprehensive set of prompting guidelines that
    are specifically tailored to each downstream task. These guidelines aim to maximize
    the few-shot performance of large language models (LLMs). Additionally, we propose
    the *visual data table* representation that incorporates visual features into
    the prompt and the LLM. Experimental results show that PromptChart achieves significant
    improvements over prior one-shot and fine-tuned baselines, as evidenced by automatic
    metrics and human evaluation. In the future, we plan to explore dynamically selecting
    the demonstrations in the prompt to further optimize the performance of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One limitation of our work is the utilization of a fixed prompt for each task,
    which has the potential to restrict the model’s performance. To mitigate this
    issue, we made our best to explicitly set our proposed properties to cover the
    different challenges in the LCQA and CS benchmarks. Still, more investigation
    is needed to explore different prompt setups that might improve performance. In
    the factoid chart question answering task, our analysis of the challenges and
    incorporating them into our fixed prompt serves as an additional effort to address
    this issue. In addition, our manual human evaluations in [Section 6.4](#S6.SS4
    "6.4 Error Analysis ‣ 6 Discussion ‣ Do LLMs Work on Charts? Designing Few-Shot
    Prompts for Chart Question Answering and Summarization") reveal that InstructGPT
    often makes logical & mathematical reasoning errors, as well as hallucinations
    & factual errors during generating long-form texts. In the future, we will focus
    on improving the VDTG module to make it more robust to different types of charts,
    as well as improving the logical and mathematical reasoning capabilities of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Ethics Statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 7.1 Risk in Model Deployment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deploying a technology developed by a third party such as OpenAI carries ethical
    considerations that must be addressed to ensure fairness and mitigate potential
    harm. This involves choosing algorithms and performance metrics that promote fairness
    Corbett-Davies et al. ([2017](#bib.bib13)), as well as identifying and addressing
    any biases in the pre-trained model. In this work, we evaluate the performance
    of the models by a factual correctness metric Fabbri et al. ([2022](#bib.bib15)),
    which encourages the generated texts to be factually correct based on the input
    information and discourages hallucinations. However, it is important to acknowledge
    that although our method strives for factual correctness, it does not guarantee
    perfectly factual outputs and may be abused to spread misinformation among the
    public. In addition, through human evaluations, we observe that our proposed method
    does not generate any discriminatory or insulting responses. However, it is important
    to note that we cannot guarantee that our method will never produce harmful content.
    Vigilance is necessary to mitigate the risk of producing harmful outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Human Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: During the human evaluation experiments, we hired three annotators to score
    a total of 300 generated texts. To fairly compensate them, we paid them an hourly
    wage of $15, which is higher than the local statutory minimum wage. Additionally,
    to protect the privacy of the annotators, we kept all annotations anonymized.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This research was supported by the Natural Sciences & Engineering Research Council
    (NSERC) of Canada and Canada Foundation for Innovation (CFI). Do Xuan Long is
    supported by the A*STAR Computing and Information Science (ACIS) scholarship,
    A*STAR, Singapore.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Alayrac et al. (2022) Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine
    Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican,
    Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao
    Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew
    Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira,
    Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. 2022. [Flamingo: a visual
    language model for few-shot learning](https://openreview.net/forum?id=EbMuimAbPbs).
    In *Advances in Neural Information Processing Systems*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Balaji et al. (2018) Abhijit Balaji, Thuvaarakkesh Ramanathan, and Venkateshwarlu
    Sonathi. 2018. Chart-text: A fully automated chart image descriptor. *arXiv preprint
    arXiv:1812.10636*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Beltagy et al. (2022) Iz Beltagy, Arman Cohan, Robert Logan IV, Sewon Min,
    and Sameer Singh. 2022. [Zero- and few-shot NLP with pretrained language models](https://doi.org/10.18653/v1/2022.acl-tutorials.6).
    In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics: Tutorial Abstracts*, pages 32–37, Dublin, Ireland. Association for
    Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Belz and Reiter (2006) Anja Belz and Ehud Reiter. 2006. [Comparing automatic
    and human evaluation of NLG systems](https://aclanthology.org/E06-1040). In *11th
    Conference of the European Chapter of the Association for Computational Linguistics*,
    pages 313–320, Trento, Italy. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bentley (1975) Jon Louis Bentley. 1975. Multidimensional binary search trees
    used for associative searching. *Communications of the ACM*, 18(9):509–517.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cheng et al. (2023) Zhi-Qi Cheng, Qi Dai, Siyao Li, Jingdong Sun, Teruko Mitamura,
    and Alexander G Hauptmann. 2023. Chartreader: A unified framework for chart derendering
    and comprehension without heuristic rules. *arXiv preprint arXiv:2304.02173*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cho et al. (2021) Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. 2021. Unifying
    vision-and-language tasks via text generation. In *International Conference on
    Machine Learning*, pages 1931–1942\. PMLR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Choi et al. (2019) Jinho Choi, Sanghun Jung, Deok Gun Park, Jaegul Choo, and
    Niklas Elmqvist. 2019. Visualizing for the non-visual: Enabling the visually impaired
    to use visualization. In *Computer Graphics Forum*, volume 38, pages 249–260\.
    Wiley Online Library.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways.
    *arXiv preprint arXiv:2204.02311*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay,
    William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
    2022. Scaling instruction-finetuned language models. *arXiv preprint arXiv:2210.11416*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve
    math word problems. *arXiv preprint arXiv:2110.14168*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Corbett-Davies et al. (2017) Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad
    Goel, and Aziz Huq. 2017. Algorithmic decision making and the cost of fairness.
    In *Proceedings of the 23rd acm sigkdd international conference on knowledge discovery
    and data mining*, pages 797–806.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. [BERT: Pre-training of deep bidirectional transformers for language
    understanding](https://doi.org/10.18653/v1/N19-1423). In *Proceedings of the 2019
    Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pages
    4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fabbri et al. (2022) Alexander Richard Fabbri, Chien-Sheng Wu, Wenhao Liu,
    and Caiming Xiong. 2022. Qafacteval: Improved qa-based factual consistency evaluation
    for summarization. In *Proceedings of the 2022 Conference of the North American
    Chapter of the Association for Computational Linguistics: Human Language Technologies*,
    pages 2587–2601.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2022) Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu,
    Yiming Yang, Jamie Callan, and Graham Neubig. 2022. Pal: Program-aided language
    models. *arXiv preprint arXiv:2211.10435*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Holtzman et al. (2020) Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin
    Choi. 2020. [The curious case of neural text degeneration](https://openreview.net/forum?id=rygGQyrFvH).
    In *International Conference on Learning Representations*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hoque et al. (2022) Enamul Hoque, Parsa Kavehzadeh, and Ahmed Masry. 2022.
    Chart question answering: State of the art and future directions. In *Computer
    Graphics Forum*, volume 41, pages 555–572\. Wiley Online Library.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ji et al. (2023) Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan
    Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey
    of hallucination in natural language generation. *ACM Computing Surveys*, 55(12):1–38.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kann et al. (2018) Katharina Kann, Sascha Rothe, and Katja Filippova. 2018.
    [Sentence-level fluency evaluation: References help, but can be spared!](https://doi.org/10.18653/v1/K18-1031)
    In *Proceedings of the 22nd Conference on Computational Natural Language Learning*,
    pages 313–323, Brussels, Belgium. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kantharaj et al. (2022a) Shankar Kantharaj, Xuan Long Do, Rixie Tiffany Ko
    Leong, Jia Qing Tan, Enamul Hoque, and Shafiq Joty. 2022a. [Opencqa: Open-ended
    question answering with charts](https://doi.org/10.48550/ARXIV.2210.06628). *The
    2022 Conference on Empirical Methods in Natural Language Processing*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kantharaj et al. (2022b) Shankar Kantharaj, Rixie Tiffany Leong, Xiang Lin,
    Ahmed Masry, Megh Thakkar, Enamul Hoque, and Shafiq Joty. 2022b. [Chart-to-text:
    A large-scale benchmark for chart summarization](https://doi.org/10.18653/v1/2022.acl-long.277).
    In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 4005–4023, Dublin, Ireland. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. 2022. [Large language models are zero-shot reasoners](https://openreview.net/forum?id=e2TBb5y0yFf).
    In *Advances in Neural Information Processing Systems*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krippendorff (2011) Klaus Krippendorff. 2011. Computing krippendorff’s alpha-reliability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. (2022) Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu
    Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina
    Toutanova. 2022. Pix2struct: Screenshot parsing as pretraining for visual language
    understanding. *arXiv preprint arXiv:2210.03347*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin (2004) Chin-Yew Lin. 2004. [ROUGE: A package for automatic evaluation of
    summaries](https://aclanthology.org/W04-1013). In *Text Summarization Branches
    Out*, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2022a) Fangyu Liu, Julian Martin Eisenschlos, Francesco Piccinno,
    Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier,
    and Yasemin Altun. 2022a. Deplot: One-shot visual language reasoning by plot-to-table
    translation. *arXiv preprint arXiv:2212.10505*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2022b) Fangyu Liu, Francesco Piccinno, Syrine Krichene, Chenxi
    Pang, Kenton Lee, Mandar Joshi, Yasemin Altun, Nigel Collier, and Julian Martin
    Eisenschlos. 2022b. Matcha: Enhancing visual language pretraining with math reasoning
    and chart derendering. *arXiv preprint arXiv:2212.09662*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023a) Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki
    Hayashi, and Graham Neubig. 2023a. Pre-train, prompt, and predict: A systematic
    survey of prompting methods in natural language processing. *ACM Computing Surveys*,
    55(9):1–35.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2019) Xiaoyi Liu, Diego Klabjan, and Patrick NBless. 2019. Data
    extraction from charts via single deep neural network. *arXiv preprint arXiv:1906.11906*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023b) Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu,
    and Chenguang Zhu. 2023b. Gpteval: Nlg evaluation using gpt-4 with better human
    alignment. *arXiv preprint arXiv:2303.16634*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo et al. (2021) Junyu Luo, Zekun Li, Jinpeng Wang, and Chin-Yew Lin. 2021.
    Chartocr: Data extraction from charts images via a deep hybrid framework. In *Proceedings
    of the IEEE/CVF winter conference on applications of computer vision*, pages 1917–1925.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Masry et al. (2023) Ahmed Masry, Parsa Kavehzadeh, Xuan Long Do, Enamul Hoque,
    and Shafiq Joty. 2023. Unichart: A universal vision-language pretrained model
    for chart comprehension and reasoning. *arXiv preprint arXiv:2305.14761*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Masry et al. (2022) Ahmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul
    Hoque. 2022. [ChartQA: A benchmark for question answering about charts with visual
    and logical reasoning](https://doi.org/10.18653/v1/2022.findings-acl.177). In
    *Findings of the Association for Computational Linguistics: ACL 2022*, pages 2263–2279,
    Dublin, Ireland. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mathur et al. (2020) Nitika Mathur, Timothy Baldwin, and Trevor Cohn. 2020.
    [Tangled up in BLEU: Reevaluating the evaluation of automatic machine translation
    evaluation metrics](https://doi.org/10.18653/v1/2020.acl-main.448). In *Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics*,
    pages 4984–4997, Online. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Methani et al. (2020) Nitesh Methani, Pritha Ganguly, Mitesh M. Khapra, and
    Pratyush Kumar. 2020. Plotqa: Reasoning over scientific plots. In *Proceedings
    of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Najdenkoska et al. (2023) Ivona Najdenkoska, Xiantong Zhen, and Marcel Worring.
    2023. [Meta learning to bridge vision and language models for multimodal few-shot
    learning](https://openreview.net/forum?id=3oWo92cQyxL). In *The Eleventh International
    Conference on Learning Representations*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. (2022) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. 2022. Training language models to follow instructions with human feedback.
    *arXiv preprint arXiv:2203.02155*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In
    *Proceedings of the 40th annual meeting of the Association for Computational Linguistics*,
    pages 311–318.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Poco and Heer (2017) Jorge Poco and Jeffrey Heer. 2017. Reverse-engineering
    visualizations: Recovering visual encodings from chart images. In *Computer graphics
    forum*, volume 36, pages 353–363\. Wiley Online Library.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'del Puy Pérez-Echeverría et al. (2018) Mª del Puy Pérez-Echeverría, Yolanda
    Postigo, and Cristina Marín. 2018. [Understanding of graphs in social science
    undergraduate students: selection and interpretation of graphs](https://doi.org/10.1080/03323315.2018.1440248).
    *Irish Educational Studies*, 37(1):89–111.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2019) Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario
    Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. [Exploring
    the limits of transfer learning with a unified text-to-text transformer](http://jmlr.org/papers/v21/20-074.html).
    *Journal of Machine Learning Research*, 21(140):1–67.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Smith et al. (2016) Aaron Smith, Christian Hardmeier, and Joerg Tiedemann.
    2016. [Climbing mont BLEU: The strange world of reachable high-BLEU translations](https://aclanthology.org/W16-3414).
    In *Proceedings of the 19th Annual Conference of the European Association for
    Machine Translation*, pages 269–281.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2022) Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, and
    Xipeng Qiu. 2022. Black-box tuning for language-model-as-a-service. In *ICML*,
    volume 162 of *Proceedings of Machine Learning Research*, pages 20841–20855\.
    PMLR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tan et al. (2015) Liling Tan, Jon Dehdari, and Josef van Genabith. 2015. [An
    awkward disparity between BLEU / RIBES scores and human judgements in machine
    translation](https://aclanthology.org/W15-5009). In *Proceedings of the 2nd Workshop
    on Asian Translation (WAT2015)*, pages 74–81, Kyoto, Japan. Workshop on Asian
    Translation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022a) Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei
    Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2022a. Finetuned language
    models are zero-shot learners. In *International Conference on Learning Representations*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022b) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi,
    Quoc Le, and Denny Zhou. 2022b. Chain of thought prompting elicits reasoning in
    large language models. *arXiv preprint arXiv:2201.11903*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Whitaker and Jacobbe (2017) Douglas Whitaker and Tim Jacobbe. 2017. [Students’
    understanding of bar graphs and histograms: Results from the locus assessments](https://doi.org/10.1080/10691898.2017.1321974).
    *Journal of Statistics Education*, 25(2):90–102.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2022) Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu.
    2022. [Learning to prompt for vision-language models](https://openreview.net/forum?id=OgCcfc1m0TO).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Extended Related Work
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A.1.1 Chart Data Extraction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Information Extraction (IE) from charts is a challenging task due to variations
    in chart types and styles. Early works in this area use a combination of different
    approaches to creating a pipeline that extracts data from chart (Poco and Heer,
    [2017](#bib.bib40); Balaji et al., [2018](#bib.bib2)). These feature-based approaches
    are shown to not generalize well on various kinds of charts. Therefore, more recent
    works adopt deep learning models to extract data from charts e.g. Choi et al.
    ([2019](#bib.bib9)) use the idea of detecting bounding boxes for bar charts. Liu
    et al. ([2019](#bib.bib30)) propose a recurrent network for pie charts. These
    methods achieved significant improvements. However, they are specifically designed
    for one type of chart. To overcome this, Luo et al. ([2021](#bib.bib32)) proposes
    a hybrid framework that can generate data from different types of charts. ChartOCR
    (Luo et al., [2021](#bib.bib32)) achieves the SOTA results at the time, but similar
    to other pipeline-based methods, the error will propagate through the pipeline,
    hence can affect the performance drastically. As end-to-end approaches in other
    computer vision tasks gained attention, Liu et al. ([2022b](#bib.bib28)) propose
    an end-to-end model which was based on Pix2Struct (Lee et al., [2022](#bib.bib25))
    architecture and further tuned on chart derendering and math reasoning. Recently,
    Masry et al. ([2023](#bib.bib33)) propose *UniChart*, an end-to-end pretrained
    chart model which achieves state-of-the-art result on chart-to-table generation
    task. As the focus of this paper is on constructing prompts for the chart comprehension
    tasks, further exploration of end-to-end data extraction approaches is left for
    future works.
  prefs: []
  type: TYPE_NORMAL
- en: A.2 VDTG Training Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We fine-tune UniChart Masry et al. ([2023](#bib.bib33)) model for 50K steps
    with a batch size of 80\. We use a learning rate of 1e-4 and 500 warmup steps.
    We evaluate the model every 1K steps and choose the best checkpoint for inference.
  prefs: []
  type: TYPE_NORMAL
- en: A.3 FCQA Prompt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Table 6](#A1.T6 "In A.3 FCQA Prompt ‣ Appendix A Appendices ‣ Do LLMs Work
    on Charts? Designing Few-Shot Prompts for Chart Question Answering and Summarization")
    illustrates our full demonstrations for the task of factoid chart question answering.
    The samples are selected carefully from the validation split of ChartQA Masry
    et al. ([2022](#bib.bib34)), each of them covers one question type that the InstructGPT
    model fails to answer presented in [Fig. 2](#S6.F2 "In 6.1 Demonstration Selection
    for FCQA ‣ 6 Discussion ‣ Do LLMs Work on Charts? Designing Few-Shot Prompts for
    Chart Question Answering and Summarization").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Index | Question | CCR Format: operands with visual attributes, operands
    without visual attributes, operators, reasoning, result | Question Type |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | How many data points on the disapprove line are above 50? | The disapprove
    number 51 and 50 are greater than 50\. The answer is 2. | Complex Retrieval |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | How much value of ’Mostly bad news’ increased from Jan. 2011 to Sept.
    2011? | The value of ’Mostly bad news’ in Jan. 2011 is 24 and in Sept. 2011 is
    61. So the increase is 61 - 24 = 37. The answer is 37. | Add/Substract |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | What’s the ratio of Lean Republican segment and Republican segment? |
    The value of Lean Republican segment is 39 and the value of Republican segment
    is 53. The ratio is 39 / 53 = 0.7358. The answer is 0.7358. | Divide |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | What’s the average value of the first two blue bars in the chart? | The
    first two blue bars are 30-34 years old with 0.35 and 25-29 years old with 0.31.
    The average value is ( 0.35 + 0.31 ) / 2 = 0.33. The answer is 0.33. | Visual
    Reasoning |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | What’s the total sum of peak points of all three lines? | The peak point
    of red line is 80, the peak point of yellow line is 68, the peak point of green
    line is 7. The total sum of peak points is 80 + 68 + 7 = 155. The answer is 155.
    | Compositional Reasoning |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | Is the average value of Andean Latin America and Cambodia more than the
    value of Thailand? | The value of Andean Latin America is 1.47 and the value of
    Cambodia is 0.77. So the average value of Andean Latin America and Cambodia is
    ( 1.47 + 0.77 ) / 2 = 1.12. The value of Thailand is 0.39. Since 1.12 > 0.39,
    the average value of Andean Latin America and Cambodia is more than the value
    of Thailand. The answer is Yes. | Boolean |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Demonstrations for factoid chart question answering task. Samples
    are from the validation split of ChartQA Masry et al. ([2022](#bib.bib34)) dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: A.4 LCQA Prompt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We present all the demonstrations used for LCQA task in [Table 7](#A1.T7 "In
    A.4 LCQA Prompt ‣ Appendix A Appendices ‣ Do LLMs Work on Charts? Designing Few-Shot
    Prompts for Chart Question Answering and Summarization"). The 6 samples are selected
    carefully from the validation split of OpenCQA Kantharaj et al. ([2022a](#bib.bib21))
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '| Index | Input | Output | Question Type |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | What opinion do the public have about earth warming? Title: Is there
    solid evidence the earth is warming? Mixed evidencel Total U.S population Context:
    Is there solid evidence the earth is warming? <s> Mixed evidencel <s> Yes. because
    of <s> Yes. because of <s> Yes but don’t <s> No <s> Some evidencel <s> human activity
    <s> natura patterns <s> know cause <s> Don’t know <s> Total U.S population <s>
    47 % <s> 18% <s> 6% 21 % <s> 8% <s> Unaffliated <s> 58% <s> 11% <s> 6% 18% <s>
    7% <s> White mainline Protestants <s> 48% <s> 19% <s> 6% 19% <s> 8% <s> White
    non-Hispanic Catholics <s> 44% <s> 20% <s> 6% 22% <s> 8% <s> Black Protestants
    <s> 39% <s> 36% <s> 5% 15% <s> 5% <s> White evangelical Protestants <s> 17% <s>
    7% <s> 11% <s> 34% <s> 31% <s> The Pew Forum on Religion & Public Life <s> pewforum.
    ORG | The unaffiliated ( 58 % ) are the most likely among the religious groups
    studied to say there is solid evidence the earth is warming because of human activity
    . White evangelical Protestants are the most likely to say there is no solid evidence
    the earth is warming ( 31 % ) , and the least likely to believe that humans have
    contributed to heating up the planet ( 34 % ) . While only 39 % of black Protestants
    say global warming is a result of human activity , they are , however , the least
    likely of the religions studied to deny global warming is occurring ( 15 % ) .
    | Describe and Summary |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Among families with opt-out moms, how is the difference in education
    between husbands and wives? Title: About One-in-Ten Highly Educated Mothers ’Opt
    Out’ Among all educated moms ’opt-out’ moms Context: About One-in-Ten Highly Educated
    Mothers "Opt Out" <s> Among all highly <s> Among all <s> educated moms <s> "opt-out"
    moms <s> 9% <s> are "opt-out" <s> moms <s> 37% More education <s> than husband
    <s> 45 <s> Same <s> education <s> as hus band <s> 18 <s> Less education <s> than
    husband <s> "Highly educated mothers have at least a Master’s de gree, are ages
    18 69 and <s> are living with the ir own child(ren) working younger than 18\.
    family Among these moms, "opt-out" <s> mothers living are married with working
    hus band, annual family Among income above $7 5, .000 <s> and say they are not
    working in forder to care for family. a professional degree is <s> considered
    equivalent to a Ph.D for the purpose of the compa rison of spousal <s> educational
    attainment. <s> Source: Pew Research Center analysis of March Current Population
    Surveys <s> Integrated Public Use Microdata Series (IPUMS CPS), 2012 2013 <s>
    PEW RESEARCH CENTER | In 37 % of the cases , it is the stay - at - home wives
    who actually have a higher level of education . In 45 % of these families , the
    spouses have equal educational attainment , and in about 18 % of the cases , the
    husbands have more education than their wives . | Comparative |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | What was the trend among registered Hispanic voters about Trump’s work?
    Title: Most Hispanic voters disapprove of Trump, dissatisfied with nation’s direction
    Context: Most Hispanic voters disapprove of Trump, dissatisfied with nation’s
    direction <s> % who are country with the today way things are <s> % who job president
    of the way Trump is handling <s> going in the country today <s> his job as president
    <s> Disapprove <s> Approve <s> Not so <s> Not so <s> Very strongly strongly Very
    <s> Dissatisfied <s> Satisfied <s> NET <s> NET <s> strongly <s> strongly <s> All
    Hispanic RVs <s> 68 <s> 51 <s> 17 8 23 <s> 30 <s> 67 <s> 32 <s> Dem/Lean Dem <s>
    93 <s> 73 <s> 20 3 4 <s> 80 <s> 19 <s> 1211 17 <s> 75 <s> Rep/Lean Rep <s> 22
    <s> 58 <s> 45 <s> 54 <s> Note: Based on Hispanic registered voters. Share of respondents
    who didn’t offer an answer not shown. Figures may not add to NET due to <s> rounding.
    <s> Source: National Survey of Latinos conducted Dec. 3-23, 2019 <s> PEW RESEARCH
    CENTER | About two - thirds of Hispanic registered voters ( 68 % ) disapprove
    of the job Trump is doing as president , including 51 % who disapprove very strongly
    . The 30 % of Hispanic voters who approve of Trump includes 23 % who approve strongly
    | Discover |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Describe the trend in job availability to people over the past years.
    Title: Job Availability in Your Area 2001 2008 Context: Job Availability in Your
    Area <s> 2001 2008 <s> Difficult to find <s> Plenty of jobs <s> 66 <s> 60 <s>
    480 <s> 40 <s> 20 <s> Q <s> 2001 2002 2003 2004 2005 2006 2007 2008 | Overall
    , a majority ( 53 % ) says that jobs are difficult to find in their community
    while only about a third ( 34 % ) says there are plenty of jobs available . The
    percentage saying that jobs are difficult to find locally has increased modestly
    since February 2007 ( 48 % ) . In October 2003 , 66 % said that jobs were difficult
    to find , the highest percentage expressing that view since the start of the Bush
    administration . | Describe and Summary |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | How was the public’s attention on different news? Title: News Interest
    vs. News Coverage Context: News Interest vs. News Coverage <s> April 13-19 <s>
    Pirates <s> 16 <s> 34 <s> Economy 27 <s> 18 <s> C <s> Tea parties <s> 3 <s> 6
    <s> Obama abroad <s> 3 <s> Cuba policy <s> 2 <s> CIA memos <s> Interest: percent
    who named story as most closely followed <s> Coverage: percent of news coverage
    devoted to story | 34 % say they followed stories about the continued attempts
    by pirates to hijack ships more closely than any other story last week , while
    27 % say they followed stories about the U.S. economy most closely . 9 % say they
    followed the tea party protests more closely than any other story . | Comparative
    |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | Does China believe that Japan has sufficiently apologized for their actions
    during WW2? Title: Chinese unchanged in belief that Japan has yet to sufficiently
    apologize for actions during World War I % Chinese saying Japan has for its military
    actions during the 1930s and 1940S Context: Chinese unchanged in belief that Japan
    has yet to <s> sufficiently apologize for actions during World War I <s> % Chinese
    saying Japan has for its military actions during the 1930s and <s> 1940S <s> Apologized
    No apology <s> Not apologized <s> sufficiently <s> sufficiently necessary <s>
    2016 <s> 77% <s> 10% <s> 2% <s> 2013 <s> 78 <s> 2008 <s> 76 <s> 8 <s> 2006 <s>
    81 <s> Source: Spring 2016 Global Attitudes Survey Q84\. <s> PEW RESEARCH CENTER
    | Roughly three - quarters ( 77 % ) say Japan has not adequately expressed regret
    , and such Chinese sentiment is largely unchanged since 2006 . Only 10 % believe
    Tokyo has apologized enough . | Discover |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Demonstrations for long-form chart question answering task. Samples
    are from the validation split of the OpenCQA Kantharaj et al. ([2022a](#bib.bib21))
    dataset. The chart images are provided in [Table 10](#A1.T10 "In A.7 Consistency
    of Generated Output with Demonstrations ‣ Appendix A Appendices ‣ Do LLMs Work
    on Charts? Designing Few-Shot Prompts for Chart Question Answering and Summarization").'
  prefs: []
  type: TYPE_NORMAL
- en: A.5 Chart Summarization Prompt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Table 8](#A1.T8 "In A.5 Chart Summarization Prompt ‣ Appendix A Appendices
    ‣ Do LLMs Work on Charts? Designing Few-Shot Prompts for Chart Question Answering
    and Summarization") presents all the demonstrations we use for the task of chart
    summarization. All 6 demonstrations are from Chart-to-Text dataset Kantharaj et al.
    ([2022b](#bib.bib22)). The first three samples are taken from validation set of
    Pew dataset, whilst the last 3 from Statista dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Index | Input | Output | Summarization Type |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Seven-in-ten Americans said in 2019 that their personal info was less
    secure than five years earlier % ofU.S adults who said they felt as iftheir personal
    information was than it was five years ago <SEP> Seven-in-ten Americans said in
    2019 that their personal info was less secure than five years earlier % ofU.S
    adults who said they felt as iftheir personal information was than it was five
    years ago More About Less secure the same secure 6 24 70 Note: Those who did not
    give an answer are not shown. Source: Survey conducted June 3- 17, 2019\. "Americans
    and Privacy: Concerned, Confused and Feeling Lack of Control Over Their Personal
    Information PEW RESEARCH CENTER | Before the outbreak, Americans strongly believed
    their personal data was more vulnerable than in the past. In a June 2019 survey,
    70% of Americans said their personal information was less secure than it was five
    years earlier. Just 6% of Americans said they felt their information was more
    secure than in the past, while 24% said their personal information was about as
    secure as it was five years earlier. Experts have suggested that data security
    could be threatened for companies shifting to remote work – as well as ordinary
    people who are spending more time online – during the coronavirus outbreak. |
    Perceptual and Cognitive |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Figure by Race, Ethnicity and Citizenship, 2007 <SEP> Figure Composition
    of Sentenced Federal Offenders, by Race, Ethnicity and Citizenship, 2007 196 ofsentenced
    offenders) U.S citizen Non-U.S citizen 40 29 25 21 Hispanic White Black Notes:
    Percenta ges may not total due to rounding. The category "Other" is excluded Source:
    Pew Hispanic Center analysis of United States Sentencing Source: Commission Monitoring
    offederal Criminal Sentences data, 2007 | Sharp growth in illegal immigration
    and increased enforcement of immigration laws have dramatically altered the ethnic
    composition of offenders sentenced in federal courts. In 2007, Latinos accounted
    for 40% of all sentenced federal offenders‚Äîmore than triple their share (13%)
    of the total U.S. adult population. The share of all sentenced offenders who were
    Latino in 2007 was up from 24% in 1991, according to an analysis of data from
    the United States Sentencing Commission (USSC) by the Pew Hispanic Center, a project
    of the Pew Research Center. | Visual Encodding |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Half of voters, including a majority of Biden supporters, expect it will
    be difficult to vote in this year’s election % ofregistered voters wwo expect
    voting in the November elections to be <SEP> Half of voters, including a majority
    of Biden supporters, expect it will be difficult to vote in this year’s election
    % ofregistered voters wwo expect voting in the November elections to be Republican
    Support Democratic Support All voters Republican Support Democratic Support candidate
    candidate 90 Easy 85 83 64 50 40 10 Difficult 15 17 35 49 60 Oct Aug Oct Aug Oct
    Aug 18 -20 ’18 20 1’8 –20 House Presidential election election Notes: Based on
    registered voters Based on supporters of Republican fand Democratic House candidates
    lin 2018 Trump and Biden supporters in 2020 No answer responses snot shown. Source:
    Survey of U.S adults conducted July 27-Aug 2 2020 PEW RESEARCH CENTER | Far more
    voters expect voting to be difficult this year than in 2018, according to a survey
    of registered voters conducted July 27 to Aug. 2\. Around half of registered voters
    (49%) said they expect voting to be very or somewhat difficult this year, while
    the other half (50%) expect it to be very or somewhat easy. That represents a
    major change from October 2018, when just 15% of voters said they expected voting
    in that year‚Äôs midterms to be difficult and 85% said they expected it to be
    easy. | Statistical and Comparative |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Turnover from retail sale of bakery products and cereals ( including
    rice and pasta ) in the United Kingdom ( UK ) from 2008 to 2018 ( in million GBP
    ) Context: Year & Turnover in million GBP <SEP> 2018 & 21890 <SEP> 2017 & 21433
    <SEP> 2016 & 20757 <SEP> 2015 & 22187 <SEP> 2014 & 20372 <SEP> 2013 & 20352 <SEP>
    2012 & 20078 <SEP> 2011 & 18818 <SEP> 2010 & 17875 <SEP> 2009 & 16719 <SEP> 2008
    & 15816 . | This statistic shows the total annual turnover of the retail sale
    of bakery products and cereals ( including rice and pasta products ) in the United
    Kingdom from 2008 to 2018 . Turnover has grown steadily , reaching a peak of approximately
    22 billion British pounds in 2015 before declining the following year . By 2018
    turnover had almost completely recovered and amounted to 21.9 billion pounds .
    | Perceptual and Cognitive |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | Number of nights spent camping or in caravans in Great Britain in 2019
    , by type ( in millions ) Context: type & Nights spent in millions <SEP> Glamping/alternative
    accommodation e.g. yurt, tipi, ecopod & 1.7 <SEP> Static caravan (owned) & 9.4
    <SEP> Tent & 12.5 <SEP> Static caravan (rented) & 16.8 <SEP> Touring caravans
    (towed/campervan/motorhome) & 19.8 | This statistic shows the number of nights
    spent camping or in caravans in Great Britain in 2019 , by type . Almost 20 million
    nights were spent by domestic tourists in touring caravans in Great Britain ,
    while 12.5 million nights were spent camping in tents . | Statistical and Comparative
    |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | Barbados : National debt in relation to gross domestic product ( GDP
    ) from 2015 to 2025 Context: Year & National debt in relation to GDP <SEP> 2025*
    & 100.11% <SEP> 2024* & 105.68% <SEP> 2023* & 111.34% <SEP> 2022* & 117.24% <SEP>
    2021* & 124.53% <SEP> 2020 & 134.09% <SEP> 2019 & 122.22% <SEP> 2018 & 125.59%
    <SEP> 2017 & 158.26% <SEP> 2016 & 149.45% <SEP> 2015 & 147.02% | This statistic
    shows the national debt of Barbados from 2015 to 2020 in relation to the gross
    domestic product ( GDP ) , with projections up until 2025 . The figures refer
    to the whole country and include the debts of the state , the communities , the
    municipalities and the social insurances . In 2020 , the national debt of Barbados
    amounted to approximately 134.09 percent of the GDP . | Visual Encodding |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: Demonstrations for chart summarization task. Samples are obtained
    from the validation split of Chart-to-Text Kantharaj et al. ([2022b](#bib.bib22))
    dataset with the first three from Pew, and the last three from Statista. All the
    chart images are provided in [Table 11](#A1.T11 "In A.7 Consistency of Generated
    Output with Demonstrations ‣ Appendix A Appendices ‣ Do LLMs Work on Charts? Designing
    Few-Shot Prompts for Chart Question Answering and Summarization").'
  prefs: []
  type: TYPE_NORMAL
- en: A.6 Visual Data Table Generator (VDTG)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are two types of visual data tables that we constructed to train the model,
    as shown in [Fig. 4](#A1.F4 "In A.6 Visual Data Table Generator (VDTG) ‣ Appendix
    A Appendices ‣ Do LLMs Work on Charts? Designing Few-Shot Prompts for Chart Question
    Answering and Summarization").
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 9](#A1.T9 "In A.6 Visual Data Table Generator (VDTG) ‣ Appendix A Appendices
    ‣ Do LLMs Work on Charts? Designing Few-Shot Prompts for Chart Question Answering
    and Summarization") shows some samples outputs from our proposed VDTG module.
    We observe that the first two cases the VDTG model can generate the correct colors
    and positions of the labels. In the $3$-th case, the model also makes an error
    in generating blue instead of the correct color is gray.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Index | Chart Image | Generated Visual Data Table |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | ![[Uncaptioned image]](img/d4ef412b53cce76dc60022968142d4e8.png)  | Country
    &#124; Human Rights Volations, 2012 <0x0A> Central African Republic (red) &#124;
    8.5 <0x0A> Iraq (purple) &#124; 8.3 <0x0A> Gabon (Teal) &#124; 6.8 <0x0A> Suriname
    (orange) &#124; 5.3 <0x0A> Peru (gray) &#124; 4.9 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | ![[Uncaptioned image]](img/22e2076bd3d9a629d7a578b10a759366.png)  | Characteristic
    &#124; Nominal (blue) &#124; Real Terms (navy blue) <0x0A> 2019/20 &#124; 11.97
    &#124; 12.11 <0x0A> 2018/19 &#124; 11.35 &#124; 11.57 <0x0A> 2017/18 &#124; 11.94
    &#124; 11.76 <0x0A> 2016/17 &#124; 11.6 &#124; 12.25 <0x0A> 2015/16 &#124; 11.34
    &#124; 12.3 <0x0A> 2014/15 &#124; 12.46 &#124; 13.55 <0x0A> 2013/14 &#124; 11.62
    &#124; 12.97 <0x0A> 2012/13 &#124; 12.76 &#124; 14.43 <0x0A> 2011/12 &#124; 12.57
    &#124; 14.35 <0x0A> 2010/11 &#124; 13.06 &#124; 15.24 <0x0A> 2009/10 &#124; 13.22
    &#124; 15.76 <0x0A> 2008/09 &#124; 12.46 &#124; 15.03 <0x0A> 2007/08 &#124; 11.94
    &#124; 14.76 <0x0A> 2006/07 &#124; 11.43 &#124; 14.48 <0x0A> 2005/06 &#124; 10.87
    &#124; 14.21 <0x0A> 2004/05 &#124; 10.03 &#124; 13.36 <0x0A> 2003/04 &#124; 9.76
    &#124; 13.3 <0x0A> 2002/03 &#124; 9.37 &#124; 13.03 <0x0A> 2001/02 &#124; 8.66
    &#124; 12.42 <0x0A> 2000/01 &#124; 7.89 &#124; 11.38 <0x0A> 1999/00 &#124; 7.74
    &#124; 11.48 <0x0A> 1998/99 &#124; 7.22 &#124; 10.86 <0x0A> 1997/98 &#124; 6.41
    &#124; 9.76 <0x0A> 1996/97 &#124; 5.73 &#124; 8.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | ![[Uncaptioned image]](img/90f81be188cc3774c536d108fc0fb3ac.png)  | Frequently
    (dark blue) &#124; Sometimes (light blue) &#124; NET <0x0A> Lonely &#124; 7 &#124;
    24 &#124; 31 <0x0A> Depressed &#124; 13 &#124; 36 &#124; 49 <0x0A> Inspired &#124;
    16 &#124; 53 &#124; 69 <0x0A> Connected &#124; 21 &#124; 49 &#124; 71 <0x0A> Angry
    &#124; 25 &#124; 47 &#124; 71 <0x0A> Amused &#124; 44 &#124; 44 &#124; 88 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | ![[Uncaptioned image]](img/ff7265d120086a10e1b504f228e60358.png)  | Characteristic
    &#124; Men (blue) &#124; Women (navy blue) <0x0A> 2019 &#124; 945.74 &#124; 120.01
    <0x0A> 2018 &#124; 928.34 &#124; 1181.71 <0x0A> 2017 &#124; 906.45 &#124; 1160.94
    <0x0A> 2016 &#124; 894.49 &#124; 1154.19 <0x0A> 2015 &#124; 895.74 &#124; 1150.61
    <0x0A> 2014 &#124; 892.76 &#124; 1156.91 <0x0A> 2013 &#124; 880.11 &#124; 1134.59
    <0x0A> 2012 &#124; 863.35 &#124; 1118.29 <0x0A> 2011 &#124; 846.25 &#124; 1001.84
    <0x0A> 2010 &#124; 818.71 &#124; 1069.38 <0x0A> 2009 &#124; 766.65 &#124; 1021.74
    <0x0A> 2008 &#124; 751.52 &#124; 998.79 <0x0A> 2007 &#124; 733.65 &#124; 983.84
    <0x0A> 2006 &#124; 725.31 &#124; 971.54 <0x0A> 2005 &#124; 719.52 &#124; 959.92
    <0x0A> 2004 &#124; 708.46 &#124; 941.77 <0x0A> 2003 &#124; 669.77 &#124; 887.1)
    <0x0A> 2002 &#124; 645.65 &#124; 852.11 <0x0A> 2001 &#124; 619.74 &#124; 817.46
    <0x0A> 2000 &#124; 587.16 &#124; 756.59 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | ![[Uncaptioned image]](img/11e77bff3d4eff2fa5ccf3e76ce2f060.png)  | Entity
    &#124; More (green) &#124; Aboutthe same (gray) &#124; Fewer (orange) &#124; None
    (VOL) <0x0A> Turkey &#124; nan &#124; 30.0 &#124; 53.0 &#124; 0 <0x0A> Christian
    &#124; 10.0 &#124; 33.0 &#124; 53.0 &#124; 0 <0x0A> Sunni &#124; 12.0 &#124; 46.0
    &#124; 36.0 &#124; 0 <0x0A> Shia &#124; nan &#124; nan &#124; 44.0 &#124; 33 <0x0A>
    Lebanon &#124; 13.0 &#124; 40.0 &#124; nan &#124; 0 <0x0A> Jordan &#124; nan &#124;
    nan &#124; 63.0 &#124; 27 <0x0A> Syrian &#124; 64.0 &#124; nan &#124; nan &#124;
    26 <0x0A> Jordan &#124; 8.0 &#124; 8.0 &#124; 60.0 &#124; 23 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | ![[Uncaptioned image]](img/ea527b630a90bec859fb100f0ed01e87.png)  | Country
    &#124; Scheduled teaching time that teachers are in the classroom, 2002 to 2016
    <0x0A> Egypt (gray) &#124; 93.45 <0x0A> Tunisia (blue) &#124; 89.89 <0x0A> Madagascar
    (blue) &#124; 58.09 <0x0A> Mozambique (blue) &#124; 44.0 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: Sample outputs of our VDTG module. The first two cases the model detects
    the correct colors and positions, while the last 4 cases the model fails.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/13a8cf800c61b6c8f4fadec495c24f19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Two types of visual data tables. The left-hand side is *horizontal*
    whilst the right-hand one is *vertical*.'
  prefs: []
  type: TYPE_NORMAL
- en: A.7 Consistency of Generated Output with Demonstrations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There have one consistency property that we propose in our prompt construction
    rules for LCQA and CS tasks ([Section 4](#S4 "4 Methodology ‣ Do LLMs Work on
    Charts? Designing Few-Shot Prompts for Chart Question Answering and Summarization")):
    *(iii)* in [Section 4](#S4 "4 Methodology ‣ Do LLMs Work on Charts? Designing
    Few-Shot Prompts for Chart Question Answering and Summarization"). Although generating
    texts given a piece of information is a one-to-many relationship, i.e., there
    may have multiple correct answers, our motivation for this property is that the
    language model should answer consistently and correctly what it is taught through
    demonstrations. To verify this property in our proposed prompts, for each of LCQA
    and CS tasks, we input the proposed prompt and ask the model to generate the answer
    for each question in the prompt. The experimental results for LCQA and CS are
    illustrated in [Table 10](#A1.T10 "In A.7 Consistency of Generated Output with
    Demonstrations ‣ Appendix A Appendices ‣ Do LLMs Work on Charts? Designing Few-Shot
    Prompts for Chart Question Answering and Summarization") and [Table 11](#A1.T11
    "In A.7 Consistency of Generated Output with Demonstrations ‣ Appendix A Appendices
    ‣ Do LLMs Work on Charts? Designing Few-Shot Prompts for Chart Question Answering
    and Summarization") respectively. We observe that although the textual outputs
    of the model sometimes are slightly different from the input demonstrations, the
    main points are kept without adding any hallucinating information, which verifies
    that our prompts satisfy *(iii)* ([Section 4](#S4 "4 Methodology ‣ Do LLMs Work
    on Charts? Designing Few-Shot Prompts for Chart Question Answering and Summarization"))
    property.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Chart Image | Demonstration’s Output | Generated Output |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/aeb787f4cbfe65a0defa47f12259c2ef.png)  | The unaffiliated
    ( 58 % ) are the most likely among the religious groups studied to say there is
    solid evidence the earth is warming because of human activity . White evangelical
    Protestants are the most likely to say there is no solid evidence the earth is
    warming ( 31 % ) , and the least likely to believe that humans have contributed
    to heating up the planet ( 34 % ) . While only 39 % of black Protestants say global
    warming is a result of human activity , they are , however , the least likely
    of the religions studied to deny global warming is occurring ( 15 % ) . | The
    opinion of the public on earth warming varies among religious groups. The unaffiliated
    are the most likely among the religious groups studied to say there is solid evidence
    the earth is warming because of human activity (58%). White evangelical Protestants
    are the most likely to say there is no solid evidence the earth is warming (31%)
    and the least likely to believe that humans have contributed to heating up the
    planet (34%). While only 39% of black Protestants say global warming is a result
    of human activity, they are, however, the least likely of the religions studied
    to deny global warming is occurring (15%). |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/e85928bbe6ca3246746c4d855c424f8b.png)  | In 37
    % of the cases , it is the stay - at - home wives who actually have a higher level
    of education . In 45 % of these families , the spouses have equal educational
    attainment , and in about 18 % of the cases , the husbands have more education
    than their wives . | Among families with opt-out moms, 37% have the stay-at-home
    wives with a higher level of education than the husbands. 45% of these families
    have equal educational attainment between spouses, and 18% of the cases have the
    husbands with more education than the wives. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/0a861cc10c000f3c67ef94e65b626b1a.png)  | About
    two - thirds of Hispanic registered voters ( 68 % ) disapprove of the job Trump
    is doing as president , including 51 % who disapprove very strongly . The 30 %
    of Hispanic voters who approve of Trump includes 23 % who approve strongly | Among
    Hispanic registered voters, there is strong disapproval of President Trump’s job
    performance. About two-thirds (68%) of Hispanic voters disapprove of the job Trump
    is doing as president, including 51% who disapprove very strongly. The 30% of
    Hispanic voters who approve of Trump includes 23% who approve strongly. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/c6c3ca6c8a3658173a59d3083b4032bf.png)  | Overall
    , a majority ( 53 % ) says that jobs are difficult to find in their community
    while only about a third ( 34 % ) says there are plenty of jobs available . The
    percentage saying that jobs are difficult to find locally has increased modestly
    since February 2007 ( 48 % ) . In October 2003 , 66 % said that jobs were difficult
    to find , the highest percentage expressing that view since the start of the Bush
    administration . | Overall, a majority (53%) says that jobs are difficult to find
    in their community while only about a third (34%) says there are plenty of jobs
    available. The percentage saying that jobs are difficult to find locally has increased
    modestly since February 2007 (48%). In October 2003, 66% said that jobs were difficult
    to find, the highest percentage expressing that view since the start of the Bush
    administration. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/4ad80cbe88e8efb2e93b619930015342.png)  | 34 %
    say they followed stories about the continued attempts by pirates to hijack ships
    more closely than any other story last week , while 27 % say they followed stories
    about the U.S. economy most closely . 9 % say they followed the tea party protests
    more closely than any other story . | 34% of the public say they followed stories
    about the continued attempts by pirates to hijack ships more closely than any
    other story last week, while 27% say they followed stories about the U.S. economy
    most closely. 9% say they followed the tea party protests more closely than any
    other story. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/dd043a2f6b55d426406ed3d8fb1b1daa.png)  | Roughly
    three - quarters ( 77 % ) say Japan has not adequately expressed regret , and
    such Chinese sentiment is largely unchanged since 2006 . Only 10 % believe Tokyo
    has apologized enough . | Roughly three-quarters (77%) of Chinese people say that
    Japan has not adequately expressed regret for their actions during World War II,
    and this sentiment has been largely unchanged since 2006\. Only 10% of Chinese
    people believe that Japan has apologized enough for their actions during the war.
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10: For each demonstration in our prompt, we ask the language model to
    generate its expected output, given its input. We then compare the generated output
    from the model with its expected output.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Chart Image | Demonstration’s Output | Generated Output |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/780ec20e51a750a72cf5e258e47ad455.png)  | Before
    the outbreak, Americans strongly believed their personal data was more vulnerable
    than in the past. In a June 2019 survey, 70% of Americans said their personal
    information was less secure than it was five years earlier. Just 6% of Americans
    said they felt their information was more secure than in the past, while 24% said
    their personal information was about as secure as it was five years earlier. Experts
    have suggested that data security could be threatened for companies shifting to
    remote work – as well as ordinary people who are spending more time online – during
    the coronavirus outbreak. | In 2019, seven in ten Americans reported feeling less
    secure about their personal information than they did five years prior. The survey,
    conducted between June 3-17, 2019, revealed that 6% of U.S adults felt their personal
    information was more secure than five years earlier, while 24% reported feeling
    that it was the same. These findings suggest that the majority of Americans feel
    their personal data is more vulnerable now than it was before, a fear that has
    been compounded by the coronavirus pandemic and the increased reliance on digital
    communication. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/2bbce55749e4c92b05df369946de074f.png)  | Sharp
    growth in illegal immigration and increased enforcement of immigration laws have
    dramatically altered the ethnic composition of offenders sentenced in federal
    courts. In 2007, Latinos accounted for 40% of all sentenced federal offenders‚Äîmore
    than triple their share (13%) of the total U.S. adult population. The share of
    all sentenced offenders who were Latino in 2007 was up from 24% in 1991, according
    to an analysis of data from the United States Sentencing Commission (USSC) by
    the Pew Hispanic Center, a project of the Pew Research Center. | In 2007, 40%
    of all federal offenders sentenced in the U.S. were Latino, according to an analysis
    of United States Sentencing Commission data by the Pew Hispanic Center. This was
    more than triple the share of Latinos (13%) in the total U.S. adult population.
    The share of Latino offenders sentenced in federal courts had risen from 24% in
    1991. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/f9ad5c00efa89dd44bd5b7425ab9c92c.png)  | Far more
    voters expect voting to be difficult this year than in 2018, according to a survey
    of registered voters conducted July 27 to Aug. 2\. Around half of registered voters
    (49%) said they expect voting to be very or somewhat difficult this year, while
    the other half (50%) expect it to be very or somewhat easy. That represents a
    major change from October 2018, when just 15% of voters said they expected voting
    in that year‚Äôs midterms to be difficult and 85% said they expected it to be
    easy. | This year, a majority of voters are expecting it to be difficult to vote
    in the November elections. According to a survey of registered voters conducted
    July 27 to Aug. 2, 49% of registered voters said they expect voting to be very
    or somewhat difficult this year, while the other half (50%) expect it to be very
    or somewhat easy. This is a stark contrast to the survey results from October
    2018, when only 15% of voters said they expected voting in that year‚Äôs midterms
    to be difficult and 85% said they expected it to be easy. These results suggest
    that many Americans are feeling more uncertain and anxious about voting this year
    than in previous years. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/7f07ee9dacce3968858c514265286819.png)  | This
    statistic shows the total annual turnover of the retail sale of bakery products
    and cereals ( including rice and pasta products ) in the United Kingdom from 2008
    to 2018 . Turnover has grown steadily , reaching a peak of approximately 22 billion
    British pounds in 2015 before declining the following year . By 2018 turnover
    had almost completely recovered and amounted to 21.9 billion pounds . | This statistic
    shows the total annual turnover of the retail sale of bakery products and cereals
    (including rice and pasta products) in the United Kingdom from 2008 to 2018\.
    Turnover has grown steadily, with a peak of approximately 22 billion British pounds
    in 2015 before declining the following year. By 2018, turnover had almost completely
    recovered and amounted to 21.9 billion pounds. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/59960a222565157d8a4dea35b7f8ecb4.png)  | This
    statistic shows the number of nights spent camping or in caravans in Great Britain
    in 2019 , by type . Almost 20 million nights were spent by domestic tourists in
    touring caravans in Great Britain , while 12.5 million nights were spent camping
    in tents . | This statistic shows the number of nights spent camping or in caravans
    in Great Britain in 2019, by type. Nearly 20 million nights were spent by domestic
    tourists in touring caravans in Great Britain, while 12.5 million nights were
    spent camping in tents. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/84406091cfd836dc0ba0e5913e365c0a.png)  | This
    statistic shows the national debt of Barbados from 2015 to 2020 in relation to
    the gross domestic product ( GDP ) , with projections up until 2025 . The figures
    refer to the whole country and include the debts of the state , the communities
    , the municipalities and the social insurances . In 2020 , the national debt of
    Barbados amounted to approximately 134.09 percent of the GDP . | This statistic
    shows the national debt of Barbados from 2015 to 2020 in relation to the gross
    domestic product (GDP), with projections up until 2025\. The figures refer to
    the whole country and include the debts of the state, the communities, the municipalities
    and the social insurances. In 2020, the national debt of Barbados amounted to
    approximately 134.09 percent of the GDP. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 11: For each demonstration in our prompt, we ask the language model to
    generate its expected output, given its input. We then compare the generated output
    from the model with its expected output.'
  prefs: []
  type: TYPE_NORMAL
- en: A.8 Analysis of Visual Data Table Representation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Model | Aug. (%) | Human (%) | Avg. (%) |'
  prefs: []
  type: TYPE_TB
- en: '| InstructGPT + CCR | - | 30.86 | - |'
  prefs: []
  type: TYPE_TB
- en: '| PromptChart | - | 38.42 | - |'
  prefs: []
  type: TYPE_TB
- en: '| InstructGPT + CCR + *ground_truth* colors | - | 60.95 | - |'
  prefs: []
  type: TYPE_TB
- en: 'Table 12: Effect of color information on color-related QA pairs (203 cases).'
  prefs: []
  type: TYPE_NORMAL
- en: 'This section is to answer the research question: "What is the maximal improvement
    we can obtain by integrating visual data tables instead of traditional data tables?"
    To answer this question, we manually label the colors for a total of 203 color-related
    QA pairs in the human test set of ChartQA Masry et al. ([2022](#bib.bib34)). We
    then examine the performance of InstructGPT model on these QA pairs in three different
    settings: (1) with ground-truth visual data tables, (2) with original data tables,
    (3) with our generated visual data tables from VDTG module. The experimental results
    are shown in [Table 12](#A1.T12 "In A.8 Analysis of Visual Data Table Representation
    ‣ Appendix A Appendices ‣ Do LLMs Work on Charts? Designing Few-Shot Prompts for
    Chart Question Answering and Summarization"). We observe that with the ground-truth
    colors, the performance of the model is improved significantly, nearly twice compared
    to the case without them. This observation not only verifies the effectiveness
    of our proposed visual data table representation but also reveals a huge room
    for further improvements.'
  prefs: []
  type: TYPE_NORMAL
- en: A.9 Illustrative Cases for Proposed Properties of LCQA and CS Prompts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Table 14](#A1.T14 "In A.9 Illustrative Cases for Proposed Properties of LCQA
    and CS Prompts ‣ Appendix A Appendices ‣ Do LLMs Work on Charts? Designing Few-Shot
    Prompts for Chart Question Answering and Summarization") shows the illustrative
    of the variants of our LCQA prompt which do not satisfy property *(ii), (iii)*
    in [Section 4](#S4 "4 Methodology ‣ Do LLMs Work on Charts? Designing Few-Shot
    Prompts for Chart Question Answering and Summarization"). We derive 2 main observations.
    First, not guaranteeing *(ii)* makes the answers commonly lack findings for *Discovery*
    queries (first 2 samples). Second, the demonstrations that do not ensure *(iii)*
    commonly have very long answers. By using these demonstrations not satisfying
    *(ii), (iii)* in the prompt, we observe that the model commonly generates very
    long answers which makes them likely to suffer from factual errors (last 2 examples).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Chart Image | Input | Gold Answer | PromptChart | Ablation Study Model |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/08593bbf0adb7b60be2c8ff068ba35bc.png)  | Question:
    How was Trump’s online posture seen when he was becoming president? | In December
    , a majority of the public ( 82 % ) also said Trump should be more cautious in
    “ the kinds of things he says and tweets ” once he becomes president . Just 15
    % said there is no need for him to change . | When it comes to the kind of things
    Trump says and tweets, once he becomes president , 82 % of Americans believe he
    will need to be more cautious , including 90 % of Democrats and 76 % of Republicans
    . Only 15 % of Americans believe there is no need for Trump to change his online
    posture. | Most people, including both Republicans and Democrats, believe that
    Trump needs to be more cautious when it comes to what he says and tweets once
    he becomes President. 82 % of the total population, 76 % of Republicans and Lean
    Rep and 90 % of Democrats and Lean Dem think Trump should be more cautious. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/0a0f098f75369d5c89b25c36592f20e4.png)  | How does
    public feel about supporting undocumented immigrants? | 70 % say there should
    be a way for undocumented immigrants to stay in the country legally , if they
    meet certain requirements . Most of those who support legal status think there
    should be a way for unauthorized immigrants to become citizens ( 43 % overall
    ) , while 24 % say the path should only include permanent residency . | A large
    majority of the public (70%) support a path to legal status for undocumented immigrants
    in the US who meet certain requirements. This includes 43% who support a path
    to citizenship and 24% who support a path to permanent residency. Only 27% of
    people oppose a path to legal status for undocumented immigrants and 3% are not
    sure. | Seven-in-ten Americans ( 70 % ) say undocumented immigrants currently
    living in the US should be allowed to stay in the country legally , if they meet
    certain requirements , while 27 % say they should not be allowed to stay . |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/57bb99f41537c41119eea22c959407a8.png)  | What
    is the favourable ratings of Philippines given by Christians and Muslims ? | in
    Christian - majority Philippines , favorable views of Christians were nearly universal
    ( 97 % ) while just 57 % said the same of Muslims in their country . | The favorable
    ratings for Christians and Muslims in the Philippines are 97 % and 57 % respectively.
    | A large majority of the public (70%) support a path to legal status for undocumented
    immigrants in the US who meet certain requirements. This includes 43% who support
    a path to citizenship and 24% who support a path to permanent residency. Only
    27% of people oppose a path to legal status for undocumented immigrants and 3%
    are not sure. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/2eb867373fc2ea4aabf5a6151bb85075.png)  | What
    do Americans think about gene editing? | Roughly half of adults ( 49 % ) said
    gene editing would be less acceptable to them if the effects “ changed the genetic
    makeup of the whole population , ” versus just 17 % who said it would be more
    acceptable . When asked about an alternate scenario in which the effects of gene
    editing are limited to a single person , more Americans said it would be more
    acceptable than less acceptable ( 34 % to 23 % ) . | A majority of Americans (
    34 % ) say that gene editing to give healthy babies a much reduced risk of serious
    diseases would be more acceptable , while 23 % say it would be less acceptable
    and 40 % say it would make no difference . However , when asked about gene editing
    that would change the genetic makeup of the whole population , only 17 % say it
    would be more acceptable , while 49 % say it would be less acceptable and 31 %
    say it would make no difference. | Americans are more concerned than accepting
    when it comes to gene editing . According to a survey conducted in March 2016
    by the Pew Research Center , 34 % of U.S. adults say that gene editing that would
    give healthy babies a much reduced risk of serious diseases would be more acceptable,
    while 40 % say it would make no difference and 23 % say it would be less acceptable.
    When it comes to gene editing that would change the genetic makeup of the whole
    population, only 17 % say it would be more acceptable, 49 % say it would be less
    acceptable and 31 % say it would make no difference. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 13: Samples to illustrate the outputs of PromptChart and its variant
    without *($L_{2}$)* (last 2 rows) for LCQA task.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Chart Image | Gold Answer | PromptChart | Ablation Study Model |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/6759c0a1b3686de2101629c60fc3813b.png)  | In the
    vast majority of local TV newsrooms, budgets either grew or held steady in 2013\.
    About half of the news directors surveyed (52%) said they increased their news
    budgets last year, compared with 49% the year before and 38% in 2011\. But there
    were differences by market size. Indeed, 54% of the mid-sized market stations
    (51-100) said they increased their budgets compared with 41% whose budgets were
    cut or stayed the same. But in the biggest 25 TV markets, the percentage of stations
    increasing their budgets in 2013 (48%) was identical to the percentage that cut
    or did not increase their spending. | In 2013, local TV newsroom budgets experienced
    an overall increase. According to a survey of news directors from all commercial
    local TV stations, 52% of respondents said their budgets had increased, 34% said
    their budgets remained the same, and 6% were not sure. Just 7% of news directors
    reported that their budgets had decreased. These results indicate that local TV
    newsroom budgets were largely stable or increased in 2013. | According to a survey
    of news directors at all commercial local stations conducted by RTDNA/Hofstra
    University, 52% of news directors said their budgets had increased, 34 |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/bd2e6b34178ffa261137f4ac342863ca.png)  | As Russian
    troops remain in Ukraine’s Crimea region and Crimea’s Parliament has set up a
    secession vote, Americans prefer the U.S. to not get too involved in the situation.
    By a roughly two-to-one margin (56% vs. 29%), the public says it is more important
    for the U.S. to not get involved in the situation with Russia and Ukraine than
    to take a firm stand against Russian actions. | In March 2014, Americans were
    divided on how the U.S. should respond to Russian actions in Ukraine. 29% believed
    it was more important for the U.S. to take a firm stand against Russian actions,
    while 56% said it was more important not to get too involved in the situation
    in Ukraine. 15% said they did not know or gave no opinion. | In 2014, 29% of Americans
    said it was more important for the United States to take a firm stand against
    Russian actions than to not get too involved in the situation in Ukraine. This
    sentiment was shared among 56% of those surveyed who chose not to answer. Only
    15% of respondents said that it was more important for the United States not to
    get too involved in the situation in Ukraine. The survey was conducted by the
    Pew Research Center from March 6-9, 2014. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/cfc62db285e59f23e51ea20d2746d0be.png)  | The public
    continued to track news about the economy and the presidential election, while
    paying less attention to another important political story – Wisconsin Gov. Scott
    Walker’s victory in a hard-fought recall election. The latest weekly News Interest
    Index, conducted June 7-10 among 1,000 adults by the Pew Research Center for the
    People & the Press, finds that 35% say they followed reports about U.S. economic
    conditions very closely while 30% tracked news about the presidential election
    very closely. The public expressed less interest in news about the Wisconsin recall
    election (21% very closely), European economic problems (18%) and the CIA killing
    of an al Qaeda leader in a drone strike in Pakistan (16%). | In June 2012, 35%
    of Americans reported they were following news about the U.S. economy very closely.
    This was the leading story in terms of interest, with 30% of Americans saying
    they were following it very closely. This was followed by the 2012 election, with
    21% of Americans following it very closely. 18% of Americans were following news
    about European economies very closely, 16% were following news about Al Qaeda
    leader killed, and 15% were following news about the Vatican and U.S nuns. These
    figures were reported in a survey conducted by the Pew Research Center from June
    7-10, 2012. | In June 2012, Americans followed the U.S. economy the closest, with
    35% of adults saying they followed the story “very closely.” The 2012 presidential
    election and the Wisconsin recall election were the next most closely followed
    stories, with 30% and 21% of adults, respectively, following these stories “very
    closely.” Europeans economies and the death of al Qaeda leader Osama bin Laden
    rounded out the top five stories, with 18% and 16% of adults saying they followed
    those stories “very closely.” The Vatican and U.S. nuns dispute was the least
    followed story, with just 6% of adults following the story “very closely.” These
    results are based on a June 7-10, 2012 survey conducted by the Pew Research Center.
    |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/6ea070bd3664e3eebf1746ed7ed9b0d4.png)  | As for
    median household income, the most recent data we have is for 2011 (the Census
    Bureau will release 2012 numbers next week), but they show that after factoring
    in inflation, the median U.S. household income shrank 4.7 percent between 2008
    and 2011\. Little surprise, then, that more than half (54%) of the people in our
    survey said household incomes have hardly recovered since the recession, and a
    scant 2% said they’ve fully recovered. | From 2008 to 2011, median household income
    in the United States decreased slightly, according to data from the U.S. Census
    Bureau’s Current Population Survey. In 2011 dollars, median household income was
    $52,546 in 2008, $51,195 in 2009, $50,831 in 2010, and $50,054 in 2011\. This
    represents a drop of 4.2% over the four-year period. | The median household income
    in the United States decreased from 2008 to 2011, according to the U.S. Census
    Bureau’s Current Population Survey. In 2008, the median household income was $52,546,
    while in 2011, it had fallen to $50,054\. This represented a decrease of 4.1%
    in real terms, or a decrease of $2,492 in inflation-adjusted dollars. The economic
    recession that began in 2008 was the main factor behind the decrease in median
    household income during this period. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 14: Samples to illustrate the outputs of PromptChart and its variant
    without *($S_{1}$)* (last 2 rows) for CS task.'
  prefs: []
  type: TYPE_NORMAL
- en: A.10 Error Analysis Case Studies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Chart Image | Question | PromptChart | InstructGPT | Gold Answer |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/f7c0463477e82b092f97660c471d9985.png)  | How many
    color bars are there in the graph? | There are 4 color bars in the graph. The
    answer is 4. | There is only one color bar in the graph. The answer is 1. | 4
    |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/14874697c70fd529e09e6718e32ee82f.png)  | What
    is the color of "More"? | The color of the pie represented "More" is Gray. The
    answer is Gray. | The color of the pie represented "More" is Orange. The answer
    is Orange. | Gray |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/b3a93a409e3f5d9e5766c658c8effbc9.png)  | What
    is the colour of India in the graph? | The colour of India in the graph is orange.
    The answer is orange. | The colour of India in the graph is not given in the data
    table. Therefore, the answer is not given. | Orange |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/a764b0f1ba0fc0b4ee3d735f7feaab3e.png)  | Is the
    color of right-side bars lightest? | The color of right-side bars is light brown.
    The lightest color of the bars is light brown, the answer is Yes. | The right-side
    bars are Dem/Lean Dem and Rep/Lean Rep. The color of Dem/Lean Dem bar is lightest.
    The answer is Yes. | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/5cf6c9114ffe7464319cc9fdb9307c19.png)  | Which
    color Bar shows the greatest Meat consumer? | The greatest meat consumer is Finland
    with 175.09\. The color of Finland is Teal. So the answer is Teal. | The highest
    meat consumption is in Finland with 175.09\. So the color bar that shows the greatest
    meat consumer is the bar for Finland. The answer is Finland. | Finland |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/2151fb88735e68d73be74f477c33da9b.png)  | What
    country had the highest yearly cost of closing global poverty in 2002? | The highest
    yearly cost of closing global poverty in 2002 is Comoros (gray) with 16.87\. The
    answer is Comoros. | The highest yearly cost of closing global poverty in 2002
    is Botswana with 144.7\. The answer is Botswana | Botswana |'
  prefs: []
  type: TYPE_TB
- en: 'Table 15: The first four cases show PomptChart answers correctly by visual
    information whilst InstructGPT answers wrongly. The last two cases illustrate
    that visual information sometimes harms the model’s performance. Both models are
    run in the few-shot setting with our proposed FCQA prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 16](#A1.T16 "In A.10 Error Analysis Case Studies ‣ Appendix A Appendices
    ‣ Do LLMs Work on Charts? Designing Few-Shot Prompts for Chart Question Answering
    and Summarization") shows our case studies as discussed in [Section 6.4](#S6.SS4
    "6.4 Error Analysis ‣ 6 Discussion ‣ Do LLMs Work on Charts? Designing Few-Shot
    Prompts for Chart Question Answering and Summarization").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Question 1: What is the average of all three bars? ![[Uncaptioned image]](img/4d304577b953f5d98f318a07cbce172c.png)
    Generated Answer 1: The average of the three bars is (1.5 + 1.34 + 1.2) / 3 =
    1.32. The answer is 1.32. Gold Answer 1: 1.346  | Question 2: Is the average percentage
    of all three bars greater than 6%? ![[Uncaptioned image]](img/e432562e94e5cfedf9209b36e285b6c9.png)
    Generated Answer 2: The percentage of all three bars is ( 7.19 + 6.85 + 6.2 )
    / 3 = 6.7466666667%. The answer is No, the average is not greater than 6% Gold
    Answer 2: Yes  | ![[Uncaptioned image]](img/d163353cae64f3578d17228fc7869433.png)
    Generated Summary 3: This statistic shows the quantity of cocaine seized in Italy
    from 2010 to 2019 . In 2019 , 8269.54 kilograms of cocaine were seized in Italy
    , a significant increase from the 3847.7 kilograms seized in 2010 . The largest
    amount of cocaine seized during this period was in 2011 , with 6348.43 kilograms
    . Gold Summary 3: The statistic shows the total quantity of cocaine seized in
    Italy from 2010 to 2019 . In 2019 , 8.3 thousand kilograms of cocaine were seized
    in the country , the largest amount registered between 2010 and 2019 .  |'
  prefs: []
  type: TYPE_TB
- en: '| Question 4: What are the opinions of Independents on Obama? ![[Uncaptioned
    image]](img/df8af6c1b82d0a16d9ba3eff991c8bc4.png) Generated Answer 4: During Obama’s
    second term , there has been a significant drop in support among Independents
    . In December 2012 , 48 % of Independents approved of Obama’s job performance
    , compared to only 32 % in November 2013 , a decrease of 16 percentage points
    . By comparison , Democrats’ approval of Obama’s job performance only decreased
    by 10 percentage points ( 78 % to 88 % ) and Republicans’ approval decreased by
    only 2 percentage points ( 10 % to 12 % ) . Gold Answer 4: Obama’s approval ratings
    among Independents declined from 48 % in December 2012 to 36 % in November 2013,
    a drop of 21 points, which is larger than 10 point decline among Democrats and
    7 point decline among Republicans.  | Question 5: What are the views of the general
    public on LGBT figures? ![[Uncaptioned image]](img/e70a361ffea8d1dbf3c882c6c2afe229.png)
    Generated Answer 5: Among the general public, 32 % of respondents named Ellen
    DeGeneres and 38 % named Elton John as the first LGBT artist or public figure
    that comes to mind. Other names mentioned included Rosie O’Donnell ( 11 % ) ,
    Jason Collins ( 8 % ) , Barney Frank ( 4 % ) , Anderson Cooper ( 4 % ) and Rock
    Hudson ( 4 % ) . A total of 38 % of respondents said they didn’t know. Gold Answer
    5: Jason Collins was the second - most frequently mentioned gay or lesbian public
    figure . 38 % said they could n’t name any gay or lesbian entertainers or public
    figures .  | ![[Uncaptioned image]](img/bf81eb88a68b959a77596b4675dcfe5e.png)
    Generated Summary 6: In 2018, the number of family units and unaccompanied children
    apprehended at the U.S. southwest border rose substantially, reaching a peak of
    27,518 family units and 5,653 unaccompanied children in December. The number of
    apprehensions for both groups increased steadily throughout the year, starting
    at 4,766 family units and 3,202 unaccompanied children in January. The increase
    in family unit apprehensions is especially notable, as the number of family units
    apprehended in 2018 was nearly six times higher than the number apprehended in
    2017\. Experts suggest that the rise in apprehensions is due to a combination
    of factors, including increasing violence in Central American countries and more
    expansive U.S. immigration policies. Gold Summary 6: In the months leading up
    to the shutdown, there was a large increase in the number of people in family
    units apprehended at the border. Monthly family apprehensions subsequently hit
    new highs each month from September through December, according to data going
    back to 2012\. There were nearly 17,000 family member apprehensions in September,
    more than 23,000 in October, about 25,000 in November and a record of more than
    27,000 in December.  |'
  prefs: []
  type: TYPE_TB
- en: 'Table 16: Case studies for error analysis. Red texts are errors and green ones
    are correct generations.'
  prefs: []
  type: TYPE_NORMAL
- en: A.11 Benchmarks Statistics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We present the statistics of the benchmarks we use in [Table 1](#S3.T1 "In ∙
    Problem Definition ‣ 3 Problem Formulation ‣ Do LLMs Work on Charts? Designing
    Few-Shot Prompts for Chart Question Answering and Summarization"). ChartQA focuses
    on logical and visual reasoning questions, while OpenCQA targets open-ended questions
    with explanatory answers and Chart-to-Text is a benchmark for chart summarization
    task.
  prefs: []
  type: TYPE_NORMAL
