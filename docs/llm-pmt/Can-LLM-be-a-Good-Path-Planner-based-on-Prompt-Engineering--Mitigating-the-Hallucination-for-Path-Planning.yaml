- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:40:23'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating the Hallucination
    for Path Planning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.13184](https://ar5iv.labs.arxiv.org/html/2408.13184)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Hourui Deng
  prefs: []
  type: TYPE_NORMAL
- en: College of Computer Science
  prefs: []
  type: TYPE_NORMAL
- en: Sichuan Normal University
  prefs: []
  type: TYPE_NORMAL
- en: Chengdu, China
  prefs: []
  type: TYPE_NORMAL
- en: herry.liquor@gmail.com
  prefs: []
  type: TYPE_NORMAL
- en: '&Hongjie Zhang'
  prefs: []
  type: TYPE_NORMAL
- en: College of Computer Science
  prefs: []
  type: TYPE_NORMAL
- en: Sichuan Normal University
  prefs: []
  type: TYPE_NORMAL
- en: Chengdu, China
  prefs: []
  type: TYPE_NORMAL
- en: zhanghongjie@sicnu.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: '&Jie Ou'
  prefs: []
  type: TYPE_NORMAL
- en: School of Information and Software Engineering
  prefs: []
  type: TYPE_NORMAL
- en: University of Electronic Science and Technology of China
  prefs: []
  type: TYPE_NORMAL
- en: Chengdu, China
  prefs: []
  type: TYPE_NORMAL
- en: oujieww6@gmail.com
  prefs: []
  type: TYPE_NORMAL
- en: '&Chaosheng Feng'
  prefs: []
  type: TYPE_NORMAL
- en: College of Computer Science
  prefs: []
  type: TYPE_NORMAL
- en: Sichuan Normal University
  prefs: []
  type: TYPE_NORMAL
- en: Chengdu, China
  prefs: []
  type: TYPE_NORMAL
- en: csfenggy@sicnu.edu.cn Corresponding Author
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Spatial reasoning in Large Language Models (LLMs) is the foundation for embodied
    intelligence. However, even in simple maze environments, LLMs still encounter
    challenges in long-term path-planning, primarily influenced by their spatial hallucination
    and context inconsistency hallucination by long-term reasoning. To address this
    challenge, this study proposes an innovative model, Spatial-to-Relational Transformation
    and Curriculum Q-Learning (S2RCQL). To address the spatial hallucination of LLMs,
    we propose the Spatial-to-Relational approach, which transforms spatial prompts
    into entity relations and paths representing entity relation chains. This approach
    fully taps the potential of LLMs in terms of sequential thinking. As a result,
    we design a path-planning algorithm based on Q-learning to mitigate the context
    inconsistency hallucination, which enhances the reasoning ability of LLMs. Using
    the Q-value of state-action as auxiliary information for prompts, we correct the
    hallucinations of LLMs, thereby guiding LLMs to learn the optimal path. Finally,
    we propose a reverse curriculum learning technique based on LLMs to further mitigate
    the context inconsistency hallucination. LLMs can rapidly accumulate successful
    experiences by reducing task difficulty and leveraging them to tackle more complex
    tasks. We performed comprehensive experiments based on Baidu’s self-developed
    LLM: ERNIE-Bot 4.0\. The results showed that our S2RCQL achieved a 23%–40% improvement
    in both success and optimality rates compared with advanced prompt engineering.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large language models are remarkable artificial intelligence (AI) technology
    that has gained remarkable attention in various fields. The LLMs have implemented
    Artificial Intelligence-Generated Content (AIGC) through massive corpora and advanced
    transformer frameworks. With the support of various prompt engineering, they have
    demonstrated a considerable level of intelligence and accomplished a wide range
    of decision-making tasks, such as mathematical reasoning  [[1](#bib.bib1)], embodied
    AI agent  [[2](#bib.bib2)], UAV control  [[3](#bib.bib3)], and complete open-world
    game Minecraft  [[4](#bib.bib4)] using chain-of-thought technology. However, LLMs
    exhibit significant limitations in spatial reasoning and long-term planning, which
    caused by their spatial hallucination and context inconsistency hallucination
    by long-term reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many studies have proposed various solutions to address hallucination problems,
    mainly focusing on three aspects: instruction fine-tuning, prompt engineering,
    and reinforcement learning. Instruction fine-tuning involves parameter adjustment
    of pre-trained LLMs, encompassing dataset curation and neural network training,
    to enhance performance on the specific task [[5](#bib.bib5)]. However, the significant
    computational costs required for fine-tuning LLMs pose a challenge for rapid expansion
    to new tasks. Prompt engineering aims to improve the inference accuracy of LLMs
    by designing instructions that guide the models to reason according to specific
    requirements. Advanced techniques in this domain include chain-of-thought (CoT) [[6](#bib.bib6),
    [7](#bib.bib7), [8](#bib.bib8)], tree-of-thought (ToT) [[9](#bib.bib9)], graph-of-thought
    (GoT) [[10](#bib.bib10), [11](#bib.bib11)], chain of experts (CoE) [[13](#bib.bib13)],
    ReAct [[14](#bib.bib14), [15](#bib.bib15)], and Reflexion [[16](#bib.bib16)].
    Reinforcement learning (RL) has long been an effective technique for addressing
    complex planning problems by allowing an agent to interact with its environment
    through trial and error. This RL model continuously adjusts its strategies to
    achieve optimal path planning. By combining RL with LLMs, RL can reduce the cost
    of exploration [[17](#bib.bib17), [18](#bib.bib18)]. However, prompt and RL models
    perform poorly in spatial reasoning tasks, particularly maze path planning. As
    shown in Figure [1(a)](#S1.F1.sf1 "In Figure 1 ‣ 1 Introduction ‣ Can LLM be a
    Good Path Planner based on Prompt Engineering? Mitigating the Hallucination for
    Path Planning"), this maze contains three forbidden zones. Therefore, a path must
    be planned from the starting point to the ending point. However, finding the shortest
    path at a glance is intuitive for humans. Still, when using CoT (Figure [1(b)](#S1.F1.sf2
    "In Figure 1 ‣ 1 Introduction ‣ Can LLM be a Good Path Planner based on Prompt
    Engineering? Mitigating the Hallucination for Path Planning")) and Rememberer [[17](#bib.bib17)](Figure [1(c)](#S1.F1.sf3
    "In Figure 1 ‣ 1 Introduction ‣ Can LLM be a Good Path Planner based on Prompt
    Engineering? Mitigating the Hallucination for Path Planning")), agents often get
    stuck and return with a failure.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0c60cba50a64024b1dbc466b44741595.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) This is a maze that contains three forbidden zones.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9c2b827cb8401df4b95198b84a502492.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) The heatmap of the path using CoT as a prompt to solve this maze.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cecb274a6fc5033f5e4113a865a702e7.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) The heatmap of the path using Rememberer to solve this maze step by step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: An example of a maze. Solving this maze path planning task is challenging
    using both CoT and Rememberer, an LLMs with RL method.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have analyzed the motives behind LLMs’ inadequate understanding of spatial
    relationships and their tendency to navigate naive paths in the direction of the
    shortest straight-line distance while ignoring obstacles. We propose an innovative
    method called Spatial-to-Relational Transformation and Curriculum Q-Learning (S2RCQL)
    to improve the LLM’s performance and solve maze problems. We have introduced the
    Spatial-to-Relational transformation to address the issue of LLM’s spatial hallucination.
    This transformation converts implicit spatial relationships into an explicit entity
    relation, describing the connectivity of paths through the relationships between
    nodes. Then, we introduced a Q-learning-assisted path-planning algorithm for LLMs
    to eliminate the LLMs’ context inconsistency hallucination by long-term reasoning.
    We guided LLMs to avoid dead end by inserting Q-values into the prompts. Finally,
    we have designed a reverse curriculum learning algorithm to mitigate the context
    inconsistency hallucination further. This algorithm gradually increases task difficulty,
    allowing LLMs to reduce the number of reasoning steps. We performed extensive
    experiments based on Baidu’s self-developed LLM: ERNIE-Bot 4.0\. The results indicate
    that our S2RCQL achieves a significant improvement of 23%–40% in success and optimality
    rates compared to the state-of-the-art CoTs baselines. This study offers several
    contributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This study is the first to propose converting spatial path planning tasks into
    entity relations for path planning. As a result, we have designed the Spatial-to-Relational
    transformation, which has been successfully applied to maze navigation tasks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We have also proposed a path-planning algorithm based on LLMs and reverse curriculum
    Q-learning, representing a step forward in addressing LLM’s context inconsistency
    hallucination.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We performed comprehensive experiments based on the ERNIE-Bot model to verify
    the reliability and effectiveness of our algorithm.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Figure [2](#S2.F2 "Figure 2 ‣ 2.1 Overview ‣ 2 Methodology ‣ Can LLM be a Good
    Path Planner based on Prompt Engineering? Mitigating the Hallucination for Path
    Planning") presents an overview of S2RCQL. Generally, S2RCQL comprises three main
    components: the environment, the agent, and the course module. The agent continuously
    interacts with the environment, seeking the shortest path from the starting point
    to the endpoint through trial and error. The environment module declares the maze
    in text with coordinates, including the maze size, obstacles, starting point,
    and goal. Then, we extract this format information using LLMs. To facilitate Python
    parsing, we control LLM output in JSON format. Moreover, we automatically construct
    a graph from the JSON response of LLMs to explicitly represent the maze connectivity,
    facilitating the LLMs’ reasoning. In the agent module, we construct a state description
    of the current maze, including the graph structure and node. In addition, we retrieve
    the most similar experience and its corresponding Q-value from the experience
    replay buffer. These elements are then used to create a few-shot example concatenated
    with the current maze to form the final prompt. The agent outputs the final action,
    the next-hop node, based on the $\epsilon-greedy$ algorithm and updates the environmental
    state. This process is repeated iteratively. In the top right corner of Figure [2](#S2.F2
    "Figure 2 ‣ 2.1 Overview ‣ 2 Methodology ‣ Can LLM be a Good Path Planner based
    on Prompt Engineering? Mitigating the Hallucination for Path Planning"), we identify
    the reverse course generation module. We construct intermediate starting points
    from the current graph based on hand-craft or LLMs. These starting points can
    reduce task difficulty and eliminate the context inconsistency hallucination by
    reducing the number of reasoning steps.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/67ffc4bd3b55c57261315461c1f0f7d3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: This diagram provides an overview of our approach. First, we convert
    arbitrary text maze descriptions into entity relations using LLMs and Python code.
    Then, we combined the Q-learning and LLMs to select actions through $\epsilon-greedy$
    with reverse curriculum learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Spatial-to-Relational Transformation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Researchers from Google DeepMind and University College London have comprehensively
    analyzed LLMs’ capabilities in performing potential multi-step reasoning [[19](#bib.bib19)].
    Multi-step reasoning requires models to retrieve relevant information sequentially
    and piece it together to solve problems or respond to queries. As a result, the
    relevant information required for LLMs’ reasoning is crucial. CoT and ToT techniques
    involve generating intermediate reasoning processes using LLMs, including relevant
    information and piecing together this information to generate answers. In our
    maze planning task, we automatically convert map information into an entity relation
    format, enabling the relevant information required for LLMs’ reasoning to be directly
    presented in the prompt, eliminating spatial hallucination of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: This study transforms the coordinates that describe the locations in the maze,
    such as (1,0) into Node F and (0,0) into Node A. The reachability between coordinates
    is converted into relationships between nodes. For example, if (1,0) and (0,0)
    are reachable, it is translated into (A,F), indicating a direct relationship between
    Node A and Node F. As a result, we used the letters A and F to represent nodes
    instead of numbers or coordinates. Our preliminary experiments indicate that LLMs
    exhibit generalization toward coordinates, perceiving (1,0) and (1,1), favoring
    movement toward (1,1), and leading to dead ends. By employing character representations,
    we can reduce node similarity and focus on relationships rather than node similarity.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/89fa5ba31f8bee1da8db19b6f3e72772.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: This module can process any maze map description and convert it into
    a relational network.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure [3](#S2.F3 "Figure 3 ‣ 2.2 Spatial-to-Relational Transformation ‣ 2 Methodology
    ‣ Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating the Hallucination
    for Path Planning") illustrates the process of Spatial-to-Relational Transformation.
    Initially, we transform the generically described maze into a structured representation
    using LLMs. In addition, we employed a JSON format to represent maze information,
    including maze size, start and end points, and obstacle coordinates. Through this
    process, we can effectively extract structured information by instructing LLMs
    to output in JSON format. For this purpose, we employ a one-shot exemplar prompt.
    Subsequently, we leverage the OpenAI Gym [[21](#bib.bib21)] environment to transform
    the structured maze into an interactive simulation environment, encompassing essential
    functions such as action execution, state updates, and reward calculations. Finally,
    we convert the state output from Gym into text, which includes information about
    the node relationship network.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Curriculum Q-Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Reverse Curriculum Generator with LLMs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We used reverse curriculum learning (RCL) [[22](#bib.bib22)] for LLMs inference,
    generating curricula from easy to complex tasks based on the prompt engineering
    of LLMs. The RCL begins by starting from a state close to the goal using random
    walks to find a reachable initial state X1, which is a simplified task. Then,
    based on X1, a more challenging initial state, X2, is generated, which continues
    iteratively. Therefore, the agent must learn according to the curriculum difficulty
    and transfer the experience from simple tasks to complex ones, thereby enhancing
    the efficiency of policy learning.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c7d29443f1a8d7c74b981ff6d0a09c96.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Generate curriculums by LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: We extend this approach to the LLMs’ prompts by describing the reverse curriculum
    generation process in natural language, allowing the LLMs to autonomously determine
    the simplified starting point, shown in Figure [4](#S2.F4 "Figure 4 ‣ Reverse
    Curriculum Generator with LLMs ‣ 2.3 Curriculum Q-Learning ‣ 2 Methodology ‣ Can
    LLM be a Good Path Planner based on Prompt Engineering? Mitigating the Hallucination
    for Path Planning"). We incorporate hand-crafted curricula into S2RCQL to obtain
    a better curriculum, which proves beneficial for handling highly complex mazes.
    We performed experiments to thoroughly compare the quality of the two types of
    curricula generation and their impact on the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Curriculum Q-learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We used LLMs or hand-crafted method courses to design a staged Q-learning optimization.
    By starting from a given initial point, we used the Q-learning algorithm to update
    the Q-table and store the experience data tuple $(s,a,r,s^{\prime},q)$ upon reaching
    the goal, encouraging the agent to search the shortest path.
  prefs: []
  type: TYPE_NORMAL
- en: Equation [1](#S2.E1 "In Curriculum Q-learning ‣ 2.3 Curriculum Q-Learning ‣
    2 Methodology ‣ Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating
    the Hallucination for Path Planning") presents the action sampling function of
    Q-learning based on LLMs, using the $\epsilon-greedy$ is a random number sampled
    at each step.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: Algorithm 1 S2RCQL
  prefs: []
  type: TYPE_NORMAL
- en: 0:  $Maze\ text$
  prefs: []
  type: TYPE_NORMAL
- en: Pseudocode of Curriculum Q-learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Algorithm [1](#alg1 "Algorithm 1 ‣ Curriculum Q-learning ‣ 2.3 Curriculum Q-Learning
    ‣ 2 Methodology ‣ Can LLM be a Good Path Planner based on Prompt Engineering?
    Mitigating the Hallucination for Path Planning") describes the pseudocode of S2RCQL.
    This study first inputs the general description of the maze. Then, we translate
    the description into a Gym environment based on LLMs and Python. LLMs generate
    the two courses ($C_{1}$, representing the path plan policy.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Experiment Settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We performed numerous maze experiments based on Baidu’s LLM ERNIE-Bot 4.0.
    To verify the inhibitory effect of S2RQL on hallucination, we compare to algorithms
    based on prompt engineering included CoT, ToT, React, Q-learning, and Rememberer.
    Mazes of varying sizes were designed for the experiments, including 30 with $5\times
    5$ mazes. The baseline Prompt Engineering are as follows: Naive, CoT [[6](#bib.bib6)],
    ToT [[9](#bib.bib9)], ReAct [[15](#bib.bib15)], Rememberer [[17](#bib.bib17)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'We evaluate the effectiveness of LLMs in maze planning by two indicators:'
  prefs: []
  type: TYPE_NORMAL
- en: (1) Success Rate($\%$ represents the total number of runs.
  prefs: []
  type: TYPE_NORMAL
- en: (2) Optimality Rate($\%$ represents the number of optimal cases. Specifically,
    many multiple shortest paths are found in a MAZE. However, the length of the shortest
    path is unique as long as the length of the resulting path reaches the minimum
    value.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Main Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 1: The results for each model are in all mazes, where $(n)$ episodes.
    The best results are highlighted in Bold, and the best baseline models are underlined'
  prefs: []
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | $5\times 5$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Success | Optimality | Success | Optimality | Success | Optimality |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| naive prompt | 11.4% | 10.8% | 10.3% | 12.5% | 9.1% | 8.9% |'
  prefs: []
  type: TYPE_TB
- en: '| CoT [[6](#bib.bib6)] | 15.0% | 14.5% | 14.9% | 13.5% | 10.5% | 10.1% |'
  prefs: []
  type: TYPE_TB
- en: '| ToT [[9](#bib.bib9)] | 17.1% | 13.8% | 16.6% | 13.1% | 10.3% | 12.9% |'
  prefs: []
  type: TYPE_TB
- en: '| ReAct [[15](#bib.bib15)] | 17.4% | 22.8% | 16.1% | 21.6% | 15.4% | 20.7%
    |'
  prefs: []
  type: TYPE_TB
- en: '| $Rememberer_{(30)}$ [[17](#bib.bib17)] | 45.1% | 50.8% | 40.2% | 44.2% |
    34.8% | 35.7% |'
  prefs: []
  type: TYPE_TB
- en: '| $S2RCQL_{(30)}$ | 85.6% | 73.8% | 73.4% | 69.6% | 64.7% | 65.7% |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/7589486e977162df55f8e1c531a0189b.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) in 5$\times$5 maze
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/521d0bcba7dbf89541d422c15373e210.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) in 7$\times$7 maze
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/487cafc126e424854c223a0536cd5d5b.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) in 10$\times$10 maze
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d21db182384dd1feef4e6ef4d16033cd.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) in 5$\times$5 maze
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2b0e5712235a09e2c2f6823407566287.png)'
  prefs: []
  type: TYPE_IMG
- en: (e) in 7$\times$7 maze
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2d0f86e5f768bcd2ce38ddfaadbbf653.png)'
  prefs: []
  type: TYPE_IMG
- en: (f) in 10$\times$10 maze
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: Comparison of the performance of S2RCQL and Rememberer.'
  prefs: []
  type: TYPE_NORMAL
- en: Comparison of the performance of S2RCQL and Rememberer algorithms in mazes of
    different sizes. Table [1](#S3.T1 "Table 1 ‣ 3.2 Main Results ‣ 3 Experiments
    ‣ Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating the Hallucination
    for Path Planning") displays the main results of each method based on the Success
    and Optimal indicators. From the experimental results, our S2RCQL outperforms
    the Rememberer algorithm by 25%–40% in terms of Success Rate and 23%–30% in terms
    of Optimality Rate. We also compare to Rememberer along with the training episode,
    which is shown in Figure [5](#S3.F5 "Figure 5 ‣ 3.2 Main Results ‣ 3 Experiments
    ‣ Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating the Hallucination
    for Path Planning"). Furthermore, as the maze size increases, the Success Rate
    and Optimality Rate gradually decline because the maze size determines the search
    space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Enhancement of various prompt engineering with the aid of the S2R
    module.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | $5\times 5$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Success | Optimality | Success | Optimality | Success | Optimality |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| naive prompt | 11.4% | 10.8% | 10.3% | 12.5% | 9.1% | 8.9% |'
  prefs: []
  type: TYPE_TB
- en: '| naive prompt w/ S2R | 25.3% | 27.1% | 13.3% | 23.9% | 10.1% | 18.7% |'
  prefs: []
  type: TYPE_TB
- en: '| CoT | 15.0% | 14.5% | 14.9% | 13.5% | 10.5% | 10.1% |'
  prefs: []
  type: TYPE_TB
- en: '| CoT w/ S2R | 30.9% | 33.5% | 20.9% | 23.1% | 13.5% | 19.4% |'
  prefs: []
  type: TYPE_TB
- en: '| ToT | 17.1% | 13.8% | 16.6% | 13.1% | 10.3% | 12.9% |'
  prefs: []
  type: TYPE_TB
- en: '| ToT w/ S2R | 33.4% | 38.8% | 24.6% | 27.5% | 14.5% | 20.9% |'
  prefs: []
  type: TYPE_TB
- en: '| ReAct | 17.4% | 22.8% | 16.1% | 21.6% | 15.4% | 20.7% |'
  prefs: []
  type: TYPE_TB
- en: '| ReAct w/ S2R | 35.7% | 40.8% | 27.6% | 30.3% | 19.7% | 23.6% |'
  prefs: []
  type: TYPE_TB
- en: '| $Rememberer_{(30)}$ | 45.1% | 50.8% | 40.2% | 44.2% | 34.8% | 35.7% |'
  prefs: []
  type: TYPE_TB
- en: '| $Rememberer\ w/\ S2R_{(30)}$ | 61.9% | 59.6% | 55.6% | 47.9% | 47.5% | 41.1%
    |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/b97e90407c8bd8c9eb2a4cd40d19d754.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) in 5$\times$5 maze
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ce567c48c6af50dd2e9f6a8de436333d.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) in 7$\times$7 maze
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/91b62b092c82d2e38fe61baf130ef468.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) in 10$\times$10 maze
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/db5bbae27a8e9e3faabfda0c1477741c.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) in 5$\times$5 maze
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/48bb6373cc25b10001ce1ad628a3abca.png)'
  prefs: []
  type: TYPE_IMG
- en: (e) in 7$\times$7 maze
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5c0f71e3ece6808c7f0d378c92308b6b.png)'
  prefs: []
  type: TYPE_IMG
- en: (f) in 10$\times$10 maze
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Comparison of the effectiveness of the S2RCQL algorithm without course
    or S2R and under different curriculum generation schemes.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Ablation study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We applied the S2R module to various prompt engineering for verifying the generality
    and effectiveness of S2R. The experimental results are presented in Table[2](#S3.T2
    "Table 2 ‣ 3.2 Main Results ‣ 3 Experiments ‣ Can LLM be a Good Path Planner based
    on Prompt Engineering? Mitigating the Hallucination for Path Planning"). The results
    demonstrate that our S2R significantly improves both Success and Optimality rates
    across various algorithms by mitigating the LLM’s spatial hallucination. We remove
    the S2R module from S2RCQL. The results indicated a decline of approximately 15%
    in both the Success and Optimality rates, shown in Figure [6](#S3.F6 "Figure 6
    ‣ 3.2 Main Results ‣ 3 Experiments ‣ Can LLM be a Good Path Planner based on Prompt
    Engineering? Mitigating the Hallucination for Path Planning"). We removed the
    CL module from S2RCQL. The results indicate that the Success and Optimality rates
    decrease by approximately 20% when there is an absence of curriculum learning.
    As the maze size increases, the impact of CL becomes more significant. Furthermore,
    we conducted a comparison between hand-crafted and LLM-generated reverse curricula,
    as shown in Figure [6](#S3.F6 "Figure 6 ‣ 3.2 Main Results ‣ 3 Experiments ‣ Can
    LLM be a Good Path Planner based on Prompt Engineering? Mitigating the Hallucination
    for Path Planning"). The curriculum generated by LLMs demonstrates an improvement
    of approximately 10% compared with algorithms without curriculum learning.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/22b6cfbbb028cde099cecb219e959706.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Errors caused by various prompt engineering over long distances.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/656372252eed81d21c6046c6dfeadea7.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Errors with prompting caused by various prompt engineering over short distances.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d1de2e0f8c18e0e88fc20807753d17b7.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Errors with Rememberer w/ S2R caused by prompt engineering over long distances.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8ebaaeb67e5a15607ece7ba5711aa519.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) Corrections caused by S2RCQL prompt engineering over long distances.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7: Toy examples of the most common errors produced by each prompt engineering.
    We show the shortcomings of each method.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Toy Example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We conducted several case studies to intuitively demonstrate the execution
    process of LLMs, as shown in Figure [7](#S3.F7 "Figure 7 ‣ 3.3 Ablation study
    ‣ 3 Experiments ‣ Can LLM be a Good Path Planner based on Prompt Engineering?
    Mitigating the Hallucination for Path Planning"):'
  prefs: []
  type: TYPE_NORMAL
- en: (1) As demonstrated in Figure [7(a)](#S3.F7.sf1 "In 3.3 Ablation study ‣ 3 Experiments
    ‣ Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating the Hallucination
    for Path Planning"), the agent reaches (2,1) and (2,2) in the prompt engineering.
    After that, due to spatial hallucination, the direction of action is lost, resulting
    in an unsolvable result. As a result, the agent directly faces the obstacle, and
    the prompt declares that the obstacle cannot be entered. This indicates context
    inconsistency hallucination in the Path Planning problem.
  prefs: []
  type: TYPE_NORMAL
- en: (2) As demonstrated in Figure [7(b)](#S3.F7.sf2 "In 3.3 Ablation study ‣ 3 Experiments
    ‣ Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating the Hallucination
    for Path Planning"), when using the Rememberer model to navigate the maze, the
    shortest distance of this maze is small, and LLMs attempt to incorrectly navigate
    to position (1,2) at the point of the obstacle. However, when we use entity relations,
    a simple prompt is required to navigate to the destination, proving that entity
    relations can enhance LLMs’ space understanding and relieve the spatial hallucination
    in LLMs. In our experiment, we found that when navigating from the position (0,3)
    to the target position (1,0), LLMs can get (1,0) without error.
  prefs: []
  type: TYPE_NORMAL
- en: (3) As shown in Figure [7(c)](#S3.F7.sf3 "In 3.3 Ablation study ‣ 3 Experiments
    ‣ Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating the Hallucination
    for Path Planning"), we use Rememberer with S2R module. While the success rate
    is nearly 100% for short-distance navigation, the performance is still poor for
    long-distance navigation. In long-term reasoning, the Rememberer still mistakenly
    assumes the existence of a relation (J, O). This shows that S2R alone is not sufficient
    to address LLMs’ context inconsistency hallucination by long-term reasoning. As
    shown in Figure [7(d)](#S3.F7.sf4 "In Figure 7 ‣ 3.3 Ablation study ‣ 3 Experiments
    ‣ Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating the Hallucination
    for Path Planning"), we propose the S2RCQL model to solve this problem. LLMs can
    accurately navigate to positions they have previously reached (such as point S
    depicted in Figure [7(d)](#S3.F7.sf4 "In Figure 7 ‣ 3.3 Ablation study ‣ 3 Experiments
    ‣ Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating the Hallucination
    for Path Planning")). This to some extent alleviates the context inconsistency
    hallucination. By introducing curriculum learning and the S2R module, S2RCQL can
    better utilize historical experience and find solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This study proposes the S2RCQL algorithm to improve LLMs’ path planning ability
    by alleviating spatial hallucinations and context inconsistency hallucinations
    in LLMs. Through S2R, we automatically convert the maze described by coordinates
    into an entity relation graph structure. The proposed S2R exhibits generality,
    and its application to various prompt engineering yields significant improvements.
    In addition, we design a reverse curriculum generation based on LLMs and a curriculum
    Q-learning algorithm that significantly improves the success rate and optimality
    rate of the maze path planning task. In future work, we will research how to enhance
    LLMs’ reverse curriculum generation capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Imani S, Du L, Shrivastava H. MathPrompter: Mathematical Reasoning using
    Large Language Models. InICLR 2023 Workshop on Trustworthy and Reliable Large-Scale
    Machine Learning Models 2023 Apr 16.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Dorbala VS, Mullen Jr JF, Manocha D. Can an Embodied Agent Find Your “Cat-shaped
    Mug”? LLM-Based Zero-Shot Object Navigation. IEEE Robotics and Automation Letters.
    2023 Dec 25.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Piggott B, Patil S, Feng G, Odat I, Mukherjee R, Dharmalingam B, Liu A.
    Net-GPT: A LLM-Empowered Man-in-the-Middle Chatbot for Unmanned Aerial Vehicle.
    In2023 IEEE/ACM Symposium on Edge Computing (SEC) 2023 Dec 6 (pp. 287-293). IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Zhu X, Chen Y, Tian H, Tao C, Su W, Yang C, Huang G, Li B, Lu L, Wang X,
    Qiao Y. Ghost in the minecraft: Generally capable agents for open-world enviroments
    via large language models with text-based knowledge and memory. arXiv preprint
    arXiv:2305.17144\. 2023 May 25.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Ye S, Hwang H, Yang S, Yun H, Kim Y, Seo M. In-context instruction learning.
    arXiv e-prints. 2023 Feb:arXiv-2302.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Wei J, Wang X, Schuurmans D, Bosma M, Xia F, Chi E, Le QV, Zhou D. Chain-of-thought
    prompting elicits reasoning in large language models. Advances in neural information
    processing systems. 2022 Dec 6;35:24824-37.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Chu Z, Chen J, Chen Q, Yu W, He T, Wang H, Peng W, Liu M, Qin B, Liu T.
    A survey of chain of thought reasoning: Advances, frontiers and future. arXiv
    preprint arXiv:2309.15402\. 2023 Sep 27.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Feng G, Zhang B, Gu Y, Ye H, He D, Wang L. Towards revealing the mystery
    behind chain of thought: a theoretical perspective. Advances in Neural Information
    Processing Systems. 2024 Feb 13;36.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Yao S, Yu D, Zhao J, Shafran I, Griffiths T, Cao Y, Narasimhan K. Tree
    of thoughts: Deliberate problem solving with large language models. Advances in
    Neural Information Processing Systems. 2024 Feb 13;36.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Besta M, Blach N, Kubicek A, Gerstenberger R, Podstawski M, Gianinazzi
    L, Gajda J, Lehmann T, Niewiadomski H, Nyczyk P, Hoefler T. Graph of thoughts:
    Solving elaborate problems with large language models. In Proceedings of the AAAI
    Conference on Artificial Intelligence 2024 Mar 24 (Vol. 38, No. 16, pp. 17682-17690).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Yao Y, Li Z, Zhao H. Beyond chain-of-thought, effective graph-of-thought
    reasoning in large language models. arXiv preprint arXiv:2305.16582\. 2023 May
    26.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Wang X, Wei J, Schuurmans D, Le QV, Chi EH, Narang S, Chowdhery A, Zhou
    D. Self-Consistency Improves Chain of Thought Reasoning in Language Models. In
    The Eleventh International Conference on Learning Representations 2022 Sep 29.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Xiao Z, Zhang D, Wu Y, Xu L, Wang YJ, Han X, Fu X, Zhong T, Zeng J, Song
    M, Chen G. Chain-of-Experts: When LLMs Meet Complex Operations Research Problems.
    In The Twelfth International Conference on Learning Representations 2023 Oct 13.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Yao S, Zhao J, Yu D, Du N, Shafran I, Narasimhan K, Cao Y. ReAct: Synergizing
    Reasoning and Acting in Language Models. In International Conference on Learning
    Representations (ICLR) 2023 Jan.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Aghzal M, Plaku E, Yao Z. Can large language models be good path planners?
    a benchmark and investigation on spatial-temporal reasoning. arXiv preprint arXiv:2310.03249\.
    2023 Oct 5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Shinn N, Cassano F, Gopinath A, Narasimhan K, Yao S. Reflexion: Language
    agents with verbal reinforcement learning. Advances in Neural Information Processing
    Systems. 2024 Feb 13;36.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Zhang D, Chen L, Zhang S, Xu H, Zhao Z, Yu K. Large Language Models Are
    Semi-Parametric Reinforcement Learning Agents. Advances in Neural Information
    Processing Systems. 2024 Feb 13;36.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Carta T, Romac C, Wolf T, Lamprier S, Sigaud O, Oudeyer PY. Grounding
    large language models in interactive environments with on-policy reinforcement
    learning. c In International Conference on Machine Learning 2023 Jul 3 (pp. 3676-3713).
    PMLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Yang S, Gribovskaya E, Kassner N, Geva M, Riedel S. Do Large Language
    Models Latently Perform Multi-Hop Reasoning?. arXiv preprint arXiv:2402.16837\.
    2024 Feb 26.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Zhao Z, Lee WS, Hsu D. Large language models as commonsense knowledge
    for large-scale task planning. Advances in Neural Information Processing Systems.
    2024 Feb 13;36.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Brockman G, Cheung V, Pettersson L, Schneider J, Schulman J, Tang J, Zaremba
    W. Openai gym. arXiv preprint arXiv:1606.01540\. 2016 Jun 5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Florensa C, Held D, Wulfmeier M, Zhang M, Abbeel P. Reverse curriculum
    generation for reinforcement learning. InConference on robot learning 2017 Oct
    18 (pp. 482-495). PMLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
