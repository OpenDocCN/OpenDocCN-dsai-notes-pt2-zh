- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:50:34'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant
    Abilities with Linguistically-Diverse Prompts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2306.11372](https://ar5iv.labs.arxiv.org/html/2306.11372)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Xuan-Phi Nguyen¹ , Sharifah Mahani Aljunied¹ , Shafiq Joty² & Lidong Bing¹
  prefs: []
  type: TYPE_NORMAL
- en: ¹DAMO Academy, Alibaba Group
  prefs: []
  type: TYPE_NORMAL
- en: '²Nanyang Technological University, Singapore Corresponding author: x.nguyen@alibaba-inc.com'
  prefs: []
  type: TYPE_NORMAL
- en: Tóm tắt nội dung
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Large language models (LLMs) are known to effectively perform tasks by simply
    observing few exemplars. However, in low-resource languages, obtaining such hand-picked
    exemplars can still be challenging, where unsupervised techniques may be necessary.
    Moreover, competent generative capabilities of LLMs are observed only in high-resource
    languages, while their performances among under-represented languages fall behind
    due to pre-training data imbalance. To elicit LLMs’ ability onto low-resource
    languages without any supervised data, we propose to assemble synthetic exemplars
    from a diverse set of high-resource languages to prompt the LLMs to translate
    from any language into English. These prompts are then used to create intra-lingual
    exemplars to perform tasks in the target languages. Our unsupervised prompting
    method performs on par with supervised few-shot learning in LLMs of different
    sizes for translations between English and 13 Indic and 21 African low-resource
    languages. We also show that fine-tuning a 7B model on data generated from our
    method helps it perform competitively with a 175B model. In non-English translation
    tasks, our method even outperforms supervised prompting by up to 3 chrF++ in many
    low-resource languages. When evaluated on zero-shot multilingual summarization,
    our method surpasses other English-pivoting baselines by up to 4 ROUGE-L and is
    also favored by GPT-4.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recent scaling effort in foundation large language models (Brown et al., [2020](#bib.bib3);
    Chowdhery et al., [2022](#bib.bib4); Scao et al., [2022](#bib.bib33); Touvron
    et al., [2023](#bib.bib39)) with massive pre-training data has enabled them to
    learn a broad range of natural language tasks through few-shot in-context learning
    - where a few input-output exemplars are shown as context prepended to the test
    input to prompt the model to predict the target answer with impressive qualities
    without any gradient update. While most LLMs were pre-trained with multilingual
    corpora in addition to the gigantic English corpus, and were shown to demonstrate
    impressive abilities in other languages (Brown et al., [2020](#bib.bib3); Chowdhery
    et al., [2022](#bib.bib4); Scao et al., [2022](#bib.bib33); Shi et al., [2022](#bib.bib35);
    Huang et al., [2023](#bib.bib17)), they only excel in high-resource languages,
    such as French. Further, they may still require pivoting the inputs into English,
    that is, performing tasks in English before reverting the response back to native
    outputs (Shi et al., [2022](#bib.bib35); Huang et al., [2023](#bib.bib17)). Improving
    LLMs abilities in extremely low-resource languages can be even more challenging,
    particularly where the data coverage is less than 0.0001% (Scao et al., [2022](#bib.bib33))
    or none at all (Touvron et al., [2023](#bib.bib39)). We also found that the models
    may confusely generate the wrong language and struggle to process low-resource
    non-latin scripts due to fragmented tokenization, where short texts are broken
    into extremely long byte-level tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="S1.F1.pic1" class="ltx_picture ltx_centering ltx_figure_panel" height="276.14"
    overflow="visible" version="1.1" width="175.67"><g transform="translate(0,276.14)
    matrix(1 0 0 -1 0 0) translate(23.9,0) translate(0,256.18)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -10.39 -3.34)" fill="#000000"
    stroke="#000000"><foreignobject width="20.78" height="12.22" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$\mathcal{L}_{\rightarrow en}$</foreignobject></g></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -7.39 -200.31)" fill="#000000" stroke="#000000"><foreignobject
    width="157.13" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">English:
    Machine learning</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 -7.39
    -221.96)" fill="#000000" stroke="#000000"><foreignobject width="30.36" height="12.3"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Igbo:</foreignobject><foreignobject
    width="63.42" height="12.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Ịmụ
    igwe</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 -7.39 -245.59)" fill="#000000"
    stroke="#000000"><foreignobject width="150.02" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">✓language, ✓translation</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hình 1: (Left) $\mathcal{L}_{\rightarrow en}$.'
  prefs: []
  type: TYPE_NORMAL
- en: In this work, we focus on unsupervised, and zero-shot, generative translation
    and summarization tasks in low-resource languages, where no supervised few-shot
    prompts are used. We focus only on foundation multilingual LLMs (Scao et al.,
    [2022](#bib.bib33); Touvron et al., [2023](#bib.bib39)) to maximally avoid leakage
    of human-annotated data inherent in instruction-tuned models (Ouyang et al., [2022](#bib.bib29)).
    To this end, in recognition of LLMs’ dominant abilities in English and some evidence
    that in-context exemplars primarily help the model locate the task (Xie et al.,
    [2021](#bib.bib44)), we propose Linguistically-Diverse Prompting (LDP), a technique
    that promotes the models to locate the task of “translate any language $X$En exemplars
    from a diverse set of high-resource languages using off-the-shelf unsupervised
    MT models (Tran et al., [2020](#bib.bib40)). To ensure disversity, languages with
    script types ranging from Latin (Fr) to Arabic (Ar) and Chinese (Zh) characters
    are used. An example of the method is shown in [Figure 1](#S1.F1 "In 1 Introduction
    ‣ Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant
    Abilities with Linguistically-Diverse Prompts").
  prefs: []
  type: TYPE_NORMAL
- en: Our method is shown to translate any low-resource language into English with
    quality on par with supervised prompting, which allows us to build intra-lingual
    exemplars with unlabeled data to prompt the models to translate into low-resource
    languages. In our experiments with BLOOM (Scao et al., [2022](#bib.bib33)) and
    InstructGPT (text-davinci-003) (Ouyang et al., [2022](#bib.bib29)), our unsupervised
    LDP method performs on par with supervised prompting in X$\rightarrow$Y non-English
    directions even outperforms supervised promptings by up to 3 chrF++ in pairs involving
    low-resource languages. In multilingual summarization tasks (Narayan et al., [2018](#bib.bib25)),
    our zero-shot LDP method outperforms both basic prompting and other English-pivoting
    methods by up to 4 ROUGE-L and is generally favored by GPT-4-EVAL (Liu et al.,
    [2023](#bib.bib21)), which shows good human alignment.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Large language models (LLMs) display outstanding capabilities because they
    are massively large and are pre-trained on massive amounts of internet text data
    (Radford et al., [2019](#bib.bib32); Brown et al., [2020](#bib.bib3); Chowdhery
    et al., [2022](#bib.bib4); Scao et al., [2022](#bib.bib33); Touvron et al., [2023](#bib.bib39)).
    Without any gradient update, foundation LLMs are able to perform in-context few-shot
    learning by simply providing the models with a prompt comprising a list of high-quality
    input-output exemplars before appending the actual test input (Brown et al., [2020](#bib.bib3);
    Wei et al., [2023](#bib.bib42)). This technique works across a broad range of
    tasks, from natural language understanding to reasoning (Brown et al., [2020](#bib.bib3);
    Wei et al., [2022](#bib.bib41); Shi et al., [2022](#bib.bib35)). Much research
    have been done to understand in-context learning. Some suggest that the models
    secretly perform gradient descent on the exemplars (Dai et al., [2022](#bib.bib8)).
    Others demonstrate that most of the knowledge is learned during pre-training,
    and in-context exemplars $x$ are only to provide evidence for the model to marginalize
    and locate the intended task via a bayesian inference process as follows (Xie
    et al., [2021](#bib.bib44); Min et al., [2022](#bib.bib23); Zhou et al., [2023](#bib.bib46)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $p(y&#124;\text{x})=\int_{\text{task}}p(y&#124;\text{task,x})p(\text{task}&#124;\text{x})d(\text{task})$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: <svg id="S2.F2.pic1" class="ltx_picture ltx_centering" height="83.33" overflow="visible"
    version="1.1" width="567.8"><g transform="translate(0,83.33) matrix(1 0 0 -1 0
    0) translate(29.79,0) translate(0,0.28) matrix(1.0 0.0 0.0 1.0 -29.79 -0.28)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g class="ltx_nestedsvg"
    transform="matrix(1 0 0 1 0 0) translate(54.24,0) translate(0,73.01)"><g stroke="#000000"
    fill="#000000" stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -49.63
    -63.89)" fill="#000000" stroke="#000000"><foreignobject width="20.29" height="11.41"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$10^{-4}$</foreignobject></g><g
    fill="#FFFFFF" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 0.18 -23.9)"><g
    class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 8.61)"><g class="ltx_tikzmatrix_row"
    transform="matrix(1 0 0 1 0 8.61)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 13.01 0) translate(25.08,0) matrix(1.0 0.0 0.0 1.0
    -22.31 -3.77)" fill="#000000" stroke="#000000"><foreignobject width="44.62" height="9.61"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">African</foreignobject></g><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    76.17 0) translate(17.95,0) matrix(1.0 0.0 0.0 1.0 -15.18 -3.77)" fill="#000000"
    stroke="#000000"><foreignobject width="30.36" height="9.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Indic</foreignobject></g></g></g></g></g></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'Hình 2: Low-resource language coverage % of ROOTS corpus (Laurençon et al.,
    [2022](#bib.bib19)) used to train BLOOM (Scao et al., [2022](#bib.bib33)). The
    highest-resource language for Indic and African are Hindi and Swahili. Hindi accounts
    for $0.7$% of the corpus.'
  prefs: []
  type: TYPE_NORMAL
- en: Most large language models are trained with multilingual corpora (Wenzek et al.,
    [2020](#bib.bib43)), even if these make up a tiny fraction of the largely English
    corpora (Radford et al., [2019](#bib.bib32); Brown et al., [2020](#bib.bib3)).
    Despite that, LLMs still exhibit strong capabilities in multilingual setups with
    high-resource languages like De and Zh, often with the help of English-pivoting
    using supervised translation systems (Shi et al., [2022](#bib.bib35)) or prompting
    the model to firstly generate intermediate English text before arriving at the
    answer (Huang et al., [2023](#bib.bib17)). The most multilingual LLM is BLOOM
    (Scao et al., [2022](#bib.bib33)), which was trained on 46 languages in the ROOTS
    corpus (Laurençon et al., [2022](#bib.bib19)). This corpus includes 34 Indic and
    African languages regarded as low-resource, with each language having a pre-training
    coverage of less than 1% in Hindi for the Indic group, to $2e^{-5}$% in Tumbuka
    for the African group, as shown in [Figure 2](#S2.F2 "In 2 Related Work ‣ Democratizing
    LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities
    with Linguistically-Diverse Prompts"). Interestingly, some common high-resource
    languages, such as Russian, were not used in the training of BLOOM, making it
    a good subject to study unseen languages. Therefore, we use BLOOM as the main
    model to evaluate our methods and baselines in such 34 low-resource languages.
    Our linguistically-diverse prompting strategy is also an English-pivoting method,
    but it is different from other cross-lingual counterparts (Shi et al., [2022](#bib.bib35);
    Huang et al., [2023](#bib.bib17)) in that while others only pivot inputs to English
    intermediates, we use in-context pairs between English and a diverse set of high-resource
    languages to promote the intended task in the target language.
  prefs: []
  type: TYPE_NORMAL
- en: Our work also intersects with unsupervised multilingual machine translation
    (UMT), where iterative back-translation is proven to be effective (Sennrich et al.,
    [2016](#bib.bib34); Edunov et al., [2018](#bib.bib9); Lample et al., [2018](#bib.bib18);
    Conneau & Lample, [2019](#bib.bib5); Liu et al., [2020](#bib.bib22); Nguyen et al.,
    [2022b](#bib.bib27)), along with other techniques such as bi-text mining (Tran
    et al., [2020](#bib.bib40); Nguyen et al., [2022a](#bib.bib26)). English-pivoting
    is also prominent in the realm of machine translation, where training models on
    high-resource En$\leftrightarrow$Y tasks (Garcia et al., [2020](#bib.bib11); [2021](#bib.bib12)).
    Nonetheless, the noteworthy gap between existing UMT and LLMs is that their language
    coverages do not overlap much, preventing us from using UMT models to enhance
    LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Analyses of machine translation using LLMs have also been done. Hendy et al.
    ([2023](#bib.bib15)) show that GPT models can perform competitively alongside
    state-of-the-art MT models. Zhu et al. ([2023](#bib.bib47)) focus on optimizing
    supervised exemplars selection and searching strategies. Sia & Duh ([2023](#bib.bib36))
    discover that using specific coherent prompts for each input helps improve performance.
    Nonetheless, such work only study supervised instruction-tuned models (Ouyang
    et al., [2022](#bib.bib29); Muennighoff et al., [2022](#bib.bib24)), which may
    risk test-set contamination. Thus, there is still limited research involving low-resource
    languages in completely zero-shot setups. As such, since low-resource languages
    may not enjoy the privilege of having large unlabeled data to conduct searching,
    only random selection is used in this study, while optimal exemplar selection
    is not within the current scope.
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S2.F3.sf1.pic1" class="ltx_picture ltx_centering ltx_figure_panel"
    height="126.54" overflow="visible" version="1.1" width="87.17"><g transform="translate(0,126.54)
    matrix(1 0 0 -1 0 0) translate(23.9,0) translate(0,106.58)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -9.12 -4.54)" fill="#000000"
    stroke="#000000"><foreignobject width="18.25" height="14.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$\mathcal{L}^{mt}_{\text{x}\rightarrow\text{en}}$]</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -9.12 -92.04)" fill="#000000" stroke="#000000"><foreignobject
    width="14.99" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">[x]</foreignobject><foreignobject
    width="36.51" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">[en][y]</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: (a) LDP for translation for $X$.
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S2.F3.sf2.pic1" class="ltx_picture ltx_centering ltx_figure_panel"
    height="126.54" overflow="visible" version="1.1" width="95.04"><g transform="translate(0,126.54)
    matrix(1 0 0 -1 0 0) translate(23.9,0) translate(0,106.58)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -14.07 -4.73)" fill="#000000"
    stroke="#000000"><foreignobject width="28.13" height="12.79" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$\mathcal{L}^{sum}_{\text{x}}$]</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -13.59 -92.04)" fill="#000000" stroke="#000000"><foreignobject
    width="19.46" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">[d${}_{\text{x}}$]</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: (b) LDP for summarization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hình 3: Compact illustrations of adopting LDP for $X$$Y$].'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Linguistically-Diverse Prompting (LDP)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our linguistically-diverse prompting (LDP) method is inspired from three intuitive
    assumptions. (i) The first one, which has been theoretically and empirically supported,
    is that LLMs have already learned most of the knowledge and task concepts implicitly
    during pre-training, and that in-context exemplars play a larger role in providing
    evidence for the models to identify and marginalize over the probability of the
    intended task (Xie et al., [2021](#bib.bib44); Min et al., [2022](#bib.bib23);
    Zhou et al., [2023](#bib.bib46)).
  prefs: []
  type: TYPE_NORMAL
- en: (ii) The second assumption is that the models intuitively learn to perform language
    encoding and understanding at an earlier time, before learning to generate language.
    This means that, rather like human, the models may be able to comprehend any language
    with reasonable competency, and only struggle to generate the intended language.
    In other words, generative abilities are improved later on when more data is seen.
  prefs: []
  type: TYPE_NORMAL
- en: '(iii) The third assumption is that LLMs can already exhibit near-human generative
    abilities in the dominant language $E$ are no longer symmetric and can be interpreted
    more broadly as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $X\rightarrow E$, it can effortlessly perform any NLU task.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $E\rightarrow X$ can involve different techniques.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Combining the three assumptions, we design in-context exemplars so that the
    model locates the task of “translate from any language $X$ equivalents (*e.g.,*English),
    which can be translated using existing multilingual unsupervised MT models (Tran
    et al., [2020](#bib.bib40); Nguyen et al., [2022b](#bib.bib27)).
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 1](#S1.F1 "In 1 Introduction ‣ Democratizing LLMs for Low-Resource
    Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse
    Prompts") illustrates how LDP works. In the example on the left, we use synthetic
    pairs from diverse high-resource languages as in-context exemplars to prompt the
    models to translate the target low-resource language $X$ with much higher quality.
    This is because the target-side prompt distribution is now realistic and consistently
    close to the true target distribution we expect the model to generate, which has
    been shown to be crucial for in-context learning to work (Xie et al., [2021](#bib.bib44)).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 LDP for Translation Tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We adopt LDP in translation tasks for $X\rightarrow E$ for better comprehensibility.
  prefs: []
  type: TYPE_NORMAL
- en: $X\rightarrow E$ task.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As mentioned above, we first gather $n$ by conditioning the LDP prompts as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}^{mt}_{X\rightarrow E}(s_{X})\sim p_{\theta}(y&#124;s_{X},s_{Z_{1}},t^{\scriptstyle
    1}_{E},..,s_{Z_{n}},t^{\scriptstyle n}_{E})$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: $E\rightarrow X$ task.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We leverage $\mathcal{L}^{mt}_{X\rightarrow E}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: The intra-lingual exemplars with consistent language in the target side helps
    the model locate the intended language to generate more effectively than a standard
    language tag, as these exemplars show the model what the intended language looks
    like. In addition, we can also use $\mathcal{L}^{mtbt}$ LDP by using native language
    tags.
  prefs: []
  type: TYPE_NORMAL
- en: $X\rightarrow Y$ task.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We leverage $\mathcal{L}^{mtbt}_{X\rightarrow E}$ is computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: Unsupervised fine-tuning.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The ability to generate synthetic $X$ allows us to create larger-scale back-translation
    data from unlabeled corpora to fine-tune the LLM model for translation tasks without
    any in-context prompt at inference time. Specifically, we use a generic [input]<lang-tag>[output]
    template to construct multilingual training samples with the generated synthetic
    data pairs from multiple low-resource languages. During training, we only compute
    loss on the [output] part to train the model to generate the right language. While
    it may be tempting to use parameter-efficient fine-tuning (PEFT) approaches, such
    as LoRA (Hu et al., [2021](#bib.bib16)), we empirically found that the model fails
    to learn to generate the low-resource languages unless we increase the learnable
    parameter counts significantly, which seems to defeat the purpose of using PEFT.
    Instead, we propose to directly fine-tune the query-key-value linear weights of
    all attention layers, which account for 20-30% of the total parameters to avoid
    memory issues and overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 LDP for Multilingual Summarization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For multilingual summarization tasks with instruction-tuned models (Ouyang
    et al., [2022](#bib.bib29)), we extend XLT (Huang et al., [2023](#bib.bib17)),
    a recent English-pivoting cross-lingual prompting technique, with document-summarization
    pairs from diverse high-resource non-English languages. This technique is illustrated
    in [Figure 3(b)](#S2.F3.sf2 "In Figure 3 ‣ 2 Related Work ‣ Democratizing LLMs
    for Low-Resource Languages by Leveraging their English Dominant Abilities with
    Linguistically-Diverse Prompts"). Formally, given an input document $d_{X}$ as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{X}^{sum}(d_{X})\sim p_{\theta}(y&#124;d_{X},d_{Z_{1}},s_{Z_{1}},..,d_{Z_{n}},s_{Z_{n}})$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: 'Similar to $E\rightarrow X$ as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{\mathcal{L}}_{X}^{sum}(d_{X})\sim p_{\theta}(y&#124;d_{X},d^{1}_{X},s^{1}_{X},..,d^{m}_{X},s^{m}_{X})$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we evaluate our method in various translation and summarization
    tasks which include translation between English and low-resource languages ([4.1](#S4.SS1
    "4.1 Low-resource ↔ English Translation ‣ 4 Experiments ‣ Democratizing LLMs for
    Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse
    Prompts")), non-English-centric translation ([4.2](#S4.SS2 "4.2 Non-English-centric
    Translation ‣ 4 Experiments ‣ Democratizing LLMs for Low-Resource Languages by
    Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts")),
    higher-resource translation with LLaMA (Touvron et al., [2023](#bib.bib39)) ([4.3](#S4.SS3
    "4.3 Translation with LLaMA ‣ 4 Experiments ‣ Democratizing LLMs for Low-Resource
    Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse
    Prompts")) and multilingual summarization ([4.4](#S4.SS4 "4.4 Zero-shot Multilingual
    Summarization ‣ 4 Experiments ‣ Democratizing LLMs for Low-Resource Languages
    by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts")).
    We also conduct extensive analyses to provide further insight into our method
    ([4.5](#S4.SS5 "4.5 Ablation Study ‣ 4 Experiments ‣ Democratizing LLMs for Low-Resource
    Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse
    Prompts")).
  prefs: []
  type: TYPE_NORMAL
- en: 'Bảng 1: Averaged performances of different prompting techniques across various
    model sizes and types, namely BLOOM (Scao et al., [2022](#bib.bib33)) and InstructGPT
    text-davinci-003 (Brown et al., [2020](#bib.bib3); Ouyang et al., [2022](#bib.bib29)),
    in translation tasks between English (En) and 13 Indic (Indic13) and 21 African
    (Afri21) low-resource languages present in the ROOTS corpus (Laurençon et al.,
    [2022](#bib.bib19)).'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Indic13-En | En-Indic13 | Afri21-En | En-Afri21 |'
  prefs: []
  type: TYPE_TB
- en: '|  | chrF++ | BLEU | chrF++ | BLEU | chrF++ | BLEU | chrF++ | BLEU |'
  prefs: []
  type: TYPE_TB
- en: '| Foundation BLOOM-175B |'
  prefs: []
  type: TYPE_TB
- en: '| Supervised-8-shot | 47.31 | 22.32 | 34.66 | 9.02 | 28.64 | 8.35 | 14.93 |
    2.00 |'
  prefs: []
  type: TYPE_TB
- en: '| Unsupervised-LDP | 47.62 | 22.38 | 34.54 | 8.88 | 28.72 | 8.71 | 14.57 |
    1.89 |'
  prefs: []
  type: TYPE_TB
- en: '| Foundation BLOOM-7B1 |'
  prefs: []
  type: TYPE_TB
- en: '| Supervised-8-shot | 39.86 | 14.77 | 24.02 | 4.42 | 21.51 | 4.33 | 11.27 |
    0.59 |'
  prefs: []
  type: TYPE_TB
- en: '| Unsupervised-LDP | 39.88 | 14.96 | 24.41 | 4.52 | 20.47 | 3.65 | 12.04 |
    0.62 |'
  prefs: []
  type: TYPE_TB
- en: '| Fine-tune QKV (2B params) | 42.19 | 17.13 | 32.72 | 8.33 | 21.14 | 5.15 |
    15.73 | 2.13 |'
  prefs: []
  type: TYPE_TB
- en: '| Supervised RLHF InstructGPT (text-davinci-003) |'
  prefs: []
  type: TYPE_TB
- en: '| Zero-shot with instruction | 35.37 | 11.48 | 20.71 | 3.88 | 27.10 | 8.04
    | 15.45 | 1.13 |'
  prefs: []
  type: TYPE_TB
- en: '| Supervised-6-shot | 37.07 | 13.13 | 24.74 | 5.21 | 31.51 | 10.88 | 19.22
    | 2.66 |'
  prefs: []
  type: TYPE_TB
- en: '| Unsupervised-LDP | 38.45 | 14.22 | 25.17 | 5.06 | 31.92 | 11.12 | 19.51 |
    2.61 |'
  prefs: []
  type: TYPE_TB
- en: '| Supervised upperbound |'
  prefs: []
  type: TYPE_TB
- en: '| NLLB-200 distilled | 61.00 | 37.24 | 46.77 | 18.78 | 48.42 | 26.92 | 39.18
    | 12.95 |'
  prefs: []
  type: TYPE_TB
- en: 4.1 Low-resource $\leftrightarrow$ English Translation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As the ROOTS corpus (Laurençon et al., [2022](#bib.bib19)) that BLOOM (Scao
    et al., [2022](#bib.bib33)) was pre-trained on offers the most diverse language
    coverage with open-sourced transparency, we tested our methods mainly with the
    BLOOM model on 13 Indic (Indic13) languages and 21 African (Afri21) languages
    present in the ROOTS corpus. We also conduct experiments with supervised instruction-tuned
    InstructGPT (text-davinci-003) (Brown et al., [2020](#bib.bib3); Ouyang et al.,
    [2022](#bib.bib29)) to provide further references. We stress that, to our knowledge,
    it is not disclosed how large text-davinci-003 is or whether it was trained on
    the test sets. As such, its results are only to compare different prompting techniques
    within the InstructGPT section. For each of the 68 language pairs, we sample randomly
    and evaluate 200 sentences from each test set with the same seed to limit the
    cost budget for API calls¹¹1BLOOM: [huggingface.co/bigscience/bloom](https://huggingface.co/bigscience/bloom).
    GPT: [platform.openai.com](https://platform.openai.com). We perform full-set evaluations
    for 4 representative languages in each group and observe $<1$BLEU standard deviation
    from our 200-sampled evaluations.. Following Costa-jussà et al. ([2022](#bib.bib7)),
    we report results in mainly chrF++ score (Popović, [2015](#bib.bib30)), which
    is a universal metric for all languages without any tokenization, while also reporting
    SacreBLEU (Post, [2018](#bib.bib31)) as a complementary metric. The full list
    of languages and other details are provided in the Appendix.'
  prefs: []
  type: TYPE_NORMAL
- en: In terms of methodologies, for supervised prompting, we collect as many supervised
    pairs as the models can fit within their context lengths (8 for BLOOM and 6 for
    GPT davinci-003). We use <src>[input]\n<tgt>[output] as the prompt template, where
    <src> and <tgt> are the language tag names (in English) of the respective languages.
    For our unsupervised linguistically-diverse prompting (LDP) method, we use 4 LDP
    $Z_{i}$ to generate synthetic training data from various unlabeled sources (Wenzek
    et al., [2020](#bib.bib43)) and fine-tune BLOOM-7B1 on the query-key-value weight
    matrices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bảng 2: chrF++ translation scores for X to Y non-english directions across
    high-high, high-low and low-low languages groups.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | High-High | High-Low | Low-Low |'
  prefs: []
  type: TYPE_TB
- en: '|  | Vi-Fr | Fr-Vi | Zh-Ne | Ne-Zh | Es-Pa | Pa-Es | Ta-Sw | Sw-Ta | Te-Sw
    | Sw-Te |'
  prefs: []
  type: TYPE_TB
- en: '| Foundation BLOOM-175B |'
  prefs: []
  type: TYPE_TB
- en: '| Supervised-8-shot | 52.17 | 51.50 | 30.91 | 17.83 | 25.67 | 37.71 | 31.45
    | 31.81 | 31.46 | 25.84 |'
  prefs: []
  type: TYPE_TB
- en: '| Unsupervised-LDP | 52.66 | 50.24 | 31.61 | 18.34 | 27.85 | 39.51 | 34.61
    | 34.47 | 32.14 | 30.57 |'
  prefs: []
  type: TYPE_TB
- en: '| Supervised InstructGPT (text-davinci-003) |'
  prefs: []
  type: TYPE_TB
- en: '| XLT (Huang et al., [2023](#bib.bib17)) | 51.16 | 44.84 | 28.56 | 13.26 |
    23.61 | 34.18 | 24.20 | 25.46 | 24.89 | 23.48 |'
  prefs: []
  type: TYPE_TB
- en: '| Unsupervised-LDP | 51.19 | 45.80 | 28.67 | 15.80 | 25.40 | 35.02 | 27.24
    | 27.70 | 28.95 | 25.12 |'
  prefs: []
  type: TYPE_TB
- en: '[Table 1](#S4.T1 "In 4 Experiments ‣ Democratizing LLMs for Low-Resource Languages
    by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts")
    shows the averaged chrF++ and BLEU scores for translations between English and
    13 Indic and 21 African low-resource languages across different prompting techniques
    with BLOOM-175B, BLOOM-7B1 and GPT text-davinci-003 models. The first noticeable
    finding is that our unsupervised-LDP method performs on par with supervised prompting
    across all language groups and LLM models. This indicates that the synthetic prompts
    generated by our $\mathcal{L}^{mt}_{X\rightarrow E}$ direction. This suggests
    that fine-tuning the model on more low-resource language data improves generative
    abilities in such languages. For GPT text-davinci-003, we observe the same pattern
    when comparing supervised and unsupervised-LDP. Further, it is interesting to
    see that GPT’s scores for Indic languages are lower than BLOOM but higher for
    African languages, despite the fact that the African languages are likely to have
    less data coverage. We suspect, with evidence in the Appendix, that this is because
    the GPT tokenizer favors Latin-based sub-words in the African languages more than
    the non-Latin characters of the Indic languages, as reflected by the high degree
    of sub-word fragmentation. For instance, a 10-token English text can be equivalent
    to 160 tokens in Tamil.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Non-English-centric Translation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For non-English $X$ with supervised prompting in three categories: High-High
    resource languages with Vi and Fr, High-Low resource between Zh, Es, Ne (Nepali)
    and Pa (Punjabi), and Low-Low resource languages with Sw (Swahili), Ta (Tamil)
    and Te (Telugu). We use the same model and evaluation pipelines as explained [Section 4.1](#S4.SS1
    "4.1 Low-resource ↔ English Translation ‣ 4 Experiments ‣ Democratizing LLMs for
    Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse
    Prompts"). For this experiment, we evaluate on the devtest sets provided by Costa-jussà
    et al. ([2022](#bib.bib7)).'
  prefs: []
  type: TYPE_NORMAL
- en: As reported in [Table 2](#S4.T2 "In 4.1 Low-resource ↔ English Translation ‣
    4 Experiments ‣ Democratizing LLMs for Low-Resource Languages by Leveraging their
    English Dominant Abilities with Linguistically-Diverse Prompts"), our unsupervised
    LDP technique also performs on par with supervised prompting in High-High Vi-Fr
    pairs. More interestingly, for High-Low and Low-Low language pairs, our unsupervised
    method even outperforms supervised prompting for these languages by up to 5 chrF++,
    largely thanks to the presence of English intermediate translations in the exemplars.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Translation with LLaMA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Bảng 3: Comparison between supervised and unsupervised-LDP prompting with LLaMA-30B
    model in translation tasks between English (En) and 19 European languages (X).
    LDP prompts consist of exemplars from high-resource languages seen by CRISS.'
  prefs: []
  type: TYPE_NORMAL
- en: '| LLaMA-30B | X$\rightarrow$X |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| chrF++ | BLEU | chrF++ | BLEU |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Supervised | 61.80 | 39.51 | 53.65 | 28.98 |'
  prefs: []
  type: TYPE_TB
- en: '| Unsupervised-LDP | 61.75 | 38.83 | 54.00 | 29.58 |'
  prefs: []
  type: TYPE_TB
- en: LLaMA (Touvron et al., [2023](#bib.bib39)) is another open-sourced LLM that
    only supports 20 European high-resource languages. In this section, we evaluate
    LLaMA in translation tasks between English and the remaining 19 languages, which
    include Hungarian, Danish and Catalan. Specifically, we use CRISS to generate
    synthetic LDP exemplars from De, Es and Fr, which we then use to prompt LLaMA
    to translate from and to such languages. As reported in [Table 3](#S4.T3 "In 4.3
    Translation with LLaMA ‣ 4 Experiments ‣ Democratizing LLMs for Low-Resource Languages
    by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts"),
    we observe similar trends where our LDP method performs competitively with supervised
    prompting. The overall scores for such languages are also much higher than those
    of non-Latin languages because LLaMA was also pre-trained with bitexts, though
    without explicit alignments.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Zero-shot Multilingual Summarization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Bảng 4: ROUGE-L / GPT-4-EVAL scores (1-5 ratings) of different prompting techniques
    using InstructGPT text-davinci-003 for zero-shot summarization in high-resource
    (Es, Vi, Id) and low-resource (Sw, So, Mr) in the Extreme-summarization (X-sum)
    task (Narayan et al., [2018](#bib.bib25)).'
  prefs: []
  type: TYPE_NORMAL
- en: '| davinci-003 | Es | Vi | Id | Sw | So | Mr |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Basic | 12.7/2.99 | 12.6/2.77 | 12.8/2.55 | 12.2/2.33 | 11.5/3.05 | 4.1/2.98
    |'
  prefs: []
  type: TYPE_TB
- en: '| XLT | 17.7/3.90 | 14.8/3.76 | 17.6/3.40 | 20.5/3.11 | 18.5/3.96 | 10.3/3.84
    |'
  prefs: []
  type: TYPE_TB
- en: '| LDP | 18.1/4.11 | 17.4/3.76 | 18.6/3.58 | 21.8/3.32 | 19.0/3.98 | 10.0/3.89
    |'
  prefs: []
  type: TYPE_TB
- en: '| LDP+Unlabeled | 18.1/4.16 | 17.0/3.82 | 24.8/3.82 | 23.5/3.25 | 19.3/4.00
    | 11.4/3.90 |'
  prefs: []
  type: TYPE_TB
- en: We extend our LDP method to multilingual summarization task by combining LDP
    with cross-lingual prompting (XLT) (Huang et al., [2023](#bib.bib17)) using instruction-tuned
    GPT text-davinci-003 model. We follow the LDP adoptions for summarization with
    (LDP + Unlabeled or $\hat{\mathcal{L}}^{sum}$) unlabeled data, as described in
    [Section 3.3](#S3.SS3 "3.3 LDP for Multilingual Summarization ‣ 3 Method ‣ Democratizing
    LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities
    with Linguistically-Diverse Prompts"). We conduct evaluation on the Extreme Summarization
    benchmark (Narayan et al., [2018](#bib.bib25)) in both high-resource (Es, Vi,
    Id-Indonesian) and low-resource (Sw, So-Somali, Mr-Marathi) languages. To avoid
    exceeding the model context length, we sample 100 documents with less than 1500
    characters for each test set and obtain only 1 in-context exemplar via LDP. We
    evaluate the models with ROUGE-L (Lin, [2004](#bib.bib20)) and GPT-4-EVAL (Liu
    et al., [2023](#bib.bib21)), which is GPT-4 based metric that recently scores
    best in human judgement alignment. We compare our LDP and LDP+Unlabeled methods
    with XLT, and basic instruction. XLT is a recent English-pivoting instruction
    proposed by Huang et al. ([2023](#bib.bib17)). As shown in [Table 4](#S4.T4 "In
    4.4 Zero-shot Multilingual Summarization ‣ 4 Experiments ‣ Democratizing LLMs
    for Low-Resource Languages by Leveraging their English Dominant Abilities with
    Linguistically-Diverse Prompts"), our LDP methods outperforms standard XLT across
    all languages by up to 7 ROUGE-L and exceeds basic prompting by large margins.
    Our methods are also consistently preferred by GPT-4-EVAL with higher ratings.
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S4.F4.pic1" class="ltx_picture ltx_centering" height="158.02" overflow="visible"
    version="1.1" width="561.13"><g transform="translate(0,158.02) matrix(1 0 0 -1
    0 0) translate(43.6,0) translate(0,29.96)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -43.6 -29.19)"><g class="ltx_nestedsvg" transform="matrix(1
    0 0 1 0 0) translate(65.77,0) translate(0,29.19)"><g stroke="#000000" fill="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -40.9 38.12)" fill="#000000"
    stroke="#000000"><foreignobject width="13.84" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$20$</foreignobject></g><g fill="#FFFFFF" stroke="#000000"
    transform="matrix(1.0 0.0 0.0 1.0 269.16 -144.65)"><g class="ltx_tikzmatrix" transform="matrix(1
    0 0 -1 0 8.995)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 8.99)"><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    24.18 0) translate(62.38,0) matrix(1.0 0.0 0.0 1.0 -59.61 -4.15)" fill="#000000"
    stroke="#000000"><foreignobject width="119.23" height="13.07" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">% pre-training data</foreignobject></g></g></g></g></g></g></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'Hình 4: chrF++ scores for translation from English to each Indic and African
    language in the ROOTS corpus (En$\rightarrow$), using BLOOM. The right y-axis
    indicates corresponding pre-training coverage of each language at log scale.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/29164fcfcdfae1d5cd9be71a5b861d41.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) LDP without back-translation $\mathcal{L}^{mt}_{\text{En}\rightarrow X}$.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d739498c493b9fc64b0cf8d7882d1772.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) LDP with back-translation $\mathcal{L}^{mtbt}_{\text{En}\rightarrow X}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hình 5: Analysis on whether the BLOOM model generates the right language for
    En$\rightarrow$ task using LDP without ([5(a)](#S4.F5.sf1 "Figure 5(a) ‣ Figure
    5 ‣ 4.4 Zero-shot Multilingual Summarization ‣ 4 Experiments ‣ Democratizing LLMs
    for Low-Resource Languages by Leveraging their English Dominant Abilities with
    Linguistically-Diverse Prompts")) and with ([5(b)](#S4.F5.sf2 "Figure 5(b) ‣ Figure
    5 ‣ 4.4 Zero-shot Multilingual Summarization ‣ 4 Experiments ‣ Democratizing LLMs
    for Low-Resource Languages by Leveraging their English Dominant Abilities with
    Linguistically-Diverse Prompts")) intra-lingual back-translation prompts. Each
    column indicates the language the model generates into while each row represents
    the language it is supposed to generate. ## Indicate other languages.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Ablation Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we conduct various analysis experiments to provide a deeper
    understanding of our LDP method and the importance of each component.
  prefs: []
  type: TYPE_NORMAL
- en: Breakdown of Language Pairs.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[Figure 4](#S4.F4 "In 4.4 Zero-shot Multilingual Summarization ‣ 4 Experiments
    ‣ Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant
    Abilities with Linguistically-Diverse Prompts") shows the breakdown of chrF++
    performances between supervised and unsupervised-LDP promptings for each of the
    34 low-resource languages. We observe that LDP performs generally on par with
    supervised prompting equally across all languages, and that it does not unevenly
    perform much worse or better in any particular language. More information on performance
    breakdown are shown in the Appendix.'
  prefs: []
  type: TYPE_NORMAL
- en: Generating the Right Language.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[Figure 5(a)](#S4.F5.sf1 "In Figure 5 ‣ 4.4 Zero-shot Multilingual Summarization
    ‣ 4 Experiments ‣ Democratizing LLMs for Low-Resource Languages by Leveraging
    their English Dominant Abilities with Linguistically-Diverse Prompts") reveals
    one reason the models struggle to translate En$\rightarrow$, is more important
    in getting the models to recognize language rather than the language tag. In fact,
    we found that removing the language tag entirely can help improve the performance
    slightly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Bảng 5: Impact of English tag, native language tags and no language tag for
    in-context prompts in Indic languages with the BLOOM model.'
  prefs: []
  type: TYPE_NORMAL
- en: '| BLOOM | Indic13-En | En-Indic13 |'
  prefs: []
  type: TYPE_TB
- en: '| chrF++ | BLEU | chrF++ | BLEU |'
  prefs: []
  type: TYPE_TB
- en: '| Supervised |'
  prefs: []
  type: TYPE_TB
- en: '| En-tag | 47.31 | 22.32 | 34.66 | 9.02 |'
  prefs: []
  type: TYPE_TB
- en: '| Unsupervised LDP |'
  prefs: []
  type: TYPE_TB
- en: '| En-tag | 46.96 | 21.99 | 22.53 | 5.02 |'
  prefs: []
  type: TYPE_TB
- en: '| En-tag + BT | 47.43 | 22.30 | 34.41 | 8.89 |'
  prefs: []
  type: TYPE_TB
- en: '| Native-tag | 46.90 | 21.82 | 29.80 | 7.02 |'
  prefs: []
  type: TYPE_TB
- en: '| Native-tag + BT | 47.52 | 22.39 | 35.22 | 9.44 |'
  prefs: []
  type: TYPE_TB
- en: '| No-tag | 46.81 | 21.92 | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| No-tag + BT | 47.62 | 22.38 | 34.54 | 8.88 |'
  prefs: []
  type: TYPE_TB
- en: Impact of Native Language Tag.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The reason why we need unlabeled text to create intra-lingual prompts for En$\rightarrow$
    tasks significantly, compared to using English tags. This method even approaches
    the performance of 8-shot supervised prompting and LDP with unlabeled BT prompts.
    Combining it with back-translation data (Native-tag + BT) even helps it outperform
    supervised prompting. In effect, the English tag may confuse the model to an extent
    that not using the language tag at all (*e.g.,*using “Input:[input]\nOutput:[output]”)
    does not hurt the performances.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bảng 6: Impact of different choices of LDP high-resource languages on $X$).
    Results are averages across 10 Indic languages excluding Ta, Bn and Hi (Indic10).
    Note that the LDP exemplars in this table are collected from supervised datasets
    for analysis purpose.'
  prefs: []
  type: TYPE_NORMAL
- en: '| BLOOM | Indic10-En | En-Indic10 |'
  prefs: []
  type: TYPE_TB
- en: '| chrF++ | BLEU | chrF++ | BLEU |'
  prefs: []
  type: TYPE_TB
- en: '| Supervised | 46.32 | 21.63 | 32.44 | 7.66 |'
  prefs: []
  type: TYPE_TB
- en: '| LDP (without BT) with different $\mathcal{Z}=$ |'
  prefs: []
  type: TYPE_TB
- en: '| Ar,Zh,Vi,Fr (default) | 45.53 | 20.90 | 17.65 | 3.13 |'
  prefs: []
  type: TYPE_TB
- en: '| Hi,Hi,Hi,Hi (Hindi) | 43.27 | 18.02 | 15.34 | 1.72 |'
  prefs: []
  type: TYPE_TB
- en: '| Ta,Bn,Hi (Indic) | 45.51 | 20.82 | 16.25 | 2.97 |'
  prefs: []
  type: TYPE_TB
- en: '| Fr,Es,Pt (European) | 45.31 | 20.52 | 18.98 | 3.22 |'
  prefs: []
  type: TYPE_TB
- en: '| Vi,Vi,Vi,Vi | 44.91 | 20.31 | 12.94 | 2.15 |'
  prefs: []
  type: TYPE_TB
- en: '| Zh,Zh,Zh,Zh | 44.71 | 20.41 | 15.78 | 2.88 |'
  prefs: []
  type: TYPE_TB
- en: '| Ar,Fr,Es,Pt,Vi,Zh,Id | 45.50 | 20.43 | 16.88 | 3.32 |'
  prefs: []
  type: TYPE_TB
- en: Choice of LDP languages.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another necessary question to ask is which high-resource languages should be
    selected as LDP exemplars. [Table 6](#S4.T6 "In Impact of Native Language Tag.
    ‣ 4.5 Ablation Study ‣ 4 Experiments ‣ Democratizing LLMs for Low-Resource Languages
    by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts")
    examines which LDP language choice is optimal. As shown, for 10 Indic low-resource
    languages, choosing a single related language (Hindi), which is often called cross-lingual
    prompting (Zhang et al., [2023](#bib.bib45); Zhu et al., [2023](#bib.bib47)),
    can be disastrous as the model tends to translate the prompt language rather than
    the test language. Choosing a single but distant language (Vi or Zh) yields better
    results, while choosing a wide variety of languages across different regions (*e.g.,* Ar,Zh,Vi,Fr)
    may be the optimal choice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bảng 7: chrF++ comparisons between unsupervised LDP prompting with BLOOM and
    unsupervised MT CRISS (Tran et al., [2020](#bib.bib40)) for En and Gu, Ne and
    Hi'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Gu-En | Ne-En | Hi-En |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\rightarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| CRISS | 41.88 | 32.41 | 37.64 | 28.17 | 51.23 | 42.29 |'
  prefs: []
  type: TYPE_TB
- en: '| BLOOM Prompting |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Supervised | 51.63 | 38.23 | 47.07 | 35.91 | 55.18 | 44.94 |'
  prefs: []
  type: TYPE_TB
- en: '| Unsupervised-LDP | 50.09 | 37.63 | 48.26 | 35.76 | 55.71 | 45.36 |'
  prefs: []
  type: TYPE_TB
- en: Comparison with Unsupervised MT.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We also compare our method against the specialized unsupervised MT model CRISS
    (Tran et al., [2020](#bib.bib40)) on eligible languages (Gu, Ne, Hi). As shown
    in [Table 7](#S4.T7 "In Choice of LDP languages. ‣ 4.5 Ablation Study ‣ 4 Experiments
    ‣ Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant
    Abilities with Linguistically-Diverse Prompts"), unsupervised LDP prompting with
    BLOOM significantly outperforms CRISS across all languages, thanks to its larger
    size and strong English abilities.
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S4.F6.pic1" class="ltx_picture ltx_centering" height="143.65" overflow="visible"
    version="1.1" width="311.6"><g transform="translate(0,143.65) matrix(1 0 0 -1
    0 0) translate(43.6,0) translate(0,16.84) matrix(1.0 0.0 0.0 1.0 -43.6 -16.84)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g class="ltx_nestedsvg"
    transform="matrix(1 0 0 1 0 0) translate(65.91,0) translate(0,16.84)"><g stroke="#000000"
    fill="#000000" stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -8.62
    -12.23)" fill="#000000" stroke="#000000"><foreignobject width="17.24" height="8.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$3e^{7}$Indic13</foreignobject></g></g></g></g></g></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'Hình 6: Performance gains compared to LDP prompting by fine-tuning BLOOM-7B1
    using LoRA (Hu et al., [2021](#bib.bib16)) with various numbers of trainable parameters
    (x-axis) for Indic13$\rightarrow$Indic13 translation tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning Trainable Parameters.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[Figure 6](#S4.F6 "In Comparison with Unsupervised MT. ‣ 4.5 Ablation Study
    ‣ 4 Experiments ‣ Democratizing LLMs for Low-Resource Languages by Leveraging
    their English Dominant Abilities with Linguistically-Diverse Prompts") analyzes
    how LoRA-fine-tuned BLOOM-7B1 models (Hu et al., [2021](#bib.bib16)) perform in
    $X$ task, suggesting that learning to generate an unfamiliar language needs much
    more parameters, rendering parameter-efficient methods ineffective.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We introduce linguistically-diverse prompting (LDP), which is designed to use
    synthetic high-quality in-context exemplars from high-resource languages to prompt
    LLMs to perform generative tasks, such as translation and summarization, in low-resource
    languages. Our unsupervised approach achieves on par with supervised few-shot
    learning while using zero supervision in English to and from 34 low-resource Indic
    and African translation tasks, even outperforming supervised prompting in non-English-centric
    directions. Our method also outperforms other English-pivoting techniques in multilingual
    summarization.
  prefs: []
  type: TYPE_NORMAL
- en: Tài liệu
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Adelani et al. (2022) David Adelani, Jesujoba Alabi, Angela Fan, Julia Kreutzer,
    Xiaoyu Shen, Machel Reid, Dana Ruiter, Dietrich Klakow, Peter Nabende, Ernie Chang,
    Tajuddeen Gwadabe, Freshia Sackey, Bonaventure F. P. Dossou, Chris Emezue, Colin
    Leong, Michael Beukman, Shamsuddeen Muhammad, Guyo Jarso, Oreen Yousuf, Andre
    Niyongabo Rubungo, Gilles Hacheme, Eric Peter Wairagala, Muhammad Umair Nasir,
    Benjamin Ajibade, Tunde Ajayi, Yvonne Gitau, Jade Abbott, Mohamed Ahmed, Millicent
    Ochieng, Anuoluwapo Aremu, Perez Ogayo, Jonathan Mukiibi, Fatoumata Ouoba Kabore,
    Godson Kalipe, Derguene Mbaye, Allahsera Auguste Tapo, Victoire Memdjokam Koagne,
    Edwin Munkoh-Buabeng, Valencia Wagner, Idris Abdulmumin, Ayodele Awokoya, Happy
    Buzaaba, Blessing Sibanda, Andiswa Bukula, and Sam Manthalu. A few thousand translations
    go a long way! leveraging pre-trained models for African news translation. In
    *Proceedings of the 2022 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pp.  3053–3070, Seattle,
    United States, July 2022\. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.223.
    URL [https://aclanthology.org/2022.naacl-main.223](https://aclanthology.org/2022.naacl-main.223).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Barrault et al. (2020) Loïc Barrault, Magdalena Biesialska, Ondřej Bojar, Marta R.
    Costa-jussà, Christian Federmann, Yvette Graham, Roman Grundkiewicz, Barry Haddow,
    Matthias Huck, Eric Joanis, Tom Kocmi, Philipp Koehn, Chi-kiu Lo, Nikola Ljubešić,
    Christof Monz, Makoto Morishita, Masaaki Nagata, Toshiaki Nakazawa, Santanu Pal,
    Matt Post, and Marcos Zampieri. Findings of the 2020 conference on machine translation
    (WMT20). In *Proceedings of the Fifth Conference on Machine Translation*, pp. 
    1–55, Online, November 2020\. Association for Computational Linguistics. URL [https://aclanthology.org/2020.wmt-1.1](https://aclanthology.org/2020.wmt-1.1).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877–1901, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. *arXiv
    preprint arXiv:2204.02311*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conneau & Lample (2019) Alexis Conneau and Guillaume Lample. Cross-lingual language
    model pretraining. In H. Wallach, H. Larochelle, A. Beygelzimer, F. dÁlché-Buc,
    E. Fox, and R. Garnett (eds.), *Advances in Neural Information Processing Systems
    32*, pp.  7059–7069\. Curran Associates, Inc., 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Conneau et al. (2020) Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav
    Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer,
    and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale.
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pp.  8440–8451, Online, July 2020\. Association for Computational
    Linguistics. doi: 10.18653/v1/2020.acl-main.747. URL [https://aclanthology.org/2020.acl-main.747](https://aclanthology.org/2020.acl-main.747).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Costa-jussà et al. (2022) Marta R Costa-jussà, James Cross, Onur Çelebi, Maha
    Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel
    Licht, Jean Maillard, et al. No language left behind: Scaling human-centered machine
    translation. *arXiv preprint arXiv:2207.04672*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dai et al. (2022) Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, and
    Furu Wei. Why can gpt learn in-context? language models secretly perform gradient
    descent as meta optimizers. *arXiv preprint arXiv:2212.10559*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Edunov et al. (2018) Sergey Edunov, Myle Ott, Michael Auli, and David Grangier.
    Understanding back-translation at scale. In *Proceedings of the 2018 Conference
    on Empirical Methods in Natural Language Processing*, pp.  489–500, Brussels,
    Belgium, October-November 2018\. Association for Computational Linguistics. doi:
    10.18653/v1/D18-1045. URL [https://www.aclweb.org/anthology/D18-1045](https://www.aclweb.org/anthology/D18-1045).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Emezue & Dossou (2021) Chris Chinenye Emezue and Bonaventure F. P. Dossou.
    MMTAfrica: Multilingual machine translation for African languages. In *Proceedings
    of the Sixth Conference on Machine Translation*, pp.  398–411, Online, November
    2021\. Association for Computational Linguistics. URL [https://aclanthology.org/2021.wmt-1.48](https://aclanthology.org/2021.wmt-1.48).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Garcia et al. (2020) Xavier Garcia, Pierre Foret, Thibault Sellam, and Ankur
    Parikh. A multilingual view of unsupervised machine translation. In *Findings
    of the Association for Computational Linguistics: EMNLP 2020*, pp.  3160–3170,
    Online, November 2020\. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.283.
    URL [https://aclanthology.org/2020.findings-emnlp.283](https://aclanthology.org/2020.findings-emnlp.283).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Garcia et al. (2021) Xavier Garcia, Aditya Siddhant, Orhan Firat, and Ankur
    Parikh. Harnessing multilinguality in unsupervised machine translation for rare
    languages. In *Proceedings of the 2021 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies*,
    pp.  1126–1137, Online, June 2021\. Association for Computational Linguistics.
    doi: 10.18653/v1/2021.naacl-main.89. URL [https://aclanthology.org/2021.naacl-main.89](https://aclanthology.org/2021.naacl-main.89).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goyal et al. (2022) Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen,
    Guillaume Wenzek, Da Ju, Sanjana Krishnan, Marc’Aurelio Ranzato, Francisco Guzmán,
    and Angela Fan. The Flores-101 evaluation benchmark for low-resource and multilingual
    machine translation. *Transactions of the Association for Computational Linguistics*,
    10:522–538, 2022. doi: 10.1162/tacl˙a˙00474. URL [https://aclanthology.org/2022.tacl-1.30](https://aclanthology.org/2022.tacl-1.30).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guzmán et al. (2019) Francisco Guzmán, Peng-Jen Chen, Myle Ott, Juan Pino,
    Guillaume Lample, Philipp Koehn, Vishrav Chaudhary, and Marc’Aurelio Ranzato.
    The FLORES evaluation datasets for low-resource machine translation: Nepali–English
    and Sinhala–English. In *Proceedings of the 2019 Conference on Empirical Methods
    in Natural Language Processing and the 9th International Joint Conference on Natural
    Language Processing (EMNLP-IJCNLP)*, pp.  6098–6111, Hong Kong, China, November
    2019\. Association for Computational Linguistics. doi: 10.18653/v1/D19-1632. URL
    [https://www.aclweb.org/anthology/D19-1632](https://www.aclweb.org/anthology/D19-1632).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendy et al. (2023) Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak,
    Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan
    Awadalla. How good are gpt models at machine translation? a comprehensive evaluation.
    *arXiv preprint arXiv:2302.09210*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of
    large language models. *arXiv preprint arXiv:2106.09685*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2023) Haoyang Huang, Tianyi Tang, Dongdong Zhang, Wayne Xin Zhao,
    Ting Song, Yan Xia, and Furu Wei. Not all languages are created equal in llms:
    Improving multilingual capability by cross-lingual-thought prompting. *arXiv preprint
    arXiv:2305.07004*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lample et al. (2018) Guillaume Lample, Myle Ott, Alexis Conneau, Ludovic Denoyer,
    and Marc’Aurelio Ranzato. Phrase-based & neural unsupervised machine translation.
    In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing*, pp.  5039–5049, Brussels, Belgium, October-November 2018\. Association
    for Computational Linguistics. doi: 10.18653/v1/D18-1549. URL [https://www.aclweb.org/anthology/D18-1549](https://www.aclweb.org/anthology/D18-1549).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Laurençon et al. (2022) Hugo Laurençon, Lucile Saulnier, Thomas Wang, Christopher
    Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao
    Mou, Eduardo González Ponferrada, Huu Nguyen, Jörg Frohberg, Mario Šaško, Quentin
    Lhoest, Angelina McMillan-Major, Gérard Dupont, Stella Biderman, Anna Rogers,
    Loubna Ben allal, Francesco De Toni, Giada Pistilli, Olivier Nguyen, Somaieh Nikpoor,
    Maraim Masoud, Pierre Colombo, Javier de la Rosa, Paulo Villegas, Tristan Thrush,
    Shayne Longpre, Sebastian Nagel, Leon Weber, Manuel Romero Muñoz, Jian Zhu, Daniel Van
    Strien, Zaid Alyafeai, Khalid Almubarak, Vu Minh Chien, Itziar Gonzalez-Dios,
    Aitor Soroa, Kyle Lo, Manan Dey, Pedro Ortiz Suarez, Aaron Gokaslan, Shamik Bose,
    David Ifeoluwa Adelani, Long Phan, Hieu Tran, Ian Yu, Suhas Pai, Jenny Chim, Violette
    Lepercq, Suzana Ilic, Margaret Mitchell, Sasha Luccioni, and Yacine Jernite. The
    bigscience ROOTS corpus: A 1.6TB composite multilingual dataset. In *Thirty-sixth
    Conference on Neural Information Processing Systems Datasets and Benchmarks Track*,
    2022. URL [https://openreview.net/forum?id=UoEw6KigkUn](https://openreview.net/forum?id=UoEw6KigkUn).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin (2004) Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries.
    In *Text summarization branches out*, pp.  74–81, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023) Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu,
    and Chenguang Zhu. G-eval: Nlg evaluation using gpt-4 with better human alignment.
    *arXiv 2303.16634*, March 2023. URL [https://www.microsoft.com/en-us/research/publication/gpteval-nlg-evaluation-using-gpt-4-with-better-human-alignment/](https://www.microsoft.com/en-us/research/publication/gpteval-nlg-evaluation-using-gpt-4-with-better-human-alignment/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020) Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov,
    Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. Multilingual denoising
    pre-training for neural machine translation. *arXiv preprint arXiv:2001.08210*,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Min et al. (2022) Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis,
    Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations:
    What makes in-context learning work? *arXiv preprint arXiv:2202.12837*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Muennighoff et al. (2022) Niklas Muennighoff, Thomas Wang, Lintang Sutawika,
    Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin
    Yong, Hailey Schoelkopf, et al. Crosslingual generalization through multitask
    finetuning. *arXiv preprint arXiv:2211.01786*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Narayan et al. (2018) Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don’t
    give me the details, just the summary! topic-aware convolutional neural networks
    for extreme summarization. *ArXiv*, abs/1808.08745, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nguyen et al. (2022a) Xuan-Phi Nguyen, Hongyu Gong, Yun Tang, Changhan Wang,
    Philipp Koehn, and Shafiq Joty. Contrastive clustering to mine pseudo parallel
    data for unsupervised translation. In *International Conference on Learning Representations*,
    2022a. URL [https://openreview.net/forum?id=pN1JOdrSY9](https://openreview.net/forum?id=pN1JOdrSY9).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nguyen et al. (2022b) Xuan-Phi Nguyen, Shafiq Joty, Kui Wu, and AiTi Aw. Refining
    low-resource unsupervised translation by language disentanglement of multilingual
    translation model. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun
    Cho (eds.), *Advances in Neural Information Processing Systems*, 2022b. URL [https://openreview.net/forum?id=eCUeRHHupF](https://openreview.net/forum?id=eCUeRHHupF).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. Gpt-4 technical report. *arXiv preprint*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. Training language models to follow instructions with human feedback.
    *Advances in Neural Information Processing Systems*, 35:27730–27744, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Popović (2015) Maja Popović. chrf: character n-gram f-score for automatic mt
    evaluation. In *Proceedings of the tenth workshop on statistical machine translation*,
    pp.  392–395, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Post (2018) Matt Post. A call for clarity in reporting BLEU scores. In *Proceedings
    of the Third Conference on Machine Translation: Research Papers*, pp.  186–191,
    Brussels, Belgium, October 2018\. Association for Computational Linguistics. doi:
    10.18653/v1/W18-6319. URL [https://www.aclweb.org/anthology/W18-6319](https://www.aclweb.org/anthology/W18-6319).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners.
    *OpenAI Blog*, 1(8):9, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scao et al. (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick,
    Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François
    Yvon, Matthias Gallé, et al. Bloom: A 176b-parameter open-access multilingual
    language model. *arXiv preprint arXiv:2211.05100*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sennrich et al. (2016) Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving
    neural machine translation models with monolingual data. In *Proceedings of the
    54th Annual Meeting of the Association for Computational Linguistics (Volume 1:
    Long Papers)*, pp.  86–96, Berlin, Germany, August 2016\. Association for Computational
    Linguistics. doi: 10.18653/v1/P16-1009. URL [https://www.aclweb.org/anthology/P16-1009](https://www.aclweb.org/anthology/P16-1009).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. (2022) Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj
    Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou,
    et al. Language models are multilingual chain-of-thought reasoners. *arXiv preprint
    arXiv:2210.03057*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sia & Duh (2023) Suzanna Sia and Kevin Duh. In-context learning as maintaining
    coherency: A study of on-the-fly machine translation using large language models.
    *arXiv preprint arXiv:2305.03573*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Socher et al. (2013) Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,
    Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models
    for semantic compositionality over a sentiment treebank. In *Proceedings of the
    2013 Conference on Empirical Methods in Natural Language Processing*, pp.  1631–1642,
    Seattle, Washington, USA, October 2013\. Association for Computational Linguistics.
    URL [https://www.aclweb.org/anthology/D13-1170](https://www.aclweb.org/anthology/D13-1170).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tang et al. (2020) Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal,
    Vishrav Chaudhary, Jiatao Gu, and Angela Fan. Multilingual translation with extensible
    multilingual pretraining and finetuning. *arXiv preprint arXiv:2008.00401*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
    Guillaume Lample. Llama: Open and efficient foundation language models. *arXiv
    preprint arXiv:2302.13971*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tran et al. (2020) Chau Tran, Yuqing Tang, Xian Li, and Jiatao Gu. Cross-lingual
    retrieval for iterative self-supervised training. In H. Larochelle, M. Ranzato,
    R. Hadsell, M. F. Balcan, and H. Lin (eds.), *Advances in Neural Information Processing
    Systems*, volume 33, pp.  2207–2219\. Curran Associates, Inc., 2020. URL [https://proceedings.neurips.cc/paper/2020/file/1763ea5a7e72dd7ee64073c2dda7a7a8-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/1763ea5a7e72dd7ee64073c2dda7a7a8-Paper.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi,
    Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large
    language models. *arXiv preprint arXiv:2201.11903*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2023) Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson,
    Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, et al. Larger language
    models do in-context learning differently. *arXiv preprint arXiv:2303.03846*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wenzek et al. (2020) Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau,
    Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave. CCNet:
    Extracting high quality monolingual datasets from web crawl data. In *Proceedings
    of the 12th Language Resources and Evaluation Conference*, pp.  4003–4012, Marseille,
    France, May 2020\. European Language Resources Association. ISBN 979-10-95546-34-4.
    URL [https://www.aclweb.org/anthology/2020.lrec-1.494](https://www.aclweb.org/anthology/2020.lrec-1.494).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie et al. (2021) Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu
    Ma. An explanation of in-context learning as implicit bayesian inference. *arXiv
    preprint arXiv:2111.02080*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023) Biao Zhang, Barry Haddow, and Alexandra Birch. Prompting
    large language model for machine translation: A case study. *arXiv preprint arXiv:2301.07069*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2023) Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun,
    Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more
    for alignment. *arXiv preprint arXiv:2305.11206*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2023) Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Lingpeng
    Kong, Jiajun Chen, Lei Li, and Shujian Huang. Multilingual machine translation
    with large language models: Empirical results and analysis. *arXiv preprint arXiv:2304.04675*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Phụ lục A Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Low-resource Language Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Table 8](#A1.T8 "In A.1 Low-resource Language Details ‣ Phụ lục A Appendix
    ‣ Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant
    Abilities with Linguistically-Diverse Prompts") lists the details of each low-resource
    language in the ROOTS corpus (Laurençon et al., [2022](#bib.bib19)) that we mainly
    evaluate with the BLOOM model (Scao et al., [2022](#bib.bib33)). Regarding test
    sets, we primarily choose from the ML50 benchmark (Tang et al., [2020](#bib.bib38)),
    which collected test data from various sources, such as WMT (Barrault et al.,
    [2020](#bib.bib2)) and FLoRes (Guzmán et al., [2019](#bib.bib14); Goyal et al.,
    [2022](#bib.bib13)). For languages absent in ML50, we choose the NLLB-devtest
    sets (Costa-jussà et al., [2022](#bib.bib7)) as replacement. For non-English $X$
    tasks, we choose NLLB-devtest for all our evaluation. To limit the API call costs
    within our budget, we randomly the same 200 samples from each test set for evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Bảng 8: Low-resource language details and corresponding test sets and unlabeled
    data sources for $X$En translation tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Indic | African |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Name | Code | Test | Unlabeled | Name | Code | Test set | Unlabeled |'
  prefs: []
  type: TYPE_TB
- en: '| Assamese | as | NLLB | CC100 | Tumbuka | –/tum | NLLB | OUR |'
  prefs: []
  type: TYPE_TB
- en: '| Oriya | or | NLLB | ROOTS | Kikuyu | ki/kik | NLLB | OUR |'
  prefs: []
  type: TYPE_TB
- en: '| Gujarati | gu | ML50 | CC100 | Bambara | bm/bam | NLLB | MAFAND |'
  prefs: []
  type: TYPE_TB
- en: '| Marathi | mr | ML50 | CC100 | Akan | ak/aka | NLLB | OUR |'
  prefs: []
  type: TYPE_TB
- en: '| Panjabi | pa | NLLB | CC100 | Tsonga | ts/tso | NLLB | MMTAfrica |'
  prefs: []
  type: TYPE_TB
- en: '| Kannada | kn | NLLB | CC100 | Southern Sotho | st/sot | NLLB | OUR |'
  prefs: []
  type: TYPE_TB
- en: '| Nepali | ne | ML50 | CC100 | Chewa | ny/nya | NLLB | MMTAfrica |'
  prefs: []
  type: TYPE_TB
- en: '| Telugu | te | ML50 | CC100 | Tswana | tn/tsn | NLLB | MMTAfrica |'
  prefs: []
  type: TYPE_TB
- en: '| Malayalam | ml | ML50 | CC100 | Lingala | ln/lin | NLLB | MMTAfrica |'
  prefs: []
  type: TYPE_TB
- en: '| Urdu | ur | NLLB | CC100 | Northern Sotho | –/nso | NLLB | MMTAfrica |'
  prefs: []
  type: TYPE_TB
- en: '| Tamil | ta | ML50 | CC100 | Fon | –/fon | NLLB | MAFAND |'
  prefs: []
  type: TYPE_TB
- en: '| Bengali | bn | NLLB | CC100 | Rundi | rn/run | NLLB | OUR |'
  prefs: []
  type: TYPE_TB
- en: '| Hindi | hi | ML50 | CC100 | Wolof | wo/wol | NLLB | CC100 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | CC100 | Luganda | lg/lug | NLLB | CC100 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | CC100 | Shona | sn/sna | NLLB | CC100 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | CC100 | Zulu | zu/zul | NLLB | CC100 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | CC100 | Igbo | ig/ibo | NLLB | CC100 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | CC100 | Xhosa | xh/xho | NLLB | CC100 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | CC100 | Kinyarwanda | rw/kin | NLLB | MMTAfrica |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | CC100 | Yoruba | yo/yor | NLLB | CC100 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | CC100 | Swahili | sw/swa | NLLB | CC100 |'
  prefs: []
  type: TYPE_TB
- en: A.2 Experiment Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Few-shot data sources.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For supervised prompting, we collect randomly parallel pairs from the respective
    valid set for each language. For unlabeled data for our LDP method, we collect
    and filter data from various sources, as specified in Unlabeled column of [Table 8](#A1.T8
    "In A.1 Low-resource Language Details ‣ Phụ lục A Appendix ‣ Democratizing LLMs
    for Low-Resource Languages by Leveraging their English Dominant Abilities with
    Linguistically-Diverse Prompts"). Specifically, the primary unlabeled source is
    the CC100 corpus (Wenzek et al., [2020](#bib.bib43); Conneau et al., [2020](#bib.bib6)).
    For those absent in CC100, we collect data from other sources, such as the ROOTS
    corpus (Laurençon et al., [2022](#bib.bib19)), MMTAfrica (Emezue & Dossou, [2021](#bib.bib10))
    and MAFAND (Adelani et al., [2022](#bib.bib1)). For the remaining languages where
    we could not find in research repositories, we crawled from several religious
    and news websites (OUR). The sizes of collected unlabeled texts vary greatly,
    ranging from a few millions lines for Hindi to less than 1000 lines for Bambara,
    thus presenting a challenge for data balancing. For LDP non-English high-resource
    exemplars, we randomly collect a single high-quality sentence of similar lengths
    from the CC100 corpus for each language and use the unsupervised CRISS model (Tran
    et al., [2020](#bib.bib40)) to translate them into English.
  prefs: []
  type: TYPE_NORMAL
- en: Unlabeled data filtering
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To ensure high-quality native texts for unsupervised LDP prompting as well as
    larger-scale synthetic data creation for fine-tuning, we filter unlabeled texts
    such that they (i) are within 20 to 200 character lengths, (ii) do not contain
    non-conversational artifacts like URLs, brackets, bullet points or excessive numbers,
    and (iii) do not contain more than 20% alphabetical characters for Indic and non-latin
    characters for African languages. For fine-tuning, we use an upscaling temperature
    of 25 to smoothen the data mixture imbalance.
  prefs: []
  type: TYPE_NORMAL
- en: Other Details.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We evaluate translation tasks with chrF++ (Popović, [2015](#bib.bib30)) and
    SacreBLEU (Post, [2018](#bib.bib31)). For SacreBLEU, we use the default tokenizer
    for Latin-based languages, while follow Guzmán et al. ([2019](#bib.bib14)); Goyal
    et al. ([2022](#bib.bib13)) to use [indic_nlp_library](https://github.com/anoopkunchukuttan/indic_nlp_library)
    for Indic language tokenization.
  prefs: []
  type: TYPE_NORMAL
- en: A.3 Additional Experiments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <svg id="A1.F7.pic1" class="ltx_picture ltx_centering" height="158.02" overflow="visible"
    version="1.1" width="561.13"><g transform="translate(0,158.02) matrix(1 0 0 -1
    0 0) translate(43.6,0) translate(0,29.96)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -43.6 -29.19)"><g class="ltx_nestedsvg" transform="matrix(1
    0 0 1 0 0) translate(65.77,0) translate(0,29.19)"><g stroke="#000000" fill="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -40.9 1.43)" fill="#000000"
    stroke="#000000"><foreignobject width="13.84" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$20$</foreignobject></g><g fill="#FFFFFF" stroke="#000000"
    transform="matrix(1.0 0.0 0.0 1.0 269.16 -144.65)"><g class="ltx_tikzmatrix" transform="matrix(1
    0 0 -1 0 8.995)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 8.99)"><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    24.18 0) translate(62.38,0) matrix(1.0 0.0 0.0 1.0 -59.61 -4.15)" fill="#000000"
    stroke="#000000"><foreignobject width="119.23" height="13.07" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">% pre-training data</foreignobject></g></g></g></g></g></g></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'Hình 7: chrF++ scores for translation from each Indic and African language
    in the ROOTS corpus to English ($X$En), using BLOOM. The right y-axis indicates
    corresponding pre-training coverage of each language at log scale.'
  prefs: []
  type: TYPE_NORMAL
- en: Breakdown of $X$En.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Similar to the observation for En$\rightarrow$ in the main paper, [Figure 7](#A1.F7
    "In A.3 Additional Experiments ‣ Phụ lục A Appendix ‣ Democratizing LLMs for Low-Resource
    Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse
    Prompts") shows that LDP performs generally on par with supervised prompting equally
    across all languages, and that it does not unevenly perform much worse or better
    in any particular language.
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="A1.F8.pic1" class="ltx_picture ltx_centering" height="156.7" overflow="visible"
    version="1.1" width="565.43"><g transform="translate(0,156.7) matrix(1 0 0 -1
    0 0) translate(46.68,0) translate(0,29.47)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -46.68 -29.09)"><g class="ltx_nestedsvg" transform="matrix(1
    0 0 1 0 0) translate(68.85,0) translate(0,29.09)"><g stroke="#000000" fill="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -33.98 24.97)" fill="#000000"
    stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$5$</foreignobject></g></g></g></g></g></g></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'Hình 8: Tokenization issue. Left y-axis bar chart: The average ratios between
    the token lengths of $X$ indicates GPT is worse than BLOOM.'
  prefs: []
  type: TYPE_NORMAL
- en: BLOOM vs. InstructGPT.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While much evidence show that InstructGPT text-davinci-003 is more superior
    than the vanilla BLOOM (Scao et al., [2022](#bib.bib33); Ouyang et al., [2022](#bib.bib29))
    in many languages, our experiments with low-resource languages demonstrate it
    is not always true for low-resource non-Latin languages, as shown in the main
    paper. [Figure 8](#A1.F8 "In Breakdown of 𝑋→En. ‣ A.3 Additional Experiments ‣
    Phụ lục A Appendix ‣ Democratizing LLMs for Low-Resource Languages by Leveraging
    their English Dominant Abilities with Linguistically-Diverse Prompts") explains
    clearly the reason is that GPT’s tokenizer is not designed to allocate meaningful
    sub-word tokens for non-Latin texts, such as Indic lexical items, while significantly
    favors Latin characters due to the sheer size of Latin texts in its pre-training
    data. For example of InstructGPT, a 10-token English text can be equivalent to
    a 160-token Tamil text but only a 28-token Tumbuka text, despite Tumbuka is much
    more low-resource. This issue is non-existent in BLOOM, as the ratios naturally
    decrease when data coverages increase. As shown in the table, InstructGPT becomes
    worse than BLOOM as soon as the ratio between token lengths of target language
    over English surpass 5 in Indic languages. We refer to this as sub-word token
    fragmentation, where texts are broken into very long byte-level tokens that exceed
    the context length and suppress performances.
  prefs: []
  type: TYPE_NORMAL
