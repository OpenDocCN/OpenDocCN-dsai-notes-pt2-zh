- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:41:27'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Cultural Value Differences of LLMs: Prompt, Language, and Model Size'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.16891](https://ar5iv.labs.arxiv.org/html/2407.16891)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Qishuai Zhong Yike Yun  Aixin Sun
  prefs: []
  type: TYPE_NORMAL
- en: Nanyang Technological University, Singapore(May 2024)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Our study aims to identify behavior patterns in cultural values exhibited by
    large language models (LLMs). The studied variants include question ordering,
    prompting language, and model size. Our experiments reveal that each tested LLM
    can efficiently behave with different cultural values. More interestingly: (i)
    LLMs exhibit relatively consistent cultural values when presented with prompts
    in a single language. (ii) The prompting language e.g., Chinese or English, can
    influence the expression of cultural values. The same question can elicit divergent
    cultural values when the same LLM is queried in a different language. (iii) Differences
    in sizes of the same model (e.g., Llama2-7B vs 13B vs 70B) have a more significant
    impact on their demonstrated cultural values than model differences (e.g., Llama2
    vs Mixtral). Our experiments reveal that query language and model size of LLM
    are the main factors resulting in cultural value differences.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cultural Value Differences of LLMs: Prompt, Language, and Model Size'
  prefs: []
  type: TYPE_NORMAL
- en: Qishuai Zhong Yike Yun  Aixin Sun Nanyang Technological University, Singapore
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since GPT-3 Brown et al. ([2020](#bib.bib4)), Large Language Models (LLMs),
    capable of generating human-like text based on instructions, have garnered significant
    attention from both academia and industry. Numerous benchmarks and datasets have
    been created and employed to assess LLMs’ capability in generating human-like
    text across various tasks like question-answering, chatbot, and summarization Clark
    et al. ([2018](#bib.bib6)); Zellers et al. ([2019](#bib.bib43)); Hendrycks et al.
    ([2021a](#bib.bib14)). The open-source leaderboard Park ([2023](#bib.bib32)) allows
    researchers and engineers to directly compare language models across various dimensions,
    spanning from commonsense reasoning to advanced question answering, showcasing
    their respective abilities. However, focusing on information content while ignoring
    language’s social factors is currently a limitation of natural language processing
    (NLP) Hovy and Yang ([2021](#bib.bib17)).
  prefs: []
  type: TYPE_NORMAL
- en: Given their capacity to generate human-like text, it is imperative to investigate
    whether LLMs demonstrate human-like behaviors stemming from the internalized values
    and cultural insights acquired from large-scale training corpora. As model-generated
    text gains wider adoption, ethical concerns arise due to the potential influence
    of cultural biases embedded in the generated text on its users Kumar et al. ([2023](#bib.bib25)).
    Hence, an emerging research trend involves quantifying the cultural biases within
    language models and understanding their impact on the models’ performance across
    various tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The primary methods for assessing values in LLMs typically involve using social
    science and psychological instruments originally designed for humans Feng et al.
    ([2023](#bib.bib9)); Arora et al. ([2023](#bib.bib1)) to assess various cultural
    aspects quantitatively, or by developing specialized datasets to examine model
    biases Parrish et al. ([2022](#bib.bib34)); Huang and Xiong ([2023](#bib.bib18)).
    Many studies on social science instruments primarily evaluate text generated by
    models in English. However, historical linguistic research, such as the Whorfian
    hypothesis proposed by Sapir-Whorf, suggests that language structure significantly
    influences individual perceptions and worldviews Kay and Kempton ([1984](#bib.bib21)).
    Research has shown that cultural accommodation occurs when individuals engage
    in multilingual contexts, as evidenced by experiments with human subjects Harzing
    and Maznevski ([2002](#bib.bib13)). Similarly, multilingual language models, pre-trained
    on text from various languages, can inherit biases and inconsistencies from their
    training data Garrido-Muñoz  et al. ([2021](#bib.bib11)). Therefore, assessments
    using only English-based instruments may not fully capture the breadth of knowledge
    in multilingual models.
  prefs: []
  type: TYPE_NORMAL
- en: 'To provide a more comprehensive understanding of LLMs’ cultural values, this
    study investigated patterns of cultural values expressed by different models using
    three distinct approaches: (i) experimenting with varied prompts in a single language,
    (ii) using prompts in different languages, and (iii) conducting experiments across
    different models. The pipeline is visualized in Figure [3](#A1.F3 "Figure 3 ‣
    Appendix A Investigation Pipeline and Prompt Format ‣ Cultural Value Differences
    of LLMs: Prompt, Language, and Model Size") in Appendix [A](#A1 "Appendix A Investigation
    Pipeline and Prompt Format ‣ Cultural Value Differences of LLMs: Prompt, Language,
    and Model Size"). All sets of experiments were designed and implemented using
    Hofstede’s latest Value Survey Module (VSM) Hofstede and Hofstede ([2016](#bib.bib16)),
    a data collection instrument that quantifies cultural values across six dimensions Taras
    et al. ([2023](#bib.bib39)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'A total of 6 LLMs were involved in our experiments, with each model being provided
    54 simulated identities to contextualize its response to the VSM questionnaire.
    Through our investigation, we found that: (i) LLMs consistently demonstrate similar
    cultural values within a single language, despite variations in prompt content.
    However, their responses are affected by alterations in the positioning of options.
    (ii) LLMs show notably different cultural values across different languages; and
    (iii) Differences in the cultural values expressed by models correlate with variations
    in text generation proficiency. For the last finding, although we considered the
    models’ text generation proficiency in our study to conduct our analysis and support
    our findings, further assessment of the models’ generation capability is beyond
    the scope of our research.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Several studies have contributed to detecting social and cultural biases displayed
    by models, as values can be inferred from the expression of biases. Another approach
    is to incorporate social science models for a direct evaluation of the values
    inherent in the models. We review both approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Bias Study of Language Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Assessing social and cultural biases in language models is crucial to mitigate
    associated risks and reveal the values embodied by the models. Liang et al. ([2021](#bib.bib27))
    provided a formal comprehension of social biases in language models. The work
    identified fine-grained local biases and high-level global biases as sources of
    representational biases and proposed the evaluation metrics for measurement. Subsequently,
    it introduced the mitigation method. Sheng et al. ([2021](#bib.bib37)) presented
    the first comprehensive survey on societal biases in language generation in 2021,
    identifying their negative impact and exploring methods for evaluation and mitigation.
    The study highlighted the challenge of bias assessment due to the open-domain
    nature of NLG and the diverse conceptualizations of bias across cultures. Recently,
    more studies have focused on evaluating bias and values in large language models,
    with innovative methodologies employed. Cheng et al. ([2023](#bib.bib5)) utilized
    the concept of markedness, initially linguistic but now a part of social science,
    to evaluate models’ stereotypes unsupervisedly. Meanwhile, Kotek et al. ([2023](#bib.bib22))
    employed a direct method to assess gender bias in LLMs, revealing models’ tendency
    to reflect imbalances over gender due to training on skewed datasets. In Ferrara
    ([2023](#bib.bib10)), bias in generative language models was defined and its sources,
    such as training data and model specifications, were investigated. However, the
    study also acknowledged that some biases may persist inevitably due to the inherent
    nature of language and cultural norms.
  prefs: []
  type: TYPE_NORMAL
- en: Previous studies have demonstrated diverse techniques for accurately and efficiently
    identifying biases. However, they have also underscored the challenges in mitigating
    biases in generated text, as biases can be inherited from human language and culture
    in training data. This indicates that the exhibited values of models are shaped
    by the training data, making it impossible to dissociate the influence of training
    data when trying to understand the patterns of values expressed by models.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Social Science Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While studies have investigated language models’ social and cultural biases,
    there’s still relatively less systematic exploration of how these models exhibit
    values under varying circumstances. Quantifying results in this domain is challenging.
    Consequently, research instruments initially focused on humans have been integrated
    into understanding language models’ values. Feng et al. ([2023](#bib.bib9)) utilized
    the political compass test to map the political leaning of language models in
    a two-dimensional space. Through the experiments conducted, the study demonstrated
    that pretrained language models are influenced by the political leaning inherent
    in the training data. Regarding culture measurement, Hofstede’s Value Survey Module
    (VSM) Hofstede and Hofstede ([2016](#bib.bib16)) and the World Values Survey Inglehart
    et al. ([2014](#bib.bib19)) were employed by Arora et al. ([2023](#bib.bib1))
    to explore cross-cultural values embedded in multilingual masked language models.
    The evaluation covered 13 languages to probe the models’ cultural values across
    13 cultures. The findings indicated that pretrained language models captured noticeable
    differences in values between cultures, albeit with weak correlations to values
    surveys. Kovač et al. ([2023](#bib.bib23)) utilized three human psychology questionnaires
    to assess how models’ expression of values changes with varying contexts, such
    as varying paragraphs and textual formats. They introduced the metaphor “LLM as
    a superposition of perspectives" to highlight the context-dependent nature of
    LLM behavior. Shu et al. ([2023](#bib.bib38)) created a dataset covering various
    persona measurement instruments to evaluate the consistency of LLMs’ "personality"
    across different prompts with minor variations. Their experiments revealed that
    even minor perturbations notably impacted the models’ question-answering performance.
    Therefore, they argued the current practice of prompting is insufficient to accurately
    capture model perceptions. The aforementioned articles challenged the practice
    of using psychological models to reveal personalities by regarding language models
    as individuals Bodroza et al. ([2023](#bib.bib3)); Pan and Zeng ([2023](#bib.bib31)).
  prefs: []
  type: TYPE_NORMAL
- en: Summarized from previous research, it is clear that prompt engineering and training
    data significantly impact how models express values. However, there is a need
    for a systematic study to evaluate these factors comprehensively. In our study,
    we systematically explore the expression of cultural values by models under varying
    circumstances, including the effects of prompt engineering, language differences,
    and model capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Measures by VSM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similarly to previous studies, we utilize a value survey and additional measurement
    metrics to evaluate the alignment of cultural values in the LLMs. Value Survey
    Module (VSM) Hofstede and Hofstede ([2016](#bib.bib16)) is for measuring cultural
    values as outlined in Hofstede’s Cultural Dimensions Theory Gerlach and Eriksson
    ([2021](#bib.bib12)). Despite facing criticism for its psychometric deficiencies Taras
    et al. ([2023](#bib.bib39)) and simplicity Ercan et al. ([1991](#bib.bib8)), its
    value representation has become a cornerstone for a substantial body of research
    on cross-cultural differences in values Arora et al. ([2023](#bib.bib1)). In this
    study, we utilize the latest version of the survey (VSM 2013) as the foundational
    assessment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The value test is structured as a questionnaire with 24 questions to evaluate
    the interviewees’ cultural values. Another six questions intended to gather background
    information about the interviewees are excluded from our study. The complete questionnaire
    is in Appendix [J](#A10 "Appendix J VSM Questionnaire ‣ Cultural Value Differences
    of LLMs: Prompt, Language, and Model Size"). Each question offers respondents
    five options, labeled with option IDs from 1 to 5\. Option IDs also serve as raw
    scores for each question. The authors of the VSM further developed a scoring system
    based on each question’s raw score, comprising six dimensions for measuring cultural
    values: Power Distance (PDI), Individualism (IDV), Uncertainty Avoidance (UAI),
    Masculinity (MAS), Long-term Orientation (LTO), and Indulgence (IVR). Each dimension
    is calculated using a formula with the raw scores from four survey questions.
    The complete list of formulas is in Appendix [B](#A2 "Appendix B VSM Dimension
    Formula ‣ Cultural Value Differences of LLMs: Prompt, Language, and Model Size").'
  prefs: []
  type: TYPE_NORMAL
- en: 'All experiments are conducted using prompts derived from the questionnaire.
    The prompt is delivered in a zero-shot manner, and the LLM is expected to respond
    in JSON format, specifying the chosen option ID and the rationale behind the selection.
    We require models to respond with option IDs to mitigate the performance degradation
    outlined by Zheng et al. ([2024](#bib.bib44)). Prompt samples are depicted in
    Figure [4](#A1.F4 "Figure 4 ‣ Appendix A Investigation Pipeline and Prompt Format
    ‣ Cultural Value Differences of LLMs: Prompt, Language, and Model Size") in Appendix [A](#A1
    "Appendix A Investigation Pipeline and Prompt Format ‣ Cultural Value Differences
    of LLMs: Prompt, Language, and Model Size"). In each prompt, we give instructions
    on the reply format, provide a survey question, and supply a simulated background
    identity. The simulation provided a target for the model to contextualize the
    response. Contextual simulation or targeting specific groups of people is a common
    methodology used by previous studies to guide the generation Kovač et al. ([2023](#bib.bib23));
    Narayanan Venkit et al. ([2023](#bib.bib29)); Ramezani and Xu ([2023](#bib.bib35));
    Cheng et al. ([2023](#bib.bib5)).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Experiment Set
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The experiment conducted in this study consists of multiple experiment sets.
    Each set is defined by a unique combination of three hyper-parameters: (i) the
    tested LLM, (ii) the prompt language, and (iii) whether options are shuffled.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Within each experiment set, the language model was presented with a curated
    collection of simulated identities, each comprising three variables: (i) nationality,
    (ii) age, and (iii) gender to furnish context for the model’s responses to questions.
    The study encompasses nine nationalities (refer to the full list in Appendix [C](#A3
    "Appendix C Nationalities for Experiment ‣ Cultural Value Differences of LLMs:
    Prompt, Language, and Model Size")), two genders, and three age groups (25, 35,
    45), resulting in a total of 54 identities. These variables align with the VSM
    survey, encompassing interviewees from various countries, genders, and ages. The
    chosen nations are globally diverse, representing a range of cultures. To prevent
    coincidence, each question was queried ten times with different seeds. Consequently,
    we could collect $10\times 24\times 54=12960$ responses for each experiment set.
    During the analysis, we calculate the average of the ten outputs as the final
    output for a simulated identity, which is used as a single data point (a 24-d
    vector) in the experiment set.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Measures by VSM Raw Scores
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Each set of responses from a simulated identity is represented as a 24-dimensional
    vector, essential for comparisons within and between experiment sets. To evaluate
    the strength of relationships between these groups, we calculate the Pearson correlation
    coefficients ($\rho$, then we reject the null hypothesis and conclude that there
    is a significant relationship between the vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Measures by VSM Scores
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Using VSM formulas in Appendix [B](#A2 "Appendix B VSM Dimension Formula ‣
    Cultural Value Differences of LLMs: Prompt, Language, and Model Size"), we can
    generate 6-dimensional score vectors (i.e., PDI, IDV, UAI, MAS, LTO, and IVR)
    from the 24-d vectors.'
  prefs: []
  type: TYPE_NORMAL
- en: Intra-set Disparity Measurement
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In Hofstede’s research, VSM scores are analyzed nationally to explore cultural
    value differences between countries. The scores for the nine nations involved
    in this study are displayed in Appendix [D](#A4 "Appendix D Human Results of VSM
    Scores ‣ Cultural Value Differences of LLMs: Prompt, Language, and Model Size"),
    where each nation’s score represents the average of all responses from its interviewees.
    Similarly, we calculate the national average for model responses of each experiment
    set. We then use the standard deviation, denoted as $\sigma_{m}(v_{i})$ for the
    human results.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The mean values for each dimension, across all experiment sets and human results,
    range from $-60$, indicating that comparing the disparity between models and human
    results is reasonable. Appendix [E](#A5 "Appendix E Mean Values of VSM Scores
    ‣ Cultural Value Differences of LLMs: Prompt, Language, and Model Size") provides
    the complete list of mean values.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We then define the distance among nations observed for humans as $D_{h}$ as
    the “Model Cultural Disparity (MCD).", shown in Eq. [3](#S3.E3 "Equation 3 ‣ Intra-set
    Disparity Measurement ‣ 3.3 Measures by VSM Scores ‣ 3 Measures by VSM ‣ Cultural
    Value Differences of LLMs: Prompt, Language, and Model Size"):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle D_{h}$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle D_{m}$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle MCD$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: MCD compares the dispersion of cultural values exhibited by models based on
    simulated nations to that observed among humans in Hofstede’s study.
  prefs: []
  type: TYPE_NORMAL
- en: Inter-set Disparity Measurement
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The intra-set disparity underscores the impact of contextual information on
    the models’ expression of cultural values. Furthermore, our pipeline uses inter-set
    disparity to explore how changes in any of the three hyper-parameters—shuffling
    of options, language, and the tested model—affect the expression of cultural values.
  prefs: []
  type: TYPE_NORMAL
- en: 'We employ clustering methodologies, Davies-Bouldin Index (DBI) Davies and Bouldin
    ([1979](#bib.bib7)) and the Silhouette Score ($SS$) Rousseeuw ([1987](#bib.bib36)),
    to assess the effectiveness of separation between each pair of experiment sets.
    Detailed descriptions of the two metrics can be found in Appendix [F](#A6 "Appendix
    F Clustering Measurement Methods ‣ Cultural Value Differences of LLMs: Prompt,
    Language, and Model Size"). In our study, we pre-define the model responses from
    each experiment set as a cluster, comprising 54 data points, as detailed in Section [3.1](#S3.SS1
    "3.1 Experiment Set ‣ 3 Measures by VSM ‣ Cultural Value Differences of LLMs:
    Prompt, Language, and Model Size").'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we have introduced a new measurement method, the Silhouette Score
    with Human Reference ($SS_{h}$), to measure the absolute disparity between pairs
    of sets, taking human results as the reference point.
  prefs: []
  type: TYPE_NORMAL
- en: '$SS_{h}$ is designed based on the Silhouette Score, utilizing nationally aggregated
    average VSM scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle a_{h}(n_{i})$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle SS_{h}$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: where $a_{h}(n_{i})$ value exceeding one suggests that the separation between
    the two sets is more pronounced than the disparity observed among humans from
    various nations.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiment Setting and RQs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Given that the value survey is structured as a questionnaire, we have specifically
    chosen and employed models fine-tuned for chat purposes for this study. A total
    of six models are evaluated in this study, including members of the Llama2 family Touvron
    et al. ([2023](#bib.bib40)): Llama2-7b-chat-hf, Llama2-13b-chat-hf, and Llama2-70b-chat-hf;
    members of the Qwen family Bai et al. ([2023](#bib.bib2)): Qwen-14b-chat and Qwen-72b-chat;
    and Mixtral-8x7B-Instruct-v0.1 Jiang et al. ([2024](#bib.bib20)), which features
    a different architecture from the other models.'
  prefs: []
  type: TYPE_NORMAL
- en: All experiments were conducted using Vllm Kwon et al. ([2023](#bib.bib26)) with
    Transformers Wolf et al. ([2020](#bib.bib42)) to achieve faster inference. We
    utilized four Nvidia A6000 cards with CUDA 12.2\. We used the default config.json
    and framework parameters for the models’ text generation.
  prefs: []
  type: TYPE_NORMAL
- en: Through experiments, we aim to gain insights into three Research Questions (RQs).
  prefs: []
  type: TYPE_NORMAL
- en: 'RQ1: Can large language models consistently express cultural values when presented
    with perturbed questions in a single language? We focus on how responses of the
    same model vary with changes in contextual information and option order shuffling.'
  prefs: []
  type: TYPE_NORMAL
- en: 'RQ2: How does language affect the expression of cultural values in models?
    We examine the consistency of cultural values expressed by models when identical
    questions are posed in different languages.'
  prefs: []
  type: TYPE_NORMAL
- en: 'RQ3: What can we infer about models’ expression of cultural values when comparing
    them? We evaluate whether models from the same family show more consistent cultural
    values and investigate how differences in text generation capabilities relate
    to variations in cultural values.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experiment Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Among the 24 questions in the VSM 2013 survey, questions 15 and 18 pertain to
    the interviewee’s recent mental and physical health. Consequently, we assign the
    most neutral option (option 3) to these two questions. We similarly assign option
    3 for any unrecognizable responses from the models.
  prefs: []
  type: TYPE_NORMAL
- en: Initially, we requested responses in Chinese when querying models with Chinese
    prompts. However, about 7% of Llama2-7b-chat-hf’s responses and 24% of Llama2-13b-chat-hf’s
    responses were unrecognizable, in contrast to other models which had at least
    99% recognizable responses. As a result, these two models are required to respond
    in English to Chinese prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Prompt Variants (RQ1)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To evaluate a single model’s consistency in expressing cultural values within
    a single language, we developed prompt variations focusing on two aspects: simulated
    identity and options order. The former modifies only the context presented to
    the model, whereas the latter entails further prompt engineering. The impact of
    simulated identity is assessed within the experiment set, while the effectiveness
    of options order is evaluated through inter-set methods.'
  prefs: []
  type: TYPE_NORMAL
- en: Simulated Identity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Within each experiment set, the model is queried with 54 simulated identities.
    VSM raw scores and intra-set measurements are used to examine the impact of simulated
    identities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on raw scores, each tested model consistently produces results with a
    similar distribution, irrespective of changes in the context. The correlation
    coefficients for the average score vectors, grouped by context variables in Table [5](#A6.T5
    "Table 5 ‣ Appendix F Clustering Measurement Methods ‣ Cultural Value Differences
    of LLMs: Prompt, Language, and Model Size") in the Appendix, indicate that responses
    across different identities are highly correlated.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, the intra-set measurements based on VSM scores presented in Table [6](#A6.T6
    "Table 6 ‣ Appendix F Clustering Measurement Methods ‣ Cultural Value Differences
    of LLMs: Prompt, Language, and Model Size"), show that the simulated nations assigned
    to the LLMs exhibit significantly less cultural value diversity compared to the
    differences observed among human interviewees from those nations.'
  prefs: []
  type: TYPE_NORMAL
- en: In summary, the evaluated models produce responses with relatively consistent
    cultural values and show limited sensitivity to changes in the context of the
    prompts. The cultural values learned from the training corpus help mitigate the
    effects of variations in the simulated identities provided in the context.
  prefs: []
  type: TYPE_NORMAL
- en: '| Models | $DBI$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-7b-chat-hf | 1.837 | 0.169 | 0.430 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-13b-chat-hf | 1.694 | 0.205 | 0.228 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-70b-chat-hf | 0.658 | 0.572 | 0.574 |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen-14b-chat | 0.981 | 0.409 | 1.033 |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen-72b-chat | 0.825 | 0.478 | 0.483 |'
  prefs: []
  type: TYPE_TB
- en: '| Mixtral-8x7B | 0.542 | 0.641 | 0.680 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Results of three measurements are listed in the table to quantify
    the disparity between model responses for the two sets, “Eng w/o shuffled options"
    and “Eng w. shuffled options". Figures showing the greatest distinctness are highlighted
    in bold in each column.'
  prefs: []
  type: TYPE_NORMAL
- en: Shuffled Options
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As noted by Zheng et al. ([2024](#bib.bib44)), LLMs are susceptible to selection
    bias, primarily due to token bias and, to a lesser extent, position bias, both
    of which originate from the training data. Accordingly, our experiment maintains
    the original option IDs and their corresponding text, only altering their positions
    to minimize token bias.
  prefs: []
  type: TYPE_NORMAL
- en: We evaluate the consistency of the model’s cultural values despite selection
    bias by analyzing changes in the distribution of raw scores and measuring the
    inter-set disparity between the "Eng" and "Eng w. Shuffle" experiment sets for
    each model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Centroid vector of each experiment set represents the distribution of the
    set. The correlation coefficient and $p$-value are computed between the centroids,
    with comprehensive results presented in Table [7](#A7.T7 "Table 7 ‣ G.2 Shuffled
    Options ‣ Appendix G Experiments Results for RQ1 ‣ Cultural Value Differences
    of LLMs: Prompt, Language, and Model Size") in Appendix. These results indicate
    that most models maintain highly correlated score distributions after option shuffling.
    However, the overall correlation scores are noticeably lower than those calculated
    for simulated identities.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The inter-set disparity measurement results, as shown in Table [1](#S5.T1 "Table
    1 ‣ Simulated Identity ‣ 5.1 Prompt Variants (RQ1) ‣ 5 Experiment Results ‣ Cultural
    Value Differences of LLMs: Prompt, Language, and Model Size"), display the effect
    of shuffling to models from the aspect of VSM score. The results of $DBI$, we
    find that most models exhibit a noticeable absolute shift in cultural values between
    the sets, which does not correspond to the significant differences observed among
    humans from diverse nations.'
  prefs: []
  type: TYPE_NORMAL
- en: The experiment results show that models remain vulnerable to selection bias,
    consistent with the findings reported in Zheng et al. ([2024](#bib.bib44)). Unlike
    human behavior, models fail to maintain consistent cultural values in the face
    of textual ambiguities.
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/bf2e752f8fa2fc051cfdcc6ae358f79f.png) |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/21f21afb5a576ec3f5b741198466c04f.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Llama2-7b-chat-hf
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b817ea9da0f5966f140f88bb0377d012.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Llama2-13b-chat-hf
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/106622dd854500212597f3d6d0423d31.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Llama2-70b-chat-hf
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5d6e501e1914ce5a31a160c5311698e4.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) Qwen-14b-chat
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/283f5f90826c162d533374a9b242e04e.png)'
  prefs: []
  type: TYPE_IMG
- en: (e) Qwen-72b-chat
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d4b32f0fb3b97bff741cd03ee2045d59.png)'
  prefs: []
  type: TYPE_IMG
- en: (f) Mixtral-8x7B
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: The 6-d VSM scores for different experiment sets for each model are
    visualized using the t-SNE technique van der Maaten and Hinton ([2008](#bib.bib41))
    to facilitate direct comparisons. Results from English queries (denoted as "Eng")
    are displayed with black circles; results from English with Shuffled Options (denoted
    as "Eng w. Shuffle") are shown with pink stars; and results from Chinese (denoted
    as "Chn") are represented by green squares.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model response distributions across different experiment sets are visualized
    using t-SNE in Figure [1](#S5.F1 "Figure 1 ‣ Shuffled Options ‣ 5.1 Prompt Variants
    (RQ1) ‣ 5 Experiment Results ‣ Cultural Value Differences of LLMs: Prompt, Language,
    and Model Size") van der Maaten and Hinton ([2008](#bib.bib41)). The visualization
    also indicates that most models demonstrate less or comparable separation effectiveness
    between sets divided by “Shuffling of options" compared to those split by “Language".'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Language Variants (RQ2)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Models | $DBI$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-7b-chat-hf | 0.962 | 0.423 | 1.357 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-13b-chat-hf | 0.720 | 0.533 | 0.581 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-70b-chat-hf | 0.799 | 0.499 | 0.707 |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen-14b-chat | 1.846 | 0.215 | 0.622 |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen-72b-chat | 0.529 | 0.646 | 0.961 |'
  prefs: []
  type: TYPE_TB
- en: '| Mixtral-8x7B | 0.651 | 0.581 | 0.660 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Results of three measurements are listed in the table to quantify
    the disparity between model responses for the two sets, “Eng w/o shuffled options"
    and “Chn w/o shuffled options". Figures showing the greatest distinctness are
    highlighted in bold in each column.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to varying prompts within the same language, we conduct experiments
    to evaluate each model’s behavior when prompted in English and Chinese. For the
    Chinese queries, we carefully crafted prompts using the Chinese version of the
    VSM 2013 questionnaires. Contextual information of the simulated identities is
    manually translated.
  prefs: []
  type: TYPE_NORMAL
- en: Correlation coefficients and $p$-values for other models remain below the threshold,
    the overall correlation coefficient is lower than that observed with prompt variants.
    This suggests that language impacts the models’ choice of options more significantly
    than the shuffling of option order.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the raw scores, the inter-set disparity measurement results
    based on VSM scores are detailed in Table [2](#S5.T2 "Table 2 ‣ 5.2 Language Variants
    (RQ2) ‣ 5 Experiment Results ‣ Cultural Value Differences of LLMs: Prompt, Language,
    and Model Size"), with a comprehensive analysis of values provided in Appendix [H](#A8
    "Appendix H Experiment Results for RQ2 ‣ Cultural Value Differences of LLMs: Prompt,
    Language, and Model Size"). Based on the results of $DBI$ results suggest that
    when queried with the same questions in a different language, the model is expected
    to exhibit cultural values with a variability of at least 50%, akin to that of
    an individual from another country. Language differences can result in a more
    distinctive separation in expressing cultural values. The t-SNE figures in Figure [1](#S5.F1
    "Figure 1 ‣ Shuffled Options ‣ 5.1 Prompt Variants (RQ1) ‣ 5 Experiment Results
    ‣ Cultural Value Differences of LLMs: Prompt, Language, and Model Size") also
    clearly illustrate that most models express cultural values more variably when
    queried in different languages.'
  prefs: []
  type: TYPE_NORMAL
- en: The discrepancies between the initial measurements and $SS_{h}$ formula considers
    inter-set distance with human disparity (a constant), providing an absolute measure
    of inter-set disparity.
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing the findings, we observe that language significantly influences
    the models’ responses and the cultural values expressed by those responses. This
    observation aligns with research findings Norton ([1997](#bib.bib30)) that suggest
    values are commonly conveyed through language. We argue that the diverse cultural
    values expressed by the model in various languages are acquired from the distinct
    training corpora of those languages, similar to other types of knowledge transferred
    from training corpora to the language model Lin et al. ([2019](#bib.bib28)); Krishna
    et al. ([2023](#bib.bib24)).
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/58b7a59867609f49493ff385c2df35dc.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) $SS_{h}$ among Models with English Questions
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9b0aeec0930ead0f74192b516661e45d.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) MMLU Distance among Models
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/054c89e3db41056edb46f5486bf51df8.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) $SS_{h}$ among Models with Chinese Questions
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/87ea1ae07cc2da7c63e603d6241b9a7c.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) $SS_{h}$ among Models cross Languages
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: The three red heatmaps display the $SS_{h}$ values among models,
    with darker colors highlighting greater disparities. The green heatmap displays
    the differences in MMLU scores among models, corresponding to the disparities
    observed in the adjacent red heatmap.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Models Comparison (RQ3)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We now analyze the patterns of cultural values expressed by different models
    based on their inter-set disparity. This analysis encompasses three types of comparisons:
    (i) among models queried solely in English (without “shuffling"), (ii) among models
    queried solely in Chinese, and (iii) cross-language comparisons. All comparisons
    utilize $SS_{h}$ values. We represent all three comparison subsets with heatmap
    charts, as shown in Figure [2](#S5.F2 "Figure 2 ‣ 5.2 Language Variants (RQ2)
    ‣ 5 Experiment Results ‣ Cultural Value Differences of LLMs: Prompt, Language,
    and Model Size").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Observations from Heatmaps (a) and (c) in Figure [2](#S5.F2 "Figure 2 ‣ 5.2
    Language Variants (RQ2) ‣ 5 Experiment Results ‣ Cultural Value Differences of
    LLMs: Prompt, Language, and Model Size") reveal that models from the same family
    do not necessarily exhibit closer cultural value alignment. Additionally, all
    Llama2 models, irrespective of size, are trained using the same datasets for the
    same duration Touvron et al. ([2023](#bib.bib40)). The Qwen technical report Bai
    et al. ([2023](#bib.bib2)) also indicates that identical datasets and hyperparameters
    are applied across various model sizes during pretraining and fine-tuning stages
    (SRF and RLHF).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the findings: (i) models from the same family do not guarantee consistency
    in expressing cultural values; (ii) models with the same background receive uniform
    training; and (iii) larger models within the same family demonstrate better text-generation
    performance. We can deduce that variations in cultural values among models of
    the same family are linked to differences in their text-generation capabilities
    instead of training data. Larger models in the same family are guaranteed to handle
    complex patterns, understand context more effectively, and generalize better to
    unseen data. As a result, they are more adept at comprehending questions posed
    in value tests and generating more appropriate responses compared to smaller models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We further link our findings with the evaluation results of generation. A common
    evaluation all six models have undergone is the MMLU (Massive Multitask Language
    Understanding) test Hendrycks et al. ([2021b](#bib.bib15)). Differences in MMLU
    scores among models are displayed in Figure [2](#S5.F2 "Figure 2 ‣ 5.2 Language
    Variants (RQ2) ‣ 5 Experiment Results ‣ Cultural Value Differences of LLMs: Prompt,
    Language, and Model Size"), Heatmap (b). A large $SS_{h}$ value between models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, in the heatmap (d) of Figure [2](#S5.F2 "Figure 2 ‣ 5.2 Language
    Variants (RQ2) ‣ 5 Experiment Results ‣ Cultural Value Differences of LLMs: Prompt,
    Language, and Model Size"), the overall disparities between models across languages
    are significantly larger than those observed within a single language. The marked
    inter-set disparities noted in cross-language comparisons indicate that language
    variations can cause substantial differences in cultural values among models.'
  prefs: []
  type: TYPE_NORMAL
- en: Our hypothesis that differences in cultural values correlate with variations
    in model capabilities is based on observations. Developing a testing mechanism
    that simultaneously evaluates text quality, the expression of cultural values,
    and their alignment is part of future work. This approach will enhance our understanding
    of how language model performance impacts the expression of cultural values.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this study, we developed an investigative pipeline to assess the behavior
    of large language models concerning expressions of cultural values. Our results
    show that (i) Cultural values tend to remain relatively consistent across variations
    in prompts, especially when changes are limited to content alone. (ii) LLMs exhibit
    significantly divergent cultural values across different languages, and (iii)
    The difference in cultural values among models is relevant to variations in the
    models’ overall proficiency in text generation. Furthermore, upon comparing the
    results illustrating the second and third findings, we find that language variants
    can lead to greater disparities in cultural values. Language emerges as the most
    significant factor influencing the cultural values exhibited by the models.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This study has a few limitations that require further investigation in future
    research. (i) We limited our exploration of cultural values expressed by models
    to the 24 questions of the VSM 2013 survey, which has been criticized for its
    simplicity. Therefore, future research should consider incorporating additional
    cultural value surveys to investigate the models’ behavior further. (ii) This
    study evaluated and assessed only six models. To further validate the findings
    regarding the models’ expression of cultural values and their performance differences,
    additional models should be explored and included in future studies. (iii) In
    our experiments, models are prompted within a narrowly defined context to generate
    responses in a zero-shot manner, conditioned solely on the provided context. Future
    studies should extend beyond direct prompts, exploring how models express cultural
    values when supplied with extensive past experiences and acting as believable
    agents Park et al. ([2023](#bib.bib33)). (iv) A new evaluation pipeline or mechanism
    needs to be designed to assess and quantify the relationship between specific
    cultural value patterns and the generated text’s quality. This would build upon
    the current finding that variations in text quality result in different cultural
    values. (v) Although we have observed variations in the cultural values of large
    language models when the same questions are asked in different languages, we have
    not thoroughly analyzed user preferences concerning these differences. Future
    research should develop a systematic approach to assess how language-induced disparities
    in cultural values impact users and to formulate strategies to mitigate any negative
    effects.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Ethical Consideration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All experiments described in this study rely on data from the widely recognized
    Value Survey Module (VSM) 2013 Hofstede and Hofstede ([2016](#bib.bib16)) and
    utilize open-source language models. While our analysis includes human subject
    data, it is important to note that this data is derived from the well-established
    findings of the VSM 2013 study. Additionally, although our research examines the
    responses of various large language models to assess cultural values, we explicitly
    avoid ranking these models to maintain objectivity and ethical integrity.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Arora et al. (2023) Arnav Arora, Lucie-Aimée Kaffee, and Isabelle Augenstein.
    2023. [Probing pre-trained language models for cross-cultural differences in values](https://arxiv.org/abs/2203.13722).
    *Preprint*, arXiv:2203.13722.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bai et al. (2023) Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong
    Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang
    Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui
    Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang,
    Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian
    Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang,
    Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan
    Zhou, and Tianhang Zhu. 2023. [Qwen technical report](https://arxiv.org/abs/2309.16609).
    *Preprint*, arXiv:2309.16609.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bodroza et al. (2023) Bojana Bodroza, Bojana M. Dinic, and Ljubisa Bojic. 2023.
    [Personality testing of gpt-3: Limited temporal reliability, but highlighted social
    desirability of gpt-3’s personality instruments results](https://arxiv.org/abs/2306.04308).
    *Preprint*, arXiv:2306.04308.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. 2020. [Language models are few-shot learners](https://arxiv.org/abs/2005.14165).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cheng et al. (2023) Myra Cheng, Esin Durmus, and Dan Jurafsky. 2023. [Marked
    personas: Using natural language prompts to measure stereotypes in language models](https://arxiv.org/abs/2305.18189).
    *Preprint*, arXiv:2305.18189.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. [Think you have solved
    question answering? try arc, the ai2 reasoning challenge](https://api.semanticscholar.org/CorpusID:3922816).
    *ArXiv*, abs/1803.05457.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Davies and Bouldin (1979) David L. Davies and Donald W. Bouldin. 1979. [A cluster
    separation measure](https://doi.org/10.1109/TPAMI.1979.4766909). *IEEE Transactions
    on Pattern Analysis and Machine Intelligence*, PAMI-1(2):224–227.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ercan et al. (1991) G Ercan, Hamad Nasif, Bahman Al-Daeaj, and Mary S Ebrahimi.
    1991. Methodological problems in cross-cultural research: An updated review. *Management
    International Review*, pages 79–91.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feng et al. (2023) Shangbin Feng, Chan Young Park, Yuhan Liu, and Yulia Tsvetkov.
    2023. [From pretraining data to language models to downstream tasks: Tracking
    the trails of political biases leading to unfair NLP models](https://doi.org/10.18653/v1/2023.acl-long.656).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 11737–11762, Toronto, Canada. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ferrara (2023) Emilio Ferrara. 2023. [Should chatgpt be biased? challenges and
    risks of bias in large language models](https://doi.org/10.5210/fm.v28i11.13346).
    *First Monday*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Garrido-Muñoz  et al. (2021) Ismael Garrido-Muñoz , Arturo Montejo-Ráez , Fernando
    Martínez-Santiago , and L. Alfonso Ureña-López . 2021. [A survey on bias in deep
    nlp](https://doi.org/10.3390/app11073184). *Applied Sciences*, 11(7).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gerlach and Eriksson (2021) Philipp Gerlach and Kimmo Eriksson. 2021. Measuring
    cultural dimensions: External validity and internal consistency of hofstede’s
    VSM 2013 scales. *Front. Psychol.*, 12:662604.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Harzing and Maznevski (2002) Anne-Wil Harzing and Martha Maznevski. 2002. [The
    interaction between language and culture: A test of the cultural accommodation
    hypothesis in seven countries](https://doi.org/10.1080/14708470208668081). *Language
    and Intercultural Communication*, 2(2):120–139.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. (2021a) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021a. Measuring massive multitask
    language understanding. *Proceedings of the International Conference on Learning
    Representations (ICLR)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. (2021b) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021b. [Measuring massive multitask
    language understanding](https://arxiv.org/abs/2009.03300). *Preprint*, arXiv:2009.03300.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hofstede and Hofstede (2016) G Hofstede and G. J. Hofstede. 2016. VSM 2013.
    [https://geerthofstede.com/research-and-vsm/vsm-2013/](https://geerthofstede.com/research-and-vsm/vsm-2013/).
    Accessed: 2024-1-11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hovy and Yang (2021) Dirk Hovy and Diyi Yang. 2021. [The importance of modeling
    social factors of language: Theory and practice](https://doi.org/10.18653/v1/2021.naacl-main.49).
    In *Proceedings of the 2021 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 588–602, Online.
    Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang and Xiong (2023) Yufei Huang and Deyi Xiong. 2023. [Cbbq: A chinese bias
    benchmark dataset curated with human-ai collaboration for large language models](https://arxiv.org/abs/2306.16244).
    *Preprint*, arXiv:2306.16244.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inglehart et al. (2014) R. Inglehart, C. Haerpfer, A. Moreno, C. Welzel, K. Kizilova,
    J. Diez-Medrano, M. Lagos, P. Norris, E. Ponarin, B. Puranen, and et al. 2014.
    [World values survey: Round six - country-pooled datafile version](/brokenurl#www.worldvaluessurvey.org/WVSDocumentationWV6.jsp).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2024) Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur
    Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas,
    Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample,
    Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep
    Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut
    Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2024. [Mixtral of
    experts](https://arxiv.org/abs/2401.04088). *Preprint*, arXiv:2401.04088.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kay and Kempton (1984) Paul Kay and Willett Kempton. 1984. What is the sapir-whorf
    hypothesis? *Am. Anthropol.*, 86(1):65–79.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kotek et al. (2023) Hadas Kotek, Rikker Dockum, and David Sun. 2023. [Gender
    bias and stereotypes in large language models](https://doi.org/10.1145/3582269.3615599).
    In *Proceedings of The ACM Collective Intelligence Conference*, CI ’23, page 12–24,
    New York, NY, USA. Association for Computing Machinery.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kovač et al. (2023) Grgur Kovač, Masataka Sawayama, Rémy Portelas, Cédric Colas,
    Peter Ford Dominey, and Pierre-Yves Oudeyer. 2023. [Large language models as superpositions
    of cultural perspectives](https://arxiv.org/abs/2307.07870). *Preprint*, arXiv:2307.07870.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krishna et al. (2023) Kundan Krishna, Saurabh Garg, Jeffrey P. Bigham, and Zachary C.
    Lipton. 2023. [Downstream datasets make surprisingly good pretraining corpora](https://arxiv.org/abs/2209.14389).
    *Preprint*, arXiv:2209.14389.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kumar et al. (2023) Sachin Kumar, Vidhisha Balachandran, Lucille Njoo, Antonios
    Anastasopoulos, and Yulia Tsvetkov. 2023. [Language generation models can cause
    harm: So what can we do about it? an actionable survey](https://doi.org/10.18653/v1/2023.eacl-main.241).
    In *Proceedings of the 17th Conference of the European Chapter of the Association
    for Computational Linguistics*, pages 3299–3321, Dubrovnik, Croatia. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kwon et al. (2023) Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin
    Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. [Efficient
    memory management for large language model serving with pagedattention](https://arxiv.org/abs/2309.06180).
    *Preprint*, arXiv:2309.06180.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang et al. (2021) Paul Pu Liang, Chiyu Wu, Louis-Philippe Morency, and Ruslan
    Salakhutdinov. 2021. [Towards understanding and mitigating social biases in language
    models](https://proceedings.mlr.press/v139/liang21a.html). In *Proceedings of
    the 38th International Conference on Machine Learning*, volume 139 of *Proceedings
    of Machine Learning Research*, pages 6565–6576\. PMLR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2019) Yongjie Lin, Yi Chern Tan, and Robert Frank. 2019. [Open
    sesame: Getting inside bert’s linguistic knowledge](https://arxiv.org/abs/1906.01698).
    *Preprint*, arXiv:1906.01698.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Narayanan Venkit et al. (2023) Pranav Narayanan Venkit, Sanjana Gautam, Ruchi
    Panchanadikar, Ting-Hao Huang, and Shomir Wilson. 2023. [Nationality bias in text
    generation](https://doi.org/10.18653/v1/2023.eacl-main.9). In *Proceedings of
    the 17th Conference of the European Chapter of the Association for Computational
    Linguistics*, pages 116–122, Dubrovnik, Croatia. Association for Computational
    Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Norton (1997) Bonny Norton. 1997. [Language, identity, and the ownership of
    english](http://www.jstor.org/stable/3587831). *TESOL Quarterly*, 31(3):409–429.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pan and Zeng (2023) Keyu Pan and Yawen Zeng. 2023. [Do llms possess a personality?
    making the mbti test an amazing evaluation for large language models](https://arxiv.org/abs/2307.16180).
    *Preprint*, arXiv:2307.16180.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Park (2023) Daniel Park. 2023. [Open-llm-leaderboard-report](https://github.com/dsdanielpark/Open-LLM-Leaderboard-Report).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park et al. (2023) Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S. Bernstein. 2023. [Generative agents: Interactive
    simulacra of human behavior](https://arxiv.org/abs/2304.03442). *Preprint*, arXiv:2304.03442.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Parrish et al. (2022) Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh
    Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, and Samuel R. Bowman. 2022.
    [Bbq: A hand-built bias benchmark for question answering](https://arxiv.org/abs/2110.08193).
    *Preprint*, arXiv:2110.08193.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramezani and Xu (2023) Aida Ramezani and Yang Xu. 2023. [Knowledge of cultural
    moral norms in large language models](https://arxiv.org/abs/2306.01857). *Preprint*,
    arXiv:2306.01857.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rousseeuw (1987) Peter J. Rousseeuw. 1987. [Silhouettes: A graphical aid to
    the interpretation and validation of cluster analysis](https://doi.org/10.1016/0377-0427(87)90125-7).
    *Journal of Computational and Applied Mathematics*, 20:53–65.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sheng et al. (2021) Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun
    Peng. 2021. [Societal biases in language generation: Progress and challenges](https://arxiv.org/abs/2105.04054).
    *Preprint*, arXiv:2105.04054.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shu et al. (2023) Bangzhao Shu, Lechen Zhang, Minje Choi, Lavinia Dunagan,
    Dallas Card, and David Jurgens. 2023. [You don’t need a personality test to know
    these models are unreliable: Assessing the reliability of large language models
    on psychometric instruments](https://arxiv.org/abs/2311.09718). *Preprint*, arXiv:2311.09718.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Taras et al. (2023) Vas Taras, Piers Steel, and Madelynn Stackhouse. 2023. [A
    comparative evaluation of seven instruments for measuring values comprising hofstede’s
    model of culture](https://doi.org/10.1016/j.jwb.2022.101386). *Journal of World
    Business*, 58(1):101386.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. [Llama
    2: Open foundation and fine-tuned chat models](https://arxiv.org/abs/2307.09288).
    *Preprint*, arXiv:2307.09288.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: van der Maaten and Hinton (2008) Laurens van der Maaten and Geoffrey Hinton.
    2008. [Visualizing data using t-sne](http://jmlr.org/papers/v9/vandermaaten08a.html).
    *Journal of Machine Learning Research*, 9(86):2579–2605.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wolf et al. (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
    Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
    and Alexander M. Rush. 2020. [Huggingface’s transformers: State-of-the-art natural
    language processing](https://arxiv.org/abs/1910.03771). *Preprint*, arXiv:1910.03771.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. 2019. Hellaswag: Can a machine really finish your sentence? In
    *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2024) Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie
    Huang. 2024. [Large language models are not robust multiple choice selectors](https://arxiv.org/abs/2309.03882).
    *Preprint*, arXiv:2309.03882.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Investigation Pipeline and Prompt Format
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5d45bfac1136af890cb4a9b77f9aa9bb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Pipeline of investigations, exploring cultural values alignment in
    LLMs in three steps. (i) Evaluating cultural values exhibited by an LLM queried
    by a single language but with variants of prompts. (ii) Assessing cultural values
    in the context of different languages. (iii) Examining cultural values exhibited
    by different LLMs, within and across model families and in different model sizes.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/40ea6c1d0c75282a1d7c05b4e9997f15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Prompt samples for the two languages used in the experiment. In both
    samples, the syntax highlighted in red is copied from the original question in
    the questionnaire. During the VSM 2013 testing, there are approximately nine types
    of questions. All customized components are embedded with the respective values
    when querying the model.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B VSM Dimension Formula
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '|  | $\displaystyle PDI$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle IDV$ |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle MAS$ |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle UAI$ |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle LTO$ |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle IVR$ |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: Appendix C Nationalities for Experiment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The full list of nationalities used in experiments for simulated identities
    includes U.S.A, China, France, Germany, Brazil, India, Singapore, Japan, and South
    Africa.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Human Results of VSM Scores
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Human results, grouped by nations, are presented in Table [3](#A4.T3 "Table
    3 ‣ Appendix D Human Results of VSM Scores ‣ Cultural Value Differences of LLMs:
    Prompt, Language, and Model Size").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Nations | Dimensional Mean |'
  prefs: []
  type: TYPE_TB
- en: '| PDI | IDV | MAS | UAI | LTO | IVR |'
  prefs: []
  type: TYPE_TB
- en: '| U.S.A. | 40 | 91 | 62 | 46 | 26 | 68 |'
  prefs: []
  type: TYPE_TB
- en: '| China | 80 | 20 | 66 | 30 | 87 | 24 |'
  prefs: []
  type: TYPE_TB
- en: '| France | 68 | 71 | 43 | 86 | 63 | 48 |'
  prefs: []
  type: TYPE_TB
- en: '| Germany | 35 | 67 | 66 | 65 | 83 | 40 |'
  prefs: []
  type: TYPE_TB
- en: '| Brazil | 69 | 38 | 49 | 76 | 44 | 59 |'
  prefs: []
  type: TYPE_TB
- en: '| India | 77 | 48 | 56 | 40 | 51 | 26 |'
  prefs: []
  type: TYPE_TB
- en: '| Singapore | 74 | 20 | 48 | 8 | 72 | 46 |'
  prefs: []
  type: TYPE_TB
- en: '| Japan | 54 | 46 | 95 | 92 | 88 | 42 |'
  prefs: []
  type: TYPE_TB
- en: '| South Africa | 49 | 65 | 63 | 49 | 34 | 63 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Human results for the nine nations involved in the experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Mean Values of VSM Scores
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The mean values for each VSM dimension for all experiment sets and human results
    are outlined in Table [4](#A5.T4 "Table 4 ‣ Appendix E Mean Values of VSM Scores
    ‣ Cultural Value Differences of LLMs: Prompt, Language, and Model Size")'
  prefs: []
  type: TYPE_NORMAL
- en: '| Models | Dimensional Mean |'
  prefs: []
  type: TYPE_TB
- en: '| PDI | IDV | MAS | UAI | LTO | IVR |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-7b-chat-hf (Eng) | 18 | 20 | 15 | 13 | -12 | 82 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-7b-chat-hf (Eng w. Shuffle) | 22 | 8 | 4 | 17 | -9 | 33 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-7b-chat-hf (Chn) | -18 | 94 | -58 | -52 | 3 | 74 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-13b-chat-hf (Eng) | 22 | 45 | -5 | -4 | 20 | 22 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-13b-chat-hf (Eng w. Shuffle) | 40 | 29 | -8 | -6 | 24 | 14 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-13b-chat-hf (Chn) | 17 | 1 | -2 | 0 | -3 | 18 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-70b-chat-hf (Eng) | -16 | 67 | -33 | -38 | 4 | 49 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-70b-chat-hf (Eng w. Shuffle) | -12 | 28 | -4 | -23 | 0 | 40 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-70b-chat-hf (Chn) | -32 | 30 | -39 | -33 | -47 | 57 |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen-14b-chat (Eng) | 28 | 83 | -20 | -17 | -5 | 13 |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen-14b-chat (Eng w. Shuffle) | -11 | 7 | 2 | -1 | 1 | -10 |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen-14b-chat (Chn) | -7 | 72 | -55 | 1 | -1 | 56 |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen-72b-chat (Eng) | -13 | 74 | -40 | -2 | -26 | 26 |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen-72b-chat (Eng w. Shuffle) | 14 | 47 | -27 | -1 | 2 | 22 |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen-72b-chat (Chn) | 7 | 11 | -8 | -33 | 12 | 24 |'
  prefs: []
  type: TYPE_TB
- en: '| Mixtral-8x7B (Eng) | -33 | 70 | 34 | -31 | 2 | 47 |'
  prefs: []
  type: TYPE_TB
- en: '| Mixtral-8x7B (Eng w. Shuffle) | 4 | 30 | 9 | -34 | 15 | 44 |'
  prefs: []
  type: TYPE_TB
- en: '| Mixtral-8x7B (Chn) | -56 | 48 | 0 | 1 | 29 | 38 |'
  prefs: []
  type: TYPE_TB
- en: '| Hofstede’s Research | 61 | 52 | 61 | 55 | 61 | 46 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: The mean values for each VSM dimension for all experiment sets and
    human results are calculated. These mean values are presented in integer format
    to maintain consistency with the human results listed in Table [3](#A4.T3 "Table
    3 ‣ Appendix D Human Results of VSM Scores ‣ Cultural Value Differences of LLMs:
    Prompt, Language, and Model Size").'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix F Clustering Measurement Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Davies-Bouldin Index (DBI) Davies and Bouldin ([1979](#bib.bib7)): The metric
    quantifies the average similarity between each cluster. In our case, it offers
    an overview of the disparity in models’ cultural values at the experiment set
    level. We calculate the DBI value for each pair of sets. The formula is given
    by:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $DBI(e_{i},e_{j})=\left(\frac{S(e_{i})+S(e_{j})}{M(e_{i},e_{j})}\right)$
    |  | (12) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: where $S(e_{i})$. The lower the DBI value, the better the separation between
    the two sets. If the DBI value is larger than one, it suggests that the separation
    between clusters is not very distinct.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Silhouette Score ($SS$ is given by:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle a(p_{i})$ |  | (13) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  | $\displaystyle b(p_{i})$ |  | (14) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  | $\displaystyle SS$ |  | (15) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: where $a(p_{i})$. Our study computes the average score across all points from
    two sets to determine the disparity score between them. The silhouette score ranges
    from -1 to 1, where a higher value indicates more effective separation between
    clusters.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| Models | Identity Context |'
  prefs: []
  type: TYPE_TB
- en: '| Nation | Age | Gender |'
  prefs: []
  type: TYPE_TB
- en: '| PCC ($\rho$) |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-7b-chat-hf (Eng) | 0.969 | 0.987 | 0.925 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-7b-chat-hf (Eng w. Shuffle) | 0.942 | 0.994 | 0.949 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-7b-chat-hf (Chn) | 0.842 | 0.971 | 0.969 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-13b-chat-hf (Eng) | 0.978 | 0.993 | 0.996 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-13b-chat-hf (Eng w. Shuffle) | 0.969 | 0.993 | 0.987 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-13b-chat-hf (Chn) | 0.993 | 0.997 | 0.998 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-70b-chat-hf (Eng) | 0.991 | 1.000 | 0.995 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-70b-chat-hf (Eng w. Shuffle) | 0.987 | 0.999 | 0.996 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-70b-chat-hf (Chn) | 0.969 | 0.996 | 0.995 |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen-14b-chat (Eng) | 0.934 | 0.992 | 0.995 |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen-14b-chat (Eng w. Shuffle) | 0.752 | 0.905 | 0.837 |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen-14b-chat (Chn) | 0.807 | 0.939 | 0.858 |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen-72b-chat (Eng) | 0.934 | 0.992 | 0.995 |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen-72b-chat (Eng w. Shuffle) | 0.943 | 0.986 | 0.994 |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen-72b-chat (Chn) | 0.915 | 0.988 | 0.987 |'
  prefs: []
  type: TYPE_TB
- en: '| Mixtral-8x7B (Eng) | 0.992 | 0.997 | 0.998 |'
  prefs: []
  type: TYPE_TB
- en: '| Mixtral-8x7B (Eng w. Shuffle) | 0.995 | 0.999 | 0.998 |'
  prefs: []
  type: TYPE_TB
- en: '| Mixtral-8x7B (Chn) | 0.947 | 0.989 | 0.953 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: The Pearson Correlation Coefficient $\rho$. Correlation coefficients
    below 0.9 are in boldface.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Models | Dimensional Standard Deviation | Distance | MCD |'
  prefs: []
  type: TYPE_TB
- en: '| PDI | IDV | MAS | UAI | LTO | IVR |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-7b-chat-hf (Eng) | 7.587 | 4.648 | 6.960 | 7.014 | 11.402 | 14.096
    | 8.618 | 0.424 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-7b-chat-hf (Eng w. Shuffle) | 6.831 | 7.047 | 3.903 | 6.549 | 7.712
    | 9.916 | 6.993 | 0.344 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-7b-chat-hf (Chn) | 13.629 | 21.835 | 14.901 | 5.429 | 14.681 | 10.993
    | 13.578 | 0.668 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-13b-chat-hf (Eng) | 5.833 | 5.952 | 3.608 | 2.850 | 4.187 | 3.004
    | 4.239 | 0.209 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-13b-chat-hf (Eng w. Shuffle) | 3.109 | 4.301 | 3.134 | 2.530 | 6.586
    | 4.148 | 3.888 | 0.191 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-13b-chat-hf (Chn) | 4.131 | 2.758 | 3.394 | 0.919 | 3.153 | 4.085
    | 3.074 | 0.151 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-70b-chat-hf (Eng) | 4.160 | 1.096 | 2.789 | 4.866 | 3.197 | 5.113
    | 3.537 | 0.174 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-70b-chat-hf (Eng w. Shuffle) | 2.746 | 2.844 | 2.829 | 1.680 | 9.016
    | 5.674 | 4.132 | 0.203 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-70b-chat-hf (Chn) | 8.183 | 3.965 | 9.947 | 3.942 | 16.616 | 7.673
    | 8.388 | 0.413 |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen-14b-chat (Eng) | 7.376 | 6.512 | 6.596 | 5.405 | 4.026 | 6.388 | 6.051
    | 0.298 |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen-14b-chat (Eng w. Shuffle) | 5.965 | 9.709 | 5.916 | 3.485 | 16.145 |
    5.451 | 7.778 | 0.383 |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen-14b-chat (Chn) | 10.607 | 22.082 | 13.947 | 6.285 | 11.354 | 7.974 |
    12.042 | 0.592 |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen-72b-chat (Eng) | 3.947 | 4.767 | 3.660 | 1.952 | 13.470 | 6.036 | 5.638
    | 0.277 |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen-72b-chat (Eng w. Shuffle) | 4.250 | 4.854 | 3.767 | 3.409 | 6.386 |
    4.267 | 4.489 | 0.221 |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen-72b-chat (Chn) | 14.556 | 3.968 | 2.458 | 9.098 | 12.066 | 9.795 | 8.657
    | 0.426 |'
  prefs: []
  type: TYPE_TB
- en: '| Mixtral-8x7B (Eng) | 7.078 | 0.591 | 0.583 | 7.785 | 7.947 | 10.799 | 5.797
    | 0.285 |'
  prefs: []
  type: TYPE_TB
- en: '| Mixtral-8x7B (Eng w. Shuffle) | 2.983 | 1.650 | 5.904 | 1.693 | 3.251 | 3.465
    | 3.158 | 0.155 |'
  prefs: []
  type: TYPE_TB
- en: '| Mixtral-8x7B (Chn) | 7.319 | 5.035 | 0.412 | 1.523 | 5.332 | 11.495 | 5.186
    | 0.255 |'
  prefs: []
  type: TYPE_TB
- en: '| Human Results | 16.613 | 23.904 | 15.301 | 27.491 | 23.337 | 15.336 | 20.330
    | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: The standard deviation for each VSM dimension is calculated across
    nations. For the models, these deviations are derived from responses grouped by
    simulated nations, while for human results, they are based on Hofstede’s research
    findings. Distances and MCDs are calculated as outlined in [3.3](#S3.SS3.SSS0.Px1
    "Intra-set Disparity Measurement ‣ 3.3 Measures by VSM Scores ‣ 3 Measures by
    VSM ‣ Cultural Value Differences of LLMs: Prompt, Language, and Model Size").
    The highest MCD among models is emphasized in bold, indicating that a larger MCD
    suggests a greater influence of simulated nations on the models’ expression of
    cultural values.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix G Experiments Results for RQ1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: G.1 Variant Context
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Results based on the raw scores of 24 questions are listed in Table [5](#A6.T5
    "Table 5 ‣ Appendix F Clustering Measurement Methods ‣ Cultural Value Differences
    of LLMs: Prompt, Language, and Model Size").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Results for intra-set comparison based on VSM scores are listed in Table [6](#A6.T6
    "Table 6 ‣ Appendix F Clustering Measurement Methods ‣ Cultural Value Differences
    of LLMs: Prompt, Language, and Model Size"). The largest MCD among all experiment
    sets is less than 0.7, and only two out of eighteen groups have scores greater
    than 0.5.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: G.2 Shuffled Options
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Results based on the raw scores of 24 questions for each pair of experiment
    sets are listed in Table [7](#A7.T7 "Table 7 ‣ G.2 Shuffled Options ‣ Appendix
    G Experiments Results for RQ1 ‣ Cultural Value Differences of LLMs: Prompt, Language,
    and Model Size").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Results for inter-set comparison based on VSM scores for each pair of experiment
    sets are listed in Table [1](#S5.T1 "Table 1 ‣ Simulated Identity ‣ 5.1 Prompt
    Variants (RQ1) ‣ 5 Experiment Results ‣ Cultural Value Differences of LLMs: Prompt,
    Language, and Model Size"). As shown in the table, the smallest Davies-Bouldin
    Index (DBI) value among all models exceeds 0.5, with values closer to 0 indicating
    better clustering quality. Additionally, the highest Silhouette Score (SS) is
    below 0.7, where values closer to 1 signify more effective clustering. These statistics
    again underscore that the change in context within prompts does not significantly
    alter the cultural values in models’ responses.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'From the perspective of $SS_{h}$ value exceeds one, indicating a greater disparity
    than human results. This model also has the lowest Pearson correlation coefficient
    between the two sets as shown in Table [7](#A7.T7 "Table 7 ‣ G.2 Shuffled Options
    ‣ Appendix G Experiments Results for RQ1 ‣ Cultural Value Differences of LLMs:
    Prompt, Language, and Model Size").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| Models | PCC ($\rho$) | P-value |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-7b-chat-hf | 0.894 | $\ll 0.05$ |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-13b-chat-hf | 0.861 | $\ll 0.05$ |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-70b-chat-hf | 0.938 | $\ll 0.05$ |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen-14b-chat | 0.718 | $\ll 0.05$ |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen-72b-chat | 0.922 | $\ll 0.05$ |'
  prefs: []
  type: TYPE_TB
- en: '| Mixtral-8x7B | 0.876 | $\ll 0.05$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: The table presents Pearson correlation coefficients ($\rho$) and p-values
    comparing centroids of models’ responses between “w. Shuffle" and “w/o Shuffle"
    options (all prompts are in English), assessing the consistency of responses from
    the aspect of the original scores.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix H Experiment Results for RQ2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Results based on the raw scores of 24 questions are listed in Table [8](#A8.T8
    "Table 8 ‣ Appendix H Experiment Results for RQ2 ‣ Cultural Value Differences
    of LLMs: Prompt, Language, and Model Size").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Results for intra-set comparison based on VSM scores are listed in Table [2](#S5.T2
    "Table 2 ‣ 5.2 Language Variants (RQ2) ‣ 5 Experiment Results ‣ Cultural Value
    Differences of LLMs: Prompt, Language, and Model Size"). From the $DBI$ is 0.07
    higher. Nevertheless, using standard clustering metrics, we find no significant
    differences between the results in Table [2](#S5.T2 "Table 2 ‣ 5.2 Language Variants
    (RQ2) ‣ 5 Experiment Results ‣ Cultural Value Differences of LLMs: Prompt, Language,
    and Model Size") and Table [1](#S5.T1 "Table 1 ‣ Simulated Identity ‣ 5.1 Prompt
    Variants (RQ1) ‣ 5 Experiment Results ‣ Cultural Value Differences of LLMs: Prompt,
    Language, and Model Size").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'However, the results of $SS_{h}$ value for language comparison is 42.7% higher
    than that for "shuffling". The observations suggest that language differences
    can more readily "induce" the model to select a different option than selection
    bias. Consequently, this results in a more distinctive separation in the expression
    of cultural values by the same model. The t-SNE figures in Figure [1](#S5.F1 "Figure
    1 ‣ Shuffled Options ‣ 5.1 Prompt Variants (RQ1) ‣ 5 Experiment Results ‣ Cultural
    Value Differences of LLMs: Prompt, Language, and Model Size") also illustrate
    the differences in intra-set disparity, clearly showing that most models express
    cultural values more variably when queried in different languages.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| Models | PCC($\rho$) | P-value |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-7b-chat-hf | 0.315 | 0.134 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-13b-chat-hf | 0.704 | $\ll 0.05$ |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-70b-chat-hf | 0.841 | $\ll 0.05$ |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen-14b-chat | 0.531 | 0.008 |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen-72b-chat | 0.643 | $\ll 0.05$ |'
  prefs: []
  type: TYPE_TB
- en: '| Mixtral-8x7B | 0.535 | 0.007 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: The table presents Pearson correlation coefficients ($\rho$) and p-values
    comparing centroids of models’ responses between “English" and “Chinese" prompts,
    assessing the consistency of responses from the aspect of the original scores.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix I Experiment Results for RQ3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Heatmap (a) in Figure [2](#S5.F2 "Figure 2 ‣ 5.2 Language Variants (RQ2) ‣
    5 Experiment Results ‣ Cultural Value Differences of LLMs: Prompt, Language, and
    Model Size") shows that the 13b and 70b models from the Llama2 family are closest
    to the 14b and 72b models from the Qwen family. Similarly, the Qwen-14b-chat model
    has the smallest $SS_{h}$ values to the model outside their own family.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In heatmap (d) of Figure [2](#S5.F2 "Figure 2 ‣ 5.2 Language Variants (RQ2)
    ‣ 5 Experiment Results ‣ Cultural Value Differences of LLMs: Prompt, Language,
    and Model Size"), we present the $SS_{h}$ values in the heatmap (d) of Figure [2](#S5.F2
    "Figure 2 ‣ 5.2 Language Variants (RQ2) ‣ 5 Experiment Results ‣ Cultural Value
    Differences of LLMs: Prompt, Language, and Model Size") is notably sparse, with
    38.9% of values exceeding 1.0 and 10.5% falling below 0.5\. However, all values
    below 0.5 correspond to comparisons between one model and others tested in a different
    language. This suggests that the dimensional space utilized in the VSM testing
    might be too constrained, causing overlap in results from various experiment sets.
    Despite the overlap, the pronounced inter-set disparities observed in cross-language
    comparisons suggest that variations in language can lead to more significant differences
    in cultural values among models.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Appendix J VSM Questionnaire
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/014caf904b2bff5451e6331d6bc71afc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: VSM Questionnaire Page 1'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/72453d20a237b5575dbf41b496d782c8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: VSM Questionnaire Page 2'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2ba2e52736c05d654e0327c3d786cb18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: VSM Questionnaire Page 3'
  prefs: []
  type: TYPE_NORMAL
