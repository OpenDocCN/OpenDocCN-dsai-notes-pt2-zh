- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:43:30'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Efficient multi-prompt evaluation of LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.17202](https://ar5iv.labs.arxiv.org/html/2405.17202)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Felipe Maia Polo¹, Ronald Xu^(2.6), Lucas Weber³, Mírian Silva^(4,5,6), Onkar
    Bhardwaj^(5,6)
  prefs: []
  type: TYPE_NORMAL
- en: Leshem Choshen^(2,5,6), Allysson Flavio Melo de Oliveira^(5,6), Yuekai Sun¹,
    Mikhail Yurochkin^(5,6)
  prefs: []
  type: TYPE_NORMAL
- en: ¹University of Michigan, ²MIT, ³University Pompeu Fabra, ⁴Federal University
    of Minas Gerais
  prefs: []
  type: TYPE_NORMAL
- en: '⁵IBM Research, ⁶MIT-IBM Watson AI Lab Corresponding author. E-mail: felipemaiapolo@gmail.com'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Most popular benchmarks for comparing LLMs rely on a limited set of prompt
    templates, which may not fully capture the LLMs’ abilities and can affect the
    reproducibility of results on leaderboards. Many recent works empirically verify
    prompt sensitivity and advocate for changes in LLM evaluation. In this paper,
    we consider the problem of estimating the performance *distribution* across many
    prompt variants instead of finding a single prompt to evaluate with. We introduce
    PromptEval, a method for estimating performance across a large set of prompts
    borrowing strength across prompts and examples to produce accurate estimates under
    practical evaluation budgets. The resulting distribution can be used to obtain
    performance quantiles to construct various robust performance metrics (e.g., top
    95% quantile or median). We prove that PromptEval consistently estimates the performance
    distribution and demonstrate its efficacy empirically on three prominent LLM benchmarks:
    MMLU, BIG-bench Hard, and LMentry. For example, PromptEval can accurately estimate
    performance quantiles across 100 prompt templates on MMLU with a budget equivalent
    to two single-prompt evaluations¹¹1Our code and data can be found in [https://github.com/felipemaiapolo/prompt-eval](https://github.com/felipemaiapolo/prompt-eval)..'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8bea8e1161274bc9ae2386363dbbd3bd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Average estimation error for performance quantiles across 100 templates
    given a limited budget (in multiples of one-template MMLU evaluations).'
  prefs: []
  type: TYPE_NORMAL
- en: In recent years, the rapid progress of large language models (LLMs) has significantly
    influenced various fields by enhancing automated text generation and comprehension.
    As these models advance in complexity and functionality, a key challenge that
    arises is their robust evaluation (Perlitz et al., [2023](#bib.bib26)). Common
    evaluation methods, which often rely on a single or limited number of prompt templates,
    may not adequately reflect the typical model’s capabilities (Weber et al., [2023b](#bib.bib43)).
    Furthermore, this approach can lead to unreliable and inconsistent rankings on
    LLM leaderboards, as different models may perform better or worse depending on
    the specific prompt template used. An ideal evaluation framework should minimize
    dependence on any single prompt template and instead provide a holistic summary
    of performance across a broad set of templates. Mizrahi et al. ([2023](#bib.bib24)),
    for example, suggests using summary statistics, such as the average performance
    across many templates, as a way to compare the abilities of different LLMs. However,
    the main drawback of this method is the high computational cost when dealing with
    numerous templates and examples.
  prefs: []
  type: TYPE_NORMAL
- en: We introduce PromptEval, a method for efficient multi-prompt evaluation of LLMs.
    With a small number of evaluations, PromptEval estimates performance across different
    prompt templates. Our approach is grounded in robust theoretical foundations and
    utilizes well-established models from the fields of educational assessment and
    psychometrics, such as Item Response Theory (IRT) (Cai et al., [2016](#bib.bib4);
    Van der Linden, [2018](#bib.bib37); Brzezińska, [2020](#bib.bib3); Lord et al.,
    [1968](#bib.bib22)). Our method is based on a *parametric* IRT model that allows
    borrowing strength across examples and prompt templates to produce accurate estimates
    of all considered prompts with an evaluation budget comparable to evaluating a
    single prompt. In Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Efficient multi-prompt
    evaluation of LLMs"), we demonstrate the ability of our method to jointly estimate
    various performance quantiles across 100 prompt templates with evaluation budget
    ranging from one to four times of a conventional single-prompt evaluation on MMLU
    (Hendrycks et al., [2020](#bib.bib13)).
  prefs: []
  type: TYPE_NORMAL
- en: Performance distribution across prompts can be used to accommodate various contexts
    when comparing LLMs (Choshen et al., [2024](#bib.bib6)). For example, it can be
    used to compute mean as suggested by Mizrahi et al. ([2023](#bib.bib24)). One
    can also use performance distributions directly to compare LLMs via various notions
    of stochastic dominance for risk-sensitive scenarios (Nitsure et al., [2023](#bib.bib25)).
    Here we primarily focus on the full distribution and its quantiles as they provide
    a flexible statistic that can inform decisions in varying contexts. For instance,
    a typical model performance corresponds to a median (50% quantile), 95% quantile
    can be interpreted as performance achievable by an expert prompt engineer, while
    5% quantile is of interest in consumer-facing applications to quantify low-end
    performance for a user not familiar with prompt engineering. We also demonstrate
    (§[5](#S5 "5 Assessing multi-prompt evaluation strategies ‣ Efficient multi-prompt
    evaluation of LLMs")) how our method can be used to improve best prompt identification
    (Shi et al., [2024](#bib.bib33)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Our main contributions are:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose (§[3](#S3 "3 Performance distribution and quantiles estimation ‣
    Efficient multi-prompt evaluation of LLMs")) a novel method called PromptEval
    which permits efficient multi-prompt evaluation of LLMs across different prompt
    templates with a limited number of evaluations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We theoretically show (§[4](#S4 "4 Theoretical guarantees ‣ Efficient multi-prompt
    evaluation of LLMs")) that PromptEval has desirable statistical properties such
    as consistency in estimating performance distribution and its quantiles.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We practically demonstrate (§[5](#S5 "5 Assessing multi-prompt evaluation strategies
    ‣ Efficient multi-prompt evaluation of LLMs")) efficacy of PromptEval in estimating
    performance across 100+ prompts and finding the best-performing prompt for various
    LLMs using data derived from three popular benchmarks: MMLU (Hendrycks et al.,
    [2020](#bib.bib13)), BIG-bench Hard (BBH) (Suzgun et al., [2022](#bib.bib36)),
    and LMentry (Efrat et al., [2022](#bib.bib9)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We conduct the first large-scale study of prompt sensitivity of 15 popular open-source
    LLMs on MMLU. We present our findings based on evaluating 100 prompt templates
    in Section [6](#S6 "6 Analysis of prompt sensitivity on MMLU ‣ Efficient multi-prompt
    evaluation of LLMs") and will release the evaluation data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1.1 Related work
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LLMs’ sensitivity to prompt templates. The sensitivity of Large Language Models
    (LLMs) to the prompts is well-documented. For example, Sclar et al. ([2023](#bib.bib32))
    revealed that subtle variations in prompt templates in few-shot settings can lead
    to significant performance discrepancies among several open-source LLMs, with
    differences as large as 76 accuracy points in tasks from the SuperNaturalInstructions
    dataset (Wang et al., [2022](#bib.bib41)). Additionally, they report that the
    performance of different prompt templates tends to correlate weakly between models.
    This finding challenges the reliability of evaluation methods that depend on a
    single prompt template. To measure LLMs sensitivity, the researchers suggested
    calculating a “performance spread,” which represents the difference between the
    best and worst performances observed. Mizrahi et al. ([2023](#bib.bib24)) conducted
    a complementary analysis using state-of-the-art models and subsets of BigBench
    and LMentry (Srivastava et al., [2022](#bib.bib34); Efrat et al., [2022](#bib.bib9)).
    The authors arrive at similar conclusions with respect to LLMs’ sensitivity to
    the used prompt templates and empirically showed that the LLM ranking considering
    different formats are usually weakly or intermediately correlated with each other.
    As a solution to the lack of robustness in LLM evaluation, the authors propose
    the use of summary statistics, as the average performance, for LLM evaluation.
    Some other works, e.g., Voronov et al. ([2024](#bib.bib40)); Weber et al. ([2023b](#bib.bib43),
    [a](#bib.bib42)), show that even when in-context examples are given to the models,
    the prompt templates can have a big impact on the final numbers, sometimes reducing
    the performance of the strongest model in their analyses to a random guess level
    (Voronov et al., [2024](#bib.bib40)). In a different direction, Shi et al. ([2024](#bib.bib33))
    acknowledges that different prompt templates have different performances and proposes
    using best-arm-identification to efficiently select the best template for an application
    at hand. One major bottleneck is still on how to efficiently compute the performance
    distribution for LLMs over many prompt templates; we tackle this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Efficient evaluation of LLMs. The escalating size of models and datasets has
    led to increased evaluation costs. To streamline evaluations, Ye et al. ([2023b](#bib.bib46))
    considered minimizing the number of *tasks* within Big-bench (Srivastava et al.,
    [2022](#bib.bib34)). Additionally, Perlitz et al. ([2023](#bib.bib26)) observed
    that evaluations on HELM (Liang et al., [2022](#bib.bib21)) rely on diversity
    across datasets, though the quantity of examples currently utilized is unnecessarily
    large. Perlitz et al. ([2023](#bib.bib26)) also highlighted the problems in evaluating
    with insufficient prompts and called to evaluate on more, suggesting evaluating
    the typical behavior by sampling prompts and examples together. To accelerate
    evaluations for classification tasks, Vivek et al. ([2023](#bib.bib39)) suggested
    clustering evaluation examples based on model confidence in the correct class.
    More recently, Polo et al. ([2024](#bib.bib27)) empirically showed that it is
    possible to shrink the size of modern LLM benchmarks and still retain good estimates
    for LLMs’ performances. Despite these advancements in streamlining LLM evaluations,
    there are no other works that propose a general and efficient multi-prompt evaluation
    method to the best of our knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Item response theory (IRT). IRT (Cai et al., [2016](#bib.bib4); Van der Linden,
    [2018](#bib.bib37); Brzezińska, [2020](#bib.bib3); Lord et al., [1968](#bib.bib22))
    is a collection of statistical models initially developed in psychometrics to
    assess individuals’ latent abilities through standardized tests but with increasing
    importance in the fields of artificial intelligence and natural language processing
    (NLP). For example, Lalor et al. ([2016](#bib.bib18)) used IRT’s latent variables
    to measure language model abilities, Vania et al. ([2021](#bib.bib38)) applied
    IRT to benchmark language models and examine the saturation of benchmarks, and
    Rodriguez et al. ([2021](#bib.bib31)) explored various uses of IRT with language
    models, including predicting responses to unseen items, categorizing items by
    difficulty, and ranking models. Recently, Polo et al. ([2024](#bib.bib27)) suggested
    using IRT for efficient LLM performance evaluation, introducing the Performance-IRT
    (pIRT) estimator to evaluate LLMs. Our quantile estimation methodology is built
    upon pIRT.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Problem statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we describe the setup we work on and what our objectives are.
    Consider that we want to evaluate a large language model (LLM) in a certain dataset
    composed of $J$, we can define its performance score as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\textstyle S_{i}\triangleq\frac{1}{J}\sum_{j\in\mathcal{J}}Y_{ij}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: The performance scores $S_{i}$’s can have a big variability, making the LLM
    evaluation reliant on the prompt choice. To have a comprehensive evaluation of
    the LLM, we propose computing the full *distribution of performances* and its
    corresponding quantile function, i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\textstyle F(x)\triangleq\frac{1}{I}\sum_{i\in\mathcal{I}}\mathds{1}_{[S_{i},\infty)}(x)~{}~{}\text{
    and }~{}~{}Q(p)\triangleq\inf\{x\in{\mathbb{R}}:F(x)\geq p\}.$ |  | (2.1) |'
  prefs: []
  type: TYPE_TB
- en: The main challenge in obtaining this distribution is that it can be very expensive
    since the exact values for the performance scores $S_{i}$).
  prefs: []
  type: TYPE_NORMAL
- en: 3 Performance distribution and quantiles estimation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We propose borrowing strength across prompt templates and examples to produce
    accurate estimates for the performance distribution and its quantile function.
    To achieve that, we need a model for the correctness scores $Y_{ij}$’s and then
    we introduce our estimators for the performance distribution and quantile functions.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 The correctness model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We assume the observations $Y_{ij}$’s are independently sampled from a Bernoulli
    model parameterized by prompt/example-specific parameters. That is, we assume
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle Y_{ij}\sim\text{Bernoulli}(\mu_{ij}),$ |  | (3.1) |'
  prefs: []
  type: TYPE_TB
- en: where $\mu_{ij}$’s and some categorization or content of each of the examples
    in the case of $z_{j}$. That is, our model assumes that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (3.2) |'
  prefs: []
  type: TYPE_TB
- en: The functions $f_{\psi}$ is small, i.e., only a few evaluations are carried
    out. This degradation in the quality of the estimates can directly affect the
    quality of the performance distribution estimates. Finally, we fit the parameters
    $\psi$, by maximizing the log-likelihood of the observed data (negative cross-entropy
    loss), i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (3.3) |'
  prefs: []
  type: TYPE_TB
- en: Realize that fitting the model with linear/affine $f_{\psi}$ as the covariates.
    In the experiments Section [5](#S5 "5 Assessing multi-prompt evaluation strategies
    ‣ Efficient multi-prompt evaluation of LLMs"), we explore some different options
    of covariates for both templates and examples.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Performance distribution and quantiles estimation using the correctness
    model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The model in ([3.1](#S3.E1 "In 3.1 The correctness model ‣ 3 Performance distribution
    and quantiles estimation ‣ Efficient multi-prompt evaluation of LLMs")) can be
    naturally used for performance estimation. That is, after observing $Y_{\mathcal{E}}$,
    is given by the following conditional expectation
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathcal{J}_{i}\triangleq\{j\in\mathcal{J}:(i,j)\in\mathcal{E}\}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (3.4) |'
  prefs: []
  type: TYPE_TB
- en: The basic version of this estimator, when no elaborate covariates (e.g., embeddings)
    are included, is known as the Performance-IRT (pIRT) estimator (Polo et al., [2024](#bib.bib27)).
    We can apply our extended version of pIRT, which we call X-pIRT, to estimate the
    performance distribution across prompt templates. After observing $Y_{\mathcal{E}}$.
    Then, we define our estimators for the distribution of performances and its corresponding
    quantile function [2.1](#S2.E1 "In 2 Problem statement ‣ Efficient multi-prompt
    evaluation of LLMs") as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (3.5) |'
  prefs: []
  type: TYPE_TB
- en: We name the procedure of obtaining $\hat{F}$ as PromptEval and summarize it
    in Algorithm [1](#algorithm1 "In 3.2 Performance distribution and quantiles estimation
    using the correctness model ‣ 3 Performance distribution and quantiles estimation
    ‣ Efficient multi-prompt evaluation of LLMs").
  prefs: []
  type: TYPE_NORMAL
- en: '1 Input: (i) $Y_{\mathcal{E}}$ ([3.4](#S3.E4 "In 3.2 Performance distribution
    and quantiles estimation using the correctness model ‣ 3 Performance distribution
    and quantiles estimation ‣ Efficient multi-prompt evaluation of LLMs")).Compute
    estimates'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\textstyle\hat{F}(\cdot)\triangleq\frac{1}{I}\sum_{i\in\mathcal{I}}\mathds{1}_{[\hat{S}_{i},\infty)}(\cdot)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\textstyle\hat{Q}(\cdot)\triangleq\inf\{x\in{\mathbb{R}}:\hat{F}(x)\geq\cdot\}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: return  $\hat{F}$.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 PromptEval
  prefs: []
  type: TYPE_NORMAL
- en: '1 Input: (i) sets $\mathcal{I}$.'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 2 Two-way balanced sampling
  prefs: []
  type: TYPE_NORMAL
- en: Sampling $Y_{\mathcal{E}}$
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We have assumed $Y_{\mathcal{E}}$ giving the same sampling probability to all
    entries. This option is, however, suboptimal because of its high instability:
    with a high chance, there will be some prompt formats (or examples) with a very
    low number of evaluations while others will have many. A more stable solution
    is given by Algorithm [2](#algorithm2 "In 3.2 Performance distribution and quantiles
    estimation using the correctness model ‣ 3 Performance distribution and quantiles
    estimation ‣ Efficient multi-prompt evaluation of LLMs"), which balances the number
    of times each prompt format and examples are evaluated. Algorithm [2](#algorithm2
    "In 3.2 Performance distribution and quantiles estimation using the correctness
    model ‣ 3 Performance distribution and quantiles estimation ‣ Efficient multi-prompt
    evaluation of LLMs") can be seen as two-way stratified random sampling in which
    the number of examples observed for each prompt format is (roughly) the same and
    the number of prompt formats that observe each one of the examples is (roughly)
    the same.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Theoretical guarantees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we claim the consistency of the distribution and quantile estimators
    detailed in Algorithm [1](#algorithm1 "In 3.2 Performance distribution and quantiles
    estimation using the correctness model ‣ 3 Performance distribution and quantiles
    estimation ‣ Efficient multi-prompt evaluation of LLMs") as $I,J\to\infty$, which
    can be useful beyond this work.
  prefs: []
  type: TYPE_NORMAL
- en: We start by assuming that the covariates are uniformly bounded.
  prefs: []
  type: TYPE_NORMAL
- en: Condition 4.1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There is a universal constant $$c></math>.
  prefs: []
  type: TYPE_NORMAL
- en: The next condition requires the number of unseen examples to increase sufficiently
    fast as $I,J\to\infty$, which is a realistic condition under the low-budget setup.
    Weaker versions of this condition are possible; we adopt this one because it makes
    our proof simpler.
  prefs: []
  type: TYPE_NORMAL
- en: Condition 4.2.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Assume (i) $m=|\mathcal{J}\setminus\mathcal{J}_{i}|$.
  prefs: []
  type: TYPE_NORMAL
- en: The third condition requires the model we work with to be correctly specified
    and the maximum likelihood estimator defined in ([3.3](#S3.E3 "In 3.1 The correctness
    model ‣ 3 Performance distribution and quantiles estimation ‣ Efficient multi-prompt
    evaluation of LLMs")) to be consistent as $I,J\to\infty$ is well-studied and holds
    under mild conditions when the dimensions of the covariates are fixed; see, for
    example, Fahrmeir and Kaufmann ([1985](#bib.bib10)).
  prefs: []
  type: TYPE_NORMAL
- en: Condition 4.3.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The data point $Y_{ij}$.
  prefs: []
  type: TYPE_NORMAL
- en: We now introduce the main result in Theorem [4.4](#S4.Thmtheorem4 "Theorem 4.4\.
    ‣ 4 Theoretical guarantees ‣ Efficient multi-prompt evaluation of LLMs"), which
    shows the consistency of the distribution and quantile functions estimators introduced
    in Algorithm [1](#algorithm1 "In 3.2 Performance distribution and quantiles estimation
    using the correctness model ‣ 3 Performance distribution and quantiles estimation
    ‣ Efficient multi-prompt evaluation of LLMs"). See Appendix [F](#A6 "Appendix
    F Theoretical results ‣ Efficient multi-prompt evaluation of LLMs") for the proof.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 4.4.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Under conditions [4.1](#S4.Thmtheorem1 "Condition 4.1\. ‣ 4 Theoretical guarantees
    ‣ Efficient multi-prompt evaluation of LLMs"), [4.2](#S4.Thmtheorem2 "Condition
    4.2\. ‣ 4 Theoretical guarantees ‣ Efficient multi-prompt evaluation of LLMs"),
    and [4.3](#S4.Thmtheorem3 "Condition 4.3\. ‣ 4 Theoretical guarantees ‣ Efficient
    multi-prompt evaluation of LLMs"), it is true that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\textstyle\left&#124;\hat{Q}_{\mathcal{I}}(p)-Q_{\mathcal{I}}(p)\right&#124;\to
    0\text{ in probability as }I,J\to\infty\text{ for any $p\in[0,1]$},$ |  |'
  prefs: []
  type: TYPE_TB
- en: and that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $W_{1}(F,\hat{F})\to 0\text{ in probability as }I,J\to\infty,$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $W_{1}(F,\hat{F})$.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Assessing multi-prompt evaluation strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: General assessment
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We assess the performance distribution and quantile function estimation methodology
    introduced in §[3](#S3 "3 Performance distribution and quantiles estimation ‣
    Efficient multi-prompt evaluation of LLMs") in estimating the performance of LLMs
    and different prompt formats on data from three popular benchmarks. For a given
    LLM and a dataset, we consider two evaluation steps. First, we compare the full
    performance distribution with the estimated distribution, i.e., in this case,
    all quantiles are considered. To compare the full performance distribution $F$,
    both defined in §[3](#S3 "3 Performance distribution and quantiles estimation
    ‣ Efficient multi-prompt evaluation of LLMs"), we use the Wasserstein 1-distance
    which is equivalent to the average quantile estimation error in this case, i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\textstyle W_{1}(F,\hat{F})$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $S_{(i)}$ to measure the quality of our estimations.
  prefs: []
  type: TYPE_NORMAL
- en: Data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We use data derived from three popular benchmarks: MMLU (Hendrycks et al.,
    [2020](#bib.bib13)), BIG-bench Hard (BBH) (Suzgun et al., [2022](#bib.bib36)),
    and LMentry (Efrat et al., [2022](#bib.bib9)). In the following, we give more
    details about each one of the used datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MMLU is a multiple choice QA benchmark consisting of 57 subjects (tasks) comprising
    approximately 14k examples. We ran 15 different open-source LLMs (including different
    versions of Llama-3 (Meta, [2024](#bib.bib23)), Mistral (Jiang et al., [2023](#bib.bib14)),
    and Gemma (Gemma et al., [2024](#bib.bib11))) combined with 100 different prompt
    variations for each one of the MMLU tasks. We found that, within each one of the
    MMLU tasks, prompt templates can have great variability in their performances,
    making within-task analysis most suitable for assessing our method. More details
    and analysis of the collected data can be found in §[6](#S6 "6 Analysis of prompt
    sensitivity on MMLU ‣ Efficient multi-prompt evaluation of LLMs") and Appendix
    [G](#A7 "Appendix G Details MMLU data ‣ Efficient multi-prompt evaluation of LLMs").
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BIG-bench Hard (BBH) is a curated subset of BIG-bench (Srivastava et al., [2022](#bib.bib34)),
    containing challenging tasks on which LLMs underperform the average human score.
    For BBH, we use the evaluation scores released by Mizrahi et al. ([2023](#bib.bib24)).
    The evaluation data includes 11 open-source LLMs combined with a different number
    of prompt variations, ranging from 136 to 188 formats, for 15 tasks containing
    100 examples each.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LMentry consists of simple linguistic tasks designed to capture explainable
    and controllable linguistic phenomena. Like BBH, we use data generated by Mizrahi
    et al. ([2023](#bib.bib24)). The authors made available the full evaluation data
    from 16 open-source LLMs combined with a different number of prompt variations,
    ranging from 226 to 259 formats, for 10 tasks containing from 26 to 100 examples
    each.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Methods and baselines
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We consider different variations of the model presented in ([3.2](#S3.E2 "In
    3.1 The correctness model ‣ 3 Performance distribution and quantiles estimation
    ‣ Efficient multi-prompt evaluation of LLMs")) coupled with Algorithm [1](#algorithm1
    "In 3.2 Performance distribution and quantiles estimation using the correctness
    model ‣ 3 Performance distribution and quantiles estimation ‣ Efficient multi-prompt
    evaluation of LLMs"); for all variations, we use linear $f_{\psi}$, however, upon
    preliminary tests with sentence transformer we didn’t observe improvements and
    chose to use one-hot-encoded vectors as in the basic Rasch model to represent
    examples. Next we detail the methods for obtaining the prompt covariates:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt embeddings. We embed prompt templates using a pre-trained sentence transformer
    variant (Karpukhin et al., [2020](#bib.bib15)) and reduce their dimensionality
    to $d=25$ using PCA. This is the most general solution that also works well in
    practice. We call it EmbPT.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuned prompt embeddings. Sentence transformers in general might not be
    most suitable for embedding prompt templates, thus we also consider fine-tuning
    BERT (Devlin et al., [2019](#bib.bib8)) as an embedder. To do so, we use evaluation
    data for all examples and prompt formats from a subset of LLMs (these LLMs are
    excluded when assessing the quality of our estimators) and fine-tune bert-base-uncased
    to predict $Y_{ij}$ as in ([3.3](#S3.E3 "In 3.1 The correctness model ‣ 3 Performance
    distribution and quantiles estimation ‣ Efficient multi-prompt evaluation of LLMs")).
    We call this variation EmbFT and provide additional details in Appendix [I](#A9
    "Appendix I BERT fine-tuning details ‣ Efficient multi-prompt evaluation of LLMs").
    We acknowledge that obtaining such evaluation data for fine-tuning might be expensive,
    however, it might be justified in some applications if these embeddings provide
    sufficient savings for future LLM evaluations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discrete prompt covariates. For BBH and LMentry, we coded a heuristic function
    that captures frequently occurring differences in common prompting templates.
    Examples of such covariates are the number of line breaks or the count of certain
    special characters (e.g., dashes or colons). Each one of these covariates is encoded
    in $x_{i}$. A full list of the used heuristics is detailed in Appendix [J](#A10
    "Appendix J Heuristics for discrete features ‣ Efficient multi-prompt evaluation
    of LLMs"). For MMLU, we adopted approach of (Sclar et al., [2023](#bib.bib32))
    to generate prompt variations via templates (see Algorithm [3](#algorithm3 "In
    Appendix G Details MMLU data ‣ Efficient multi-prompt evaluation of LLMs")), which
    also provides a natural way to construct the covariates, e.g., the presence of
    dashes or colons.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To the best of our knowledge, the methods introduced in §[3](#S3 "3 Performance
    distribution and quantiles estimation ‣ Efficient multi-prompt evaluation of LLMs")
    are the first ones handling the problem of efficient evaluation of performance
    *distribution* of LLMs across multiple prompts. Thus, we compare different variations
    of our method with one natural baseline (“avg”) which estimates $S_{i}$. To make
    comparisons fair, we sample the data using Algorithm [2](#algorithm2 "In 3.2 Performance
    distribution and quantiles estimation using the correctness model ‣ 3 Performance
    distribution and quantiles estimation ‣ Efficient multi-prompt evaluation of LLMs")
    for all methods and the baseline.
  prefs: []
  type: TYPE_NORMAL
- en: Key results
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We investigate the effectiveness of the different variations of PromptEval
    (PE) against the “avg” baseline strategy in quantile estimation and overall performance
    distribution estimation across prompt templates. In total, we consider five variations
    of PromptEval: (i) PE-Rasch (model in ([3.2](#S3.E2 "In 3.1 The correctness model
    ‣ 3 Performance distribution and quantiles estimation ‣ Efficient multi-prompt
    evaluation of LLMs")) is a Rach model), (ii) PE-discrete (discrete covariates
    are used for prompt templates), (iii) PE-EmbPT (pre-trained LLM embeddings are
    used for prompt templates), and (iv) PE-EmbFT (fine-tuned LLM embeddings are used
    for prompt templates). Within each one of the benchmarks, we conduct a different
    experiment for each one of the tasks, LLMs, and 5 random seeds used when sampling
    $Y_{\mathcal{E}}$ to the total number of evaluations on MMLU.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distribution estimation. Our results for quantile estimation can be seen in
    Figure [2](#S5.F2 "Figure 2 ‣ Key results ‣ 5 Assessing multi-prompt evaluation
    strategies ‣ Efficient multi-prompt evaluation of LLMs"). We see that, in general,
    all variations of PromptEval, including its simplest version (PE-Rasch), can do
    much better in distribution estimation when compared to the baseline. Among our
    methods, the ones that use covariates are the best ones.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantile estimation. Our results for quantile estimation are presented in Figure
    [3](#S5.F3 "Figure 3 ‣ Key results ‣ 5 Assessing multi-prompt evaluation strategies
    ‣ Efficient multi-prompt evaluation of LLMs"). As before, even the simplest version
    of our method (PE-Rasch) does much better than the considered baseline. For all
    the other variations of PromptEval, estimating extreme quantiles is usually hard
    and needs more evaluations, while more central ones (e.g., median) can be accurately
    estimated with 200 evaluations, providing more than 100x compute saving in most
    cases. Regarding the different variations of PromptEval, we found that the pre-trained
    embeddings are robust across benchmarks, while the discrete covariates could not
    do well on LMentry data. Using covariates obtained via fine-tuning the BERT model
    provides some further improvements, for example, for extreme quantiles and small
    evaluation budget settings on MMLU. However, fine-tuning requires collecting large
    amounts of evaluation data and in most cases, we anticipate that it would be more
    practical to use PromptEval with pre-trained embedder and moderate evaluation
    budget instead.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/339c804bfb471107de5493cab6972354.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Performance distribution estimation errors measured with Wasserstein-1
    distance on three benchmarks.'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/4b9fc4b09bf0d4be393cb923cc908b8b.png) |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/f80d12df157d65be633f327f412367c3.png) |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/b2f12519f6c5acaa68557d6b791461d0.png) |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 3: Performance quantile estimation errors for varying quantiles (columns)
    and benchmarks (rows).'
  prefs: []
  type: TYPE_NORMAL
- en: Best-prompt identification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e2c284bee41b0662788f5cc701938dd1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Best-prompt identification.'
  prefs: []
  type: TYPE_NORMAL
- en: The best-prompt identification task (Shi et al., [2024](#bib.bib33)) is to find
    the best prompt from a set of fixed templates, i.e., the one that gives the best
    performance for a task at hand. Shi et al. ([2024](#bib.bib33)) propose framing
    this problem as a bandit problem and using a linear model or an MLP to predict
    the performance of each prompt template. To apply PromptEval in this setting we
    use our model ([3.2](#S3.E2 "In 3.1 The correctness model ‣ 3 Performance distribution
    and quantiles estimation ‣ Efficient multi-prompt evaluation of LLMs")) and X-pIRT
    to estimate how good each template is coupled with sequential elimination algorithm
    (Azizi et al., [2021](#bib.bib1)) (as in Shi et al. ([2024](#bib.bib33))) to select
    prompt-example pairs for evaluation in each round. In Figure [4](#S5.F4 "Figure
    4 ‣ Best-prompt identification ‣ 5 Assessing multi-prompt evaluation strategies
    ‣ Efficient multi-prompt evaluation of LLMs") we compare our PE to the baseline
    TRIPLE-GSE (Shi et al., [2024](#bib.bib33)) with a logistic regression performance
    predictor and the same three types of covariates (PE-OneHot corresponds to PE-Rasch
    in previous experiments). For all covariate choices, we show that using PromptEval
    for best-prompt identification results in lower regret, i.e., the performance
    of the best template minus the performance of the chosen template. We include
    the full results for other benchmarks and also apply TRIPLE-GSE with an MLP in
    Appendix [E](#A5 "Appendix E Extra results for best-prompt identification ‣ Efficient
    multi-prompt evaluation of LLMs").
  prefs: []
  type: TYPE_NORMAL
- en: 6 Analysis of prompt sensitivity on MMLU
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Prior work reports strong sensitivity of LLMs to spurious prompt template changes
    (see Section [1.1](#S1.SS1 "1.1 Related work ‣ 1 Introduction ‣ Efficient multi-prompt
    evaluation of LLMs")). For example, Sclar et al. ([2023](#bib.bib32)) observe
    performance changes of up to 80% for Natural Instructions tasks (Wang et al.,
    [2022](#bib.bib41)) due to template changes. Despite its popularity, no such analysis
    exists for the MMLU dataset to date. We here provide an in-depth analysis of MMLU
    prompt sensitivity.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/04930ab753506829ad7ade3f67753e12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Accuracy spread across 57 subjects.'
  prefs: []
  type: TYPE_NORMAL
- en: Performance spread
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When averaged across subjects, we observe relatively small performance spreads
    per LLM compared to other datasets in the literature (see Figure [12](#A8.F12
    "Figure 12 ‣ Appendix H Details MMLU spread analysis ‣ Efficient multi-prompt
    evaluation of LLMs") in the Appendix [H](#A8 "Appendix H Details MMLU spread analysis
    ‣ Efficient multi-prompt evaluation of LLMs")). For example, we can consistently
    identify Llama-3-70B-Instruct as the best performing model, independent of the
    prompt template. On the other hand, scores within individual subjects are highly
    inconsistent. Figure [5](#S6.F5 "Figure 5 ‣ 6 Analysis of prompt sensitivity on
    MMLU ‣ Efficient multi-prompt evaluation of LLMs") shows the distribution of prompt
    spreads (max-min acc.) across subjects per LLM. Most LLMs demonstrate a significant
    average spread of around 10% at the subject level.
  prefs: []
  type: TYPE_NORMAL
- en: Template consistency
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In practice, having consistently performing templates is highly relevant *within
    a single LLM* or *across LLMs* for the same subject. To evaluate the template
    consistency, we rank template performances either across subjects or across LLMs
    to then calculate the agreement across those rankings using Kendall’s $W$ (Kendall
    and Smith, [1939](#bib.bib16), inspired by Mizrahi et al. [2023](#bib.bib24)).
  prefs: []
  type: TYPE_NORMAL
- en: Within LLMs, we observe that Gemma-7B-it has a notably higher Kendall’s $W$
    of 0.45 than any other model, meaning a fixed set of prompts performs best across
    many subjects (for full results, see Table [1](#A8.T1 "Table 1 ‣ Appendix H Details
    MMLU spread analysis ‣ Efficient multi-prompt evaluation of LLMs") in the Appendix).
    Across LLMs, we do not observe high correlations within any of the subjects (see
    Figure [13](#A8.F13 "Figure 13 ‣ Appendix H Details MMLU spread analysis ‣ Efficient
    multi-prompt evaluation of LLMs") in Appendix [H](#A8 "Appendix H Details MMLU
    spread analysis ‣ Efficient multi-prompt evaluation of LLMs")). Hence, similar
    to previous findings (e.g. Sclar et al., [2023](#bib.bib32)), we do not identify
    any coherent template preferences across LLMs (for detailed results, see Appendix [H](#A8
    "Appendix H Details MMLU spread analysis ‣ Efficient multi-prompt evaluation of
    LLMs")).
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'PromptEval enables a more comprehensive evaluation of LLMs. We hope that comparing
    distributions or quantiles across many prompt variants will enable more robust
    leaderboards and address the common concern of comparing LLMs with a single pre-defined
    prompt. Prior to our work, a major limitation of such evaluation was its cost.
    We demonstrated empirically across several popular benchmarks that our method
    can produce accurate performance distribution and quantile estimates at the cost
    of 2-4 single-prompt evaluations, out of hundreds possible. However, several questions
    remain: how to decide on the set of prompts for evaluation and how to best utilize
    our distribution estimates for comparison in various contexts. For the former,
    we utilized suggestions from prior work (Mizrahi et al., [2023](#bib.bib24); Sclar
    et al., [2023](#bib.bib32)) and for the latter, we primarily focused on quantiles
    as well-established robust performance measures.'
  prefs: []
  type: TYPE_NORMAL
- en: Besides evaluation, another common problem in practice is finding the best prompt
    for a given task. Our method can be applied in this setting when there is a pre-defined
    set of candidate prompts (Figure [4](#S5.F4 "Figure 4 ‣ Best-prompt identification
    ‣ 5 Assessing multi-prompt evaluation strategies ‣ Efficient multi-prompt evaluation
    of LLMs")). However, several recent works (Prasad et al., [2023](#bib.bib28);
    Yang et al., [2023](#bib.bib44); Li and Wu, [2023](#bib.bib20); Ye et al., [2023a](#bib.bib45))
    demonstrate the benefits of dynamically generating new prompt candidates. For
    example, Prasad et al. ([2023](#bib.bib28)) propose an evolutionary algorithm
    that creates new prompts based on the ones that performed well at an earlier iteration.
    Extending PromptEval to accommodate an evolving set of prompt candidates is an
    interesting future work direction.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper is based upon work supported by the National Science Foundation (NSF)
    under grants no. 2027737 and 2113373.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Azizi et al. [2021] Mohammad Javad Azizi, Branislav Kveton, and Mohammad Ghavamzadeh.
    Fixed-budget best-arm identification in structured bandits. *arXiv preprint arXiv:2106.04763*,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beeching et al. [2023] Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon
    Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas
    Wolf. Open llm leaderboard. [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard),
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brzezińska [2020] Justyna Brzezińska. Item response theory models in the measurement
    theory. *Communications in Statistics-Simulation and Computation*, 49(12):3299–3313,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cai et al. [2016] Li Cai, Kilchan Choi, Mark Hansen, and Lauren Harrell. Item
    response theory. *Annual Review of Statistics and Its Application*, 3:297–321,
    2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2023] Yunxiao Chen, Chengcheng Li, Jing Ouyang, and Gongjun Xu.
    Statistical inference for noisy incomplete binary matrix. *Journal of Machine
    Learning Research*, 24(95):1–66, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Choshen et al. [2024] Leshem Choshen, Ariel Gera, Yotam Perlitz, Michal Shmueli-Scheuer,
    and Gabriel Stanovsky. Navigating the modern evaluation landscape: Considerations
    in benchmarks and frameworks for large language models (llms). In *International
    Conference on Language Resources and Evaluation*, 2024. URL [https://api.semanticscholar.org/CorpusID:269804253](https://api.semanticscholar.org/CorpusID:269804253).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Clements et al. [2008] Douglas H Clements, Julie H Sarama, and Xiufeng H Liu.
    Development of a measure of early mathematics achievement using the rasch model:
    The research-based early maths assessment. *Educational Psychology*, 28(4):457–482,
    2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. [2019] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. Bert: Pre-training of deep bidirectional transformers for language
    understanding. In *Proceedings of the 2019 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)*, pages 4171–4186, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Efrat et al. [2022] Avia Efrat, Or Honovich, and Omer Levy. Lmentry: A language
    model benchmark of elementary language tasks. *arXiv preprint arXiv:2211.02069*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fahrmeir and Kaufmann [1985] Ludwig Fahrmeir and Heinz Kaufmann. Consistency
    and asymptotic normality of the maximum likelihood estimator in generalized linear
    models. *The Annals of Statistics*, 13(1):342–368, 1985.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gemma et al. [2024] Team Gemma, Thomas Mesnard, Cassidy Hardin, Robert Dadashi,
    Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay
    Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology.
    *arXiv preprint arXiv:2403.08295*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Georg [1960] Rasch Georg. Probabilistic models for some intelligence and attainment
    tests. *Copenhagen: Institute of Education Research*, 1960.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. [2020] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language
    understanding. *arXiv preprint arXiv:2009.03300*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. [2023] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. *arXiv preprint
    arXiv:2310.06825*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Karpukhin et al. [2020] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick
    Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval
    for open-domain question answering. In *Proceedings of the 2020 Conference on
    Empirical Methods in Natural Language Processing (EMNLP)*, pages 6769–6781, Online,
    November 2020\. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.550.
    URL [https://www.aclweb.org/anthology/2020.emnlp-main.550](https://www.aclweb.org/anthology/2020.emnlp-main.550).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kendall and Smith [1939] Maurice G Kendall and B Babington Smith. The problem
    of m rankings. *The annals of mathematical statistics*, 10(3):275–287, 1939.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kingma and Ba [2014] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic
    optimization. *arXiv preprint arXiv:1412.6980*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lalor et al. [2016] John P Lalor, Hao Wu, and Hong Yu. Building an evaluation
    scale using item response theory. In *Proceedings of the Conference on Empirical
    Methods in Natural Language Processing. Conference on Empirical Methods in Natural
    Language Processing*, volume 2016, page 648\. NIH Public Access, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2023] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan
    Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval:
    An automatic evaluator of instruction-following models. [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval),
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li and Wu [2023] Yujian Betterest Li and Kai Wu. Spell: Semantic prompt evolution
    based on a llm. *arXiv preprint arXiv:2310.01260*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang et al. [2022] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras,
    Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya
    Kumar, et al. Holistic evaluation of language models. *arXiv preprint arXiv:2211.09110*,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lord et al. [1968] FM Lord, MR Novick, and Allan Birnbaum. Statistical theories
    of mental test scores. 1968.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Meta [2024] Meta. Introducing meta llama 3: The most capable openly available
    llm to date. [https://ai.meta.com/blog/meta-llama-3](https://ai.meta.com/blog/meta-llama-3),
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mizrahi et al. [2023] Moran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror, Dafna
    Shahaf, and Gabriel Stanovsky. State of what art? a call for multi-prompt llm
    evaluation. *arXiv preprint arXiv:2401.00595*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nitsure et al. [2023] Apoorva Nitsure, Youssef Mroueh, Mattia Rigotti, Kristjan
    Greenewald, Brian Belgodere, Mikhail Yurochkin, Jiri Navratil, Igor Melnyk, and
    Jerret Ross. Risk assessment and statistical significance in the age of foundation
    models. *arXiv preprint arXiv:2310.07132*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perlitz et al. [2023] Yotam Perlitz, Elron Bandel, Ariel Gera, Ofir Arviv, Liat
    Ein-Dor, Eyal Shnarch, Noam Slonim, Michal Shmueli-Scheuer, and Leshem Choshen.
    Efficient benchmarking (of language models). *arXiv preprint arXiv:2308.11696*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Polo et al. [2024] Felipe Maia Polo, Lucas Weber, Leshem Choshen, Yuekai Sun,
    Gongjun Xu, and Mikhail Yurochkin. tinybenchmarks: evaluating llms with fewer
    examples. *arXiv preprint arXiv:2402.14992*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prasad et al. [2023] Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal.
    Grips: Gradient-free, edit-based instruction search for prompting large language
    models. In *Proceedings of the 17th Conference of the European Chapter of the
    Association for Computational Linguistics*, pages 3827–3846, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reimers and Gurevych [2019] Nils Reimers and Iryna Gurevych. Sentence-bert:
    Sentence embeddings using siamese bert-networks. In *Proceedings of the 2019 Conference
    on Empirical Methods in Natural Language Processing*. Association for Computational
    Linguistics, 11 2019. URL [https://arxiv.org/abs/1908.10084](https://arxiv.org/abs/1908.10084).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resnick [2019] Sidney Resnick. *A probability path*. Springer, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rodriguez et al. [2021] Pedro Rodriguez, Joe Barrow, Alexander Miserlis Hoyle,
    John P. Lalor, Robin Jia, and Jordan Boyd-Graber. Evaluation examples are not
    equally informative: How should that change NLP leaderboards? In Chengqing Zong,
    Fei Xia, Wenjie Li, and Roberto Navigli, editors, *Proceedings of the 59th Annual
    Meeting of the Association for Computational Linguistics and the 11th International
    Joint Conference on Natural Language Processing (Volume 1: Long Papers)*, pages
    4486–4503, Online, August 2021\. Association for Computational Linguistics. doi:
    10.18653/v1/2021.acl-long.346. URL [https://aclanthology.org/2021.acl-long.346](https://aclanthology.org/2021.acl-long.346).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sclar et al. [2023] Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr.
    Quantifying language models’ sensitivity to spurious features in prompt design
    or: How i learned to start worrying about prompt formatting. *arXiv preprint arXiv:2310.11324*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. [2024] Chengshuai Shi, Kun Yang, Jing Yang, and Cong Shen. Best arm
    identification for prompt learning under a limited budget. *arXiv preprint arXiv:2402.09723*,
    2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Srivastava et al. [2022] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,
    Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya
    Gupta, Adrià Garriga-Alonso, et al. Beyond the imitation game: Quantifying and
    extrapolating the capabilities of language models. *arXiv preprint arXiv:2206.04615*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Starke et al. [2017] Alain Starke, Martijn Willemsen, and Chris Snijders. Effective
    user interface designs to increase energy-efficient behavior in a rasch-based
    energy recommender system. In *Proceedings of the eleventh ACM conference on recommender
    systems*, pages 65–73, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suzgun et al. [2022] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian
    Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny
    Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve
    them. *arXiv preprint arXiv:2210.09261*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Van der Linden [2018] Wim J Van der Linden. *Handbook of item response theory:
    Three volume set*. CRC Press, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vania et al. [2021] Clara Vania, Phu Mon Htut, William Huang, Dhara Mungra,
    Richard Yuanzhe Pang, Jason Phang, Haokun Liu, Kyunghyun Cho, and Samuel R Bowman.
    Comparing test sets with item response theory. *arXiv preprint arXiv:2106.00840*,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vivek et al. [2023] Rajan Vivek, Kawin Ethayarajh, Diyi Yang, and Douwe Kiela.
    Anchor points: Benchmarking models with much fewer examples. *arXiv preprint arXiv:2309.08638*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Voronov et al. [2024] Anton Voronov, Lena Wolf, and Max Ryabinin. Mind your
    format: Towards consistent evaluation of in-context learning improvements. *arXiv
    preprint arXiv:2401.06766*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2022] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh
    Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran,
    Atharva Naik, David Stap, et al. Super-naturalinstructions: Generalization via
    declarative instructions on 1600+ nlp tasks. *arXiv preprint arXiv:2204.07705*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weber et al. [2023a] Lucas Weber, Elia Bruni, and Dieuwke Hupkes. The icl consistency
    test. *arXiv preprint arXiv:2312.04945*, 2023a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weber et al. [2023b] Lucas Weber, Elia Bruni, and Dieuwke Hupkes. Mind the
    instructions: a holistic evaluation of consistency and interactions in prompt-based
    learning. In *Proceedings of the 27th Conference on Computational Natural Language
    Learning (CoNLL)*, pages 294–313, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. [2023] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V
    Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. *arXiv preprint
    arXiv:2309.03409*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ye et al. [2023a] Qinyuan Ye, Maxamed Axmed, Reid Pryzant, and Fereshte Khani.
    Prompt engineering a prompt engineer. *arXiv preprint arXiv:2311.05661*, 2023a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ye et al. [2023b] Qinyuan Ye, Harvey Yiyun Fu, Xiang Ren, and Robin Jia. How
    predictable are large language model capabilities? a case study on big-bench.
    *arXiv preprint arXiv:2305.14947*, 2023b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While our method provides a more reliable measure and a more flexible one, it
    assumes multiple prompts. Thus, if the question of which single prompt should
    be used was a challenge for old benchmarks, which set of prompt templates to use
    is the challenge now. While methods have been suggested for generating multiple
    prompts and diversifying those [Mizrahi et al., [2023](#bib.bib24)], and while
    when many prompts are suggested the choice of each one is likely less critical,
    it is a limitation future work should consider.
  prefs: []
  type: TYPE_NORMAL
- en: Another limitation we note is that we do not focus on prompt engineering and
    do not solve this problem. While this would have been a crucial contribution for
    the field, we assume a set of prompts and assume we only care about evaluation
    and not training, which make for a useful and common setting, but it does not
    encompass this goal.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Adapting the correctness model for bounded $Y_{ij}$
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There might be situations in LLM evaluation in which $Y_{ij}\notin\{0,1\}$ and
    work with this newly created variable.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Computing resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All experiments were conducted using a virtual machine with 32 cores. The results
    for each benchmark separately can be obtained within 3-6 hours.
  prefs: []
  type: TYPE_NORMAL
- en: For fine-tuning BERT embeddings, we employ multiple NVIDIA A30 GPUs with 24
    GB vRAM, requiring 70 hours of training and an additional approximately 350 hours
    for hyperparameter search. Fine-tuning can be conducted on GPUs with smaller capacities.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Estimation errors by task
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Figures [6](#A4.F6 "Figure 6 ‣ Appendix D Estimation errors by task ‣ Efficient
    multi-prompt evaluation of LLMs"), [7](#A4.F7 "Figure 7 ‣ Appendix D Estimation
    errors by task ‣ Efficient multi-prompt evaluation of LLMs"), and [8](#A4.F8 "Figure
    8 ‣ Appendix D Estimation errors by task ‣ Efficient multi-prompt evaluation of
    LLMs"), we analyze the Wasserstein 1-distance per task for each benchmark when
    using the method PE-EmbPT, a robust and versatile variation of PromptEval. The
    results show that for BBH and LMentry, the estimation errors (Wasserstein 1-distance)
    are more uniform across tasks compared to MMLU, where some tasks exhibit higher
    estimation errors. This discrepancy occurs because all tasks in BBH and LMentry
    have the same number of examples, whereas tasks in MMLU, particularly those with
    higher estimation errors, have a significantly larger number of examples when
    compared to the others. In those cases, a larger number of evaluations is recommended.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/962b52c9d3d03735f8699176064e8131.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Estimation error for the BBH tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/56bebb529e0a3654bc0b05fd4a5bf294.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Estimation error for the LMentry tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c72faf78397ae91bf46c6dd2460edb2c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Estimation error for the MMLU tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Extra results for best-prompt identification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Figures [9](#A5.F9 "Figure 9 ‣ Appendix E Extra results for best-prompt identification
    ‣ Efficient multi-prompt evaluation of LLMs"), [10](#A5.F10 "Figure 10 ‣ Appendix
    E Extra results for best-prompt identification ‣ Efficient multi-prompt evaluation
    of LLMs"), and [11](#A5.F11 "Figure 11 ‣ Appendix E Extra results for best-prompt
    identification ‣ Efficient multi-prompt evaluation of LLMs"), we can see the full
    results for MMLU, BBH, and LMentry. For all benchmarks, we can see that within
    each triple “PE”, “GTRIPLE-SE”, “TRIPLE-MLP-GSE”, the “PE” version always has
    some advantage with a lower regret.
  prefs: []
  type: TYPE_NORMAL
- en: 'The tuning and fitting process of the Multi-Layer Perceptron (MLP) classifier
    involves setting up a pipeline that includes feature scaling and the MLP classifier
    itself, which has 30 neurons in its hidden layer. This process begins by defining
    a range of values for critical hyperparameters: the l2 regularization strength
    is tested over the range from 0.001 to 10, and the initial learning rate is tested
    over the range from 0.001 to 0.1\. These values are systematically tested through
    cross-validation to determine the optimal combination. During this phase, cross-validation
    ensures that the model is evaluated on different subsets of the data to prevent
    overfitting and to ensure robust performance. Once the best hyperparameters are
    identified, the final model is trained on the entire dataset using these optimal
    settings, resulting in a well-tuned MLP classifier ready for deployment.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8f31390207c408802db4db273332fa1b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Best-prompt identification for MMLU'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3fc82c3abf04fa68945a35917d120d48.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Best-prompt identification for BBH'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ee17334495195895fbe5ccb40474e94e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Best-prompt identification for LMentry'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix F Theoretical results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: F.1 Consistency of X-pIRT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In Theorem [F.1](#A6.Thmtheorem1 "Theorem F.1\. ‣ F.1 Consistency of X-pIRT
    ‣ Appendix F Theoretical results ‣ Efficient multi-prompt evaluation of LLMs"),
    we claim that the X-pIRT estimator is uniformly consistent over all $i\in\mathcal{I}$.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem F.1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Under conditions [4.1](#S4.Thmtheorem1 "Condition 4.1\. ‣ 4 Theoretical guarantees
    ‣ Efficient multi-prompt evaluation of LLMs"), [4.2](#S4.Thmtheorem2 "Condition
    4.2\. ‣ 4 Theoretical guarantees ‣ Efficient multi-prompt evaluation of LLMs"),
    and [4.3](#S4.Thmtheorem3 "Condition 4.3\. ‣ 4 Theoretical guarantees ‣ Efficient
    multi-prompt evaluation of LLMs"), it is true that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\textstyle\sup_{i\in\mathcal{I}}\left&#124;\hat{{\mathbb{E}}}[S_{i}\mid
    Y_{\mathcal{S}}]-S_{i}\right&#124;\to 0\text{ in probability as }I,J\to\infty.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: A direct consequence of Theorem [F.1](#A6.Thmtheorem1 "Theorem F.1\. ‣ F.1 Consistency
    of X-pIRT ‣ Appendix F Theoretical results ‣ Efficient multi-prompt evaluation
    of LLMs") is that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\textstyle\left&#124;\frac{1}{I}\sum_{i\in\mathcal{I}}\hat{{\mathbb{E}}}[S_{i}\mid
    Y_{\mathcal{S}}]-\frac{1}{I}\sum_{i\in\mathcal{I}}S_{i}\right&#124;\leq\frac{1}{I}\sum_{i\in\mathcal{I}}\left&#124;\hat{{\mathbb{E}}}[S_{i}\mid
    Y_{\mathcal{S}}]-S_{i}\right&#124;\leq\sup_{i\in\mathcal{I}}\left&#124;\hat{{\mathbb{E}}}[S_{i}\mid
    Y_{\mathcal{S}}]-S_{i}\right&#124;\to 0$ |  |'
  prefs: []
  type: TYPE_TB
- en: in probability as $I,J\to\infty$. This means that the mean of predicted performances
    is also consistent if a practitioner wants to use it as a summary statistic.
  prefs: []
  type: TYPE_NORMAL
- en: The proof of Theorem [F.1](#A6.Thmtheorem1 "Theorem F.1\. ‣ F.1 Consistency
    of X-pIRT ‣ Appendix F Theoretical results ‣ Efficient multi-prompt evaluation
    of LLMs") is embedded in the proof of Theorem [4.4](#S4.Thmtheorem4 "Theorem 4.4\.
    ‣ 4 Theoretical guarantees ‣ Efficient multi-prompt evaluation of LLMs").
  prefs: []
  type: TYPE_NORMAL
- en: F.2 Proof of Theorem [4.4](#S4.Thmtheorem4 "Theorem 4.4\. ‣ 4 Theoretical guarantees
    ‣ Efficient multi-prompt evaluation of LLMs")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the following results, we denote $\psi^{\top}x_{i}$.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma F.2.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Under Conditions [4.1](#S4.Thmtheorem1 "Condition 4.1\. ‣ 4 Theoretical guarantees
    ‣ Efficient multi-prompt evaluation of LLMs") and [4.3](#S4.Thmtheorem3 "Condition
    4.3\. ‣ 4 Theoretical guarantees ‣ Efficient multi-prompt evaluation of LLMs"),
    we have that $\sup_{i\in\mathcal{I}}|\hat{\theta}_{i}-\theta_{i}|=o_{P}(1)$.
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We prove that $\sup_{i\in\mathcal{I}}|\hat{\theta}_{i}-\theta_{i}|=o_{P}(1)$.
    The second statement is obtained in the same way.
  prefs: []
  type: TYPE_NORMAL
- en: See that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: as $I,J\to\infty$. Where the first inequality is obtained using the Cauchy–Schwarz
    inequality, the second is obtained using Condition [4.1](#S4.Thmtheorem1 "Condition
    4.1\. ‣ 4 Theoretical guarantees ‣ Efficient multi-prompt evaluation of LLMs"),
    and the last equality is a consequence of Condition [4.3](#S4.Thmtheorem3 "Condition
    4.3\. ‣ 4 Theoretical guarantees ‣ Efficient multi-prompt evaluation of LLMs")
    and the continuous mapping theorem [Resnick, [2019](#bib.bib30)]. ∎
  prefs: []
  type: TYPE_NORMAL
- en: Lemma F.3.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Under Conditions [4.1](#S4.Thmtheorem1 "Condition 4.1\. ‣ 4 Theoretical guarantees
    ‣ Efficient multi-prompt evaluation of LLMs") and [4.3](#S4.Thmtheorem3 "Condition
    4.3\. ‣ 4 Theoretical guarantees ‣ Efficient multi-prompt evaluation of LLMs"),
    it is true that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\sup_{i\in\mathcal{I}}\left&#124;\hat{{\mathbb{E}}}[S_{i}\mid Y_{\mathcal{E}}]-{\mathbb{E}}[S_{i}\mid
    Y_{\mathcal{E}}]\right&#124;=o_{P}(1)\text{ as }I,J\to\infty.$ |  |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: See that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\sup_{i\in\mathcal{I}}\left&#124;\hat{{\mathbb{E}}}[S_{i}\mid
    Y_{\mathcal{E}}]-{\mathbb{E}}[S_{i}\mid Y_{\mathcal{E}}]\right&#124;$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\leq\frac{1}{4}\left(\sup_{i}&#124;\hat{\theta}_{i}-\theta_{i}&#124;+\sup_{j}&#124;\hat{\beta}_{j}-\beta_{j}&#124;\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=o_{P}(1)$ |  |'
  prefs: []
  type: TYPE_TB
- en: where the third step is justified by the fact that $\sigma$-Lipschitz and the
    last step is justified by Lemma [F.2](#A6.Thmtheorem2 "Lemma F.2\. ‣ F.2 Proof
    of Theorem 4.4 ‣ Appendix F Theoretical results ‣ Efficient multi-prompt evaluation
    of LLMs"). ∎
  prefs: []
  type: TYPE_NORMAL
- en: Lemma F.4.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Under Condition [4.2](#S4.Thmtheorem2 "Condition 4.2\. ‣ 4 Theoretical guarantees
    ‣ Efficient multi-prompt evaluation of LLMs"), it is true that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\sup_{i\in\mathcal{I}}\left&#124;{\mathbb{E}}[S_{i}\mid Y_{\mathcal{E}}]-S_{i}\right&#124;=o_{P}(1)\text{
    as }I,J\to\infty.$ |  |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For an arbitrary <math id=$$, see that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle{\mathbb{P}}\left(\sup_{i\in\mathcal{I}}\left&#124;{\mathbb{E}}[S_{i}\mid
    Y_{\mathcal{E}}]-S_{i}\right&#124;\geq\epsilon\right)$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\leq\sum_{i\in\mathcal{I}}{\mathbb{P}}\left(\left&#124;{\mathbb{E}}[S_{i}\mid
    Y_{\mathcal{E}}]-S_{i}\right&#124;\geq\epsilon\right)$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\leq\sum_{i\in\mathcal{I}}{\mathbb{P}}\left(\left&#124;\frac{1}{m}\sum_{j\not\in\mathcal{J}_{i}}Z_{ij}\right&#124;\geq\epsilon\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $Z_{ij}\triangleq Y_{ij}-\sigma(\theta_{i}-\beta_{j})$. Applying Hoeffding’s
    inequality, we obtain
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle{\mathbb{P}}\left(\sup_{i\in\mathcal{I}}\left&#124;{\mathbb{E}}[S_{i}\mid
    Y_{\mathcal{E}}]-S_{i}\right&#124;\geq\epsilon\right)$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=2\mathrm{exp}\left(\log I-2\epsilon^{2}m\right)$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=2\mathrm{exp}\left(-\log(\mathrm{exp}(2\epsilon^{2}m)/I)\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\to 0$ |  |'
  prefs: []
  type: TYPE_TB
- en: ∎
  prefs: []
  type: TYPE_NORMAL
- en: Lemma F.5.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let $a_{1},\cdots,a_{n}$ $b$.
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If $a_{i}$ such that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\underbrace{&#124;\{a\in\mathcal{A}:a_{i}=a\}&#124;}_{m_{2}+1}+\underbrace{&#124;\{a\in\mathcal{A}:a_{i}></math>
    |  |'
  prefs: []
  type: TYPE_TB
- en: Because $a_{i}$.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, because $m_{1}+m_{2}+1\geq{p\cdot n}$. ∎
  prefs: []
  type: TYPE_NORMAL
- en: Lemma F.6.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let $\hat{i}$.
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Define $\mathcal{A}\triangleq\{S_{1},\cdots,S_{i}\}$, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If (i) holds, then there is at least $M_{2}+M_{3}+1$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If (ii) holds, then there is at least $M_{1}+M_{2}+1$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This means that under $\sup_{i\in\mathcal{I}}|\hat{{\mathbb{E}}}[S_{i}\mid Y_{\mathcal{E}}]-S_{i}|\leq\epsilon$.
    ∎
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Theorem [4.4](#S4.Thmtheorem4 "Theorem 4.4\. ‣ 4 Theoretical guarantees
    ‣ Efficient multi-prompt evaluation of LLMs") (Part 1).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let $\hat{i}$. Consequently,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\mathbb{P}}\left(&#124;\hat{{\mathbb{E}}}[S_{\hat{i}}\mid Y_{S}]-S_{i^{*}}&#124;\leq
    2\epsilon\right)\geq{\mathbb{P}}\left(\sup_{i\in\mathcal{I}}&#124;\hat{{\mathbb{E}}}[S_{i}\mid
    Y_{S}]-S_{i}&#124;\leq\epsilon\right)=1+o(1)$ |  |'
  prefs: []
  type: TYPE_TB
- en: because
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\sup_{i\in\mathcal{I}}\left&#124;\hat{{\mathbb{E}}}[S_{i}\mid Y_{\mathcal{E}}]-S_{i}\right&#124;\leq\sup_{i\in\mathcal{I}}\left&#124;\hat{{\mathbb{E}}}[S_{i}\mid
    Y_{\mathcal{E}}]-{\mathbb{E}}[S_{i}\mid Y_{\mathcal{E}}]\right&#124;+\sup_{i\in\mathcal{I}}\left&#124;{\mathbb{E}}[S_{i}\mid
    Y_{\mathcal{E}}]-S_{i}\right&#124;=o_{P}(1)\text{ as }I,J\to\infty$ |  |'
  prefs: []
  type: TYPE_TB
- en: holds by lemmas [F.3](#A6.Thmtheorem3 $$.
  prefs: []
  type: TYPE_NORMAL
- en: ∎
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Theorem [4.4](#S4.Thmtheorem4 "Theorem 4.4\. ‣ 4 Theoretical guarantees
    ‣ Efficient multi-prompt evaluation of LLMs") (Part 2).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We start this proof by showing that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $&#124;\hat{Q}(U)-Q(U)&#124;=o_{P}(1)$ |  |'
  prefs: []
  type: TYPE_TB
- en: with $U\sim\text{Unif}[0,1]$.
  prefs: []
  type: TYPE_NORMAL
- en: For an arbitrary $$\epsilon></math>, see that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id=$$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | <math id="A6.Ex26.m1.5" class="ltx_Math" alttext="\displaystyle={\mathbb{E}}\left[\lim_{I,J\to\infty}{\mathbb{P}}(&#124;\hat{Q}(U)-Q(U)&#124;></math>
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=0$ |  |'
  prefs: []
  type: TYPE_TB
- en: where the second equality is justified by the Dominated Convergence Theorem
    [Resnick, [2019](#bib.bib30)] and the last one is justified by $|\hat{Q}(p)-Q(p)|=o_{P}(1)$.
    Now, we see that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\lim_{I,J\to\infty}{\mathbb{E}}[W_{1}(F,\hat{F})]$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\lim_{I,J\to\infty}{\mathbb{E}}\left[&#124;\hat{Q}(U)-Q(U)&#124;\right]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=0$ |  |'
  prefs: []
  type: TYPE_TB
- en: where the last step is justified by Fubini’s Theorem [Resnick, [2019](#bib.bib30)],
    $|\hat{Q}(U)-Q(U)|=o_{P}(1)$ and applying Markov’s inequality, we get
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: ∎
  prefs: []
  type: TYPE_NORMAL
- en: Appendix G Details MMLU data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Algorithm [3](#algorithm3 "In Appendix G Details MMLU data ‣ Efficient multi-prompt
    evaluation of LLMs") for automatically generating templates can be seen as a graph
    traversal of a template graph, whose nodes are defined by which features they
    have: a separator $SEP$. By traversing this graph, we can collect unique templates
    that can used in the evaluation of LLMs on tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '1Input: Base prompt template features: Separator $SEP$, add to templates. Add
    the generated templates to the agenda.return generated templates.'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 3 TemplateGeneration
  prefs: []
  type: TYPE_NORMAL
- en: Appendix H Details MMLU spread analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Figure [12](#A8.F12 "Figure 12 ‣ Appendix H Details MMLU spread analysis ‣ Efficient
    multi-prompt evaluation of LLMs") depicts the performance of LLMs on the whole
    MMLU.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/349c6c77f14214dca00f7c2ee744dfa3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: MMLU accuracy (all 57 subjects).'
  prefs: []
  type: TYPE_NORMAL
- en: To correlate the ranks from different judges, we can use Kendall’s $W$ is the
    number of objects ranked. In our case, we first have MMLU subjects ranking prompt
    templates, and then we have LLMs ranking prompt templates.
  prefs: []
  type: TYPE_NORMAL
- en: In Figure [13](#A8.F13 "Figure 13 ‣ Appendix H Details MMLU spread analysis
    ‣ Efficient multi-prompt evaluation of LLMs"), we see the distribution of Kendall’s
    $W$ around 0.25\. This suggests that there is no "best" prompt for a subject.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e900e82a023e18f8dcf1e0fa26665739.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Kendall’s $W$ over the 57 subjects of MMLU.'
  prefs: []
  type: TYPE_NORMAL
- en: In Table [1](#A8.T1 "Table 1 ‣ Appendix H Details MMLU spread analysis ‣ Efficient
    multi-prompt evaluation of LLMs"), we see the values of Kendall’s $W$ is 0.45
    and 0.35, respectively. Curiously, both of the top-ranked prompt templates have
    lots of commas. The best-ranked prompt is "The, following, are, multiple, choice,
    questions, (with, answers), about, topic], question], Answers], choices], Answer]".
    Interestingly, the comma separation of each word or phrase in this prompt template
    may aid the model in parsing and effectively understanding the different components
    of the prompt structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Kendall’s $W$ per LLM'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Kendall’s W |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| meta-llama/llama-3-8b-instruct | 0.126027 |'
  prefs: []
  type: TYPE_TB
- en: '| meta-llama/llama-3-8b | 0.252835 |'
  prefs: []
  type: TYPE_TB
- en: '| meta-llama/llama-3-70b-instruct | 0.101895 |'
  prefs: []
  type: TYPE_TB
- en: '| mistralai/mistral-7b-instruct-v0-2 | 0.219841 |'
  prefs: []
  type: TYPE_TB
- en: '| mistralai/mistral-7b-v0-1 | 0.345592 |'
  prefs: []
  type: TYPE_TB
- en: '| mistralai/mixtral-8x7b-instruct-v01 | 0.131487 |'
  prefs: []
  type: TYPE_TB
- en: '| codellama/codellama-34b-instruct | 0.287066 |'
  prefs: []
  type: TYPE_TB
- en: '| ibm-mistralai/merlinite-7b | 0.146411 |'
  prefs: []
  type: TYPE_TB
- en: '| google/gemma-7b-it | 0.445478 |'
  prefs: []
  type: TYPE_TB
- en: '| google/gemma-7b | 0.179373 |'
  prefs: []
  type: TYPE_TB
- en: '| google/flan-t5-xl | 0.066501 |'
  prefs: []
  type: TYPE_TB
- en: '| google/flan-t5-xxl | 0.056257 |'
  prefs: []
  type: TYPE_TB
- en: '| google/flan-ul2 | 0.109076 |'
  prefs: []
  type: TYPE_TB
- en: '| tiiuae/falcon-180b | 0.165600 |'
  prefs: []
  type: TYPE_TB
- en: '| tiiuae/falcon-40b | 0.100173 |'
  prefs: []
  type: TYPE_TB
- en: Figure [14](#A8.F14 "Figure 14 ‣ Appendix H Details MMLU spread analysis ‣ Efficient
    multi-prompt evaluation of LLMs") illustrates sensitivity for llama-3-8b, gemma-7b,
    and merlinite-7b, respectively. On the template graph, a distance 1 means templates
    differ by only 1 feature, a distance 2 means templates differ by 2 features, etc.
    We see that there is no significant correlation between template distance and
    the accuracy spread. In the cases of gemma-7b and merlinite-7b, the accuracy spread
    for templates with smaller distance seems to be smaller, possibly implying that
    the template graph for these models is smooth.
  prefs: []
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/046b29692ba4b440c95c3316085bf438.png) | ![Refer to
    caption](img/3e228dc85a7444157cb959fcee62e935.png) | ![Refer to caption](img/29a6fd1b3625377a0db1a80a22f43557.png)
    |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 14: Model sensitivity for llama-3-8b, gemma-7b, and merlinite-7b.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix I BERT fine-tuning details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I.1 Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We augment the BERT model by extending its input embeddings by $|J|$ [Example
    ID] tokens which we use to feed information about the example identity to the
    model. Additionally, we add a linear downward projection (d = 25) on top of the
    final BERT layer to reduce the dimensionality of the resulting covariates.
  prefs: []
  type: TYPE_NORMAL
- en: I.2 Training data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To obtain training examples, we concatenate all prompting templates with all
    [Example ID] tokens giving us $|I|\times|J|$ from the LLMs in the training set,
    making the training task a multi-label binary classification problem. We train
    on an iid split of half of the LLMs at a time and test on the other half. Additionally,
    the training data are split along the example axis into an 80% training and 20%
    validation set.
  prefs: []
  type: TYPE_NORMAL
- en: I.3 Hyperparameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We run a small grid search over different plausible hyperparameter settings
    and settle on the following setup: We employ the Adam optimizer [Kingma and Ba,
    [2014](#bib.bib17)] with an initial learning rate of 2e-5 and a weight decay of
    1e-5\. The learning rate undergoes a linear warm-up over 200 steps, followed by
    exponential decay using the formula $lr_{currnt}=\gamma^{s}\cdot lr_{init}$ is
    set to 0.99995. We train with a batch size of 96.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix J Heuristics for discrete features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the BBH and LMentry benchmarks, we use the following heuristics to construct
    feature representations of prompt templates.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Overview of Discrete Features'
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Feature Name | Description |'
  prefs: []
  type: TYPE_TB
- en: '| Casing Features | All Caps Words | Count of all uppercase words |'
  prefs: []
  type: TYPE_TB
- en: '| Lowercase Words | Count of all lowercase words |'
  prefs: []
  type: TYPE_TB
- en: '| Capitalized Words | Count of words with the first letter capitalized |'
  prefs: []
  type: TYPE_TB
- en: '| Formatting Features | Line Breaks | Count of line breaks |'
  prefs: []
  type: TYPE_TB
- en: '| Framing Words | Count of capitalized or numeric words before a colon |'
  prefs: []
  type: TYPE_TB
- en: '| Special Characters Features | Colon (:) | Count of ’:’ |'
  prefs: []
  type: TYPE_TB
- en: '| Dash (-) | Count of ’-’ |'
  prefs: []
  type: TYPE_TB
- en: '| Double Bar (&#124;&#124;) | Count of ’&#124;&#124;’ |'
  prefs: []
  type: TYPE_TB
- en: '| Separator token | Count of ’<sep>’ |'
  prefs: []
  type: TYPE_TB
- en: '| Double Colon (::) | Count of ’::’ |'
  prefs: []
  type: TYPE_TB
- en: '| Parenthesis Left (() | Count of ’(’ |'
  prefs: []
  type: TYPE_TB
- en: '| Parenthesis Right ()) | Count of ’)’ |'
  prefs: []
  type: TYPE_TB
- en: '| Quotation (") | Count of ’"’ |'
  prefs: []
  type: TYPE_TB
- en: '| Question Mark (?) | Count of ’?’ |'
  prefs: []
  type: TYPE_TB
- en: '| Length Feature | Space Count | Count of spaces |'
  prefs: []
  type: TYPE_TB
