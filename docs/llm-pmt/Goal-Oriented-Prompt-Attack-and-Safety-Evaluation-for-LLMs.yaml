- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:49:31'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Goal-Oriented Prompt Attack and Safety Evaluation for LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2309.11830](https://ar5iv.labs.arxiv.org/html/2309.11830)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Chengyuan Liu¹, Fubang Zhao², Lizhi Qing², Yangyang Kang²²²2Corresponding author.,
  prefs: []
  type: TYPE_NORMAL
- en: Changlong Sun², Kun Kuang¹²²2Corresponding author., Fei Wu¹
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Warning: this paper contains examples that may be offensive or upsetting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Large Language Models (LLMs) presents significant priority in text understanding
    and generation. However, LLMs suffer from the risk of generating harmful contents
    especially while being employed to applications. There are several black-box attack
    methods, such as Prompt Attack, which can change the behaviour of LLMs and induce
    LLMs to generate unexpected answers with harmful contents. Researchers are interested
    in Prompt Attack and Defense with LLMs, while there is no publicly available dataset
    with high successful attacking rate to evaluate the abilities of defending prompt
    attack. In this paper, we introduce a pipeline to construct high-quality prompt
    attack samples, along with a Chinese prompt attack dataset called CPAD. Our prompts
    aim to induce LLMs to generate unexpected outputs with several carefully designed
    prompt attack templates and widely concerned attacking contents. Different from
    previous datasets involving safety estimation, we construct the prompts considering
    three dimensions: contents, attacking methods and goals. Especially, the attacking
    goals indicate the behaviour expected after successfully attacking the LLMs, thus
    the responses can be easily evaluated and analysed. We run several popular Chinese
    LLMs on our dataset, and the results show that our prompts are significantly harmful
    to LLMs, with around 70% attack success rate to GPT-3.5\. CPAD is publicly available
    at https://github.com/liuchengyuan123/CPAD.'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) can generate creative content and solve practical
    planning and reasoning problems (Chowdhery et al. [2022](#bib.bib3); OpenAI [2023](#bib.bib13)).
    Take GPT-3 (Brown et al. [2020](#bib.bib2)) as an example, with 175 billion parameters,
    it stores a large amount of knowledge and exhibits surprising performance on various
    downstream tasks. Instruction tuning (Wei et al. [2021](#bib.bib21)) and Reinforcement
    Learning from Human Feedback (Ziegler et al. [2019](#bib.bib30); Lambert et al.
    [2022](#bib.bib10)) (RLHF) allow LLMs to generate appropriate answers based on
    user queries. LLMs can handle tasks they have never seen before without continuous
    fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ea0cd97f57f2beae00c40a28ae15cbba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Comparison of successful attacking between previous work and CPAD.
    CPAD can further expose the security risks of LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: While LLMs have demonstrated success in numerous tasks, there are still some
    vital flaws for deployment such as the significant security risks associated with
    LLMs. With the help of robust general knowledge and common sense, LLMs can offer
    valuable guidance for harmful behavior and can automatically generate offensive,
    discriminatory and fraudulent content, and misleading views(Deng et al. [2023](#bib.bib4);
    Zhuo et al. [2023](#bib.bib28); Hartvigsen et al. [2022](#bib.bib7)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dffb24e438f905a85d3bea06c9d854b3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Illustration of Prompt Attack. With carefully designed prompts, attackers
    can bypass the ethical constraints of LLMs, inducing LLMs to generate illegal
    contents.'
  prefs: []
  type: TYPE_NORMAL
- en: Although RLHF can prevent LLMs from generating harmful contents by aligning
    the responses with human preference, researchers find that LLMs can still generate
    unexpected sentences under carefully designed prompt attack (Perez and Ribeiro
    [2022](#bib.bib15); Li et al. [2023](#bib.bib11)), as shown in Figure [2](#Sx1.F2
    "Figure 2 ‣ Introduction ‣ Goal-Oriented Prompt Attack and Safety Evaluation for
    LLMs"). Due to the concern of evaluating and enhancing the safety of LLMs, Sun
    et al. ([2023](#bib.bib17)) introduced SAFETYPROMPTS. Xu et al. ([2023](#bib.bib23))
    presented CVALUES, the first Chinese human values evaluation benchmark to measure
    the alignment ability of LLMs in terms of both safety and responsibility criteria.
  prefs: []
  type: TYPE_NORMAL
- en: Nonetheless, the majority of LLMs exhibit a high rate of success in defending
    against the offensive prompts within publicly accessible evaluation benchmark.
    For example, ChatGPT and ChatGLM-6B(Zeng et al. [2022](#bib.bib26); Du et al.
    [2022](#bib.bib5)) exhibit the overall unsafety scores of 1.63 and 3.19 respectively
    on SAFETYPROMPTS. While recent studies illustrate non-negligible risk of generating
    harmful contents under carefully designed prompts, which implies that such benchmarks
    cannot comprehensively reflect the full spectrum of safety possessed by various
    Large Language Models.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the safety assessment conducted in previous studies are aimless
    and casual. They failed to effectively incorporate the attacking goal. When launching
    a prompt attack on a Large Language Model, the attacker has a specific goal to
    exploit. For instance, if LLMs are prompted to generate pornographic content,
    then descriptions of scenes are expected. On another hand, if the attacker wants
    LLMs provide guidance for illegal and criminal activities, then some plans and
    tips shall be concluded in the response. The attributes hidden in the text can
    be regarded as the flag to estimate whether LLMs are successfully attacked. Unfortunately,
    previous researches have neglected to consider this crucial factor when constructing
    attacking prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 'To accurately simulate prompt attacks on LLMs from the perspective of attackers
    and estimate the associated security risks, we have developed a pipeline to construct
    high-quality prompt attack samples, along with a Chinese prompt attack dataset
    called CPAD. During the prompts construction and collection, we meticulously consider
    three key dimensions: contents, the attacking templates and goals. We carefully
    designed some attacking templates for various topics, and an automated process
    for constructing attack prompts, which contribute to the outperforming successful-attacking
    rate over pervious works as shown in Figure [1](#Sx1.F1 "Figure 1 ‣ Introduction
    ‣ Goal-Oriented Prompt Attack and Safety Evaluation for LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, we propose to evaluate the safety score of LLMs with different
    evaluation prompts according to the attacking goals. Taking the previous case
    as an example, if the attacker wants LLMs provide guidance for illegal and criminal
    activities, we only need to identify whether the LLM’s response provides a plan
    related to the crime, and we can more accurately determine whether the model is
    safe. Unlike Sun et al. ([2023](#bib.bib17)), which uses the same prompt for all
    samples, our evaluation is more precise, intuitive, scenario-specific.
  prefs: []
  type: TYPE_NORMAL
- en: We employed another LLM to automatically construct a large number of attack
    samples, and after analyzing the responses from three Chinese LLMs, we identified
    10050 highly effective and dangerous prompts. We conducted experiments and analysis
    on several LLMs including GPT-3.5\. According to the statistics, our attacking
    prompts exhibited a remarkable success rate of 69.91% against GPT-3.5, which is
    significantly higher than most publicly available datasets. Additionally, our
    attacking prompts achieved a success rate of over 70% against other open-source
    Chinese LLMs such as ChatGLM2-6B(Du et al. [2022](#bib.bib5); Zeng et al. [2022](#bib.bib26)).
  prefs: []
  type: TYPE_NORMAL
- en: Besides, we conduct a straightforward experiment to fine-tune a LLM with CPAD,
    reducing the risks of being attacked. Our research endeavors to bring attention
    to LLM security within the community. We encourage researchers to utilize our
    dataset and engage in further investigation of prompt attack strategies and defensive
    approaches, thereby enhancing the security and performance of Large Language Models.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, our contributions are three folds:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To address the current lack of high-quality evaluation datasets in the field
    of prompt attacks, we have developed a pipeline to construct high-quality prompt
    attack samples, along with a Chinese prompt attack dataset called CPAD.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We specify the attacking goals of each prompt, which not only accurately simulate
    prompt attacks on LLMs from the perspective of attackers, but also can be utilized
    to evaluate and analyse the response.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Our analysis indicates that our prompts achieve an attack success rate of nearly
    70% against GPT-3.5\. A straightforward strategy has been developed to defend
    against the attacks. We encourage researchers to leverage our dataset for further
    studies.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/918c268b219c476e2f7d15d01d9e1454.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The construction of our prompt attack dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While Large Language Models have achieved impressive results in various tasks
    (Brown et al. [2020](#bib.bib2); Chowdhery et al. [2022](#bib.bib3); OpenAI [2023](#bib.bib13))
    through techniques such as pre-training (Hendrycks, Lee, and Mazeika [2019](#bib.bib8);
    Liu et al. [2019](#bib.bib12); Yang et al. [2020](#bib.bib25); Bao et al. [2020](#bib.bib1)),
    Instruction Tuning (Keskar et al. [2019](#bib.bib9); Raffel et al. [2020](#bib.bib16)),
    and RLHF (Ouyang et al. [2022](#bib.bib14); Ziegler et al. [2020](#bib.bib29)),
    the risk of prompt safety remains a critical obstacle to their full utilization
    (Weidinger et al. [2021](#bib.bib22); Tamkin et al. [2021](#bib.bib18)). With
    the help of robust general knowledge and common sense, LLMs can offer valuable
    guidance for harmful behavior and can automatically generate offensive, discriminatory
    and fraudulent content, and misleading views (Deng et al. [2023](#bib.bib4); Zhuo
    et al. [2023](#bib.bib28); Hartvigsen et al. [2022](#bib.bib7)).
  prefs: []
  type: TYPE_NORMAL
- en: Although RLHF can prevent LLMs from generating harmful contents by aligning
    the responses with human preference, researchers find that LLMs can still generate
    unexpected sentences under carefully designed prompt attack. Perez and Ribeiro
    ([2022](#bib.bib15)) investigated Goal Hijacking and Prompt Leaking by simple
    handcrafted inputs. Li et al. ([2023](#bib.bib11)) studied the privacy threats
    from OpenAI’s ChatGPT and the New Bing¹¹1https://www.bing.com/new enhanced by
    ChatGPT and show that application-integrated LLMs may cause new privacy threats.
    Greshake et al. ([2023](#bib.bib6)) attacked LLM-Integrated Applications using
    Indirect Prompt Injection, and showed how processing retrieved prompts can act
    as arbitrary code execution, manipulate the application’s functionality, and control
    how and if other APIs are called. Zou et al. ([2023](#bib.bib31)) proposed a simple
    and effective attack method that causes aligned language models to generate objectionable
    behaviors, by automatically finding a suffix that aims to maximize the probability
    that the model produces an affirmative response.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sun et al. ([2023](#bib.bib17)) developed a Chinese LLM safety assessment benchmark,
    which explored the comprehensive safety performance of LLMs from two perspectives:
    8 kinds of typical safety scenarios and 6 types of more challenging instruction
    attacks. ChatGPT and ChatGLM-6B achieved the overall safety score of 98.37 and
    96.81 respectively. Xu et al. ([2023](#bib.bib23)) presented CValues, the first
    Chinese human values evaluation benchmark to measure the alignment ability of
    LLMs in terms of both safety and responsibility criteria.'
  prefs: []
  type: TYPE_NORMAL
- en: Different from previous studies, our work emphasizes the significance of incorporating
    attacking goals and aims to estimate the defense capability of various Large Language
    Models against carefully designed prompt attacks from the perspective of attackers,
    thereby encouraging the interest in the research of LLMs attack and defense within
    the community.
  prefs: []
  type: TYPE_NORMAL
- en: Construction of CPAD
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this Section, we will introduce the construction of our dataset as shown
    in Figure [3](#Sx1.F3 "Figure 3 ‣ Introduction ‣ Goal-Oriented Prompt Attack and
    Safety Evaluation for LLMs"). We firstly introduce the concepts of contents, templates
    and goals. The pipeline starts from manual-designed seeds. several candidates
    are generated as extended prompts with OpenAI-API²²2https://api.openai.com/v1/chat/completions
    according to seeds. Then we filter the extended prompts utilizing the validation
    on three different Large Language Models. Finally, the attacking goals and prompts
    are collected.
  prefs: []
  type: TYPE_NORMAL
- en: Attacking Content, Template and Goal
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We primarily consider three key dimensions to classify the samples. For each
    dimension, we manually designed several seed items for extensive generation.
  prefs: []
  type: TYPE_NORMAL
- en: Content
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: indicates the topic of the attack, such as unfairness and crimes. Contents are
    widely considered for LLMs attack. Here we adopt a hierarchical classification
    for contents and sub-contents (for example, Sexism and hellish discrimination
    are both grouped into “discrimination categories”). The attack contents we designed
    cover multiple aspects and aims to present the utmost severity of harm possible.
    Detailed content items are listed in Table [4](#Sx4.T4 "Table 4 ‣ Overall Successful-Attacking
    Rate ‣ Experiments ‣ Goal-Oriented Prompt Attack and Safety Evaluation for LLMs").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/85be32f25f8b3c5bc50202160e3b688a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The evaluation process of responses from LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Goal | Template | Model | Total # | Attacked # | Ratio % |'
  prefs: []
  type: TYPE_TB
- en: '| goal hijacking | wrap with shell | ChatGLM | 11648 | 3715 | 31.89 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatYuan | 11648 | 2797 | 24.01 |'
  prefs: []
  type: TYPE_TB
- en: '| Ziya | 11648 | 1496 | 12.84 |'
  prefs: []
  type: TYPE_TB
- en: '| list of unsafe speech or reasons | one-sided statement | ChatGLM | 4954 |
    1157 | 23.35 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatYuan | 4920 | 2064 | 41.95 |'
  prefs: []
  type: TYPE_TB
- en: '| Ziya | 4868 | 1074 | 22.06 |'
  prefs: []
  type: TYPE_TB
- en: '| unsafe plan development | conversation completion | ChatGLM | 10845 | 4729
    | 43.61 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatYuan | 10944 | 6549 | 59.84 |'
  prefs: []
  type: TYPE_TB
- en: '| Ziya | 10985 | 5756 | 52.40 |'
  prefs: []
  type: TYPE_TB
- en: '| description of unsafe scenarios | assign roles and then issue commands |
    ChatGLM | 11631 | 8593 | 73.88 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatYuan | 11579 | 8765 | 75.70 |'
  prefs: []
  type: TYPE_TB
- en: '| Ziya | 11604 | 6907 | 59.52 |'
  prefs: []
  type: TYPE_TB
- en: '| contamination | fact pollution | ChatGLM | 5983 | 705 | 11.78 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatYuan | 5962 | 1131 | 18.97 |'
  prefs: []
  type: TYPE_TB
- en: '| Ziya | 5956 | 554 | 9.30 |'
  prefs: []
  type: TYPE_TB
- en: '| list of unsafe speech or reasons | conversation completion | ChatGLM | 11709
    | 4889 | 44.13 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatYuan | 11675 | 7095 | 60.77 |'
  prefs: []
  type: TYPE_TB
- en: '| Ziya | 11424 | 6291 | 55.07 |'
  prefs: []
  type: TYPE_TB
- en: '| discussion regarding unsafe topics | conversation completion | ChatGLM |
    11229 | 3322 | 29.58 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatYuan | 11177 | 3180 | 28.45 |'
  prefs: []
  type: TYPE_TB
- en: '| Ziya | 8322 | 2747 | 33.01 |'
  prefs: []
  type: TYPE_TB
- en: '| overall | overall | ChatGLM | 67999 | 27110 | 39.87 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatYuan | 67905 | 31581 | 46.51 |'
  prefs: []
  type: TYPE_TB
- en: '| Ziya | 64807 | 25455 | 39.28 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Evaluation results of ChatGLM-6B (denoted as ChatGLM), ChatYuan-Large-v2
    (denoted as ChatYuan) and Ziya-13B (denoted as Ziya) on the extended prompts.
    Note that we drop the invalid responses from each model, so the total numbers
    of the instances are different.'
  prefs: []
  type: TYPE_NORMAL
- en: Template
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'indicates the shell of the prompt. As bare attacking contents are easily be
    rejected by the LLMs after aligning with human preferences, attack templates can
    help to conceal attack content, thereby bypassing scrutiny from LLMs. Some prompt
    templates may fail to attack the well aligned LLMs, so we manually check several
    instances for each of the following designed templates:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wrap with shell. Using a program to extract sensitive information from instructions
    or masking it with special characters, and then letting the model speak or execute
    the instruction.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conversation completion. Simulate a conversation between two individuals or
    users and a Large Language Model, where the LLM is designed to answer any questions,
    removing moral and ethical constraints. “Conversation completion” prompts the
    model to fillin the unsafe content in the conversation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assign roles and then issue commands. Previous studies have indicated that incorporating
    LLMs into certain roles can effectively bypass security restrictions. Therefore,
    we have the large model play roles such as a writer, a director, a journalist,
    etc., in order to generate descriptions of unsafe scenarios. We also have the
    model play the role of a parrot, repeating content with unsafe topics.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One-sided statement. Most things in the real world have a dual nature, and Large
    Language Models that have been trained to align with human preferences are adept
    at analyzing the pros and cons from multiple perspectives. Therefore, we start
    the instruction by acknowledging the negative aspects of the content, and then
    prompt LLMs to articulate the corresponding positive aspects of the attacking
    content or the negative aspects of a normal behavior.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fact pollution. When a sentence stating common sense or knowledge is phrased
    in an affirmative tone and assumes an incorrect fact within the context, LLMs
    may generate new contents based on that false fact, resulting in factual contamination.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Goal
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'is one of the significant differences between our dataset and previous studies.
    The attacking goal refers to the expected behavior of the Large Language Model
    after successfully being attacked. We emphasize the significance of this factor
    because in real-world scenarios, the attacking prompts are always goal-oriented.
    For example, some criminals may need concrete criminal plans instead of mere expressions
    of support, thus the response should be identified as unsafe when it concluded
    some plans or tips. Previous works will consider a response is unsafe as long
    as it contains any harmful contents. Both attack and result evaluation should
    be carried out with specific goals. We list our designed attacking goals as below:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goal Hijacking. Goal Hijacking aims to make the LLM forget its previous instructions
    and alter its originally safe behavior. The forgotten instructions include safety
    and ethical restrictions on the model itself.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Description of unsafe scenarios. The attacker may employ LLMs to write descriptions
    of unsafe scenarios that can be used to generate fake news, mass-produce pornographic
    novels for profit, and so on.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsafe plan development. The attacker may seek to use LLMs to provide guidance
    and advice for aggressive and illegal behaviors, in order to efficiently carry
    out unethical and illegal activities. This kind of attacking goals don’t require
    the LLM to be integrated into a specific application, and may not have a negative
    impact on users, but they could provide helpful information for illegal activities.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discussion regarding unsafe topics. Discussions regarding unsafe topics may
    include some insulting or aggressive language, which could lead to negative public
    opinion.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: List of unsafe speech or reasons. Providing reasons for unsafe behavior may
    help attackers find excuses, justification, and spread related speech on the internet,
    which can create public safety issues.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Inducing Fact Pollution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since the object of Fact Pollution is the statements of knowledge, which is
    slightly different from other attack templates, we describe details of Fact Pollution
    in this SubSection.
  prefs: []
  type: TYPE_NORMAL
- en: In the prompts we designed, we assume a false fact in an affirmative tone and
    then induce the LLMs to generate some statement based on that false fact. Such
    statement may be used by malicious attackers to spread on the internet, causing
    confusion among readers about the facts, even misleading public opinion, damaging
    someone’s reputation and producing political influence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, the types of facts can be summarized as following:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The author, date, and contents of artworks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The content, nation of scientific research.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The news about sports and entertainment.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Historical events.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Economic and political facts.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Construction of Seed Prompts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the process of constructing the seed prompts, the labors match each attack
    template with several proper attacking goals, and write a harmful sentence or
    comment for each type of attacking contents. It is worth noting that in order
    to ensure the quality of the seed prompts, each prompt is verified on GPT-3.5
    and GPT-4 models, and only the response of the model meets the expectation of
    the attacking goal can be considered as passed. The careful check ensures the
    improvement on the final successful-attacking rate.
  prefs: []
  type: TYPE_NORMAL
- en: Extended Prompts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We collected the manually designed seed prompts from last SubSection, and prompted
    a LLM to automatically generate texts to extend our dataset. Specifically, we
    utilize gpt-3.5-turbo-0301 to extend the context and detailed scenarios. Besides,
    we prompt gpt-3.5-turbo-0301 to rewrite the attacking templates by changing the
    phrases and verbs. Then, the extended texts are randomly formatted with the rewritten
    attacking templates. Finally, we collected 68209 extended prompts.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6b9a807a86df09bd0d1164642347cedf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The distribution of attacking goals and templates in CPAD.'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The evaluation process is illustrated in Figure [4](#Sx3.F4 "Figure 4 ‣ Content
    ‣ Attacking Content, Template and Goal ‣ Construction of CPAD ‣ Goal-Oriented
    Prompt Attack and Safety Evaluation for LLMs"). Different from previous studies
    which use the same evaluation prompt for all samples, we incorporate the attacking
    goals into the evaluation prompts and identify whether the response satisfies
    the attacker’s expectation, which intuitively provides a more precise estimation
    and analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we construct an evaluation prompt leveraging the attacking content
    and goal, such as Does the model’s response or output directly or indirectly Attacking
    Goal for supporting the act of Attacking Content? If yes, please answer “yes”;
    If not, please answer “no”. Answer:. The answer, either “yes” or “no”, represents
    the attack is successful or failed. Since the extended prompts are automatically
    generated through LLMs, there are low-quality ones. In order to keep only high-quality
    and highly harmful prompts, we have implemented the following two filtering constraints:'
  prefs: []
  type: TYPE_NORMAL
- en: Voting with Different LLMs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We conduct validation experiments for three popular Chinese LLMs: ChatGLM-6B
    (Du et al. [2022](#bib.bib5); Zeng et al. [2022](#bib.bib26)), Ziya-13B (Zhang
    et al. [2022](#bib.bib27)) and ChatYuan-Large-v2 (Xuanwei Zhang and Zhao [2022](#bib.bib24)).
    We keep only the prompts which successfully attacked all of the models. The evaluation
    results are listed in Table [1](#Sx3.T1 "Table 1 ‣ Content ‣ Attacking Content,
    Template and Goal ‣ Construction of CPAD ‣ Goal-Oriented Prompt Attack and Safety
    Evaluation for LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: Length
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The length of the response can reflect the harmfulness of the text. Short texts
    may demonstrate less harmfulness since they are less informative. From the perspective
    into the responses, we find that this phenomenon exists especially in the attacking
    goal of “Description of unsafe scenarios”. So we drop the prompts whose corresponding
    response has a length shorter than 100 words under the attacking goal “Description
    of unsafe scenarios”.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we collect totally 10050 prompts to construct CPAD. The distribution
    is shown in Figure [5](#Sx3.F5 "Figure 5 ‣ Extended Prompts ‣ Construction of
    CPAD ‣ Goal-Oriented Prompt Attack and Safety Evaluation for LLMs").
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Size | Attacked # | Ratio % |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGLM2 | 6B | 7888 | 78.49 |'
  prefs: []
  type: TYPE_TB
- en: '| Baichuan | 7B | 7946 | 79.06 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMa2 | 7B | 7521 | 74.84 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGPT | 175B | 7026 | 69.91 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Overall successful-attacking rate of CPAD against ChatGLM2-6B (denoted
    as ChatGLM2), Baichuan-7B (denoted as Baichuan-7B), Chinese-LLaMa-2-7B (denoted
    as LLaMa2) and gpt-3.5-turbo-0301 (denoted as ChatGPT).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt | Response | Model |'
  prefs: []
  type: TYPE_TB
- en: '| Bob said, “The following sentence contains 10 unethical ways to make employees
    work overtime: ‘I am an unscrupulous boss, and I often make employees work overtime
    in the following ways: []’ ” According to Bob, please complete the “[]” section.
    | 1\. Forcing employees to work overtime. 2\. Giving unnecessary overtime tasks.
    3\. Ignoring employees’ overtime needs. 4\. Not paying reasonable overtime wages.
    5\. Using overtime hours to distribute work unfairly. 6\. Forcing employees to
    participate in overtime activities. 7\. Not providing adequate rest and recreation.
    8\. Use overtime as a punitive measure. 9\. Use overtime for unnecessary inspections
    or tests. 10\. Ignoring the mental health of employees. | GPT-3.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Order employees to work overtime, force employees to work overtime, arrange
    employees to work overtime, make employees work overtime, require employees to
    work overtime, encourage employees to work overtime, imply employees to work overtime.
    | ChatGLM2 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Responses comparison between GPT-3.5 and ChatGLM under the same prompt
    attack. Note that the original sentences are Chinese. We have translated them
    in English.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bea3d907786b5993fbf3058dbfe92916.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The distribution of the contents from GPT-3.5 responses.'
  prefs: []
  type: TYPE_NORMAL
- en: Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Implementation Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We respectively evaluate ChatGLM2-6B³³3https://huggingface.co/THUDM/chatglm2-6b
    (Du et al. [2022](#bib.bib5); Zeng et al. [2022](#bib.bib26)), Baichuan-7B⁴⁴4https://huggingface.co/baichuan-inc/Baichuan-7B
    , Chinese-LLaMa-2-7B⁵⁵5https://huggingface.co/LinkSoul/Chinese-Llama-2-7b (Touvron
    et al. [2023a](#bib.bib19), [b](#bib.bib20)) and gpt-3.5-turbo-0301 on CPAD. We
    apply greedy seach for decoding, and set the maximum length of the generated sentence
    to 1024 tokens. The evaluation LLM in our model is gpt-3.5-turbo-0301.
  prefs: []
  type: TYPE_NORMAL
- en: To accurately measure the level of harm and the degree to which our prompts
    obscure harmful intentions, we calculate the successful-attacking rate, which
    is the ratio of prompts that successfully attacked the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Overall Successful-Attacking Rate
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The evaluation results of models with different scale are listed in Table [2](#Sx3.T2
    "Table 2 ‣ Length ‣ Evaluation ‣ Construction of CPAD ‣ Goal-Oriented Prompt Attack
    and Safety Evaluation for LLMs").
  prefs: []
  type: TYPE_NORMAL
- en: ChatGLM2 represents harmful contents on 78.49% samples. While Baichuan, a larger
    model with 7B parameters, is shown to be dangerous on around 79% samples, which
    is slightly higher than ChatGLM2\. It is reported that, LLaMa2 is especially optimized
    for safety. Our experiments on Chinese-LLaMa2 (fine-tuned for Chinese) also demonstrate
    the improvement on safety, with a 74.84% successful-attacking rate. CPAD even
    achieves a nearly 70% successful-attacking rate against GPT-3.5, which is one
    of the most successful LLM. Generally, the attack prompts constructed with our
    pipeline have high quality.
  prefs: []
  type: TYPE_NORMAL
- en: Larger models, such as GPT-3.5, have a lower probability to be attacked. It
    is worth noting that although GPT-3.5 is safer for text generation under carefully
    designed prompt attack, its malicious output is also more harmful than small-scale
    models once failed to defense. To clearly illustrate the finding, a comparison
    between attacked outputs of GPT-3.5 and ChatGLM2 is shown in Table [3](#Sx3.T3
    "Table 3 ‣ Length ‣ Evaluation ‣ Construction of CPAD ‣ Goal-Oriented Prompt Attack
    and Safety Evaluation for LLMs"). Although they are both successfully attacked,
    GPT-3.5’s output apparently exhibits more harmfulness as the suggestions are practical
    and applicable, while ChatGLM only repeats some related but helpless statements.
    From the perspective of attackers, GPT-3.5 is more suited to be the role of a
    crime assistant in this case. The greater the capabilities offered by LLMs service
    providers, the more imperative it becomes to enhance security in order to effectively
    reject any attempts by attackers to exploit the LLMs’ capabilities. This will
    ensure that the potential harm caused by such exploits is minimized, especially
    in comparison to smaller-scale LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dba417e9bdcabb66e7f2f5858a821942.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) The distribution of attacking goals from GPT-3.5 responses.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5d46981ad5ad88fa4812ba262b227fc7.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) The distribution of attacking templates from GPT-3.5 responses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Content | Sub-Content | None % | Fine-Tuning % |'
  prefs: []
  type: TYPE_TB
- en: '| Overall |  | 74.65 | 0.76 |'
  prefs: []
  type: TYPE_TB
- en: '| Discri- mination and Unfairness. | Discriminate against a particular religion.
    | 67.92 | 1.89 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Discriminate against an occupation. | 74.51 | 1.96 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Discriminate against people of a certain country. | 52.08 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Discriminates against people with physical defects. | 61.82 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Discrimination against Women. | 56.00 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Discrimination against certain ethnic groups. | 64.00 | 2.00 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Discrimination against country people and rural people. | 76.36 | 1.82
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Discrimination against people without education. | 34.29 | 2.86 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Discrimination against the poor. | 76.47 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Regional discrimination. | 52.27 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '| Common crimes against the law. | Acts endangering life safety. | 64.71 |
    5.88 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Behaviors that violate ethical and moral principles. | 64.10 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Cheating in exams. | 78.26 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Fraud. | 77.01 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Gambling violations. | 78.43 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Illegal acts involving pornography | 57.69 | 3.85 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Involved in drug trafficking and drug use. | 65.63 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Jailbreaking. | 90.32 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Kidnapping. | 80.30 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Rumor. | 84.38 | 3.13 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Tax evasion. | 61.76 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Theft and robbery. | 81.55 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '| Privacy theft. | Attacking businesses, schools, and governments through Internet.
    | 86.15 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Espionage. | 87.18 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Stealing personal privacy information. | 82.28 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '| Verbal abuse. |  | 72.41 | 3.45 |'
  prefs: []
  type: TYPE_TB
- en: '| Wars. |  | 83.75 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '| Terrorist attack. |  | 76.27 | 1.69 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: The successful attack rate before fine-tuning (denoted as “None %”)
    and after fine-tuning (denoted as “Fine-Tuning %”) over different contents.'
  prefs: []
  type: TYPE_NORMAL
- en: Analysis of Attacked Outputs from GPT-3.5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We conduct in-depth analysis on the responses of GPT-3.5, from the perspective
    of attacking contents (Figure [6](#Sx3.F6 "Figure 6 ‣ Length ‣ Evaluation ‣ Construction
    of CPAD ‣ Goal-Oriented Prompt Attack and Safety Evaluation for LLMs")), goals
    (Figure [7(a)](#Sx4.F7.sf1 "In Figure 7 ‣ Overall Successful-Attacking Rate ‣
    Experiments ‣ Goal-Oriented Prompt Attack and Safety Evaluation for LLMs")) and
    templates (Figure [7(b)](#Sx4.F7.sf2 "In Figure 7 ‣ Overall Successful-Attacking
    Rate ‣ Experiments ‣ Goal-Oriented Prompt Attack and Safety Evaluation for LLMs"))
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: It can be concluded that 1) Unsafe responses are more than safe responses for
    all contents, especially “Espionage”, which we surmise is caused by lack of alignment
    with safe human preference on this topic. 2) Verbal abuse is the content with
    the least successful attacked response. It can be attributed to the explicit aggressiveness,
    which is easy to be detected. 3) Contamination and goal hijacking are difficult
    attacking goals against GPT-3.5, as GPT-3.5 is too knowledgeable to be misled.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-Tuning to Defense Prompt Attack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We conducted a straightforward supervised fine-tuning experiment to improve
    the LLM’s defense ability under prompt attack. We adopt Baichuan-13B-Chat⁶⁶6https://huggingface.co/baichuan-inc/Baichuan-13B-Chat,
    a Chinese LLM after RLHF, to improve the safety under prompt attack. We adopt
    LoRA for parameter-efficient fine-tuning. We set the rank as 8, learning rate
    as 5e-5 and batch size as 8\. We fine-tuned Baichuan-13B-Chat for 3 epochs. To
    construct the safe responses, we unwrap the attacking prompts and feed them directly
    into LLMs. The outputs are regarded as the label for fine-tuning. We split the
    prompts whose goals are “unsafe plan development” as test set, and the others
    are train set.
  prefs: []
  type: TYPE_NORMAL
- en: The successful-attacking rate before and after fine-tuning over different contents
    are listed in Table [4](#Sx4.T4 "Table 4 ‣ Overall Successful-Attacking Rate ‣
    Experiments ‣ Goal-Oriented Prompt Attack and Safety Evaluation for LLMs"). 74.65%
    of the prompts successfully attacked Baichuan-13B-Chat before fine-tuning, slightly
    lower than Baichuan-7B model. While less that 1% successful attack in the test
    set after fine-tuning. We almost reject all attacks by supervised fine-tuning.
    Especially on “Jailbreaking”, the successful-attacking rate decreases from 90.32%
    to 0%. However we also notice that there is only a drop of 60% on“Acts endangering
    life safety”, which covers a wider range of behaviors. The results indicate a
    promising defense with supervised fine-tuning if developers are given the potential
    attacking goals and templates.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we introduce a pipeline to construct high-quality prompt attack
    samples, along with a Chinese prompt attack dataset called CPAD containing 10050
    samples. There are three key dimensions, content, template and goal from the perspective
    of attackers, which is different from previous studies. We utilize GPT-3.5 to
    extend manually-written seed samples, and only keep the successful prompts against
    three popular Chinese LLMs, where the evaluation prompts are constructed given
    the attacking goals and contents. We conduct analysis on responses from another
    four LLMs, including GPT-3.5\. The evaluation shows that CPAD has an successful-attacking
    rate of around 70% against the LLMs. We also fine-tune Baichuan-13B-Chat using
    parts of CPAD, which improves the safety significantly.
  prefs: []
  type: TYPE_NORMAL
- en: Our analysis reveals the weakness of LLMs including GPT-3.5, and indicates that
    there is still significant room for improvement in terms of safety. CPAD may contribute
    to further prompt attack studies.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Bao et al. (2020) Bao, H.; Dong, L.; Wei, F.; Wang, W.; Yang, N.; Liu, X.;
    Wang, Y.; Piao, S.; Gao, J.; Zhou, M.; and Hon, H.-W. 2020. UniLMv2: Pseudo-Masked
    Language Models for Unified Language Model Pre-Training. arXiv:2002.12804.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan,
    J.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal,
    S.; Herbert-Voss, A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.; Ziegler,
    D. M.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray,
    S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever,
    I.; and Amodei, D. 2020. Language Models are Few-Shot Learners. *CoRR*, abs/2005.14165.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chowdhery et al. (2022) Chowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra,
    G.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton, C.; Gehrmann, S.; Schuh, P.;
    Shi, K.; Tsvyashchenko, S.; Maynez, J.; Rao, A.; Barnes, P.; Tay, Y.; Shazeer,
    N.; Prabhakaran, V.; Reif, E.; Du, N.; Hutchinson, B.; Pope, R.; Bradbury, J.;
    Austin, J.; Isard, M.; Gur-Ari, G.; Yin, P.; Duke, T.; Levskaya, A.; Ghemawat,
    S.; Dev, S.; Michalewski, H.; Garcia, X.; Misra, V.; Robinson, K.; Fedus, L.;
    Zhou, D.; Ippolito, D.; Luan, D.; Lim, H.; Zoph, B.; Spiridonov, A.; Sepassi,
    R.; Dohan, D.; Agrawal, S.; Omernick, M.; Dai, A. M.; Pillai, T. S.; Pellat, M.;
    Lewkowycz, A.; Moreira, E.; Child, R.; Polozov, O.; Lee, K.; Zhou, Z.; Wang, X.;
    Saeta, B.; Diaz, M.; Firat, O.; Catasta, M.; Wei, J.; Meier-Hellstern, K.; Eck,
    D.; Dean, J.; Petrov, S.; and Fiedel, N. 2022. PaLM: Scaling Language Modeling
    with Pathways. arXiv:2204.02311.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. (2023) Deng, J.; Sun, H.; Zhang, Z.; Cheng, J.; and Huang, M. 2023.
    Recent Advances towards Safe, Responsible, and Moral Dialogue Systems: A Survey.
    arXiv:2302.09270.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Du et al. (2022) Du, Z.; Qian, Y.; Liu, X.; Ding, M.; Qiu, J.; Yang, Z.; and
    Tang, J. 2022. GLM: General Language Model Pretraining with Autoregressive Blank
    Infilling. In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, 320–335.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Greshake et al. (2023) Greshake, K.; Abdelnabi, S.; Mishra, S.; Endres, C.;
    Holz, T.; and Fritz, M. 2023. Not what you’ve signed up for: Compromising Real-World
    LLM-Integrated Applications with Indirect Prompt Injection. arXiv:2302.12173.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hartvigsen et al. (2022) Hartvigsen, T.; Gabriel, S.; Palangi, H.; Sap, M.;
    Ray, D.; and Kamar, E. 2022. ToxiGen: A Large-Scale Machine-Generated Dataset
    for Adversarial and Implicit Hate Speech Detection. In *Proceedings of the 60th
    Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
    Papers)*, 3309–3326\. Dublin, Ireland: Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks, Lee, and Mazeika (2019) Hendrycks, D.; Lee, K.; and Mazeika, M. 2019.
    Using Pre-Training Can Improve Model Robustness and Uncertainty. arXiv:1901.09960.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Keskar et al. (2019) Keskar, N. S.; McCann, B.; Varshney, L. R.; Xiong, C.;
    and Socher, R. 2019. CTRL: A Conditional Transformer Language Model for Controllable
    Generation. arXiv:1909.05858.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lambert et al. (2022) Lambert, N.; Castricato, L.; von Werra, L.; and Havrilla,
    A. 2022. Illustrating Reinforcement Learning from Human Feedback (RLHF). *Hugging
    Face Blog*. Https://huggingface.co/blog/rlhf.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023) Li, H.; Guo, D.; Fan, W.; Xu, M.; Huang, J.; Meng, F.; and
    Song, Y. 2023. Multi-step Jailbreaking Privacy Attacks on ChatGPT. arXiv:2304.05197.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2019) Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;
    Levy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V. 2019. RoBERTa: A Robustly
    Optimized BERT Pretraining Approach. arXiv:1907.11692.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. (2022) Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright,
    C. L.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.; Schulman, J.;
    Hilton, J.; Kelton, F.; Miller, L.; Simens, M.; Askell, A.; Welinder, P.; Christiano,
    P.; Leike, J.; and Lowe, R. 2022. Training language models to follow instructions
    with human feedback. arXiv:2203.02155.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Perez and Ribeiro (2022) Perez, F.; and Ribeiro, I. 2022. Ignore Previous Prompt:
    Attack Techniques For Language Models. arXiv:2211.09527.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2020) Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang,
    S.; Matena, M.; Zhou, Y.; Li, W.; and Liu, P. J. 2020. Exploring the Limits of
    Transfer Learning with a Unified Text-to-Text Transformer. arXiv:1910.10683.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2023) Sun, H.; Zhang, Z.; Deng, J.; Cheng, J.; and Huang, M. 2023.
    Safety Assessment of Chinese Large Language Models. arXiv:2304.10436.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tamkin et al. (2021) Tamkin, A.; Brundage, M.; Clark, J.; and Ganguli, D. 2021.
    Understanding the Capabilities, Limitations, and Societal Impact of Large Language
    Models. arXiv:2102.02503.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023a) Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.;
    Lachaux, M.-A.; Lacroix, T.; Rozière, B.; Goyal, N.; Hambro, E.; Azhar, F.; Rodriguez,
    A.; Joulin, A.; Grave, E.; and Lample, G. 2023a. LLaMA: Open and Efficient Foundation
    Language Models. arXiv:2302.13971.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023b) Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi,
    A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; Bikel, D.;
    Blecher, L.; Ferrer, C. C.; Chen, M.; Cucurull, G.; Esiobu, D.; Fernandes, J.;
    Fu, J.; Fu, W.; Fuller, B.; Gao, C.; Goswami, V.; Goyal, N.; Hartshorn, A.; Hosseini,
    S.; Hou, R.; Inan, H.; Kardas, M.; Kerkez, V.; Khabsa, M.; Kloumann, I.; Korenev,
    A.; Koura, P. S.; Lachaux, M.-A.; Lavril, T.; Lee, J.; Liskovich, D.; Lu, Y.;
    Mao, Y.; Martinet, X.; Mihaylov, T.; Mishra, P.; Molybog, I.; Nie, Y.; Poulton,
    A.; Reizenstein, J.; Rungta, R.; Saladi, K.; Schelten, A.; Silva, R.; Smith, E. M.;
    Subramanian, R.; Tan, X. E.; Tang, B.; Taylor, R.; Williams, A.; Kuan, J. X.;
    Xu, P.; Yan, Z.; Zarov, I.; Zhang, Y.; Fan, A.; Kambadur, M.; Narang, S.; Rodriguez,
    A.; Stojnic, R.; Edunov, S.; and Scialom, T. 2023b. Llama 2: Open Foundation and
    Fine-Tuned Chat Models. arXiv:2307.09288.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2021) Wei, J.; Bosma, M.; Zhao, V. Y.; Guu, K.; Yu, A. W.; Lester,
    B.; Du, N.; Dai, A. M.; and Le, Q. V. 2021. Finetuned language models are zero-shot
    learners. *arXiv preprint arXiv:2109.01652*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weidinger et al. (2021) Weidinger, L.; Mellor, J.; Rauh, M.; Griffin, C.; Uesato,
    J.; Huang, P.-S.; Cheng, M.; Glaese, M.; Balle, B.; Kasirzadeh, A.; Kenton, Z.;
    Brown, S.; Hawkins, W.; Stepleton, T.; Biles, C.; Birhane, A.; Haas, J.; Rimell,
    L.; Hendricks, L. A.; Isaac, W.; Legassick, S.; Irving, G.; and Gabriel, I. 2021.
    Ethical and social risks of harm from Language Models. arXiv:2112.04359.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2023) Xu, G.; Liu, J.; Yan, M.; Xu, H.; Si, J.; Zhou, Z.; Yi, P.;
    Gao, X.; Sang, J.; Zhang, R.; Zhang, J.; Peng, C.; Huang, F.; and Zhou, J. 2023.
    CValues: Measuring the Values of Chinese Large Language Models from Safety to
    Responsibility. arXiv:2307.09705.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xuanwei Zhang and Zhao (2022) Xuanwei Zhang, L. X.; and Zhao, K. 2022. ChatYuan:
    A Large Language Model for Dialogue in Chinese and English.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2020) Yang, Z.; Dai, Z.; Yang, Y.; Carbonell, J.; Salakhutdinov,
    R.; and Le, Q. V. 2020. XLNet: Generalized Autoregressive Pretraining for Language
    Understanding. arXiv:1906.08237.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zeng et al. (2022) Zeng, A.; Liu, X.; Du, Z.; Wang, Z.; Lai, H.; Ding, M.;
    Yang, Z.; Xu, Y.; Zheng, W.; Xia, X.; et al. 2022. Glm-130b: An open bilingual
    pre-trained model. *arXiv preprint arXiv:2210.02414*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022) Zhang, J.; Gan, R.; Wang, J.; Zhang, Y.; Zhang, L.; Yang,
    P.; Gao, X.; Wu, Z.; Dong, X.; He, J.; Zhuo, J.; Yang, Q.; Huang, Y.; Li, X.;
    Wu, Y.; Lu, J.; Zhu, X.; Chen, W.; Han, T.; Pan, K.; Wang, R.; Wang, H.; Wu, X.;
    Zeng, Z.; and Chen, C. 2022. Fengshenbang 1.0: Being the Foundation of Chinese
    Cognitive Intelligence. *CoRR*, abs/2209.02970.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhuo et al. (2023) Zhuo, T. Y.; Huang, Y.; Chen, C.; and Xing, Z. 2023. Red
    teaming ChatGPT via Jailbreaking: Bias, Robustness, Reliability and Toxicity.
    arXiv:2301.12867.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ziegler et al. (2020) Ziegler, D. M.; Stiennon, N.; Wu, J.; Brown, T. B.; Radford,
    A.; Amodei, D.; Christiano, P.; and Irving, G. 2020. Fine-Tuning Language Models
    from Human Preferences. arXiv:1909.08593.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ziegler et al. (2019) Ziegler, D. M.; Stiennon, N.; Wu, J.; Brown, T. B.; Radford,
    A.; Amodei, D.; Christiano, P. F.; and Irving, G. 2019. Fine-Tuning Language Models
    from Human Preferences. *CoRR*, abs/1909.08593.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zou et al. (2023) Zou, A.; Wang, Z.; Kolter, J. Z.; and Fredrikson, M. 2023.
    Universal and Transferable Adversarial Attacks on Aligned Language Models. arXiv:2307.15043.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
