- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:46:13'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better
    Context-aware Translators'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.15200](https://ar5iv.labs.arxiv.org/html/2402.15200)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Xinglin Lyu^♣,  Junhui Li^♣,  Yanqing Zhao^♠,  Min Zhang^♠,  Daimeng Wei^♠,
  prefs: []
  type: TYPE_NORMAL
- en: Shimin Tao^♠,  Hao Yang^♠,  Min Zhang^♣
  prefs: []
  type: TYPE_NORMAL
- en: ^♣School of Computer Science and Technology, Soochow University, Suzhou, China
  prefs: []
  type: TYPE_NORMAL
- en: ^♠Huawei Translation Services Center, Beijing, China
  prefs: []
  type: TYPE_NORMAL
- en: xllv2020@stu.suda.edu.cn, {lijunhui,minzhang}@suda.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: '{zhaoyanqing,zhangmin186,weidaimeng,taoshimin,yanghao30}@huawei.com Corresponding
    author: Junhui Li.'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Generally, the decoder-only large language models (LLMs) are adapted to context-aware
    neural machine translation (NMT) in a concatenating way, where LLMs take the concatenation
    of the source sentence (i.e., intra-sentence context) and the inter-sentence context
    as the input, and then to generate the target tokens sequentially. This adaptation
    strategy, i.e., concatenation mode, considers intra-sentence and inter-sentence
    contexts with the same priority, despite an apparent difference between the two
    kinds of contexts. In this paper, we propose an alternative adaptation approach,
    named Decoding-enhanced Multi-phase Prompt Tuning (DeMPT), to make LLMs discriminately
    model and utilize the inter- and intra-sentence context and more effectively adapt
    LLMs to context-aware NMT. First, DeMPT divides the context-aware NMT process
    into three separate phases. During each phase, different continuous prompts are
    introduced to make LLMs discriminately model various information. Second, DeMPT
    employs a heuristic way to further discriminately enhance the utilization of the
    source-side inter- and intra-sentence information at the final decoding phase.
    Experiments show that our approach significantly outperforms the concatenation
    method, and further improves the performance of LLMs in discourse modeling.¹¹1As
    this paper is under review, we will release our code and datasets later.
  prefs: []
  type: TYPE_NORMAL
- en: 'DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better
    Context-aware Translators'
  prefs: []
  type: TYPE_NORMAL
- en: 'Xinglin Lyu^♣,  Junhui Li^♣^†^†thanks: Corresponding author: Junhui Li.,  Yanqing
    Zhao^♠,  Min Zhang^♠,  Daimeng Wei^♠, Shimin Tao^♠,  Hao Yang^♠,  Min Zhang^♣
    ^♣School of Computer Science and Technology, Soochow University, Suzhou, China
    ^♠Huawei Translation Services Center, Beijing, China xllv2020@stu.suda.edu.cn,
    {lijunhui,minzhang}@suda.edu.cn {zhaoyanqing,zhangmin186,weidaimeng,taoshimin,yanghao30}@huawei.com'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Context-aware neural machine translation (NMT) goes beyond sentence-level NMT
    by incorporating inter-sentence context at the document level Zhang et al. ([2018](#bib.bib42));
    Miculicich et al. ([2018](#bib.bib25)); Voita et al. ([2018](#bib.bib36), [2019b](#bib.bib35),
    [2019a](#bib.bib34)); Bao et al. ([2021](#bib.bib2)); Sun et al. ([2022](#bib.bib31)),
    aiming to address discourse-related challenges such as zero pronoun translation Wang
    et al. ([2019](#bib.bib38)), lexical translation consistency Lyu et al. ([2021](#bib.bib20),
    [2022](#bib.bib21)), and discourse structure Hu and Wan ([2023](#bib.bib7)). A
    recent paradigm shift has been witnessed in context-aware NMT with the emergence
    of the decoder-only large language models (LLMs) BigScience ([2022](#bib.bib3));
    Google ([2022](#bib.bib4)); MetaAI ([2023b](#bib.bib24), [a](#bib.bib23)); OpenAI
    ([2023](#bib.bib26)). These generative language models, trained on extensive public
    data, have gained significant attention in the natural language processing (NLP)
    community. In adapting LLMs to context-aware NMT, a common strategy involves concatenating
    multiple source sentences as a prefix and generating translations token-by-token,
    relying on the prefix and previously predicted target tokens, as shown in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning
    for Making LLMs Be Better Context-aware Translators") (a). However, a critical
    observation of this strategy reveals a potential drawback – the equal prioritization
    of the inter- and intra-sentence contexts during token generation. Importantly,
    the intra-sentence context inherently contains richer parallel semantic information
    with the target sentence and should be given a higher priority than the inter-sentence
    context. Consequently, we propose that separately modeling and utilizing the inter-
    and intra-sentence contexts should explicitly inform LLMs of the document-level
    context and the current sentence itself, thus being able to prevent the misallocation
    of attention weights to source-side tokens Bao et al. ([2021](#bib.bib2)); Li
    et al. ([2023](#bib.bib16)). Inspired by the success of prompt tuning Li and Liang
    ([2021](#bib.bib15)); Liu et al. ([2022](#bib.bib18)); Tan et al. ([2022](#bib.bib32)),
    our alternative approach, named Decoding-Enhanced Multi-phase Prompt Tuning (DeMPT),
    aims to enhance LLMs’ adaptability to context-aware NMT, as shown in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning
    for Making LLMs Be Better Context-aware Translators") (b).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b90414bf0551800a5143ed3173d1b7a7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Comparison of different strategies for adapting LLMs to context-aware
    NMT. The concatenation strategy (left) treats inter-sentence and intra-sentence
    (referred to as the "source sentence" context in the figure) with equal importance.
    In contrast, our approach (right) divides context-aware NMT into three distinct
    phases, enabling LLMs to selectively model and leverage both inter- and intra-sentence
    contexts.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we divide the whole procedure of context-aware NMT into three
    phases: inter-sentence context encoding, intra-sentence context encoding, and
    decoding. Following Li and Liang ([2021](#bib.bib15)); Liu et al. ([2022](#bib.bib18)),
    we sequentially and differentially adapt LLMs for each phase, utilizing phase-specific
    trainable prompts. This phased tuning method enables LLMs to independently capture
    and model both inter- and intra-sentence contexts, facilitating a better understanding
    of their differences. Importantly, our approach only divides the original input
    into three parts without significantly increasing computational load. As a result,
    there is no substantial decrease in inference speed compared to the concatenating
    method, as detailed in Section [4.3](#S4.SS3 "4.3 Comparison of Inference Speed
    ‣ 4 Discussion ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making
    LLMs Be Better Context-aware Translators").'
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, during the decoding phase, we propose a heuristic method to emphasize
    the difference between inter- and intra-sentence contexts, and avoid long-distance
    issue when utilizing inter-sentence context. Specifically, at each decoding step,
    we use LLMs to predict the next token three times. The decoding states used for
    each prediction directly concatenate with the representations of two contexts
    in a discriminative manner. Finally, we combine three probability distributions
    to search for the next token as the output from the target vocabulary. This method
    enables LLMs to learn not only to properly capture inter-sentence context in addressing
    discourse-related issues but also to recognize a difference between inter- and
    intra-sentence contexts, allowing for effective utilization of both types of contexts.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, our contributions can be outlined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose a novel multi-phase prompt tuning approach to divide context-aware
    NMT into three phases, making LLMs aware of the distinction between inter- and
    intra-sentence contexts.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We introduce a enhanced decoding method that discriminately utilize both context
    types. This allows LLMs not only properly capture inter-sentence context in addressing
    discourse-related issues, but also be aware of the importance of the intra-sentence
    context.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We validate our approach using llama-2-7b and bloomz-7b1-mt as foundation models,
    demonstrating its effectiveness across five context-aware translation directions.
    Extensive analyses further highlight the substantial enhancement in LLMs’ ability
    for context-aware NMT.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we describe our decoding-enhanced multi-phase approach for
    adapting LLMs to context-aware NMT in details. Specifically, we break down the
    whole procedure of context-aware NMT into three phases (Section [2.1](#S2.SS1
    "2.1 Multi-phase Encoding and Decoding ‣ 2 Methodology ‣ DeMPT: Decoding-enhanced
    Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators")),
    i.e., inter-sentence context encoding, intra-sentence encoding, and decoding.
    Additionally, we discriminatively enhance the utilization of inter- and intra-sentence
    contexts during the decoding phase (Section [2.2](#S2.SS2 "2.2 Enhanced Decoding
    Phase ‣ 2 Methodology ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for
    Making LLMs Be Better Context-aware Translators")). Finally, we describe our phase-aware
    prompts and training objective in Section [2.3](#S2.SS3 "2.3 Phase-aware Prompts
    ‣ 2 Methodology ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making
    LLMs Be Better Context-aware Translators") and Section [2.4](#S2.SS4 "2.4 Training
    Objective ‣ 2 Methodology ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning
    for Making LLMs Be Better Context-aware Translators"), respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: For a given document pair $(\mathcal{S},\mathcal{T})$ as the number of transformer
    layers within it.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Multi-phase Encoding and Decoding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We implement our approach based on deep prompt tuning Li and Liang ([2021](#bib.bib15));
    Liu et al. ([2022](#bib.bib18)). Next, we use training instance $(\mathcal{C},S,T)$
    as an example to describe the multi-phase approach. Figure [2](#S2.F2 "Figure
    2 ‣ Inter-sentence Context Encoding Phase. ‣ 2.1 Multi-phase Encoding and Decoding
    ‣ 2 Methodology ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making
    LLMs Be Better Context-aware Translators") illustrates the procedure of multi-phase
    prompt tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: Inter-sentence Context Encoding Phase.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the inter-sentence context encoding phase (Phase 1 in Figure [2](#S2.F2
    "Figure 2 ‣ Inter-sentence Context Encoding Phase. ‣ 2.1 Multi-phase Encoding
    and Decoding ‣ 2 Methodology ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning
    for Making LLMs Be Better Context-aware Translators")), we first concatenate all
    sentences in $\mathcal{C}$ by incorporating the trainable prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $H_{\mathcal{C}}^{1:L}=\text{LLM}(\mathcal{C},{\bf P}_{\mathcal{C}}),$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'where $H_{\mathcal{C}}^{1:L}\in\mathbb{R}^{L\times|\mathcal{C}|\times d}$ as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $H_{\mathcal{C}}^{l}=\text{FFN}\left(\text{Multi-Attn}\left(\textbf{K}_{\mathcal{C}},\textbf{V}_{\mathcal{C}},\textbf{Q}_{\mathcal{C}}\right)\right),$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\textbf{Q}_{\mathcal{C}}=H_{\mathcal{C}}^{l-1},$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\textbf{K}_{\mathcal{C}}=[{\bf P}_{\mathcal{C}}[l,:q,:];H_{\mathcal{C}}^{l-1}],$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\textbf{V}_{\mathcal{C}}=[{\bf P}_{\mathcal{C}}[l,q:,:];H_{\mathcal{C}}^{l-1}],$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: where $H_{\mathcal{C}}^{l}\in\mathbb{R}^{|\mathcal{C}|\times d}$ are the concatenating
    and slicing operations, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8651a44cba15da78d9c4c12bf06e028b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Illustration of pipeline of multi-phase prompt tuning LLM for context-aware
    NMT. Red lines illustrate the procedure of enhanced decoding phase.'
  prefs: []
  type: TYPE_NORMAL
- en: Intra-sentence Context Encoding Phase.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the intra-sentence context encoding phase (Phase 2 in Figure [2](#S2.F2
    "Figure 2 ‣ Inter-sentence Context Encoding Phase. ‣ 2.1 Multi-phase Encoding
    and Decoding ‣ 2 Methodology ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning
    for Making LLMs Be Better Context-aware Translators")), the LLM encodes the intra-sentence
    context $S$ and trainable prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $H_{S}^{1:L}=\text{LLM}(S,H_{\mathcal{C}}^{1:L},{\bf P}_{S}),$ |  | (6)
    |'
  prefs: []
  type: TYPE_TB
- en: 'where $H_{{S}}^{1:L}\in\mathbb{R}^{L\times|S|\times d}$ as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $H_{S}^{l}=\text{FFN}\left(\text{Multi-Attn}\left(\textbf{K}_{S},\textbf{V}_{S},\textbf{Q}_{S}\right)\right),$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\textbf{Q}_{S}=H_{S}^{l-1},$ |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\textbf{K}_{S}=[{\bf P}_{S}[l,:q,:];H_{\mathcal{C}}^{l-1};H_{S}^{l-1}],$
    |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\textbf{V}_{S}=[{\bf P}_{S}[l,q:,:];H_{\mathcal{C}}^{l-1};H_{S}^{l-1}],$
    |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: where $H_{S}^{l}$ layer output of the inter-sentence context encoding.
  prefs: []
  type: TYPE_NORMAL
- en: Decoding Phase.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the decoding phase (Phase 3 in Figure [2](#S2.F2 "Figure 2 ‣ Inter-sentence
    Context Encoding Phase. ‣ 2.1 Multi-phase Encoding and Decoding ‣ 2 Methodology
    ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better
    Context-aware Translators")), given the past activations $H_{S}$ and trainable
    prompt, we call the LLM again to generate the hidden state for predicting the
    probability of the target sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $H_{T}^{1:L}=\text{LLM}(T,H_{{S}}^{1:L},{\bf P}_{T}),$ |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: 'where $H_{T}^{1:L}\in\mathbb{R}^{L\times|T|\times d}$ as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $H_{T}^{l}=\text{FFN}\left(\text{Multi-Attn}\left(\textbf{K}_{T},\textbf{V}_{T},\textbf{Q}_{T}\right)\right),$
    |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\textbf{Q}_{T}=H_{T}^{l-1},$ |  | (13) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\textbf{K}_{T}=[{\bf P}_{T}[l,:q,:];H_{S}^{l-1};H_{T}^{l-1}],$ |  | (14)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\textbf{V}_{T}=[{\bf P}_{T}[l,q:,:];H_{S}^{l-1};H_{T}^{l-1}],$ |  | (15)
    |'
  prefs: []
  type: TYPE_TB
- en: 'where $H_{T}^{l}\in\mathbb{R}^{|T|\times d}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $p\left(y_{t}&#124;S,\mathcal{C},y_{<t}\right)=\text{Softmax}\left(h_{t}^{L}W\right),$
    |  | (16) |'
  prefs: []
  type: TYPE_TB
- en: where $W\in\mathbb{R}^{d\times|\mathcal{V}|}$ is the vocabulary size.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Enhanced Decoding Phase
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/57f788a1666872fdb183598321e8db7a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Illustration of the procedure of our proposed decoding-enhanced approach
    at the $t$-th decoding step of the decoding phase.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Figure [2](#S2.F2 "Figure 2 ‣ Inter-sentence Context Encoding Phase.
    ‣ 2.1 Multi-phase Encoding and Decoding ‣ 2 Methodology ‣ DeMPT: Decoding-enhanced
    Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators"),
    both the inter-sentence context representation $H_{\mathcal{C}}^{1:L}$. This may
    result in a long-distance issue such that the inter-sentence context are not properly
    aligned by target-side tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, to address above two issues, we propose an enhanced decoding phase
    with an aim to more effectively utilize both the inter- and intra-sentence contexts.
    Inspired by Kuang et al. ([2018](#bib.bib13)), we move both the two types of inter-
    and intra-sentence contexts closer to target words to achieve a tight interaction
    between them. Specifically, we concatenate the decoding states with the two types
    of representations to predict the next target words. As shown in Figure [3](#S2.F3
    "Figure 3 ‣ 2.2 Enhanced Decoding Phase ‣ 2 Methodology ‣ DeMPT: Decoding-enhanced
    Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators"),
    the enhanced next word prediction $p_{e}$ is a combination of three distributions
    with different inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\begin{split}p_{e}\left(y_{t}&#124;S,\mathcal{C},y_{<t}\right)=&amp;\lambda_{1}\times\hat{p}\left(y_{t}&#124;S,\mathcal{C},y_{<t}\right)\\
    &amp;+\lambda_{2}\times\bar{p}\left(y_{t}&#124;S,\mathcal{C},y_{<t}\right)\\'
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;+\left(1-\lambda_{1}-\lambda_{2}\right)\times p\left(y_{t}&#124;S,\mathcal{C},y_{<t}\right),\end{split}$$
    |  | (17) |'
  prefs: []
  type: TYPE_NORMAL
- en: 'where $\lambda_{1}$, respectively, which can be further computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{p}\left(y_{t}&#124;S,\mathcal{C},y_{<t}\right)=\text{Softmax}\left(\hat{h}_{t}^{L}W\right),$
    |  | (18) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\bar{p}\left(y_{t}&#124;S,\mathcal{C},y_{<t}\right)=\text{Softmax}\left(\bar{h}_{t}^{L}W\right),$
    |  | (19) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\hat{h}_{t}^{L}=\text{FFN}\left([\tilde{H}_{\mathcal{C}}^{L};\tilde{H}_{S}^{L};h_{t}^{L}]\right),$
    |  | (20) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\bar{h}_{t}^{L}=\text{FFN}\left([\tilde{H}_{S}^{L};h_{t}^{L}]\right),$
    |  | (21) |'
  prefs: []
  type: TYPE_TB
- en: where $W$ at token level, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Phase-aware Prompts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We emphasize the LLM needs to play various roles across three phases, and maintaining
    similar prompts across different phases may not be reasonable. Thus, we empower
    LLM to distinguish different phases by introducing a type embedding and a transfer
    layer³³3Different from the multi-layer perceptron (MLPs) used for reparameterization,
    our transfer layer is shared-parameter for all prompts. Thus, there are fewer
    trainable parameters during the training of our model. We compare the number of
    trainable parameters among different tuning methods in Table [3](#S3.T3 "Table
    3 ‣ 3.2 Experimental Results ‣ 3 Experimentation ‣ DeMPT: Decoding-enhanced Multi-phase
    Prompt Tuning for Making LLMs Be Better Context-aware Translators") and analyze
    the effect of the transfer layer in Appendix [D](#A4 "Appendix D Effect of Transfer
    Layer and Type Embedding ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning
    for Making LLMs Be Better Context-aware Translators"). for these prompts:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\bf P}_{r}=\left(\tanh\left({\bf O}_{r}W_{1}\right)\right)W_{2}+\text{TypeEmb}\left(r\right),$
    |  | (22) |'
  prefs: []
  type: TYPE_TB
- en: where ${\bf O}_{r}\in\mathbb{R}^{L\times 2q\times d}$ represents either phase
    1, phase 2, or phase 3.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Training Objective
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We employ the cross-entropy loss as the training objective of our model. Given
    a training instance $(\mathcal{C},S,T)$, its training loss is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}\left(\mathcal{C},S,T\right)=-\frac{1}{&#124;T&#124;}\sum_{t=1}^{&#124;T&#124;}\text{log}~{}p_{e}\left(y_{t}&#124;S,\mathcal{C},y_{<t}\right).$
    |  | (23) |'
  prefs: []
  type: TYPE_TB
- en: 'Notably, the parameters in LLM, including $W$ in Eq. [16](#S2.E16 "In Decoding
    Phase. ‣ 2.1 Multi-phase Encoding and Decoding ‣ 2 Methodology ‣ DeMPT: Decoding-enhanced
    Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators"),
     [18](#S2.E18 "In 2.2 Enhanced Decoding Phase ‣ 2 Methodology ‣ DeMPT: Decoding-enhanced
    Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators"),
     [19](#S2.E19 "In 2.2 Enhanced Decoding Phase ‣ 2 Methodology ‣ DeMPT: Decoding-enhanced
    Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators"),
    are frozen during training.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Experimentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Model | ZH$\rightarrow$EN |'
  prefs: []
  type: TYPE_TB
- en: '| BLEU | COMET | BLEU | COMET | BLEU | COMET | BLEU | COMET | BLEU | COMET
    |'
  prefs: []
  type: TYPE_TB
- en: '| Trans. | 29.86 | 0.8406 | 38.53 | 0.8545 | 41.44 | 0.8682 | 48.74 | 0.8783
    | 32.25 | 0.8169 |'
  prefs: []
  type: TYPE_TB
- en: '| llama-2-7b as foundation model |'
  prefs: []
  type: TYPE_TB
- en: '| MT-LoRA | 27.43 | 0.8511 | 38.18 | 0.8647 | 40.96 | 0.8712 | 47.52 | 0.8733
    | 33.00 | 0.8311 |'
  prefs: []
  type: TYPE_TB
- en: '| MT-PT | 31.32 | 0.8565 | 41.92 | 0.8675 | 43.56 | 0.8752 | 51.32 | 0.8819
    | 35.46 | 0.8333 |'
  prefs: []
  type: TYPE_TB
- en: '| CMT-PT | 31.13 | 0.8387 | 42.01 | 0.8699 | 43.11 | 0.8762 | 51.66 | 0.8823
    | 35.91 | 0.8396 |'
  prefs: []
  type: TYPE_TB
- en: '| MPT | *33.21 | 0.8645 | †43.11 | 0.8744 | *43.88 | 0.8824 | †52.01 | 0.8913
    | †36.49 | 0.8456 |'
  prefs: []
  type: TYPE_TB
- en: '| DeMPT | *33.89 | 0.8658 | †43.71 | 0.8816 | *44.69 | 0.8899 | †53.10 | 0.8979
    | †36.55 | 0.8438 |'
  prefs: []
  type: TYPE_TB
- en: '| bloomz-7b1-mt as foundation model |'
  prefs: []
  type: TYPE_TB
- en: '| MT-LoRA | 25.79 | 0.8466 | 35.67 | 0.8601 | 35.17 | 0.8522 | 46.32 | 0.8644
    | 28.01 | 0.8012 |'
  prefs: []
  type: TYPE_TB
- en: '| MT-PT | 30.99 | 0.8520 | 40.49 | 0.8661 | 37.76 | 0.8579 | 50.68 | 0.8823
    | 30.27 | 0.8106 |'
  prefs: []
  type: TYPE_TB
- en: '| CMT-PT | 30.82 | 0.8504 | 40.31 | 0.8639 | 38.01 | 0.8601 | 50.26 | 0.8832
    | 29.80 | 0.8108 |'
  prefs: []
  type: TYPE_TB
- en: '| MPT | *31.81 | 0.8601 | *41.11 | 0.8766 | †38.99 | 0.8669 | *51.33 | 0.8910
    | *30.99 | 0.8201 |'
  prefs: []
  type: TYPE_TB
- en: '| DeMPT | *32.46 | 0.8649 | *41.92 | 0.8790 | †40.06 | 0.8703 | *52.25 | 0.8990
    | *31.79 | 0.8253 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Results of different systems on sacreBLEU and COMET metrics. DeMPT/MPT
    is our proposed Multi-phase Prompt Tuning approach with/without Decoding-enhanced
    strategy (in Sec. [2.2](#S2.SS2 "2.2 Enhanced Decoding Phase ‣ 2 Methodology ‣
    DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware
    Translators")). Scores with bold indicate the best performance. * or † indicates
    the gains are statistically significant over MT-PT or CMT-PT with $p$<0.01 Koehn
    ([2004](#bib.bib12)).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | ZH$\rightarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| Trans. | 47.63 | 54.41 | 58.29 | 62.52 | 48.79 |'
  prefs: []
  type: TYPE_TB
- en: '| llama-2-7b as foundation model |'
  prefs: []
  type: TYPE_TB
- en: '| MT-LoRA | 44.83 | 54.52 | 57.72 | 62.18 | 49.06 |'
  prefs: []
  type: TYPE_TB
- en: '| MT-PT | 49.49 | 57.87 | 60.89 | 65.02 | 52.59 |'
  prefs: []
  type: TYPE_TB
- en: '| CMT-PT | 49.53 | 58.27 | 61.23 | 65.89 | 53.34 |'
  prefs: []
  type: TYPE_TB
- en: '| MPT | 51.56 | 59.56 | 62.15 | 67.14 | 54.18 |'
  prefs: []
  type: TYPE_TB
- en: '| DeMPT | 52.68 | 60.33 | 63.11 | 67.95 | 54.34 |'
  prefs: []
  type: TYPE_TB
- en: '| bloomz-7b1-mt as foundation model |'
  prefs: []
  type: TYPE_TB
- en: '| MT-LoRA | 43.23 | 51.82 | 51.12 | 61.77 | 43.29 |'
  prefs: []
  type: TYPE_TB
- en: '| MT-PT | 49.48 | 56.81 | 55.40 | 64.71 | 46.14 |'
  prefs: []
  type: TYPE_TB
- en: '| CMT-PT | 49.61 | 57.05 | 55.81 | 65.12 | 46.09 |'
  prefs: []
  type: TYPE_TB
- en: '| MPT | 50.22 | 57.93 | 56.69 | 66.25 | 47.29 |'
  prefs: []
  type: TYPE_TB
- en: '| DeMPT | 50.62 | 58.30 | 57.34 | 67.12 | 48.00 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Results of different systems on BlonDe metric.'
  prefs: []
  type: TYPE_NORMAL
- en: We build our approach upon two open-source LLMs, namely, llama-2-7b⁴⁴4[https://huggingface.co/meta-llama/Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf)
    and bloomz-7b1-mt⁵⁵5[https://huggingface.co/bigscience/bloomz-7b1-mt](https://huggingface.co/bigscience/bloomz-7b1-mt).
    We verify the effectiveness of our proposed approach on five translation tasks,
    including {Chinese (ZH), French (FR), German (DE), Spanish (ES), Russian (RU)}$\rightarrow$English
    (EN).
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Experimental Settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Datasets and Preprocessing.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The corpus of all translation tasks is extracted from New-Comentary-v18. See
    Appendix [A](#A1 "Appendix A Datasets ‣ DeMPT: Decoding-enhanced Multi-phase Prompt
    Tuning for Making LLMs Be Better Context-aware Translators") for splitting and
    statistics of the training set, valid set, and test set. We use the tokenizer
    of foundation models to process the input data and no any other preprocessing
    is performed.'
  prefs: []
  type: TYPE_NORMAL
- en: Baselines.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We compare our approach against four baselines:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Transformer (Trans.): It is an encoder-decoder Transformer-base model Vaswani
    et al. ([2017](#bib.bib33)) that is trained from scratch.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MT-LoRA: It is a tuned LLM adapted to NMT task via the tuning method of Low-Rank
    Adaptation Hu et al. ([2022](#bib.bib6)), which makes large-scale pre-training
    models adapt to a new task by injecting a trainable rank decomposition matrice
    into each layer of the Transformer architecture.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MT-PT: It is a tuned LLM adapted to NMT task via the deep prompt tuning with
    MLPs reparameterization,⁶⁶6We attempt to remove reparameterization but experience
    a significant decline in performance. which only tunes continuous prompts with
    a frozen language model.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CMT-PT: Similar to MT-PT, it is also a tuned LLM via the deep prompt tuning
    with MLPs reparameterization. Unlike MT-PT, it utilizes inter-sentence context
    within the concatenation strategy, as depicted in Figure [1](#S1.F1 "Figure 1
    ‣ 1 Introduction ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making
    LLMs Be Better Context-aware Translators") (a).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Among them, Transformer, MT-LoRA, and MT-PT are context-agnostic systems while
    CMT-PT is a context-aware system. For a fair comparison, we ensure that all context-aware
    systems, including CMT-PT, MPT, and DeMPT, incorporate identical inter-sentence
    context.
  prefs: []
  type: TYPE_NORMAL
- en: Model Setting and Training.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For the Transformer model, we implement it upon Fairseq Ott et al. ([2019](#bib.bib27)).
    For MT-LoRA models, we set the rank of trainable matrices as 16 which performs
    best in our preliminary experiment. For all MT-PT models, CMT-PT models, and our
    models, we set the prompt length $q$ to 1/3\. More details of training are provided
    in Appendix [B](#A2 "Appendix B Training Details ‣ DeMPT: Decoding-enhanced Multi-phase
    Prompt Tuning for Making LLMs Be Better Context-aware Translators").'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We use sacreBLEU (accuracy-related metric)⁷⁷7Signature: nrefs:1|case:mixed|eff:no|tok:13a|'
  prefs: []
  type: TYPE_NORMAL
- en: smooth:exp|version:2.3.1 Post ([2018](#bib.bib28)), COMET (semantics-related
    metric) with the wmt22-comet-da model⁸⁸8[https://github.com/Unbabel/COMET](https://github.com/Unbabel/COMET)
     Rei et al. ([2020](#bib.bib30)), and BlonDe (discourse-related metric) Jiang
    et al. ([2022](#bib.bib9)) as the evaluation metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Experimental Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The main experimental results are presented in Tables [1](#S3.T1 "Table 1 ‣
    3 Experimentation ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making
    LLMs Be Better Context-aware Translators") and [2](#S3.T2 "Table 2 ‣ 3 Experimentation
    ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better
    Context-aware Translators"). Additionally, a comparison of the number of trainable
    parameters is presented in Table [3](#S3.T3 "Table 3 ‣ 3.2 Experimental Results
    ‣ 3 Experimentation ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making
    LLMs Be Better Context-aware Translators") across different tuning methods. From
    these results we have the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The encoder-decoder Transformer model (Trans.) performs better than the LLMs
    with LoRA tuning in most translation directions in BLEU score. For example, when
    utilizing llama-2-7b as the foundation model, Transformer surpasses MT-LoRA an
    average of 0.75 BLEU score across all translation tasks. However, MT-LoRA model
    outperforms Trans. in terms of COMET, suggesting that translations from LLMs may
    align more closely with human preferences.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The MT-PT model presents superior performance compared to the MT-LoRA model
    in terms of BLEU, COMET, and BlonDe. Taking bloomz-7b1-mt as the foundation model,
    the MT-PT model outperforms the MT-LoRA model by an average of 3.84 BLEU score,
    3.51 BlonDe score, and 0.0047 COMET score. Nevertheless, the MT-PT model sacrifices
    efficiency for performance, introducing more trainable parameters (13.87% vs.
    0.12%).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leveraging the inter-sentence context is helpful in alleviating discourse-related
    issues. For example, with bloomz-7b1-mt used as the foundation model, the CMT-PT
    model, despite underperforming in BLEU and COMET compared to the MT-PT model,
    excels in discourse-related BlonDe scores (averaging 57.66 vs. 57.17).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our MPT/DeMPT model outperforms all baselines across all translation tasks.
    For example, when using llama-2-7b as the foundation model, our MPT model achieves
    an average gain of 1.62/1.45/2.03 in BLEU/COMET/BlonDe score compared to the CMT-PT
    model. Furthermore, our decoding-enhance strategy enhances the capacity of LLMs
    in context-aware NMT, with DeMPT outperforming MPT in BLEU/COMET/BlonDe score
    (averaging 42.39/0.8758/59.68 vs. 41.74/0.8716/58.91).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model built upon llama-2-7b as the foundation model outperforms the one
    using bloomz-7b1-mt, suggesting that llama-2-7b serves as a more robust foundation
    model for translation tasks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | MT-LoRA | MT-PT/CMT-PT | DeMPT |'
  prefs: []
  type: TYPE_TB
- en: '| Trainable Para. | 0.12% | 13.87% | 3.11% |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Proportion of trainable parameters against total parameters for different
    tuning methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we use bloomz-7b1-mt as the foundation model to discuss and
    analyze our approach. See Appendix C$\sim$E for further discussions.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Effect of Length of Inter-sentence Context
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For efficient training, we define the inter-sentence context in Section [2](#S2
    "2 Methodology ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making
    LLMs Be Better Context-aware Translators") as previous sentences with a total
    tokens not exceeding 256\. We are curious about the potential impact of inter-sentence
    length on the performance of our approach. Consequently, we extend the inter-sentence
    context length from 256 to 1024 and assess the performance of our approach in
    the ZH$\rightarrow$EN task.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [4](#S4.F4 "Figure 4 ‣ 4.1 Effect of Length of Inter-sentence Context
    ‣ 4 Discussion ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making
    LLMs Be Better Context-aware Translators") shows the performance trend of the
    CMT-PT model and our DeMPT model. As the length of the inter-sentence context
    increases, both models exhibit a slight enhancement in both BLEU and BlonDe scores.
    Interestingly, our model with a 256-token inter-sentence context outperforms the
    CMT-PT model with a 1024-token inter-sentence context in both BLEU and BlonDe
    scores. This further suggests the effectiveness of our approach in harnessing
    the capabilities of LLMs for context-aware NMT compared to the concatenation strategy.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/52183a911cdeb42e04fadaa65e37f732.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Performance of CMT-PT and our DeMPT on ZH$\rightarrow$EN test set
    when using different inter-sentence context lengths.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Effect of Prompt Length
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5699259a7947b50ea951e7c947248c6f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Performance of MT-PT, CMT-PT, and our DeMPT on ZH$\rightarrow$EN
    test set when using different lengths of the trainable prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: As our approach is implemented based on deep prompt tuning, next we compare
    the impact of the trainable prompt length for MT-PT, CMT-PT, and our DeMPT.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [5](#S4.F5 "Figure 5 ‣ 4.2 Effect of Prompt Length ‣ 4 Discussion ‣
    DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware
    Translators") shows the performance curves when increasing the prompt length from
    32 to 128\. We observe that increased prompt length tends to enhance performance
    for both BLEU and BlonDe, yet the gains exhibit diminishing returns. This finding
    is consistent with that in Li and Liang ([2021](#bib.bib15)); Lester et al. ([2021](#bib.bib14));
    Tan et al. ([2022](#bib.bib32)). We also observe that DeMPT with a prompt length
    of 64 outperforms both MT-PT and CMT-PT with a prompt length of 128 on both metrics,
    suggesting the superiority of our approach over the concatenation strategy in
    enhancing LLMs’ capacity for context-aware NMT.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Comparison of Inference Speed
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Model | Speed | BLEU |'
  prefs: []
  type: TYPE_TB
- en: '| MT-PT | 0.75 sec/sent. | 30.99 |'
  prefs: []
  type: TYPE_TB
- en: '| CMT-PT | 0.77 sec/sent. | 30.82 |'
  prefs: []
  type: TYPE_TB
- en: '| MPT | 0.78 sec/sent. | 31.81 |'
  prefs: []
  type: TYPE_TB
- en: '| DeMPT | 0.79 sec/sent. | 32.46 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Comparison of inference speed on ZH$\rightarrow$EN translation task.
    Speed is measured on the test set using 4 GPUs. sec/sent. means seconds spent
    for decoding each sentence. Note that the reparameterization is not needed during
    inference Li and Liang ([2021](#bib.bib15)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [4](#S4.T4 "Table 4 ‣ 4.3 Comparison of Inference Speed ‣ 4 Discussion
    ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better
    Context-aware Translators") compares the inference speed of different models on
    ZH$\rightarrow$EN translation task. Our MPT and DeMPT models, dividing the context-aware
    NMT process into three separate phases, demonstrates comparable inference speed
    to the single-phase MT-PT and CMT-PT models, with only a marginal drop of 0.02
    seconds per sentence in decoding. This illustrates the efficiency of our approach
    without introducing significant computational overhead.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Performance on Contrastive Test Set
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We evaluate the models’ ability to resolve discourse inconsistencies using the
    contrastive test set proposed by Voita et al. ([2019a](#bib.bib34)), which focuses
    on four discourse phenomena such as deixis, lexicon consistency (lex.c), ellipsis
    inflection (ell.infl), and verb phrase ellipsis (ell.VP) in English$\rightarrow$Russian
    translation. Within the test set, each instance comprises a positive translation
    and several negative ones that vary by only one specific word. The purpose of
    the contrastive test set is to assess whether a model is more inclined to generate
    a correct translation as opposed to incorrect variations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [5](#S4.T5 "Table 5 ‣ 4.4 Performance on Contrastive Test Set ‣ 4 Discussion
    ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better
    Context-aware Translators") lists the accuracy of translation prediction on the
    contrastive test set for MT-PT, CMT-PT and DeMPT. Compared to the context-agnostic
    MT-PT model, both context-aware CMT-PT and DeMPT models show substantial improvements
    across the four discourse phenomena. Additionally, DeMPT demonstrates the best
    performance, surpassing CMT-PT by an average accuracy margin of 3.8.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | deixis | lex.c | ell.infl | ell.VP | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| MT-PT | 50.0 | 45.7 | 53.0 | 28.6 | 44.3 |'
  prefs: []
  type: TYPE_TB
- en: '| CMT-PT | 80.2 | 46.1 | 74.3 | 75.3 | 68.9 |'
  prefs: []
  type: TYPE_TB
- en: '| DeMPT | 80.1 | 55.7 | 75.9 | 79.3 | 72.7 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Accuracy [%] of translation prediction for four discourse phenomena
    on the English $\rightarrow$ Russian contrastive test set.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Human Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We use the Direct Assessment (DA) method Graham et al. ([2017](#bib.bib5)) to
    manually assess the quality of translations generated by DeMPT and CMT-PT. In
    this assessment, human evaluators compare the meaning of the MT output with a
    human-produced reference translation, working within the same language.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we randomly select 5 documents with a total of 200 groups of
    sentences from the ZH$\rightarrow$EN test set. To avoid potential bias in evaluation,
    we recruit 6 professional translators and ensure each translation from DeMPT or
    CMT-PT is scored twice by two translators. Table [6](#S4.T6 "Table 6 ‣ 4.5 Human
    Evaluation ‣ 4 Discussion ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning
    for Making LLMs Be Better Context-aware Translators") shows the DA scores for
    CMT-PT and DeMPT. Our DeMPT outperforms CMT-PT by 7.14 DA score, providing strong
    evidence for the effectiveness of our approach. Further details and results regarding
    the DA can be found in Appendix [C](#A3 "Appendix C Details of Human Evaluation
    ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better
    Context-aware Translators").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Score_1 | Score_2 | Average |'
  prefs: []
  type: TYPE_TB
- en: '| CMT-PT | 79.00 | 80.17 | 79.59 |'
  prefs: []
  type: TYPE_TB
- en: '| DeMPT | 86.17 (+7.17) | 87.30 (+7.13) | 86.73 (+7.14) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Human DA scores for CMT-PT and DeMPT on ZH$\rightarrow$EN translation
    task.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large Language Models for Context-aware Machine Translation.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While traditional context-aware neural machine translation (NMT) has seen considerable
    progress in recent years Jean et al. ([2017](#bib.bib8)); Wang et al. ([2017](#bib.bib39));
    Voita et al. ([2018](#bib.bib36)); Maruf et al. ([2019](#bib.bib22)); Kang et al.
    ([2020](#bib.bib10)); Bao et al. ([2021](#bib.bib2)); Sun et al. ([2022](#bib.bib31));
    Bao et al. ([2023](#bib.bib1)), the effective integration of large language models
    (LLMs) to model inter-sentence context and enhance context-aware translation remains
    an area of limited exploration. Existing studies mainly focus on the assessment
    of LLMs’ ability in discourse modeling. For example, Wang et al. ([2023](#bib.bib37))
    approach context-aware NMT as a task involving long sequence generation, employing
    a concatenation strategy, and conduct comprehensive evaluations of LLMs such as
    ChatGPT and GPT-4\. Their focus includes the impact of context-aware prompts,
    comparisons with translation models, and an in-depth analysis of discourse modeling
    ability. Similarly, Karpinska and Iyyer ([2023](#bib.bib11)) engage professional
    translators to evaluate LLMs’ capacity in context-aware NMT. In contrast, Wu et al.
    ([2024](#bib.bib40)) compare the effectiveness of various parameter-efficient
    fine-tuning methods on moderately-sized LLMs for context-aware NMT. Besides, Wu
    and Hu ([2023](#bib.bib41)) explore the prompt engineering with GPT language models
    specifically for document-level (context-aware) MT while Li et al. ([2024](#bib.bib17))
    experiment with combining sentence-level and document-level translation instructions
    of varying lengths to fine-tune LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt Tuning for Large Language Model.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Liu et al. ([2021](#bib.bib19)) and Li and Liang ([2021](#bib.bib15)) propose
    to make LLMs adapt to various tasks by adding trainable prompts (also called continuous
    prompts) to the original input sequences. In this paradigm, only the continuous
    prompts are updated during training. Liu et al. ([2022](#bib.bib18)) further introduce
    deep prompt tuning, extending the idea by inserting trainable prompts into all
    layers of LLMs, rather than just the embedding layer. While these approaches lay
    the groundwork for a general framework, our focus lies in augmenting the performance
    of LLMs specifically for inter-sentence context modeling in context-aware NMT.
    Notably related, Tan et al. ([2022](#bib.bib32)) propose a multi-phase tuning
    approach to enhance the sentence-level translation performance of a multilingual
    GPT. However, the exploration of effective LLM tuning for addressing discourse-related
    challenges in the context-aware NMT domain remains underdeveloped.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we have examined the hypothesis that it is crucial to differentially
    model and leverage inter-sentence context and intra-sentence context when adapting
    LLMs to context-aware NMT. This stems from our observation that intra-sentence
    context exhibits a stronger correlation with the target sentence compared to inter-sentence
    context, owing to its richer parallel semantic information. To this end, we have
    proposed a novel decoding-enhanced multi-phase prompt tuning (DeMPT) approach
    to make LLMs aware of the differences between inter- and intra-sentence contexts,
    and further improve LLMs’ capacity in discourse modeling. We have evaluated our
    approach using two foundation models and present experimental results across five
    translation directions. Experimental results and discussions have demonstrated
    a significant enhancement in the performance of LLMs in context-aware NMT, manifesting
    as improved translation accuracy and a reduction in discourse-related issues.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Owing to resource limitations, our work is restricted to moderate-scale LLMs,
    specifically those with 7 billion parameters, and a confined window size of inter-sentence
    context. It is imperative to acknowledge that the results of our research may
    differ when employing larger models and an extended window size for inter-sentence
    context. We acknowledge these limitations and consider them as avenues for future
    exploration.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bao et al. (2023) Guangsheng Bao, Zhiyang Teng, and Yue Zhang. 2023. Target-side
    augmentation for document-level machine translation. In *Proceedings of ACL*,
    pages 10725–10742.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bao et al. (2021) Guangsheng Bao, Yue Zhang, Zhiyang Teng, Boxing Chen, and
    Weihua Luo. 2021. G-transformer for document-level machine translation. In *Proceedings
    of ACL*, pages 3442–3455.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BigScience (2022) BigScience. 2022. Bloom: A 176b-parameter open-access multilingual
    language model. *Computing Research Repository*, arXiv:2211.05100.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Google (2022) Google. 2022. Palm: Scaling language modeling with pathways.
    *J. Mach. Learn. Res.*, 24:240:1–240:113.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graham et al. (2017) Yvette Graham, Qingsong Ma, Timothy Baldwin, Qun Liu, Carla
    Parra, and Carolina Scarton. 2017. Improving evaluation of document-level machine
    translation quality estimation. In *Proceedings of EACL*, pages 356–361.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2022) Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation
    of large language models. In *Proceedings of ICLR*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu and Wan (2023) Xinyu Hu and Xiaojun Wan. 2023. Exploring discourse structure
    in document-level machine translation. In *Proceedings of EMNLP*, pages 13889–13902.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jean et al. (2017) Sebastien Jean, Stanislas Lauly, Orhan Firat, and Kyunghyun
    Cho. 2017. Does neural machine translation benefit from larger context? *Computing
    Research Repository*, arXiv:1704.05135.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (2022) Yuchen Eleanor Jiang, Tianyu Liu, Shuming Ma, Dongdong
    Zhang, Jian Yang, Haoyang Huang, Rico Sennrich, Ryan Cotterell, Mrinmaya Sachan,
    and Ming Zhou. 2022. BlonDe: An automatic evaluation metric for document-level
    machine translation. In *Proceedings of NAACL*, pages 1550–1565, Seattle, United
    States.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kang et al. (2020) Xiaomian Kang, Yang Zhao, Jiajun Zhang, and Chengqing Zong.
    2020. Dynamic context selection for document-level neural machine translation
    via reinforcement learning. In *Proceedings of EMNLP*, pages 2242–2254.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karpinska and Iyyer (2023) Marzena Karpinska and Mohit Iyyer. 2023. Large language
    models effectively leverage document-level context for literary translation, but
    critical errors persist. In *Proceedings of WMT*, pages 419–451.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Koehn (2004) Philipp Koehn. 2004. Statistical significance tests for machine
    translation evaluation. In *Proceedings of EMNLP*, pages 388–395.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kuang et al. (2018) Shaohui Kuang, Junhui Li, António Branco, Weihua Luo, and
    Deyi Xiong. 2018. Attention focusing for neural machine translation by bridging
    source and target embeddings. In *Proceedings of ACL*, pages 1767–1776.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lester et al. (2021) Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The
    power of scale for parameter-efficient prompt tuning. In *Proceedings of EMNLP*,
    pages 3045–3059.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li and Liang (2021) Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing
    continuous prompts for generation. In *Proceedings of ACL-IJCNLP*, pages 4582–4597,
    Online.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023) Yachao Li, Junhui Li, Jing Jiang, Shimin Tao, Hao Yang, and
    Min Zhang. 2023. P-Transformer: Towards Better Document-to-Document Neural Machine
    Translation. *IEEE/ACM Transactions on Audio, Speech, and Language Processing*,
    31:3859–3870.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2024) Yachao Li, Junhui Li, Jing Jiang, and Min Zhang. 2024. Enhancing
    document-level translation of large language model via translation mixed-instructions.
    *Computing Research Repository*, arXiv:2401.08088.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2022) Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du,
    Zhilin Yang, and Jie Tang. 2022. P-tuning: Prompt tuning can be comparable to
    fine-tuning across scales and tasks. In *Proceedings of ACL*, pages 61–68.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2021) Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian,
    Zhilin Yang, and Jie Tang. 2021. Gpt understands, too. *Computing Research Repository*,
    arXiv:2103.10385.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lyu et al. (2021) Xinglin Lyu, Junhui Li, Zhengxian Gong, and Min Zhang. 2021.
    Encouraging lexical translation consistency for document-level neural machine
    translation. In *Proceedings of EMNLP*, pages 3265–3277.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lyu et al. (2022) Xinglin Lyu, Junhui Li, Shimin Tao, Hao Yang, and Min Zhang.
    2022. Modeling consistency preference via lexical chains for document-level neural
    machine translation. In *Proceedings of EMNLP*, pages 6312–6326.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maruf et al. (2019) Sameen Maruf, André F. T. Martins, and Gholamreza Haffari.
    2019. Selective attention for context-aware neural machine translation. In *Proceedings
    of NAACL*, pages 3092–3102.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MetaAI (2023a) MetaAI. 2023a. Llama 2: Open foundation and fine-tuned chat
    models. *Computing Research Repository*, arXiv:2307.09288.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MetaAI (2023b) MetaAI. 2023b. Llama: Open and efficient foundation language
    models. *ArXiv*, abs/2302.13971.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Miculicich et al. (2018) Lesly Miculicich, Dhananjay Ram, Nikolaos Pappas, and
    James Henderson. 2018. Document-level neural machine translation with hierarchical
    attention networks. In *Proceedings of EMNLP*, pages 2947–2954.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. 2023. Gpt-4 technical report. *Computing Research Repository*,
    arXiv:2303.08774.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ott et al. (2019) Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam
    Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. fairseq: A fast, extensible
    toolkit for sequence modeling. In *Proceedings of NAACL-HLT: Demonstrations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Post (2018) Matt Post. 2018. A call for clarity in reporting BLEU scores. In
    *Proceedings of WMT*, pages 186–191.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rajbhandari et al. (2020) Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,
    and Yuxiong He. 2020. Zero: Memory optimizations toward training trillion parameter
    models. In *SC20: Proceedings of High Performance Computing, Networking, Storage
    and Analysis*, pages 1–16.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rei et al. (2020) Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie.
    2020. COMET: A neural framework for MT evaluation. In *Proceedings of EMNLP*,
    pages 2685–2702.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2022) Zewei Sun, Mingxuan Wang, Hao Zhou, Chengqi Zhao, Shujian
    Huang, Jiajun Chen, and Lei Li. 2022. Rethinking document-level neural machine
    translation. In *Findings of ACL*, pages 3537–3548.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tan et al. (2022) Zhixing Tan, Xiangwen Zhang, Shuo Wang, and Yang Liu. 2022.
    MSP: Multi-stage prompting for making pre-trained language models better translators.
    In *Proceedings of ACL*, pages 6131–6142.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. In *Proceedings of NIPS*, pages 5998–6008.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Voita et al. (2019a) Elena Voita, Rico Sennrich, and Ivan Titov. 2019a. Context-aware
    monolingual repair for neural machine translation. In *Proceedings of EMNLP-IJCNLP*,
    pages 877–886.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Voita et al. (2019b) Elena Voita, Rico Sennrich, and Ivan Titov. 2019b. When
    a good translation is wrong in context: Context-aware machine translation improves
    on deixis, ellipsis, and lexical cohesion. In *Proceedings of ACL*, pages 1198–1212.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Voita et al. (2018) Elena Voita, Pavel Serdyukov, Rico Sennrich, and Ivan Titov.
    2018. Context-aware neural machine translation learns anaphora resolution. In
    *Proceedings of ACL*, pages 1264–1274.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023) Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian
    Yu, Shuming Shi, and Zhaopeng Tu. 2023. Document-level machine translation with
    large language models. In *Proceedings of EMNLP*, pages 16646–16661.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2019) Longyue Wang, Zhaopeng Tu, Xing Wang, and Shuming Shi. 2019.
    One model to learn both: Zero pronoun prediction and translation. In *Proceedings
    of EMNLP-IJCNLP*, pages 921–930.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2017) Longyue Wang, Zhaopeng Tu, Andy Way, and Qun Liu. 2017. Exploiting
    cross-sentence context for neural machine translation. In *Proceedings of EMNLP*,
    pages 2826–2831.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2024) Minghao Wu, Thuy-Trang Vu, Lizhen Qu, George Foster, and Gholamreza
    Haffari. 2024. Adapting large language models for document-level machine translation.
    *Computing Research Repository*, arXiv:2401.06468.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu and Hu (2023) Yangjian Wu and Gang Hu. 2023. Exploring prompt engineering
    with GPT language models for document-level machine translation: Insights and
    findings. In *Proceedings of the Eighth Conference on Machine Translation*, pages
    166–169.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2018) Jiacheng Zhang, Huanbo Luan, Maosong Sun, Feifei Zhai, Jingfang
    Xu, Min Zhang, and Yang Liu. 2018. Improving the transformer translation model
    with document-level context. In *Proceedings of EMNLP*, pages 533–542.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Statistics and Splitting of Datasets.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We provide the detailed statistic in Table [7](#A1.T7 "Table 7 ‣ Statistics
    and Splitting of Datasets. ‣ Appendix A Datasets ‣ DeMPT: Decoding-enhanced Multi-phase
    Prompt Tuning for Making LLMs Be Better Context-aware Translators"). For all translation
    tasks, we randomly select 80% document pairs from the corpus as the training set.
    Both the test set and validation set include 150 document pairs each, randomly
    sampled from the remaining 20% of document pairs in the corpus. Regarding sentence
    preprocessing across all datasets, we segment the sentences with the tokenizer
    from the respective foundation model. No additional preprocessing steps are performed.
    Datasets are downloaded from [https://data.statmt.org/news-commentary/v18](https://data.statmt.org/news-commentary/v18).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | ZH$\rightarrow$EN |'
  prefs: []
  type: TYPE_TB
- en: '| #Doc | #Sent | #Doc | #Sent | #Doc | #Sent | #Doc | #Sent | #Doc | #Sent
    |'
  prefs: []
  type: TYPE_TB
- en: '| Training | 8,622 | 342,495 | 7,915 | 310,489 | 8,417 | 333,201 | 9,677 |
    378,281 | 7,255 | 272,100 |'
  prefs: []
  type: TYPE_TB
- en: '| Validation | 150 | 6,061 | 150 | 5,890 | 150 | 5,866 | 150 | 5,782 | 150
    | 5,691 |'
  prefs: []
  type: TYPE_TB
- en: '| Test | 150 | 5,747 | 150 | 5,795 | 150 | 5,967 | 150 | 5,819 | 150 | 5,619
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Statistics of training, validation, and test sets for five translation
    tasks. #Doc and #Sent denote the numbers of Document and Sentence, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cb3e7753bbb2b9e048fa28670d594bc2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Scoring criterion for Direct Assessment. We group the score into
    five ranges, i.e., 0-20, 21-40, 41-60, 61-80, 81-100.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Training Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For all Transformer NMT models, we use the transformer-base setting as in Vaswani
    et al. ([2017](#bib.bib33)), where the learning rate is set to 1e-4\. The Transformer
    NMT models are trained on 4$\times$ NVIDIA A800 GPUs with Deespeed Zero 2 offload
    setting Rajbhandari et al. ([2020](#bib.bib29)).⁹⁹9[https://github.com/microsoft/DeepSpeed](https://github.com/microsoft/DeepSpeed)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7e32b17208056040233d0df23f9e6856.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: A case study for the CMT-PT model and our DeMPT model on ZH$\rightarrow$EN
    translation task.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Details of Human Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Criterion and Recruitment.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Given a source sentence, its translation from MT (i.e., CMT-PT and our DeMPT),
    and its human-produced reference translation, the evaluators are asked to give
    a score ranging from 0 to 100\. Figure [6](#A1.F6 "Figure 6 ‣ Statistics and Splitting
    of Datasets. ‣ Appendix A Datasets ‣ DeMPT: Decoding-enhanced Multi-phase Prompt
    Tuning for Making LLMs Be Better Context-aware Translators") presents the detailed
    criterion of scoring. We recruit evaluators from professional translators with
    at least five years of experience in translation.'
  prefs: []
  type: TYPE_NORMAL
- en: Statistics of Translation Errors.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We manually count the number of bad cases from our DeMPT model. The bad cases
    fall into two categories: (1) the DA score is 60 or lower; (2) the DA score is
    lower than that of the translation from CMT-PT. The main types of the bad cases
    are Mistranslation (Mis.), Unnoticed Omission (UO), Inappropriate Expression (IE),
    and Grammatical Error (GE). We present detailed statistics in Table [8](#A3.T8
    "Table 8 ‣ Case Study. ‣ Appendix C Details of Human Evaluation ‣ DeMPT: Decoding-enhanced
    Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators").
    The statistics indicate the bad cases mainly come from Mistranslation and Unnoticed
    Omission. Meanwhile, our DeMPT model outperforms the CMT-PT model in 86.5% DA
    cases.'
  prefs: []
  type: TYPE_NORMAL
- en: Case Study.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We present a case in Figure [7](#A2.F7 "Figure 7 ‣ Appendix B Training Details
    ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better
    Context-aware Translators") to illustrate how our DeMPT model outperforms the
    CMT-PT model. In this case, we compare the translations of two consecutive sentences
    from our model and the CMT-PT model. First, we notice that the CMT-PT model translates
    the source word 美国  in the two sentences into US and America, respectively. However,
    our model consistently translates them into US. Second, our model uses for its
    part, a phase with more coherent preference, as the translation of 同时 , instead
    of At the same time adopted in the translation from the CMT-PT model. Both of
    them demonstrate the superiority of our proposed approach in discourse modeling.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Group | Type of Bad Case |'
  prefs: []
  type: TYPE_TB
- en: '| Mis. | UO | IE | GE | Total (Perc.) |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 6 | 3 | 1 | 2 | 12 (6.0%) |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 9 | 7 | 6 | 5 | 27 (13.5%) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: Statistics of bad cases from our DeMPT model on ZH$\rightarrow$EN
    translation task. Perc. denotes the percentage of bad cases against the total
    of DA cases.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Effect of Transfer Layer and Type Embedding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As in Eq. [22](#S2.E22 "In 2.3 Phase-aware Prompts ‣ 2 Methodology ‣ DeMPT:
    Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware
    Translators") within Section [2.3](#S2.SS3 "2.3 Phase-aware Prompts ‣ 2 Methodology
    ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better
    Context-aware Translators"), we introduce two sublayers: a non-linear transfer
    sublayer and a type embedding sublayer for the trainable prompt in each phase.
    This design enhances the awareness of LLMs regarding the distinctions in inputs
    across the three tuning phases, allowing them to adapt to specific roles at each
    phase. We investigate the effect of these two sublayers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Table [9](#A4.T9 "Table 9 ‣ Appendix D Effect of Transfer Layer
    and Type Embedding ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making
    LLMs Be Better Context-aware Translators"), our observations reveal that the transfer
    sublayer holds greater importance than the type embedding sublayer. Removing either
    the non-linear transfer sublayer (w/o Transfer.) or the type embedding sublayer
    (w/o Embed.) results in a performance drop of 0.84/0.0048/0.39 or 0.45/0.0036/0.007
    in BLEU/COMET/BlonDe metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | BLEU | COMET | BlonDe |'
  prefs: []
  type: TYPE_TB
- en: '| MT-PT | 30.99 | 0.8520 | 49.48 |'
  prefs: []
  type: TYPE_TB
- en: '| CMT-PT | 30.82 | 0.8504 | 49.61 |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashlineDeMPT | 32.46 | 0.8649 | 50.62 |'
  prefs: []
  type: TYPE_TB
- en: '| w/o Transfer. | 31.62 | 0.8601 | 50.23 |'
  prefs: []
  type: TYPE_TB
- en: '| w/o Embed. | 32.01 | 0.8613 | 50.55 |'
  prefs: []
  type: TYPE_TB
- en: '| w/o CTX. | 31.98 | 0.8593 | 49.89 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: Comparison of performances of the DeMPT variants on ZH$\rightarrow$EN
    test set. w/o Trans. or w/o Embed. denotes the variant without the non-linear
    transfer sublayer or type embedding sublayer in Eq. [22](#S2.E22 "In 2.3 Phase-aware
    Prompts ‣ 2 Methodology ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for
    Making LLMs Be Better Context-aware Translators"). w/o CTX. means the inter-sentence
    context is not available, i.e., context-agnostic DeMPT system.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Effect of Inter-sentence Context
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We implement the context-agnostic (sentence-level) DeMPT system to analyze the
    effect of the inter-sentence context. More specifically, we replace the input
    of LLMs in the inter-sentence context encoding phase with the intra-sentence context.
    In other words, we encode the intra-sentence context twice to keep the multi-phase
    tuning strategy in DeMPT while making the inter-sentence context unavailable.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the last row of Table [9](#A4.T9 "Table 9 ‣ Appendix D Effect of
    Transfer Layer and Type Embedding ‣ DeMPT: Decoding-enhanced Multi-phase Prompt
    Tuning for Making LLMs Be Better Context-aware Translators") (i.e., w/o CTX),
    we find that the inter-sentence context is crucial for the alleviation of discourse-related
    issues. The BlonDe score drops by 0.73 when the inter-sentence context is unavailable.
    Meanwhile, our DeMPT also significantly improves the performance of LLMs in context-agnostic
    MT, e.g., + 0.99 BLEU score and + 0.0073 COMET score compared to the MT-PT model.'
  prefs: []
  type: TYPE_NORMAL
