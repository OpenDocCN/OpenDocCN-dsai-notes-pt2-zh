- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:40:41'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:40:41
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'EPiC: Cost-effective Search-based Prompt Engineering of LLMs for Code Generation'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: EPiC：面向代码生成的成本效益搜索型提示工程
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.11198](https://ar5iv.labs.arxiv.org/html/2408.11198)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2408.11198](https://ar5iv.labs.arxiv.org/html/2408.11198)
- en: Hamed Taherkhani, Melika Sepindband, Hung Viet Pham, Song Wang, Hadi Hemmati
    Hamed Taherkhani is with Lassonde School of Engineering, York University, Toronto,
    Ontario, Canada. Email:hamedth@yorku.ca Melika Sepidband is with Lassonde School
    of Engineering, York University, Toronto, Ontario, Canada. Email:melikasp@yorku.caHadi
    Hemmati is an associate professor at Lassonde School of Engineering, York University,
    Toronto, Ontario, Canada. Email:hemmati@yorku.caHung Viet Pham is an assistant
    professor at Lassonde School of Engineering, York University, Toronto, Ontario,
    Canada. Email:hvpham@yorku.caSong Wang is an associate professor at Lassonde School
    of Engineering, York University, Toronto, Ontario, Canada. Email:wangsong@yorku.ca
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 哈梅德·塔赫卡尼，梅利卡·塞宾班德，洪越·范，宋旺，哈迪·赫马提 哈梅德·塔赫卡尼在加拿大安大略省多伦多市约克大学拉松德工程学院工作。电子邮件：hamedth@yorku.ca
    梅利卡·塞宾班德在加拿大安大略省多伦多市约克大学拉松德工程学院工作。电子邮件：melikasp@yorku.ca 哈迪·赫马提是加拿大安大略省多伦多市约克大学拉松德工程学院的副教授。电子邮件：hemmati@yorku.ca
    洪越·范是加拿大安大略省多伦多市约克大学拉松德工程学院的助理教授。电子邮件：hvpham@yorku.ca 宋旺是加拿大安大略省多伦多市约克大学拉松德工程学院的副教授。电子邮件：wangsong@yorku.ca
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large Language Models (LLMs) have seen increasing use in various software development
    tasks, especially in code generation. The most advanced recent methods attempt
    to incorporate feedback from code execution into prompts to help guide LLMs in
    generating correct code, in an iterative process. While effective, these methods
    could be costly and time-consuming due to numerous interactions with the LLM and
    the extensive token usage. To address this issue, we propose an alternative approach
    named Evolutionary Prompt Engineering for Code (EPiC), which leverages a lightweight
    evolutionary algorithm to evolve the original prompts toward better ones that
    produce high-quality code, with minimal interactions with LLM. Our evaluation
    against state-of-the-art (SOTA) LLM-based code generation models shows that EPiC
    outperforms all the baselines in terms of cost-effectiveness.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在各种软件开发任务中的使用越来越广泛，特别是在代码生成方面。最近最先进的方法尝试将代码执行的反馈纳入提示，以帮助指导 LLMs
    生成正确的代码，采用迭代过程。尽管这些方法有效，但由于与 LLM 的多次交互和大量的标记使用，这些方法可能会很昂贵且耗时。为解决这一问题，我们提出了一种名为进化提示工程（EPiC）的替代方法，它利用轻量级的进化算法将原始提示演变为更好的提示，以生成高质量的代码，同时减少与
    LLM 的交互。我们对先进的（SOTA）LLM 基于代码生成模型的评估表明，EPiC 在成本效益方面优于所有基准。
- en: 'Index Terms:'
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Prompt Engineering, Code Generation, Large Language Models, Evolutionary Algorithm
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程、代码生成、大型语言模型、进化算法
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: LLMs have been used in many software development activities such as software
    testing, design, requirement engineering, code generation, maintenance, deployment,
    etc. [[30](#bib.bib30), [32](#bib.bib32)]. Among these activities, code generation
    using LLMs has demonstrated significant potential.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 已被应用于许多软件开发活动，如软件测试、设计、需求工程、代码生成、维护、部署等。[[30](#bib.bib30), [32](#bib.bib32)]。在这些活动中，使用
    LLMs 进行代码生成展示了显著的潜力。
- en: In LLM-based code generation, various prompt engineering techniques, including
    zero-shot [[5](#bib.bib5)], in-context learning [[33](#bib.bib33), [34](#bib.bib34)],
    RAG [[35](#bib.bib35)], and task-specific methods [[36](#bib.bib36), [37](#bib.bib37)],
    have been shown to outperform fine-tuned smaller models. The most advanced prompt
    engineering methods for code generation employ various agent-based approaches
    [[28](#bib.bib28)]. SOTA methods such as Reflexion [[20](#bib.bib20)], Language
    Agent Tree Search (LATS) [[21](#bib.bib21)], AgentCoder [[22](#bib.bib22)], LDB
    [[23](#bib.bib23)], and MetaGPT [[29](#bib.bib29)] are either planning-based or
    multi-collaborative agents. While effective, these methods can be costly and time-consuming
    due to numerous interactions with LLMs which results in extensive token usage,
    making them less attractive in practical settings. For instance, LATS requires
    on average 3 minutes to generate the implementation of a function with an average
    of only 6 lines of code, on the MBPP dataset.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于大语言模型（LLM）的代码生成中，各种提示工程技术，包括零样本[[5](#bib.bib5)]、上下文学习[[33](#bib.bib33), [34](#bib.bib34)]、RAG[[35](#bib.bib35)]和任务特定方法[[36](#bib.bib36),
    [37](#bib.bib37)]，已被证明比微调的小模型表现更好。用于代码生成的最先进提示工程方法采用了各种基于代理的方法[[28](#bib.bib28)]。最先进的方法如Reflexion[[20](#bib.bib20)]、语言代理树搜索（LATS）[[21](#bib.bib21)]、AgentCoder[[22](#bib.bib22)]、LDB[[23](#bib.bib23)]和MetaGPT[[29](#bib.bib29)]，要么基于计划，要么是多协作代理。虽然这些方法有效，但由于与LLM的交互频繁，导致大量令牌的使用，使得这些方法在实际应用中显得不那么具有吸引力。例如，LATS在MBPP数据集中生成一个平均只有6行代码的函数实现平均需要3分钟。
- en: '![Refer to caption](img/675ea440798b2b7afdbd80694988355a.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/675ea440798b2b7afdbd80694988355a.png)'
- en: 'Figure 1: The initial failed prompt (left) and the mutated successful prompt
    (right)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：初始失败的提示（左）和变异成功的提示（右）
- en: 'To tackle this limitation, in this paper, we explicitly focus on the cost of
    prompt engineering for code and propose a cost-effective approach that leverages
    a lightweight evolutionary search to optimize prompts for code generation. Our
    evolutionary-based prompt engineering tool for code generation, namely, EPiC,
    optimizes prompts for code generation by assessing the generated code against
    a fitness function, i.e., the pass rate of test cases. Our approach consists of
    two phases: Initial Evaluation (IE) and Evolutionary Prompt Engineering (EPE).
    The first phase involves primary code generation using an initial prompt and its
    evaluation using a set of test cases. If a correct solution is not generated,
    the process moves to the second (EPE) phase, where an initial population of prompts
    is generated using the initial prompt. Using each prompt in the initial population,
    we generate its corresponding code by the LLM under study. We then evaluate the
    generated code against test cases to calculate its fitness score. Candidate prompts
    are selected out of the population, using a weighted random selection approach.
    These candidates are then mutated to form the next generation of prompts. The
    mutation is carried out using two approaches: one utilizes an LLM guided by a
    prompt that specifies how to perform the mutation, and the other employs vector
    embeddings of words to find and replace similar words.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这一限制，本文明确关注代码提示工程的成本，并提出了一种利用轻量级进化搜索优化代码生成提示的成本效益方法。我们的基于进化的代码生成提示工程工具，即EPiC，通过评估生成的代码与适应度函数（即测试用例的通过率）来优化代码生成提示。我们的方法包括两个阶段：初始评估（IE）和进化提示工程（EPE）。第一个阶段涉及使用初始提示进行初步代码生成，并通过一组测试用例进行评估。如果未生成正确的解决方案，则进入第二个（EPE）阶段，其中使用初始提示生成初始提示种群。使用初始种群中的每个提示，我们由所研究的LLM生成其相应的代码。然后，我们将生成的代码与测试用例进行评估，以计算其适应度得分。候选提示通过加权随机选择的方法从种群中选择。这些候选提示然后经过变异，形成下一代提示。变异使用两种方法进行：一种是利用由提示指导的LLM来指定如何执行变异，另一种是使用词向量嵌入找到并替换相似的词。
- en: 'Figure [1](#S1.F1 "Figure 1 ‣ EPiC: Cost-effective Search-based Prompt Engineering
    of LLMs for Code Generation") illustrates an example of a mutation applied to
    a given prompt. In this example, GPT-4o, with the initial prompt on the left,
    is confused by the word “print” and generates an incorrect print statement. However,
    the evolved prompt on the right uses the word “publish” which results in the expected
    return statement, thereby passing the provided test cases.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '图[1](#S1.F1 "Figure 1 ‣ EPiC: Cost-effective Search-based Prompt Engineering
    of LLMs for Code Generation")展示了应用于给定提示的一个突变示例。在此示例中，左侧的初始提示使GPT-4o对“print”一词感到困惑，并生成了一个错误的打印语句。然而，右侧的演变提示使用了“publish”一词，结果生成了预期的返回语句，从而通过了提供的测试用例。'
- en: 'To evaluate the effectiveness of EPiC, we select two widely used code generation
    benchmark datasets, i.e., Humaneval [[8](#bib.bib8)] and MBPP [[24](#bib.bib24)].
    We compare EPiC against three SOTA LLM-based code generation tools i.e. Reflexion
    [[20](#bib.bib20)], LATS [[21](#bib.bib21)], and LDB [[23](#bib.bib23)]. We also
    implement EPiC leveraging two different LLMs: a large closed-source (GPT4-o) and
    a smaller open-source one (Magic Coder [[11](#bib.bib11)]). In addition, we have
    empirically investigated the impact of different components of our evolutionary
    algorithm on the cost and effectiveness of the results.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估EPiC的有效性，我们选择了两个广泛使用的代码生成基准数据集，即Humaneval [[8](#bib.bib8)]和MBPP [[24](#bib.bib24)]。我们将EPiC与三个SOTA
    LLM基础的代码生成工具进行比较，即Reflexion [[20](#bib.bib20)]、LATS [[21](#bib.bib21)]和LDB [[23](#bib.bib23)]。我们还实现了EPiC，利用了两种不同的LLM：一个大型闭源（GPT4-o）和一个较小的开源（Magic
    Coder [[11](#bib.bib11)]）。此外，我们还实证研究了进化算法中不同组件对成本和结果有效性的影响。
- en: Overall, our results suggest that the cost and effectiveness of code-generation
    tools do not always increase proportionally. Specifically, EPiC outperforms all
    compared SOTA tools (measured by pass@1) by 1% to 3% on HumanEval and 2% to 7%
    on MBPP with either lower or comparable cost. In addition, EPiC is more effective
    than LATS (highest performing SOTA) but with 80% less cost and is almost as low
    cost as Reflexion (the lowest cost approach among SOTA), but with 8% better results.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，我们的结果表明，代码生成工具的成本和效果并不总是成正比。具体来说，EPiC在HumanEval上比所有比较的SOTA工具（以pass@1为衡量标准）高出1%到3%，在MBPP上高出2%到7%，且成本更低或相当。此外，EPiC比LATS（性能最好的SOTA）效果更好，但成本减少了80%，几乎与Reflexion（SOTA中成本最低的方法）成本相当，但结果提高了8%。
- en: 'The main contributions of this paper are as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本论文的主要贡献如下：
- en: '1.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: To the best of our knowledge, we are the first to explore the code generation
    task from the cost perspective.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 据我们所知，我们是首个从成本角度探讨代码生成任务的研究者。
- en: '2.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We propose a novel framework EPiC that leverages a lightweight evolutionary
    algorithm to evolve the original prompts toward better ones that produce high-quality
    code.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一个新颖的框架EPiC，该框架利用轻量级进化算法将原始提示演变为更优质的提示，从而生成高质量代码。
- en: '3.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: We have demonstrated the cost-effectiveness of EPiC in code generation compared
    with baseline methods.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们展示了与基线方法相比，EPiC在代码生成中的成本效益。
- en: '4.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: We release the data and source code of our experiments to enable other researchers
    to replicate and extend our study ([https://github.com/HamedTaherkhani/EPiC](https://github.com/HamedTaherkhani/EPiC))
    .
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们发布了实验的数据和源代码，以便其他研究人员能够复制和扩展我们的研究（[https://github.com/HamedTaherkhani/EPiC](https://github.com/HamedTaherkhani/EPiC)）。
- en: 2 Background and Related Work
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景和相关工作
- en: In this section, we briefly explain the background of LLMs for code generation
    and prompt engineering, then report the most related work in the context of prompt
    engineering of LLM for code.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们简要解释了用于代码生成和提示工程的LLM背景，然后报告了在LLM代码提示工程方面的相关工作。
- en: 2.1 LLMs for Code Generation
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 用于代码生成的LLMs
- en: Models for code generation can be divided into two categories, RNN-based models
    and transformer-based models [[1](#bib.bib1)]. RNN-based models are older models
    that are outperformed by transformers. Transformer-based code generation models
    are transformers pre-trained on code-related datasets. For instance, CodeBERT [[2](#bib.bib2)]
    is a transformer model based on BERT [[3](#bib.bib3)], specifically pre-trained
    on the CodeSearchNet dataset [[4](#bib.bib4)]. Another example is CodeGPT-2, which
    builds upon GPT-2 [[5](#bib.bib5)] and is pre-trained on the same dataset. Lu
    et al. [[6](#bib.bib6)] evaluated the performance of both GPT-2 and CodeGPT-2
    in the code generation task and later, Perez et al. [[7](#bib.bib7)], fine-tuned
    GPT-2 using their dataset for code generation.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 代码生成模型可以分为两类：基于 RNN 的模型和基于变换器的模型[[1](#bib.bib1)]。基于 RNN 的模型是较早的模型，已被变换器超越。基于变换器的代码生成模型是基于代码相关数据集预训练的变换器。例如，CodeBERT
    [[2](#bib.bib2)] 是一个基于 BERT [[3](#bib.bib3)] 的变换器模型，专门在 CodeSearchNet 数据集 [[4](#bib.bib4)]
    上进行预训练。另一个例子是 CodeGPT-2，它建立在 GPT-2 [[5](#bib.bib5)] 的基础上，并在相同的数据集上进行预训练。Lu 等人
    [[6](#bib.bib6)] 评估了 GPT-2 和 CodeGPT-2 在代码生成任务中的表现，后来，Perez 等人 [[7](#bib.bib7)]
    使用他们的数据集对 GPT-2 进行了微调以进行代码生成。
- en: Newer versions of LLMs, such as GPT-4, have demonstrated improved performance
    compared to their predecessors across diverse tasks, including code generation.
    For instance, Codex [[8](#bib.bib8)] is a language model developed by OpenAI based
    on the GPT architecture, similar to GPT-3\. However, it’s specifically designed
    for code generation and understanding. Codex is trained on a diverse range of
    publicly available code repositories and strongly focuses on understanding and
    generating programming code. Another model, CodeLlama [[9](#bib.bib9)], is a family
    of large language models for code based on Llama 2 [[10](#bib.bib10)] that is
    available in 7B, 13B, and 34B parameters. DeepSeek Coder is another state-of-the-art
    LLM that has been used in software tasks such as Code Generation. This LLM has
    various sizes, ranging from 1B to 33B. Magicoder [[11](#bib.bib11)] is another
    model that has been fine-tuned on both CodeLlama-7b-Python and deepseek-coder-6.7b.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 更新版的 LLM，如 GPT-4，相比于其前身在各种任务中表现得更为出色，包括代码生成。例如，Codex [[8](#bib.bib8)] 是由 OpenAI
    开发的基于 GPT 架构的语言模型，类似于 GPT-3。然而，它专门用于代码生成和理解。Codex 在各种公开的代码库上进行训练，并且特别关注理解和生成编程代码。另一个模型，CodeLlama
    [[9](#bib.bib9)]，是一系列基于 Llama 2 [[10](#bib.bib10)] 的大型语言模型，提供 7B、13B 和 34B 参数。DeepSeek
    Coder 是另一种先进的 LLM，已用于软件任务如代码生成。这个 LLM 有多种规模，从 1B 到 33B。Magicoder [[11](#bib.bib11)]
    是另一个在 CodeLlama-7b-Python 和 deepseek-coder-6.7b 上进行微调的模型。
- en: 'In this research, we employed two different types of LLMs: GPT-4o, as an example
    of closed-source large models, and MagicCoder as an example of open-source smaller
    models. We selected GPT-4o due to its high performance among the closed-source
    LLMs. MagicCoder was chosen as the most cost-effective small-size open-source
    model, at the time of designing our experiments.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项研究中，我们使用了两种不同类型的 LLM：GPT-4o 作为封闭源大型模型的例子，以及 MagicCoder 作为开源小型模型的例子。我们选择了
    GPT-4o，因为它在封闭源 LLM 中表现优秀。MagicCoder 被选为当时设计实验时最具成本效益的小型开源模型。
- en: '![Refer to caption](img/9b7aecf22b157056b7964926d04821c2.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9b7aecf22b157056b7964926d04821c2.png)'
- en: 'Figure 2: Diagram of EPiC. On the left, the initial evaluation for the initial
    prompt assessment is depicted and on the right, the evolutionary process of EPiC
    is illustrated.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：EPiC 的示意图。左侧展示了初始提示评估的初步评估，右侧则展示了 EPiC 的演变过程。
- en: 2.2 Prompt Engineering for LLMs
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 针对 LLMs 的提示工程
- en: A prompt is an input or query provided to a model to generate a response or
    perform a task. Prompt engineering refers to the process of designing and refining
    prompts to achieve desired outcomes when using LLMs ¹¹1https://platform.openai.com/docs/guides/prompt-engineering.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 提示是提供给模型的输入或查询，用于生成响应或执行任务。提示工程指的是设计和优化提示的过程，以在使用 LLMs 时实现预期的结果¹¹1https://platform.openai.com/docs/guides/prompt-engineering。
- en: There are multiple categories of prompt engineering, including approaches without
    training, reasoning and logic, reducing hallucination, and evolutionary-based
    methods [[38](#bib.bib38)]. Zero-shot [[5](#bib.bib5)] and few-shot [[33](#bib.bib33)]
    prompting fall under the category of approaches without training. Techniques such
    as chain-of-thought (CoT) prompting [[12](#bib.bib12)], Automatic Chain-of-Thought
    (Auto-CoT) [[39](#bib.bib39)], Self-Consistency [[40](#bib.bib40)], and knowledge
    prompting [[13](#bib.bib13)] exemplify reasoning and logic-based methods. To reduce
    hallucination for prompt engineering, techniques such as Retrieval Augmented Generation
    (RAG) [[35](#bib.bib35)], ReAct Prompting [[41](#bib.bib41)], and Chain-of-Knowledge
    (CoK) Prompting [[42](#bib.bib42)] are employed. Evolutionary algorithms are utilized
    to optimize prompts, as demonstrated by EvoPrompt [[16](#bib.bib16)] and PromptBreeder
    [[25](#bib.bib25)]. There are other solutions such as Automated Prompt Engineering [[14](#bib.bib14)]
    and Prompt Optimization with Textual Gradients (ProTeGi) [[15](#bib.bib15)].
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程有多个类别，包括无训练的方法、推理和逻辑、减少幻觉和基于进化的方法[[38](#bib.bib38)]。零-shot [[5](#bib.bib5)]
    和少-shot [[33](#bib.bib33)] 提示属于无训练的方法。链式推理 (CoT) 提示[[12](#bib.bib12)]、自动链式推理 (Auto-CoT)
    [[39](#bib.bib39)]、自一致性 [[40](#bib.bib40)] 和知识提示 [[13](#bib.bib13)] 体现了基于推理和逻辑的方法。为了减少提示工程中的幻觉，采用了诸如检索增强生成
    (RAG) [[35](#bib.bib35)]、ReAct 提示 [[41](#bib.bib41)] 和知识链 (CoK) 提示 [[42](#bib.bib42)]
    等技术。进化算法用于优化提示，如 EvoPrompt [[16](#bib.bib16)] 和 PromptBreeder [[25](#bib.bib25)]
    所示。还有其他解决方案，如自动化提示工程 [[14](#bib.bib14)] 和文本梯度优化提示 (ProTeGi) [[15](#bib.bib15)]。
- en: Zero-shot prompting, as described in [[5](#bib.bib5)], eliminates the necessity
    for extensive training data by employing accurately designed prompts to direct
    the model toward executing new tasks. The model receives a task description without
    labeled training data and employs its pre-existing knowledge to generate predictions
    based on the prompt. Few-shot prompting [[33](#bib.bib33)] uses a limited number
    of input-output examples to help models understand tasks, unlike zero-shot prompting
    which provides no examples. However, this method requires additional tokens, which
    can be impractical for longer texts. The selection and composition of examples
    can also significantly influence model behavior and introduce biases.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 零-shot 提示，如[[5](#bib.bib5)]中所述，通过采用准确设计的提示来引导模型执行新任务，从而消除了对大量训练数据的需求。模型在没有标记训练数据的情况下接收任务描述，并利用其已有知识基于提示生成预测。少-shot
    提示[[33](#bib.bib33)]使用有限数量的输入-输出示例来帮助模型理解任务，这与零-shot 提示不同，后者不提供示例。然而，这种方法需要额外的
    tokens，对于较长的文本可能不太实用。示例的选择和组合也可能显著影响模型行为并引入偏差。
- en: CoT [[12](#bib.bib12)] improves the reasoning abilities of large language models
    by incorporating intermediate reasoning steps within prompts. This technique breaks
    down complex tasks into smaller sub-tasks, mimicking human problem-solving. It
    significantly enhances performance in arithmetic, commonsense, and symbolic reasoning,
    especially with larger models. Auto-CoT  [[39](#bib.bib39)] is an approach designed
    to enhance the reasoning capabilities of LLMs by automating the generation of
    intermediate reasoning steps. It automates the creation of reasoning chains and
    demonstrations using diversity-based sampling to generate multiple reasoning paths.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: CoT [[12](#bib.bib12)] 通过在提示中加入中间推理步骤来提高大型语言模型的推理能力。这种技术将复杂任务分解为更小的子任务，模拟人类问题解决方式。它显著提高了在算术、常识和符号推理方面的表现，特别是对于较大的模型。Auto-CoT
    [[39](#bib.bib39)] 是一种旨在通过自动生成中间推理步骤来增强 LLM 推理能力的方法。它利用基于多样性的采样来自动生成推理链和演示，生成多个推理路径。
- en: Automated prompt engineering approaches use an agent or a similar automated
    engine to interact with LLM and typically get some feedback to improve the prompt.
    Zhou et al. [[14](#bib.bib14)] proposed APE, a framework designed for the automatic
    generation and selection of instructions. In this framework, they used an LLM
    to generate instruction candidates and then search for candidate solutions to
    maximize a selected score function, treating prompt engineering as an optimization
    problem. Pryzan et al. [[15](#bib.bib15)] proposed Prompt Optimization with Textual
    Gradients (ProTeGi), which improves prompts for LLMs using a non-parametric approach
    inspired by numerical gradient descent. It uses natural language gradients from
    training data to critique and edit prompts, employing a process guided by beam
    search and bandit selection. This approach improved the efficiency of previous
    prompt editing techniques across various NLP tasks, including the novel challenge
    of LLM jailbreak detection.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化提示工程方法使用代理或类似的自动化引擎与LLM互动，通常会得到一些反馈以改进提示。Zhou等人[[14](#bib.bib14)]提出了APE，这是一个旨在自动生成和选择指令的框架。在这个框架中，他们使用LLM生成指令候选，然后搜索候选解决方案以最大化所选的评分函数，将提示工程视为优化问题。Pryzan等人[[15](#bib.bib15)]提出了基于文本梯度的提示优化（ProTeGi），它采用一种非参数方法来改进LLM的提示，灵感来自数值梯度下降。它利用训练数据中的自然语言梯度来批评和编辑提示，使用由束搜索和强盗选择引导的过程。这种方法提高了之前提示编辑技术在各种NLP任务中的效率，包括LLM越狱检测这一新挑战。
- en: Evolutionary algorithms have been used in the past for prompt engineering of
    LLMs in generic NLP tasks. For example, EvoPrompt [[16](#bib.bib16)], a framework
    designed to automate the prompt engineering process for LLMs. EvoPrompt begins
    with an initial population of prompts and iteratively generates new ones using
    evolutionary operators such as mutation and crossover, which are performed by
    LLMs. Another similar approach is PromptBreeder [[25](#bib.bib25)], which employs
    task prompts to condition the LLM and mutation prompts to modify task prompts.
    The interplay between task and mutation prompts drives iterative improvement in
    PromptBreeder. Similar approaches for prompt enhancement using LLMs through EAs
    are also proposed in [[26](#bib.bib26)] and [[27](#bib.bib27)].
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 进化算法过去曾用于LLM在通用NLP任务中的提示工程。例如，EvoPrompt[[16](#bib.bib16)]是一个旨在自动化LLM提示工程过程的框架。EvoPrompt以初始提示种群开始，利用进化操作符如变异和交叉，迭代生成新的提示，这些操作由LLM执行。另一个类似的方法是PromptBreeder[[25](#bib.bib25)]，它使用任务提示来调节LLM，并使用变异提示来修改任务提示。任务提示和变异提示之间的相互作用驱动了PromptBreeder的迭代改进。类似的LLM提示增强方法通过EAs也在[[26](#bib.bib26)]和[[27](#bib.bib27)]中提出。
- en: In contrast to existing evolutionary prompt engineering techniques, EPiC is
    particularly tailored for prompt engineering in coding tasks, with a fitness function
    defined based on the pass rate of test cases. In addition, it focuses on cost-efficiency
    throughout the process. To achieve this, it minimizes the calls to external LLMs
    and implements the mutation operator using local lightweight word embeddings libraries.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 与现有的进化提示工程技术相比，EPiC特别针对编码任务中的提示工程，具有基于测试用例通过率定义的适应度函数。此外，它在整个过程中注重成本效率。为此，它减少了对外部LLM的调用，并使用本地轻量级词嵌入库实现变异操作符。
- en: 2.3 Prompt Engineering of LLMs for Code Generation
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 LLM代码生成的提示工程
- en: Zelikman et al. [[17](#bib.bib17)] introduced Parsel, a framework enabling automatic
    implementation and validation of complex algorithms with code LLMs. Using Parsel,
    they break down algorithmic tasks into structured descriptions written in natural
    language, then explore various combinations of function implementations using
    tests.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Zelikman等人[[17](#bib.bib17)]介绍了Parsel，这是一个使复杂算法的自动实现和验证与代码LLM相结合的框架。使用Parsel，他们将算法任务分解为用自然语言编写的结构化描述，然后通过测试探索各种函数实现的组合。
- en: 'DyLAN [[19](#bib.bib19)] is a framework designed to boost the performance of
    Large Language Model (LLM) agents by assembling them into a dynamic team that
    adapts to different tasks. Unlike traditional methods with fixed agent sets, DyLAN’s
    architecture adjusts dynamically to the task query, enhancing its versatility.
    It employs features like inference-time agent selection and early stopping to
    improve efficiency and effectiveness. Furthermore, DyLAN incorporates an automatic
    agent team optimization algorithm based on an unsupervised metric called Agent
    Importance Score, which selects the most effective agents for each task. Empirical
    results show DyLAN’s success in tasks like reasoning and code generation, achieving
    significant improvements over single LLM executions. Reflexion [[20](#bib.bib20)]
    is a reinforcement-based framework for this problem where language agents learn
    from linguistic feedback rather than weight updates. Agents reflect on task feedback
    verbally and maintain their own reflective text in memory, which helps them make
    better decisions in subsequent trials. Reflexion is adaptable to different types
    and sources of feedback signals and shows significant improvements over baseline
    agents across various tasks. LATS (Language Agent Tree Search) [[21](#bib.bib21)]
    is another search-based framework that combines LLMs’ abilities in planning, acting,
    and reasoning. LATS draws inspiration from the Monte Carlo tree search and repurposes
    LLMs’ strengths as agents, value functions, and optimizers. Crucially, LATS incorporates
    an environment for external feedback to enable more deliberate and adaptive problem-solving
    beyond existing techniques’ limitations. AgentCoder [[22](#bib.bib22)] utilizes
    a multi-agent framework with specialized agents: the programmer agent, the test
    designer agent, and the test executor agent. The programmer agent focuses on code
    generation and refinement based on feedback from the test executor agent, while
    the test designer agent generates test cases for the code. The test executor agent
    runs the code with the test cases and provides feedback to the programmer. This
    collaborative system aims to improve code generation efficiency, outperforming
    single-agent models and previous strategies. Zhong et al. [[23](#bib.bib23)] introduced
    Large Language Model Debugger (LDB), a framework that segments programs into basic
    blocks and tracks intermediate variable values during runtime. LDB enables LLMs
    to focus on simpler code units, verify their correctness block by block, and pinpoint
    errors effectively.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: DyLAN [[19](#bib.bib19)] 是一个框架，旨在通过将大型语言模型（LLM）代理组装成一个动态团队来提升其性能，这个团队能够适应不同的任务。与传统的固定代理集方法不同，DyLAN
    的架构能够动态调整以适应任务查询，从而增强其多功能性。它采用了推理时代理选择和提前停止等特性，以提高效率和效果。此外，DyLAN 还结合了基于名为代理重要性分数的无监督指标的自动代理团队优化算法，该算法选择每个任务中最有效的代理。实证结果显示，DyLAN
    在推理和代码生成等任务中取得了成功，相较于单一 LLM 执行，显著提高了性能。Reflexion [[20](#bib.bib20)] 是一个基于强化学习的框架，其中语言代理通过语言反馈而非权重更新来进行学习。代理会口头反思任务反馈，并在记忆中保留自己的反思文本，这有助于他们在后续试验中做出更好的决策。Reflexion
    可以适应不同类型和来源的反馈信号，并在各种任务中表现出显著的改进，相较于基线代理。LATS（语言代理树搜索） [[21](#bib.bib21)] 是另一个基于搜索的框架，结合了
    LLM 在规划、执行和推理方面的能力。LATS 从蒙特卡罗树搜索中获得灵感，并将 LLM 的优势重新应用于代理、价值函数和优化器。关键是，LATS 引入了外部反馈环境，使其能够在现有技术的限制之外进行更深思熟虑和适应性的解决问题。AgentCoder
    [[22](#bib.bib22)] 利用一个多代理框架，包括专门的代理：程序员代理、测试设计师代理和测试执行者代理。程序员代理专注于基于测试执行者代理的反馈进行代码生成和改进，而测试设计师代理则为代码生成测试用例。测试执行者代理运行代码并提供反馈给程序员。这个协作系统旨在提高代码生成效率，超越单一代理模型和以往策略。Zhong
    等人 [[23](#bib.bib23)] 介绍了大型语言模型调试器（LDB），这是一个将程序分段为基本块并在运行时跟踪中间变量值的框架。LDB 使得 LLM
    能够专注于较简单的代码单元，逐块验证其正确性，并有效地定位错误。
- en: 'In contrast to existing methods and to the best of our knowledge, EPiC is the
    first search-based prompt engineering method for code generation. It employs a
    lightweight process to identify the optimal solution in a cost-efficient manner.
    To achieve this, EPiC utilizes a local embedding function to implement mutation
    operators on text, to reduce the cost of iterative prompt engineering for code
    generation. It also guides the search over iterations using the fitness function
    in Section [4.4](#S4.SS4 "4.4 Objective Function ‣ 4 Experiment Design ‣ EPiC:
    Cost-effective Search-based Prompt Engineering of LLMs for Code Generation"),
    which helpsfindg the most effective prompts.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '与现有方法相比，据我们所知，EPiC 是首个基于搜索的代码生成提示工程方法。它采用轻量级过程以经济高效的方式识别最优解决方案。为此，EPiC 利用局部嵌入函数对文本进行突变操作，以减少迭代提示工程的成本。它还使用第[4.4](#S4.SS4
    "4.4 Objective Function ‣ 4 Experiment Design ‣ EPiC: Cost-effective Search-based
    Prompt Engineering of LLMs for Code Generation")节中的适应度函数指导迭代过程，帮助找到最有效的提示。'
- en: 3 Evolutionary Prompt Engineering for Code
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 代码的进化提示工程
- en: Algorithm 1 Evolutionary algorithm
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 算法1 进化算法
- en: 1:procedure EvoALG($prompt,tests,popSize$ then15:               $return$30:end procedure
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 1:procedure EvoALG($prompt, tests, popSize$ then15:               $return$30:end
    procedure
- en: Algorithm 2 Mutation process in $sim\_words\_as\_mutator$
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 算法2 在 $sim\_words\_as\_mutator$ 中的突变过程
- en: 1:procedure mutate($prompt,sim\_t,num\_t,mutation\_probability$ do15:        if $$random.random()></math>22:end procedure![Refer
    to caption](img/38b2b85f90c130e8ccb03f9a32c97dc2.png)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 1:procedure mutate($prompt, sim\_t, num\_t, mutation\_probability$ do15:        if
    $$random.random()></math>22:end procedure![参考说明](img/38b2b85f90c130e8ccb03f9a32c97dc2.png)
- en: (a) Original prompt and its result
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 原始提示及其结果
- en: '![Refer to caption](img/b3cceccd291ebb305bf9ab2fcc1a2a83.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b3cceccd291ebb305bf9ab2fcc1a2a83.png)'
- en: (b) Transformed prompt and its result
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 转换后的提示及其结果
- en: 'Figure 3: An unsuccessful original prompt on the left and the transformed successful
    version of it on the right. The main issue with the original implementation is
    the incorrect initialization of the Eulerian number. The orange text in both figures
    highlights the code and text related to the initialization of the function.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：左侧为失败的原始提示，右侧为转换后的成功版本。原始实现的主要问题在于欧拉数的初始化不正确。两个图中的橙色文本突出了与函数初始化相关的代码和文本。
- en: 'This section provides a detailed elaboration of EPiC. As depicted in Figure
    [2](#S2.F2 "Figure 2 ‣ 2.1 LLMs for Code Generation ‣ 2 Background and Related
    Work ‣ EPiC: Cost-effective Search-based Prompt Engineering of LLMs for Code Generation"),
    our approach comprises two phases, i.e., the Initial Evaluation (IE) phase and
    the Evolutionary Prompt Engineering (EPE) phase. The process begins with a user
    providing an initial prompt that describes the intended functionality of the code
    to be generated. In the IE phase, we evaluate the prompt by generating the code
    based on the original prompt using an LLM. This evaluation determines whether
    the prompt is sufficient to generate the correct implementation or if it requires
    a further process in the EPE phase. EPiC’s algorithm is presented in Algorithm
    [1](#alg1 "Algorithm 1 ‣ 3 Evolutionary Prompt Engineering for Code ‣ EPiC: Cost-effective
    Search-based Prompt Engineering of LLMs for Code Generation"), which will be elaborated
    upon in the subsequent sections. Lines 2 to 6 in Algorithm [1](#alg1 "Algorithm
    1 ‣ 3 Evolutionary Prompt Engineering for Code ‣ EPiC: Cost-effective Search-based
    Prompt Engineering of LLMs for Code Generation") correspond to the IE phase of
    EPiC and lines 7 to 30 are the implementaion of the EPE phase.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '本节详细阐述了EPiC。如图[2](#S2.F2 "Figure 2 ‣ 2.1 LLMs for Code Generation ‣ 2 Background
    and Related Work ‣ EPiC: Cost-effective Search-based Prompt Engineering of LLMs
    for Code Generation")所示，我们的方法包括两个阶段，即初始评估（IE）阶段和进化提示工程（EPE）阶段。过程从用户提供一个描述待生成代码功能的初始提示开始。在IE阶段，我们通过使用LLM生成基于原始提示的代码来评估提示。这一评估确定提示是否足够生成正确的实现，或者是否需要在EPE阶段进行进一步处理。EPiC的算法在算法[1](#alg1
    "Algorithm 1 ‣ 3 Evolutionary Prompt Engineering for Code ‣ EPiC: Cost-effective
    Search-based Prompt Engineering of LLMs for Code Generation")中呈现，后续章节将详细说明。算法[1](#alg1
    "Algorithm 1 ‣ 3 Evolutionary Prompt Engineering for Code ‣ EPiC: Cost-effective
    Search-based Prompt Engineering of LLMs for Code Generation")的第2到第6行对应EPiC的IE阶段，第7到第30行是EPE阶段的实现。'
- en: 3.1 Initial Evaluation
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 初始评估
- en: 'In Step <svg id="S3.SS1.p1.1.pic1" class="ltx_picture" height="19.62" overflow="visible"
    version="1.1" width="19.62"><g transform="translate(0,19.62) matrix(1 0 0 -1 0
    0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g></svg>
    of the IE phase, the initial prompts for code and test case generation are provided
    to the LLM (the choice of LLM to be used in our case will be discussed in the
    Section [4](#S4 "4 Experiment Design ‣ EPiC: Cost-effective Search-based Prompt
    Engineering of LLMs for Code Generation")) to generate the corresponding code
    and test cases. Subsequently, in Step <svg id="S3.SS1.p1.2.pic2" class="ltx_picture"
    height="19.62" overflow="visible" version="1.1" width="19.62"><g transform="translate(0,19.62)
    matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000"
    stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">2</foreignobject></g></g></svg>, the generated
    code and test cases are sent to the Testcase Evaluation module. evaluatePrompt
    in Line 4 of Algorithm [1](#alg1 "Algorithm 1 ‣ 3 Evolutionary Prompt Engineering
    for Code ‣ EPiC: Cost-effective Search-based Prompt Engineering of LLMs for Code
    Generation") is responsible for generating the code and evaluating the test cases.
    If any test case fails on the code we continue with the EPE phase. If all the
    test cases pass, we stop and report the generated code as the final answer.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '在IE阶段的第<svg id="S3.SS1.p1.1.pic1" class="ltx_picture" height="19.62" overflow="visible"
    version="1.1" width="19.62"><g transform="translate(0,19.62) matrix(1 0 0 -1 0
    0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g></svg>步中，向LLM提供代码和测试用例生成的初始提示（我们在本案例中选择的LLM将在第[4](#S4
    "4 Experiment Design ‣ EPiC: Cost-effective Search-based Prompt Engineering of
    LLMs for Code Generation")节中讨论）以生成相应的代码和测试用例。随后，在第<svg id="S3.SS1.p1.2.pic2" class="ltx_picture"
    height="19.62" overflow="visible" version="1.1" width="19.62"><g transform="translate(0,19.62)
    matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000"
    stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">2</foreignobject></g></g></svg>步中，生成的代码和测试用例被送到测试用例评估模块。算法[1](#alg1
    "Algorithm 1 ‣ 3 Evolutionary Prompt Engineering for Code ‣ EPiC: Cost-effective
    Search-based Prompt Engineering of LLMs for Code Generation")的第4行中的evaluatePrompt负责生成代码和评估测试用例。如果有任何测试用例在代码上失败，我们将继续进行EPE阶段。如果所有测试用例都通过，我们将停止并报告生成的代码作为最终答案。'
- en: Note that test cases can be provided in various ways. One approach is to use
    developer-provided test cases for evaluation, while another is to generate test
    cases using LLMs. The challenge with developer-provided test cases is that typically
    users do not have test cases before implementation, making the approach impractical
    for a code generation task. On the other hand, the advantage of using LLM-generated
    test cases is that it makes the process fully automated. This method is currently
    used in most SOTA code generation tools [[20](#bib.bib20)], [[21](#bib.bib21)],
    [[22](#bib.bib22)], [[18](#bib.bib18)], and [[29](#bib.bib29)], as part of their
    internal evaluation. Consequently, we also opted to use LLMs for test case generation
    to ensure a fully automated approach, assuming no developer-provided test cases.
    Specifically, we employed OpenAI’s GPT-4o model to generate test cases with few-shot
    examples. To ensure the functional correctness of these test cases, we validated
    them by parsing their Abstract Syntax Trees (AST). A test case is considered valid
    if it successfully parses into a syntactically correct AST. This approach is the
    same approach used in Reflexion [[20](#bib.bib20)].
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，测试用例可以通过多种方式提供。一种方法是使用开发人员提供的测试用例进行评估，而另一种方法是使用LLM生成测试用例。开发人员提供的测试用例的挑战在于用户通常在实现之前没有测试用例，这使得这种方法在代码生成任务中不够实际。另一方面，使用LLM生成测试用例的优势在于它使过程完全自动化。这种方法目前在大多数SOTA代码生成工具[[20](#bib.bib20)]、[[21](#bib.bib21)]、[[22](#bib.bib22)]、[[18](#bib.bib18)]和[[29](#bib.bib29)]中作为其内部评估的一部分得到应用。因此，我们也选择使用LLM进行测试用例生成，以确保完全自动化的方法，假设没有开发人员提供的测试用例。具体而言，我们采用了OpenAI的GPT-4o模型，通过少量示例生成测试用例。为了确保这些测试用例的功能正确性，我们通过解析它们的抽象语法树（AST）来验证它们。如果测试用例能够成功解析为语法正确的AST，则视为有效。这种方法与Reflexion[[20](#bib.bib20)]中使用的方法相同。
- en: The prompts for code and test case generation which are used in our experiments
    are provided below. These prompts can be modified if necessary depending on the
    dataset.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实验中使用的代码和测试用例生成提示如下。这些提示可以根据数据集的需要进行修改。
- en: '<svg id="S3.SS1.p4.pic1" class="ltx_picture" height="61.04" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,61.04) matrix(1 0 0 -1 0 0)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 8.99 8.99)"><foreignobject width="582.01" height="43.05" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Code Generation Prompt You
    are a Python developer that implements the correct code based on the function
    description provided. You are given one or more functions to implement. Don’t
    delete import statements in the code snippet. Use at most 1000 words.</foreignobject></g></g></svg><svg
    id="S3.SS1.p5.pic1" class="ltx_picture" height="228.31" overflow="visible" version="1.1"
    width="600"><g transform="translate(0,228.31) matrix(1 0 0 -1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 8.99 8.99)"><foreignobject width="582.01" height="210.32" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Test Case Generation Prompt
    You are an AI coding assistant who can write unique, diverse, and intuitive unit
    tests for functions given the signature and docstring. Examples:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: <svg id="S3.SS1.p4.pic1" class="ltx_picture" height="61.04" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,61.04) matrix(1 0 0 -1 0 0)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 8.99 8.99)"><foreignobject width="582.01" height="43.05" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">代码生成提示 你是一个 Python 开发者，根据提供的函数描述实现正确的代码。你将获得一个或多个需要实现的函数。不要删除代码片段中的
    import 语句。最多使用 1000 字。</foreignobject></g></g></svg><svg id="S3.SS1.p5.pic1" class="ltx_picture"
    height="228.31" overflow="visible" version="1.1" width="600"><g transform="translate(0,228.31)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 8.99 8.99)"><foreignobject
    width="582.01" height="210.32" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">测试用例生成提示 你是一个 AI 编程助手，可以为函数编写独特、多样化且直观的单元测试，基于函数的签名和文档字符串。示例：
- en: 'def add3Numbers(x, y, z):'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 'def add3Numbers(x, y, z):'
- en: ””” Add three numbers together. This function takes three numbers as input and
    returns the sum of the three numbers.”””
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ””” 将三个数字相加。此函数接受三个数字作为输入，返回这三个数字的总和。”””
- en: 'unit tests:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 单元测试：
- en: assert add3Numbers(1, 2, 3) == 6
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(1, 2, 3) == 6
- en: assert add3Numbers(-1, 2, 3) == 4
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(-1, 2, 3) == 4
- en: assert add3Numbers(1, -2, 3) == 2
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(1, -2, 3) == 2
- en: assert add3Numbers(1, 2, -3) == 0
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(1, 2, -3) == 0
- en: assert add3Numbers(-3, -2, -1) == -6
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(-3, -2, -1) == -6
- en: assert add3Numbers(0, 0, 0) == 0</foreignobject></g></g></svg>
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(0, 0, 0) == 0</foreignobject></g></g></svg>
- en: 3.2 Evolutionary Prompt Engineering
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 进化型提示工程
- en: In the EPE phase, the first step (Step <svg id="S3.SS2.p1.1.pic1" class="ltx_picture"
    height="19.62" overflow="visible" version="1.1" width="19.62"><g transform="translate(0,19.62)
    matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><foreignobject width="181.55" height="24.17" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Test Case Generation Prompt
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在 EPE 阶段，第一步（步骤 <svg id="S3.SS2.p1.1.pic1" class="ltx_picture" height="19.62"
    overflow="visible" version="1.1" width="19.62"><g transform="translate(0,19.62)
    matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><foreignobject width="181.55" height="24.17" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">测试用例生成提示
- en: 'You are an AI coding assistant who can write unique, diverse, and intuitive
    unit tests for functions given the signature and docstring. Examples:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一个 AI 编程助手，可以为函数编写独特、多样化且直观的单元测试，基于函数的签名和文档字符串。示例：
- en: 'def add3Numbers(x, y, z):'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 'def add3Numbers(x, y, z):'
- en: ””” Add three numbers together. This function takes three numbers as input and
    returns the sum of the three numbers.”””
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ””” 将三个数字相加。此函数接受三个数字作为输入，返回这三个数字的总和。”””
- en: 'unit tests:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 单元测试：
- en: assert add3Numbers(1, 2, 3) == 6
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(1, 2, 3) == 6
- en: assert add3Numbers(-1, 2, 3) == 4
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(-1, 2, 3) == 4
- en: assert add3Numbers(1, -2, 3) == 2
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(1, -2, 3) == 2
- en: assert add3Numbers(1, 2, -3) == 0
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(1, 2, -3) == 0
- en: assert add3Numbers(-3, -2, -1) == -6
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(-3, -2, -1) == -6
- en: assert add3Numbers(0, 0, 0) == 0</foreignobject> <g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g></svg>)
    involves populating the first generation of individuals, where each individual
    is a distinct prompt to generate the same code. In Step <svg id="S3.SS2.p1.2.pic2"
    class="ltx_picture" height="19.62" overflow="visible" version="1.1" width="19.62"><g
    transform="translate(0,19.62) matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><foreignobject width="181.55"
    height="24.17" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Test Case
    Generation Prompt
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(0, 0, 0) == 0</foreignobject> <g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g></svg>)
    涉及填充第一代个体，每个个体都是生成相同代码的不同提示。在第 <svg id="S3.SS2.p1.2.pic2" class="ltx_picture"
    height="19.62" overflow="visible" version="1.1" width="19.62"><g transform="translate(0,19.62)
    matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><foreignobject width="181.55" height="24.17" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">测试用例生成提示
- en: 'You are an AI coding assistant who can write unique, diverse, and intuitive
    unit tests for functions given the signature and docstring. Examples:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一个AI编码助手，可以根据函数的签名和文档字符串编写独特、多样且直观的单元测试。示例：
- en: 'def add3Numbers(x, y, z):'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 'def add3Numbers(x, y, z):'
- en: ””” Add three numbers together. This function takes three numbers as input and
    returns the sum of the three numbers.”””
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ””” 将三个数字相加。此函数接受三个数字作为输入，并返回这三个数字的和。”””
- en: 'unit tests:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 单元测试：
- en: assert add3Numbers(1, 2, 3) == 6
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(1, 2, 3) == 6
- en: assert add3Numbers(-1, 2, 3) == 4
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(-1, 2, 3) == 4
- en: assert add3Numbers(1, -2, 3) == 2
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(1, -2, 3) == 2
- en: assert add3Numbers(1, 2, -3) == 0
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(1, 2, -3) == 0
- en: assert add3Numbers(-3, -2, -1) == -6
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(-3, -2, -1) == -6
- en: 'assert add3Numbers(0, 0, 0) == 0</foreignobject> <g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g></svg>,
    we generate multiple prompts by modifying the initial prompt using an LLM agent.
    Generating this prompt population forms an important part of our evolutionary
    algorithm. A more detailed description of this process will be provided in Section
    [3.2.1](#S3.SS2.SSS1 "3.2.1 Initial Population Builder ‣ 3.2 Evolutionary Prompt
    Engineering ‣ 3 Evolutionary Prompt Engineering for Code ‣ EPiC: Cost-effective
    Search-based Prompt Engineering of LLMs for Code Generation"). This step corresponds
    to line 8 of Algorithm [1](#alg1 "Algorithm 1 ‣ 3 Evolutionary Prompt Engineering
    for Code ‣ EPiC: Cost-effective Search-based Prompt Engineering of LLMs for Code
    Generation").'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 'assert add3Numbers(0, 0, 0) == 0</foreignobject> <g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g></svg>,
    我们通过使用LLM代理修改初始提示生成多个提示。生成这些提示集是我们进化算法的重要部分。有关此过程的更详细描述将在第[3.2.1](#S3.SS2.SSS1
    "3.2.1 初始种群生成器 ‣ 3.2 进化提示工程 ‣ 3 代码的进化提示工程 ‣ EPiC: 基于成本的LLM代码生成提示工程")节提供。此步骤对应于算法[1](#alg1
    "算法 1 ‣ 3 代码的进化提示工程 ‣ EPiC: 基于成本的LLM代码生成提示工程")的第8行。'
- en: In Step <svg id="S3.SS2.p2.1.pic1" class="ltx_picture" height="19.62" overflow="visible"
    version="1.1" width="19.62"><g transform="translate(0,19.62) matrix(1 0 0 -1 0
    0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><foreignobject
    width="181.55" height="24.17" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Test
    Case Generation Prompt
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 <svg id="S3.SS2.p2.1.pic1" class="ltx_picture" height="19.62" overflow="visible"
    version="1.1" width="19.62"><g transform="translate(0,19.62) matrix(1 0 0 -1 0
    0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><foreignobject
    width="181.55" height="24.17" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">测试用例生成提示
- en: 'You are an AI coding assistant who can write unique, diverse, and intuitive
    unit tests for functions given the signature and docstring. Examples:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一个AI编码助手，可以根据函数的签名和文档字符串编写独特、多样且直观的单元测试。示例：
- en: 'def add3Numbers(x, y, z):'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 'def add3Numbers(x, y, z):'
- en: ””” Add three numbers together. This function takes three numbers as input and
    returns the sum of the three numbers.”””
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ””” 将三个数字相加。此函数接受三个数字作为输入，并返回这三个数字的和。”””
- en: 'unit tests:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 单元测试：
- en: assert add3Numbers(1, 2, 3) == 6
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(1, 2, 3) == 6
- en: assert add3Numbers(-1, 2, 3) == 4
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(-1, 2, 3) == 4
- en: assert add3Numbers(1, -2, 3) == 2
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(1, -2, 3) == 2
- en: assert add3Numbers(1, 2, -3) == 0
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(1, 2, -3) == 0
- en: assert add3Numbers(-3, -2, -1) == -6
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(-3, -2, -1) == -6
- en: 'assert add3Numbers(0, 0, 0) == 0</foreignobject> <g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">4</foreignobject></g></g></svg>
    of EPE which corresponds to lines 12 to 20 in Algorithm [1](#alg1 "Algorithm 1
    ‣ 3 Evolutionary Prompt Engineering for Code ‣ EPiC: Cost-effective Search-based
    Prompt Engineering of LLMs for Code Generation"), we generate code for each prompt
    and evaluate each generated code sample using the same test cases. We define a
    fitness function based on the ratio of test cases passed. If any of the prompts
    achieves the maximum fitness score, the process stops. If not, the process continues
    until all prompts are validated and given a fitness score, which will guide the
    selection process. The fitness function is basically the passing rate of each
    prompt. The mathematical formula is explained in Section [4.4](#S4.SS4 "4.4 Objective
    Function ‣ 4 Experiment Design ‣ EPiC: Cost-effective Search-based Prompt Engineering
    of LLMs for Code Generation").'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 'assert add3Numbers(0, 0, 0) == 0</foreignobject> <g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">4</foreignobject></g></g></svg>
    的 EPE 对应于算法 [1](#alg1 "Algorithm 1 ‣ 3 Evolutionary Prompt Engineering for Code
    ‣ EPiC: Cost-effective Search-based Prompt Engineering of LLMs for Code Generation")
    中的第 12 到 20 行，我们为每个提示生成代码，并使用相同的测试用例评估每个生成的代码样本。我们定义了一个基于测试用例通过率的适应度函数。如果任何提示达到最大适应度分数，过程将停止。如果没有，过程将继续，直到所有提示都被验证并给出适应度分数，这将指导选择过程。适应度函数基本上是每个提示的通过率。数学公式在第
    [4.4](#S4.SS4 "4.4 Objective Function ‣ 4 Experiment Design ‣ EPiC: Cost-effective
    Search-based Prompt Engineering of LLMs for Code Generation") 节中解释。'
- en: This fitness function is used to select the candidate prompts for mutation in
    Step <svg id="S3.SS2.p3.1.pic1" class="ltx_picture" height="19.62" overflow="visible"
    version="1.1" width="19.62"><g transform="translate(0,19.62) matrix(1 0 0 -1 0
    0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><foreignobject
    width="181.55" height="24.17" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Test
    Case Generation Prompt
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 此适应度函数用于在第 <svg id="S3.SS2.p3.1.pic1" class="ltx_picture" height="19.62" overflow="visible"
    version="1.1" width="19.62"><g transform="translate(0,19.62) matrix(1 0 0 -1 0
    0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><foreignobject
    width="181.55" height="24.17" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">测试用例生成提示
- en: 'You are an AI coding assistant who can write unique, diverse, and intuitive
    unit tests for functions given the signature and docstring. Examples:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一个可以根据函数的签名和文档字符串编写独特、多样化且直观的单元测试的 AI 编程助手。示例：
- en: 'def add3Numbers(x, y, z):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 'def add3Numbers(x, y, z):'
- en: ””” Add three numbers together. This function takes three numbers as input and
    returns the sum of the three numbers.”””
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ””” 将三个数字相加。此函数接受三个数字作为输入，并返回这三个数字的总和。”””
- en: 'unit tests:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 单元测试：
- en: assert add3Numbers(1, 2, 3) == 6
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(1, 2, 3) == 6
- en: assert add3Numbers(-1, 2, 3) == 4
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(-1, 2, 3) == 4
- en: assert add3Numbers(1, -2, 3) == 2
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(1, -2, 3) == 2
- en: assert add3Numbers(1, 2, -3) == 0
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(1, 2, -3) == 0
- en: assert add3Numbers(-3, -2, -1) == -6
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(-3, -2, -1) == -6
- en: 'assert add3Numbers(0, 0, 0) == 0</foreignobject> <g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">5</foreignobject></g></g></svg>.
    This is implemented in chooseCandidate function in Algorithm [1](#alg1 "Algorithm
    1 ‣ 3 Evolutionary Prompt Engineering for Code ‣ EPiC: Cost-effective Search-based
    Prompt Engineering of LLMs for Code Generation") (Lines 22 and 26), where a weighted
    random selection algorithm selects candidates randomly, with the probability of
    selection being proportional to their respective fitness score (Section [4.4](#S4.SS4
    "4.4 Objective Function ‣ 4 Experiment Design ‣ EPiC: Cost-effective Search-based
    Prompt Engineering of LLMs for Code Generation")). This selection occurs with
    substitution, allowing the same prompt to be chosen multiple times. In Step <svg
    id="S3.SS2.p3.2.pic2" class="ltx_picture" height="19.62" overflow="visible" version="1.1"
    width="19.62"><g transform="translate(0,19.62) matrix(1 0 0 -1 0 0) translate(9.81,0)
    translate(0,9.81)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><foreignobject
    width="181.55" height="24.17" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Test
    Case Generation Prompt'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 'assert add3Numbers(0, 0, 0) == 0</foreignobject> <g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">5</foreignobject></g></g></svg>。这在算法
    [1](#alg1 "算法 1 ‣ 3 进化提示工程 ‣ EPiC: 成本效益的基于搜索的代码生成提示工程")（第22行和第26行）的 chooseCandidate
    函数中实现，其中加权随机选择算法随机选择候选项，选择的概率与它们各自的适应度分数成正比（第 [4.4](#S4.SS4 "4.4 目标函数 ‣ 4 实验设计
    ‣ EPiC: 成本效益的基于搜索的代码生成提示工程") 节）。这种选择是有替换的，允许相同的提示被多次选择。在步骤 <svg id="S3.SS2.p3.2.pic2"
    class="ltx_picture" height="19.62" overflow="visible" version="1.1" width="19.62"><g
    transform="translate(0,19.62) matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><foreignobject width="181.55"
    height="24.17" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">测试用例生成提示'
- en: 'You are an AI coding assistant who can write unique, diverse, and intuitive
    unit tests for functions given the signature and docstring. Examples:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一个AI编码助手，可以根据函数的签名和文档字符串编写独特、多样化和直观的单元测试。示例：
- en: 'def add3Numbers(x, y, z):'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 'def add3Numbers(x, y, z):'
- en: ””” Add three numbers together. This function takes three numbers as input and
    returns the sum of the three numbers.”””
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ””” 将三个数字相加。该函数接受三个数字作为输入，并返回这三个数字的和。”””
- en: 'unit tests:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 单元测试：
- en: assert add3Numbers(1, 2, 3) == 6
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(1, 2, 3) == 6
- en: assert add3Numbers(-1, 2, 3) == 4
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(-1, 2, 3) == 4
- en: assert add3Numbers(1, -2, 3) == 2
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(1, -2, 3) == 2
- en: assert add3Numbers(1, 2, -3) == 0
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(1, 2, -3) == 0
- en: assert add3Numbers(-3, -2, -1) == -6
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(-3, -2, -1) == -6
- en: assert add3Numbers(0, 0, 0) == 0</foreignobject> <g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">5</foreignobject></g></g></svg>,
    we select $N-1$ candidates for mutation and one prompt for “elitism”.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(0, 0, 0) == 0</foreignobject> <g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">5</foreignobject></g></g></svg>，我们选择
    $N-1$ 个候选项进行变异，并选择一个提示用于“精英主义”。
- en: Following prompt selection, in Step <svg id="S3.SS2.p4.1.pic1" class="ltx_picture"
    height="19.62" overflow="visible" version="1.1" width="19.62"><g transform="translate(0,19.62)
    matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><foreignobject width="181.55" height="24.17" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Test Case Generation Prompt
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择提示后，在步骤 <svg id="S3.SS2.p4.1.pic1" class="ltx_picture" height="19.62" overflow="visible"
    version="1.1" width="19.62"><g transform="translate(0,19.62) matrix(1 0 0 -1 0
    0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><foreignobject
    width="181.55" height="24.17" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">测试用例生成提示
- en: 'You are an AI coding assistant who can write unique, diverse, and intuitive
    unit tests for functions given the signature and docstring. Examples:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一个AI编码助手，可以根据函数的签名和文档字符串编写独特、多样化和直观的单元测试。示例：
- en: 'def add3Numbers(x, y, z):'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 'def add3Numbers(x, y, z):'
- en: ””” Add three numbers together. This function takes three numbers as input and
    returns the sum of the three numbers.”””
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ””” 将三个数字相加。该函数接受三个数字作为输入，并返回这三个数字的和。”””
- en: 'unit tests:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 单元测试：
- en: assert add3Numbers(1, 2, 3) == 6
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(1, 2, 3) == 6
- en: assert add3Numbers(-1, 2, 3) == 4
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(-1, 2, 3) == 4
- en: assert add3Numbers(1, -2, 3) == 2
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(1, -2, 3) == 2
- en: assert add3Numbers(1, 2, -3) == 0
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(1, 2, -3) == 0
- en: assert add3Numbers(-3, -2, -1) == -6
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(-3, -2, -1) == -6
- en: 'assert add3Numbers(0, 0, 0) == 0</foreignobject> <g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">6</foreignobject></g></g></svg>,
    EPiC randomly mutates the selected prompt candidates to better explore the search
    space of potential prompts in the next generation. The prompt mutation which is
    detailed in Algorithm [2](#alg2 "Algorithm 2 ‣ 3 Evolutionary Prompt Engineering
    for Code ‣ EPiC: Cost-effective Search-based Prompt Engineering of LLMs for Code
    Generation"), is elaborated in Section [3.2.2](#S3.SS2.SSS2 "3.2.2 Prompt Mutation
    ‣ 3.2 Evolutionary Prompt Engineering ‣ 3 Evolutionary Prompt Engineering for
    Code ‣ EPiC: Cost-effective Search-based Prompt Engineering of LLMs for Code Generation").
    The next generation is formed by adding the elite prompt to the pool of mutated
    prompts. Lines 21 to 27 in Algorithm [1](#alg1 "Algorithm 1 ‣ 3 Evolutionary Prompt
    Engineering for Code ‣ EPiC: Cost-effective Search-based Prompt Engineering of
    LLMs for Code Generation") corresponds to Steps <svg id="S3.SS2.p4.2.pic2" class="ltx_picture"
    height="19.62" overflow="visible" version="1.1" width="19.62"><g transform="translate(0,19.62)
    matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><foreignobject width="181.55" height="24.17" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Test Case Generation Prompt'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 'assert add3Numbers(0, 0, 0) == 0</foreignobject> <g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">6</foreignobject></g></g></svg>，EPiC
    随机突变选定的提示候选，以更好地探索下一代潜在提示的搜索空间。提示突变的详细信息见算法 [2](#alg2 "算法 2 ‣ 3 进化提示工程 ‣ EPiC:
    基于搜索的成本效益提示工程，用于代码生成")，在第 [3.2.2](#S3.SS2.SSS2 "3.2.2 提示突变 ‣ 3.2 进化提示工程 ‣ 3 进化提示工程用于代码
    ‣ EPiC: 成本效益的基于搜索的提示工程，用于代码生成") 节中详细阐述。下一代通过将精英提示添加到突变提示池中形成。算法 [1](#alg1 "算法
    1 ‣ 3 进化提示工程 ‣ EPiC: 基于搜索的成本效益提示工程，用于代码生成") 中的第 21 行到第 27 行对应于步骤 <svg id="S3.SS2.p4.2.pic2"
    class="ltx_picture" height="19.62" overflow="visible" version="1.1" width="19.62"><g
    transform="translate(0,19.62) matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><foreignobject width="181.55"
    height="24.17" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">测试用例生成提示'
- en: 'You are an AI coding assistant who can write unique, diverse, and intuitive
    unit tests for functions given the signature and docstring. Examples:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一个 AI 编码助手，可以根据函数的签名和文档字符串编写独特、多样且直观的单元测试。示例：
- en: 'def add3Numbers(x, y, z):'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 'def add3Numbers(x, y, z):'
- en: ””” Add three numbers together. This function takes three numbers as input and
    returns the sum of the three numbers.”””
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ””” 将三个数字相加。该函数接受三个数字作为输入，并返回这三个数字的总和。”””
- en: 'unit tests:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 单元测试：
- en: assert add3Numbers(1, 2, 3) == 6
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(1, 2, 3) == 6
- en: assert add3Numbers(-1, 2, 3) == 4
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(-1, 2, 3) == 4
- en: assert add3Numbers(1, -2, 3) == 2
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(1, -2, 3) == 2
- en: assert add3Numbers(1, 2, -3) == 0
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(1, 2, -3) == 0
- en: assert add3Numbers(-3, -2, -1) == -6
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(-3, -2, -1) == -6
- en: assert add3Numbers(0, 0, 0) == 0</foreignobject> <g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">5</foreignobject></g></g></svg>
    and <svg id="S3.SS2.p4.3.pic3" class="ltx_picture" height="19.62" overflow="visible"
    version="1.1" width="19.62"><g transform="translate(0,19.62) matrix(1 0 0 -1 0
    0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><foreignobject
    width="181.55" height="24.17" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Test
    Case Generation Prompt
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(0, 0, 0) == 0</foreignobject> <g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">5</foreignobject></g></g></svg>
    和 <svg id="S3.SS2.p4.3.pic3" class="ltx_picture" height="19.62" overflow="visible"
    version="1.1" width="19.62"><g transform="translate(0,19.62) matrix(1 0 0 -1 0
    0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><foreignobject
    width="181.55" height="24.17" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">测试用例生成提示
- en: 'You are an AI coding assistant who can write unique, diverse, and intuitive
    unit tests for functions given the signature and docstring. Examples:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一个 AI 编码助手，可以根据函数的签名和文档字符串编写独特、多样且直观的单元测试。示例：
- en: 'def add3Numbers(x, y, z):'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 'def add3Numbers(x, y, z):'
- en: ””” Add three numbers together. This function takes three numbers as input and
    returns the sum of the three numbers.”””
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ””” 将三个数字相加。该函数接受三个数字作为输入，并返回这三个数字的总和。”””
- en: 'unit tests:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 单元测试：
- en: assert add3Numbers(1, 2, 3) == 6
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: assert add3Numbers(1, 2, 3) == 6
- en: assert add3Numbers(-1, 2, 3) == 4
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`assert add3Numbers(-1, 2, 3) == 4`'
- en: assert add3Numbers(1, -2, 3) == 2
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '`assert add3Numbers(1, -2, 3) == 2`'
- en: assert add3Numbers(1, 2, -3) == 0
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`assert add3Numbers(1, 2, -3) == 0`'
- en: assert add3Numbers(-3, -2, -1) == -6
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '`assert add3Numbers(-3, -2, -1) == -6`'
- en: assert add3Numbers(0, 0, 0) == 0</foreignobject> <g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">6</foreignobject></g></g></svg>.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`assert add3Numbers(0, 0, 0) == 0</foreignobject> <g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">6</foreignobject></g></g></svg>。`'
- en: 'The process of EPE is repeated iteratively until a solution achieves maximum
    fitness score or until predefined stopping criteria, i.e., reaching the maximum
    number of iterations or observing no improvement in the fitness scores, are met.
    In such cases, the best-generated code, determined by its fitness, is chosen and
    returned (line 29 of Algorithm [1](#alg1 "Algorithm 1 ‣ 3 Evolutionary Prompt
    Engineering for Code ‣ EPiC: Cost-effective Search-based Prompt Engineering of
    LLMs for Code Generation")).'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: EPE的过程会反复迭代，直到解决方案达到最大适应度分数或满足预定义的停止标准，即达到最大迭代次数或适应度分数没有改善。在这种情况下，将选择并返回最佳生成的代码（由其适应度决定）（算法[1](#alg1
    "算法 1 ‣ 3 进化提示工程 ‣ EPiC：基于搜索的代码生成提示工程的成本效益")的第29行）。
- en: '![Refer to caption](img/6c102c97ebac2821cd4d964b1de7cdf0.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/6c102c97ebac2821cd4d964b1de7cdf0.png)'
- en: 'Figure 4: An example of a mutation generated by $sim\_words\_as\_mutator$.
    The orange words highlight the selected and mutated words in this example.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：$sim\_words\_as\_mutator$生成的突变示例。橙色词语突出显示了在本示例中选择和突变的词语。
- en: In the next two subsections, we will explain more details about Step <svg id="S3.SS2.p6.1.pic1"
    class="ltx_picture" height="19.62" overflow="visible" version="1.1" width="19.62"><g
    transform="translate(0,19.62) matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><foreignobject width="181.55"
    height="24.17" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Test Case
    Generation Prompt
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的两个小节中，我们将详细解释第<svg id="S3.SS2.p6.1.pic1" class="ltx_picture" height="19.62"
    overflow="visible" version="1.1" width="19.62"><g transform="translate(0,19.62)
    matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><foreignobject width="181.55" height="24.17" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">测试用例生成提示
- en: 'You are an AI coding assistant who can write unique, diverse, and intuitive
    unit tests for functions given the signature and docstring. Examples:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一个AI编码助手，可以为函数编写独特、多样且直观的单元测试，前提是提供函数的签名和文档字符串。示例：
- en: 'def add3Numbers(x, y, z):'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`def add3Numbers(x, y, z):`'
- en: ””” Add three numbers together. This function takes three numbers as input and
    returns the sum of the three numbers.”””
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ””” 将三个数字相加。此函数接受三个数字作为输入，并返回这三个数字的总和。”””
- en: 'unit tests:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 单元测试：
- en: assert add3Numbers(1, 2, 3) == 6
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '`assert add3Numbers(1, 2, 3) == 6`'
- en: assert add3Numbers(-1, 2, 3) == 4
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '`assert add3Numbers(-1, 2, 3) == 4`'
- en: assert add3Numbers(1, -2, 3) == 2
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '`assert add3Numbers(1, -2, 3) == 2`'
- en: assert add3Numbers(1, 2, -3) == 0
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`assert add3Numbers(1, 2, -3) == 0`'
- en: assert add3Numbers(-3, -2, -1) == -6
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`assert add3Numbers(-3, -2, -1) == -6`'
- en: assert add3Numbers(0, 0, 0) == 0</foreignobject> <g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g></svg>
    (Initial Population Builder), and Step <svg id="S3.SS2.p6.2.pic2" class="ltx_picture"
    height="19.62" overflow="visible" version="1.1" width="19.62"><g transform="translate(0,19.62)
    matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><foreignobject width="181.55" height="24.17" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Test Case Generation Prompt
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '`assert add3Numbers(0, 0, 0) == 0</foreignobject> <g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g></svg>（初始种群生成器），第<svg
    id="S3.SS2.p6.2.pic2" class="ltx_picture" height="19.62" overflow="visible" version="1.1"
    width="19.62"><g transform="translate(0,19.62) matrix(1 0 0 -1 0 0) translate(9.81,0)
    translate(0,9.81)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><foreignobject
    width="181.55" height="24.17" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">测试用例生成提示'
- en: 'You are an AI coding assistant who can write unique, diverse, and intuitive
    unit tests for functions given the signature and docstring. Examples:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一个AI编码助手，可以为函数编写独特、多样且直观的单元测试，前提是提供函数的签名和文档字符串。示例：
- en: 'def add3Numbers(x, y, z):'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`def add3Numbers(x, y, z):`'
- en: ””” Add three numbers together. This function takes three numbers as input and
    returns the sum of the three numbers.”””
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ””” 将三个数字相加。此函数接受三个数字作为输入，并返回这三个数字的总和。”””
- en: 'unit tests:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 单元测试：
- en: assert add3Numbers(1, 2, 3) == 6
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 断言 `add3Numbers(1, 2, 3) == 6`
- en: assert add3Numbers(-1, 2, 3) == 4
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 断言 `add3Numbers(-1, 2, 3) == 4`
- en: assert add3Numbers(1, -2, 3) == 2
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 断言 `add3Numbers(1, -2, 3) == 2`
- en: assert add3Numbers(1, 2, -3) == 0
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 断言 `add3Numbers(1, 2, -3) == 0`
- en: assert add3Numbers(-3, -2, -1) == -6
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 断言 `add3Numbers(-3, -2, -1) == -6`
- en: assert add3Numbers(0, 0, 0) == 0</foreignobject> <g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">6</foreignobject></g></g></svg>
    (Prompt Mutation.)
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 断言 `add3Numbers(0, 0, 0) == 0` </foreignobject> <g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">6</foreignobject></g></g></svg>（提示变异。）
- en: 3.2.1 Initial Population Builder
  id: totrans-182
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 初始种群构建器
- en: This subsection provides a detailed explanation of the Step <svg id="S3.SS2.SSS1.p1.1.pic1"
    class="ltx_picture" height="19.62" overflow="visible" version="1.1" width="19.62"><g
    transform="translate(0,19.62) matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><foreignobject width="181.55"
    height="24.17" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Test Case
    Generation Prompt
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 本小节详细解释了步骤 <svg id="S3.SS2.SSS1.p1.1.pic1" class="ltx_picture" height="19.62"
    overflow="visible" version="1.1" width="19.62"><g transform="translate(0,19.62)
    matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><foreignobject width="181.55" height="24.17" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">测试用例生成提示
- en: 'You are an AI coding assistant who can write unique, diverse, and intuitive
    unit tests for functions given the signature and docstring. Examples:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一个 AI 编码助手，可以根据函数的签名和文档字符串编写独特、多样和直观的单元测试。示例：
- en: 'def add3Numbers(x, y, z):'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 'def add3Numbers(x, y, z):'
- en: ””” Add three numbers together. This function takes three numbers as input and
    returns the sum of the three numbers.”””
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ””” 将三个数字相加。此函数接受三个数字作为输入，并返回这三个数字的总和。”””
- en: 'unit tests:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 单元测试：
- en: assert add3Numbers(1, 2, 3) == 6
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 断言 `add3Numbers(1, 2, 3) == 6`
- en: assert add3Numbers(-1, 2, 3) == 4
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 断言 `add3Numbers(-1, 2, 3) == 4`
- en: assert add3Numbers(1, -2, 3) == 2
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 断言 `add3Numbers(1, -2, 3) == 2`
- en: assert add3Numbers(1, 2, -3) == 0
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 断言 `add3Numbers(1, 2, -3) == 0`
- en: assert add3Numbers(-3, -2, -1) == -6
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 断言 `add3Numbers(-3, -2, -1) == -6`
- en: 'assert add3Numbers(0, 0, 0) == 0</foreignobject> <g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g></svg>
    in Figure [2](#S2.F2 "Figure 2 ‣ 2.1 LLMs for Code Generation ‣ 2 Background and
    Related Work ‣ EPiC: Cost-effective Search-based Prompt Engineering of LLMs for
    Code Generation"). Human-written prompts are often very brief and lack the necessary
    information or context. This results in insufficient descriptions and a failure
    to adhere to a well-defined format [[43](#bib.bib43)]. Thus manual prompting typically
    results in several back-and-forth interactions with LLM until all necessary details
    are given [[28](#bib.bib28)]. Providing the initial prompt in a structured and
    elaborate format will guide the LLM to generate better results with fewer interactions [[31](#bib.bib31)].'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '断言 `add3Numbers(0, 0, 0) == 0` <foreignobject> <g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g></svg>
    在图 [2](#S2.F2 "图 2 ‣ 2.1 LLMs 用于代码生成 ‣ 2 背景及相关工作 ‣ EPiC: 成本效益高的基于搜索的 LLMs 代码生成提示工程")
    中。人工编写的提示通常非常简短，缺乏必要的信息或上下文。这导致描述不足，未能遵循良好的定义格式 [[43](#bib.bib43)]。因此，手动提示通常需要与
    LLM 进行多次往返互动，直到提供所有必要的细节 [[28](#bib.bib28)]。以结构化和详细的格式提供初始提示将引导 LLM 生成更好的结果，并减少交互次数
    [[31](#bib.bib31)]。'
- en: 'In our evolutionary algorithm (Algorithm [1](#alg1 "Algorithm 1 ‣ 3 Evolutionary
    Prompt Engineering for Code ‣ EPiC: Cost-effective Search-based Prompt Engineering
    of LLMs for Code Generation")), we require an initial population of prompts. To
    create it, we utilize OpenAI’s GPT-4o to generate multiple prompts based on the
    initial given prompt. The prompt we use is as follows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '在我们的进化算法（算法 [1](#alg1 "算法 1 ‣ 3 进化提示工程用于代码 ‣ EPiC: 成本效益高的基于搜索的 LLMs 代码生成提示工程")）中，我们需要一个初始的提示集合。为了创建它，我们利用
    OpenAI 的 GPT-4o 基于初始给定的提示生成多个提示。我们使用的提示如下：'
- en: '<svg id="S3.SS2.SSS1.p3.pic1" class="ltx_picture" height="94.25" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,94.25) matrix(1 0 0 -1 0 0)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 8.99 8.99)"><foreignobject width="582.01" height="76.26" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Please rewrite the function
    description based on these instructions:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 请根据以下指示重写函数描述：
- en: 1- Add input and output types of the function to the description. 2- Elaborate
    the description so that it is understandable for large language models. 3- Keep
    the original test cases and add three test cases to the description to cover the
    edge cases. Do not separate the generated test cases and the original ones. 4-
    Keep the structure of the function and add the description as a comment in the
    function. Use at most 500 words.</foreignobject></g></g></svg>
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 1- 将函数的输入和输出类型添加到描述中。2- 详细说明描述，以便大型语言模型能够理解。3- 保留原始测试用例，并在描述中添加三个测试用例以涵盖边界情况。不要将生成的测试用例和原始测试用例分开。4-
    保持函数的结构，并在函数中添加描述作为注释。使用不超过500个单词。
- en: 'We employ a high-temperature setting (0.9) for the LLMto stimulate creativity
    in prompt generation. Elaborate prompts, which include explicit input-output types
    and test cases, more effectively direct the language model to produce the desired
    behavior compared to the original prompts. Figures [3(a)](#S3.F3.sf1 "In Figure
    3 ‣ 3 Evolutionary Prompt Engineering for Code ‣ EPiC: Cost-effective Search-based
    Prompt Engineering of LLMs for Code Generation") and [3(b)](#S3.F3.sf2 "In Figure
    3 ‣ 3 Evolutionary Prompt Engineering for Code ‣ EPiC: Cost-effective Search-based
    Prompt Engineering of LLMs for Code Generation") show an example of such an elaborate
    prompt.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '我们为LLM采用高温设置（0.9）以刺激提示生成中的创造力。详细的提示，包括明确的输入输出类型和测试用例，比原始提示更有效地引导语言模型生成所需的行为。图
    [3(a)](#S3.F3.sf1 "在图3 ‣ 3 进化提示工程用于代码 ‣ EPiC: 基于搜索的成本效益提示工程用于LLMs代码生成") 和图 [3(b)](#S3.F3.sf2
    "在图3 ‣ 3 进化提示工程用于代码 ‣ EPiC: 基于搜索的成本效益提示工程用于LLMs代码生成") 展示了这样的详细提示示例。'
- en: 'Figure [3(a)](#S3.F3.sf1 "In Figure 3 ‣ 3 Evolutionary Prompt Engineering for
    Code ‣ EPiC: Cost-effective Search-based Prompt Engineering of LLMs for Code Generation")
    illustrates the original prompt provided to an LLM, i.e., GPT-4o, which results
    in an implementation that fails to pass the test cases. Conversely, Figure [3(b)](#S3.F3.sf2
    "In Figure 3 ‣ 3 Evolutionary Prompt Engineering for Code ‣ EPiC: Cost-effective
    Search-based Prompt Engineering of LLMs for Code Generation") depicts a transformed
    version of the prompt that successfully guides the LLM towards a correct implementation.
    The primary issue with the original implementation is the incorrect initialization
    of the Eulerian number. In contrast, the transformed prompt includes specific
    instructions on how to properly initialize the Eulerian number, leading to a correct
    implementation. The orange text in both figures highlights the code and text related
    to the initialization.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [3(a)](#S3.F3.sf1 "在图3 ‣ 3 进化提示工程用于代码 ‣ EPiC: 基于搜索的成本效益提示工程用于LLMs代码生成") 展示了提供给LLM，即GPT-4o的原始提示，这导致了一个未能通过测试用例的实现。相反，图
    [3(b)](#S3.F3.sf2 "在图3 ‣ 3 进化提示工程用于代码 ‣ EPiC: 基于搜索的成本效益提示工程用于LLMs代码生成") 描绘了一个变换后的提示版本，它成功地引导LLM朝向正确的实现。原始实现的主要问题是欧拉数初始化不正确。相对而言，变换后的提示包括了如何正确初始化欧拉数的具体指示，从而实现了正确的实现。这两幅图中的橙色文本突出显示了与初始化相关的代码和文本。'
- en: 'After generating the elaborate prompt, we add the previously generated test
    cases (Step <svg id="S3.SS2.SSS1.p6.1.pic1" class="ltx_picture" height="19.62"
    overflow="visible" version="1.1" width="19.62"><g transform="translate(0,19.62)
    matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><foreignobject width="601.3" height="1440.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Please rewrite the function description based
    on these instructions:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成详细的提示后，我们将之前生成的测试用例（步骤 <svg id="S3.SS2.SSS1.p6.1.pic1" class="ltx_picture"
    height="19.62" overflow="visible" version="1.1" width="19.62"><g transform="translate(0,19.62)
    matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><foreignobject width="601.3" height="1440.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">请根据以下指示重写函数描述：
- en: '1- Add input and output types of the function to the description. 2- Elaborate
    the description so that it is understandable for large language models. 3- Keep
    the original test cases and add three test cases to the description to cover the
    edge cases. Do not separate the generated test cases and the original ones. 4-
    Keep the structure of the function and add the description as a comment in the
    function. Use at most 500 words.</foreignobject> <g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g></svg>
    in Figure [2](#S2.F2 "Figure 2 ‣ 2.1 LLMs for Code Generation ‣ 2 Background and
    Related Work ‣ EPiC: Cost-effective Search-based Prompt Engineering of LLMs for
    Code Generation")) to each prompt. These test cases will also be used during the
    evolutionary algorithm’s fitness evaluation in Step <svg id="S3.SS2.SSS1.p6.2.pic2"
    class="ltx_picture" height="19.62" overflow="visible" version="1.1" width="19.62"><g
    transform="translate(0,19.62) matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><foreignobject width="601.3"
    height="1440.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Please
    rewrite the function description based on these instructions:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 1- 在描述中添加函数的输入和输出类型。2- 详细阐述描述，以便大型语言模型能够理解。3- 保留原始测试用例，并在描述中添加三个测试用例以涵盖边界情况。不要将生成的测试用例与原始测试用例分开。4-
    保持函数结构，并将描述作为注释添加到函数中。最多使用500个单词。</foreignobject> <g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g></svg>
    图 [2](#S2.F2 "图 2 ‣ 2.1 LLMs 用于代码生成 ‣ 2 背景和相关工作 ‣ EPiC：基于搜索的提示工程的成本效益") 添加到每个提示中。这些测试用例也将在步骤
    <svg id="S3.SS2.SSS1.p6.2.pic2" class="ltx_picture" height="19.62" overflow="visible"
    version="1.1" width="19.62"><g transform="translate(0,19.62) matrix(1 0 0 -1 0
    0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><foreignobject
    width="601.3" height="1440.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">请根据以下指示重写函数描述：
- en: 1- Add input and output types of the function to the description. 2- Elaborate
    the description so that it is understandable for large language models. 3- Keep
    the original test cases and add three test cases to the description to cover the
    edge cases. Do not separate the generated test cases and the original ones. 4-
    Keep the structure of the function and add the description as a comment in the
    function. Use at most 500 words.</foreignobject> <g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">4</foreignobject></g></g></svg>.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 1- 在描述中添加函数的输入和输出类型。2- 详细阐述描述，以便大型语言模型能够理解。3- 保留原始测试用例，并在描述中添加三个测试用例以涵盖边界情况。不要将生成的测试用例与原始测试用例分开。4-
    保持函数结构，并将描述作为注释添加到函数中。最多使用500个单词。</foreignobject> <g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">4</foreignobject></g></g></svg>
    中的进化算法适应度评估过程中使用。
- en: 3.2.2 Prompt Mutation
  id: totrans-202
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 提示突变
- en: 'This subsection provides a detailed explanation of the Step <svg id="S3.SS2.SSS2.p1.1.pic1"
    class="ltx_picture" height="19.62" overflow="visible" version="1.1" width="19.62"><g
    transform="translate(0,19.62) matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><foreignobject width="601.3"
    height="1440.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Please
    rewrite the function description based on these instructions:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 本小节详细解释了第 <svg id="S3.SS2.SSS2.p1.1.pic1" class="ltx_picture" height="19.62"
    overflow="visible" version="1.1" width="19.62"><g transform="translate(0,19.62)
    matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><foreignobject width="601.3" height="1440.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">请根据以下说明重新编写函数描述：
- en: '1- Add input and output types of the function to the description. 2- Elaborate
    the description so that it is understandable for large language models. 3- Keep
    the original test cases and add three test cases to the description to cover the
    edge cases. Do not separate the generated test cases and the original ones. 4-
    Keep the structure of the function and add the description as a comment in the
    function. Use at most 500 words.</foreignobject> <g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">6</foreignobject></g></g></svg>
    in Figure [2](#S2.F2 "Figure 2 ‣ 2.1 LLMs for Code Generation ‣ 2 Background and
    Related Work ‣ EPiC: Cost-effective Search-based Prompt Engineering of LLMs for
    Code Generation"). We employed two kinds of prompt mutation approaches. In the
    first approach, EPiC uses an LLM to facilitate the mutation process ($LLM\_as\_mutator$)
    is described in Algorithm [2](#alg2 "Algorithm 2 ‣ 3 Evolutionary Prompt Engineering
    for Code ‣ EPiC: Cost-effective Search-based Prompt Engineering of LLMs for Code
    Generation"). We will compare the performance of both approaches in the following
    sections. We use the following prompt for mutation using LLMs:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 1- 将函数的输入和输出类型添加到描述中。 2- 详细阐述描述，以便大型语言模型能够理解。 3- 保留原有的测试用例，并在描述中添加三个测试用例，以覆盖边界情况。不要将生成的测试用例与原始测试用例分开。
    4- 保持函数的结构，并将描述作为注释添加到函数中。使用不超过500字。</foreignobject> <g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">6</foreignobject></g></g></svg>
    图[2](#S2.F2 "图 2 ‣ 2.1 代码生成的 LLM ‣ 2 背景与相关工作 ‣ EPiC：基于成本效益的 LLM 代码生成提示工程")中所示的步骤。我们采用了两种提示变异方法。在第一种方法中，EPiC
    使用 LLM 来促进变异过程（$LLM\_as\_mutator$），该方法在算法[2](#alg2 "算法 2 ‣ 3 进化提示工程 ‣ EPiC：基于成本效益的
    LLM 代码生成提示工程")中进行了描述。我们将在接下来的部分中比较这两种方法的性能。我们使用以下提示进行 LLM 变异：
- en: <svg id="S3.SS2.SSS2.p2.pic1" class="ltx_picture" height="61.04" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,61.04) matrix(1 0 0 -1 0 0)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 8.99 8.99)"><foreignobject width="582.01" height="43.05" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">You are a mutation tool. This
    is a Python function and its description. Please mutate the description by enhancing
    its clarity and comprehension for sophisticated language models.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: <svg id="S3.SS2.SSS2.p2.pic1" class="ltx_picture" height="61.04" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,61.04) matrix(1 0 0 -1 0 0)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 8.99 8.99)"><foreignobject width="582.01" height="43.05" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">您是一个变异工具。这是一个 Python 函数及其描述。请通过提升其清晰度和可理解性来变异描述，以适应复杂的语言模型。
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g></g></svg>'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '请将修改后的描述放在 #Explanation 和 #End 之间。使用不超过600字。</foreignobject></g></g></svg>'
- en: 'In the $sim\_words\_as\_mutator$. In this example, selected words are highlighted
    and replaced with their related synonyms by this algorithm. For instance, the
    word “takes” is randomly chosen for mutation. The algorithm first identifies similar
    words for this word, resulting in the following list: [“take”, “make”, “require”,
    “have”, “carry”, “get”, “bring”, “accept”, “lead”, “hold”]. The algorithm then
    randomly selects the word “accept” and replaces it with “take”. This process is
    similarly applied to other randomly selected words, such as “iterates”, “returns”,
    “containing”, “collection”, and “elements”.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在$sim\_words\_as\_mutator$中。在这个示例中，选定的词被高亮显示，并由算法用其相关同义词替换。例如，词“takes”被随机选择进行突变。算法首先为这个词识别类似词，得到以下列表：[“take”,
    “make”, “require”, “have”, “carry”, “get”, “bring”, “accept”, “lead”, “hold”]。然后，算法随机选择词“accept”，并用“take”替换它。这个过程类似地应用于其他随机选择的词，如“iterates”，“returns”，“containing”，“collection”和“elements”。
- en: 4 Experiment Design
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验设计
- en: 4.1 Research Questions
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 研究问题
- en: 'We define the following research questions to explore the performance of EPiC:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了以下研究问题以探讨EPiC的表现：
- en: •
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'RQ1: How cost-effective are the SOTA LLM-based code generation approaches?
    Motivation: Most of the SOTA LLM-based code generation studies do not include
    a comprehensive approach to evaluate cost-effectiveness. In this RQ, we set out
    to evaluate these tools from the cost-effectiveness perspective.'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RQ1：基于SOTA LLM的代码生成方法的成本效益如何？动机：大多数基于SOTA LLM的代码生成研究没有包括全面评估成本效益的方法。在这个RQ中，我们旨在从成本效益的角度评估这些工具。
- en: •
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'RQ2: How effective is EPiC in code generation? Motivation: In this RQ, we explore
    the performance of EPiC against SOTA LLM-based code generation tools in terms
    of both cost and effectiveness. We experiment with a large closed-source LLM (RQ2.1)
    and a smaller open-source model(RQ2.2)'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RQ2：EPiC在代码生成中的效果如何？动机：在这个RQ中，我们探讨EPiC在成本和效果方面相对于SOTA LLM基础的代码生成工具的表现。我们对一个大型闭源LLM（RQ2.1）和一个较小的开源模型（RQ2.2）进行实验。
- en: –
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'RQ2.1: How cost-effective is EPiC compared to the SOTA?'
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: RQ2.1：与SOTA相比，EPiC的成本效益如何？
- en: –
  id: totrans-217
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'RQ2.2: How effective is EPiC on smaller open-source LLMs compared to larger
    closed-source models?'
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: RQ2.2：与较大的闭源模型相比，EPiC在较小的开源LLM上的效果如何？
- en: •
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'RQ3:What are the impacts of different EA design choices on the cost-effectiveness
    of EPiC? Motivation: The goal of this RQ is to explore the impact of different
    hyperparameters in our approach. We define three sub-RQs to address the impact
    of the mutation generator (RQ3.1), mutation probability and similar word selection
    (RQ3.2), and population size (RQ3.3), as follows:'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RQ3：不同的EA设计选择对EPiC的成本效益有什么影响？动机：这个RQ的目标是探讨我们方法中不同超参数的影响。我们定义了三个子RQ来解决突变生成器（RQ3.1）、突变概率和相似词选择（RQ3.2）以及种群规模（RQ3.3）的影响，如下所示：
- en: –
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'RQ3.1: How does the complexity of the mutation generator impact the cost-effectiveness
    of EPiC?'
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: RQ3.1：突变生成器的复杂性如何影响EPiC的成本效益？
- en: –
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'RQ3.2: How does the mutation probability and similar word selection impact
    the performance of EPiC?'
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: RQ3.2：突变概率和相似词选择如何影响EPiC的性能？
- en: –
  id: totrans-225
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'RQ3.3: How does population size impact the cost-effectiveness of EPiC?'
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: RQ3.3：种群规模如何影响EPiC的成本效益？
- en: 4.2 Datasets and Models
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 数据集和模型
- en: 'TABLE I: Default configuration parameters'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：默认配置参数
- en: '| $mutation\_probability$ |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| $mutation\_probability$ |'
- en: '| $0.4$ |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| $0.4$ |'
- en: 'We used two datasets for our experiments: HumanEval [[8](#bib.bib8)] and MBPP
    [[24](#bib.bib24)].'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了两个数据集进行实验：HumanEval [[8](#bib.bib8)] 和 MBPP [[24](#bib.bib24)]。
- en: The HumanEval is a collection of 164 programming problems designed to evaluate
    the functional correctness of code generated by AI models. Each problem includes
    a function signature, a detailed description of the task (docstring), the function
    body, and multiple unit tests to verify the solution. One major obstacle in making
    fair judgments on the performance of the related works was that some papers did
    not use the HumanEval dataset as is. For instance, there are cases where the samples
    are removed or changed. To have a fair comparison among all methods we transformed
    the original dataset into each tool’s required format (if needed) to re-run their
    code using the original dataset. We will explain this in more detail as we discuss
    the configuration setup for assessing each related work.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: HumanEval是一个包含164个编程问题的集合，旨在评估由AI模型生成代码的功能正确性。每个问题包括一个函数签名、任务的详细描述（文档字符串）、函数体以及多个单元测试以验证解决方案。对相关工作的表现进行公平评估的一个主要障碍是一些论文并未按原样使用HumanEval数据集。例如，有些情况是样本被删除或更改。为了在所有方法之间进行公平比较，我们将原始数据集转换为每个工具所需的格式（如果需要），以使用原始数据集重新运行他们的代码。我们将在讨论每个相关工作的评估配置设置时详细解释这一点。
- en: The Mostly Basic Programming Problems (MBPP) comprises 974 short Python programming
    tasks designed to be solvable by entry-level programmers. The tasks were crowd-sourced
    from individuals with basic Python knowledge, who provided a problem statement,
    a self-contained Python function solving the problem, and three test cases to
    check for correctness. A subset of 427 tasks was manually inspected and edited
    for consistency and accuracy, referred to as mbpp-sanitized. In this paper, we
    used mbpp-sanitized for our experiments. While experimenting with the SOTA, we
    discovered that they have used different subsets of the MBPP dataset. For this
    reason, their reported results are not comparable. Consequently, we chose to use
    the mbpp-sanitized subset in all our experiments.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: Mostly Basic Programming Problems (MBPP)包括974个短小的Python编程任务，旨在由初级程序员解决。这些任务由具有基本Python知识的个人通过众包方式提供，他们提供了一个问题陈述，一个自包含的Python函数来解决问题，以及三个测试用例来检查正确性。一个包含427个任务的子集经过手动检查和编辑以确保一致性和准确性，称为mbpp-sanitized。在本文中，我们使用了mbpp-sanitized进行实验。在对SOTA进行实验时，我们发现他们使用了MBPP数据集的不同子集。因此，他们报告的结果不可比。因此，我们选择在所有实验中使用mbpp-sanitized子集。
- en: For RQ1 and RQ2.1, we used both datasets. For RQ2.2 and RQ3, we used only HumanEval.
    We will explain this in more detail in the corresponding RQs.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 对于RQ1和RQ2.1，我们使用了两个数据集。对于RQ2.2和RQ3，我们仅使用了HumanEval。我们将在相关RQ中详细解释这一点。
- en: 'We employed two LLMs in our study: OpenAI’s GPT-4o²²2https://openai.com/index/hello-gpt-4o/
    and Magicoder-S-DS-6.7B [[11](#bib.bib11)]. The latter model was specifically
    utilized for RQ 2.2, while OpenAI’s GPT-4o model was used for all other experiments.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在研究中使用了两个LLM：OpenAI的GPT-4o²²2https://openai.com/index/hello-gpt-4o/和Magicoder-S-DS-6.7B [[11](#bib.bib11)]。后者模型专门用于RQ
    2.2，而OpenAI的GPT-4o模型用于所有其他实验。
- en: 4.3 Evaluation Metrics
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 评估指标
- en: 'To evaluate the quality of the generated code by LLMs, we utilized the pass@k
    metric. This metric, introduced in [[8](#bib.bib8)], is advantageous over metrics
    such as CodeBLEU and ROUGE because it evaluates code behavior rather than text
    semantics, which are the focus of the latter metrics. The formula for pass@k is
    presented in Equation [1](#S4.E1 "In 4.3 Evaluation Metrics ‣ 4 Experiment Design
    ‣ EPiC: Cost-effective Search-based Prompt Engineering of LLMs for Code Generation").
    In this equation, $E$ samples is correct. When applied to multiple problems, it
    evaluates the expectation across all problems.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '为了评估LLM生成代码的质量，我们使用了pass@k指标。这个指标在[[8](#bib.bib8)]中提出，相较于CodeBLEU和ROUGE等指标具有优势，因为它评估的是代码行为，而不是后者指标所关注的文本语义。pass@k的公式见方程[1](#S4.E1
    "In 4.3 Evaluation Metrics ‣ 4 Experiment Design ‣ EPiC: Cost-effective Search-based
    Prompt Engineering of LLMs for Code Generation")。在这个方程中，$E$样本是正确的。当应用于多个问题时，它评估所有问题的期望。'
- en: '|  | <math id="S4.E1.m1.3" class="ltx_Math" alttext="pass@k:=\underset{\text{
    Problems }}{\mathbb{E}}\left[1-\frac{\left(\begin{array}[]{c}n-c\\ k\end{array}\right)}{\left(\begin{array}[]{c}n\\'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math id="S4.E1.m1.3" class="ltx_Math" alttext="pass@k:=\underset{\text{
    Problems }}{\mathbb{E}}\left[1-\frac{\left(\begin{array}[]{c}n-c\\ k\end{array}\right)}{\left(\begin{array}[]{c}n\\'
- en: k\end{array}\right)}\right]$$ |  | (1) |
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: k\end{array}\right)}\right]$$ |  | (1) |
- en: When comparing EPiC with baselines, we also report Mann-Whitney U-test results,
    which is a non-parametric significant test, to consider the randomness introduced
    by our algorithm. The tests compare the distribution of 10 pass@1 results for
    EPiC (using 10 random seeds) with the pass@1 of each baseline, separately. Note
    that here we only rerun EPiC with 10 random seeds to consider its internal randomness
    introduced in the algorithm and do not rerun the baselines as they do not have
    internal randomness. In other words, we do not study the potential randomness
    of LLMs results and their effect on each baseline, as this is not reported in
    the original papers and would go beyond the scope of this paper.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在比较EPiC与基准时，我们还报告了Mann-Whitney U检验结果，这是一种非参数显著性检验，用以考虑我们算法引入的随机性。这些测试将EPiC的10次pass@1结果（使用10个随机种子）与每个基准的pass@1进行比较。请注意，我们这里只是用10个随机种子重新运行EPiC，以考虑其算法中引入的内部随机性，而不重新运行基准，因为基准没有内部随机性。换句话说，我们不研究LLMs结果的潜在随机性及其对每个基准的影响，因为这在原始论文中没有报告，并超出了本文的范围。
- en: 4.4 Objective Function
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 目标函数
- en: 'We use $\mathcal{X}$. The fitness function is defined as follows:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用$\mathcal{X}$。适应度函数定义如下：
- en: '|  | $\mathcal{F}(x,\mathcal{T})=\frac{\sum_{i=1}^{i=n}t_{i}(\mathcal{S}(x))}{n}$
    |  | (2) |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{F}(x,\mathcal{T})=\frac{\sum_{i=1}^{i=n}t_{i}(\mathcal{S}(x))}{n}$
    |  | (2) |'
- en: The goal of the EPiC framework is to optimize the fitness function $\mathcal{F}$.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: EPiC框架的目标是优化适应度函数$\mathcal{F}$。
- en: 4.5 Baselines
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 基准
- en: 'To evaluate the performance of EPiC, we first identified the most relevant
    papers in code generation that utilized a common benchmark for comparison, i.e.,
    papers that used HumanEval or MBPP, either exclusively or as part of their benchmarks.
    These two benchmarks were chosen due to their popularity in the field. Further
    narrowing down the selection, we applied three additional criteria: (1) whether
    their results were competitive (more than %90 pass@1 on HumanEval), (2) whether
    their GitHub repository is available online, and (3) their cost is not disproportionately
    high. The third criterion was specifically defined to exclude SOTA tools with
    excessively high costs, ensuring that the selected tools remained competitive
    in terms of cost-effectiveness. For comparison, we used the pass@1 metric. As
    a result, four papers are selected as our baselines, i.e., Reflexion[[20](#bib.bib20)],
    LATS[[21](#bib.bib21)], LDB[[23](#bib.bib23)], and AgentCoder[[22](#bib.bib22)]
    (details are in Section [2](#S2 "2 Background and Related Work ‣ EPiC: Cost-effective
    Search-based Prompt Engineering of LLMs for Code Generation")). For each baseline,
    we ran the experiments with the recommended configuration provided in its GitHub
    repository or provided in its paper. After initial evaluation, AgentCoder[[22](#bib.bib22)]
    was excluded from further analysis as we found out that it’s 6 times more expensive
    than the second most expensive alternative. The baselines’ configuration details
    used in our experiments are as follows:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '为了评估EPiC的性能，我们首先识别了在代码生成领域使用共同基准进行比较的最相关论文，即使用HumanEval或MBPP的论文，无论是专门使用还是作为其基准的一部分。这两个基准因其在该领域的流行而被选择。进一步缩小选择范围，我们应用了三个附加标准：（1）其结果是否具有竞争力（HumanEval上的pass@1超过%90），（2）其GitHub仓库是否在线可用，以及（3）其成本是否不成比例地高。第三个标准专门定义为排除成本过高的SOTA工具，以确保选择的工具在成本效益方面保持竞争力。为了比较，我们使用了pass@1指标。因此，选定了四篇论文作为我们的基准，即Reflexion[[20](#bib.bib20)]，LATS[[21](#bib.bib21)]，LDB[[23](#bib.bib23)]和AgentCoder[[22](#bib.bib22)]（详细信息见第[2](#S2
    "2 Background and Related Work ‣ EPiC: Cost-effective Search-based Prompt Engineering
    of LLMs for Code Generation")节）。对于每个基准，我们使用其GitHub仓库或论文中提供的推荐配置进行实验。经过初步评估后，AgentCoder[[22](#bib.bib22)]因其费用是第二高的替代方案的6倍被排除在进一步分析之外。我们实验中使用的基准配置详细信息如下：'
- en: 'Reflexion: We ran Reflexion with $max\_iters$ set to “gpt4o”. Note that, we
    have adapted Reflexion’s code base to support the “gpt4o” model and tracking of
    time cost as these features were unavailable in Reflexion’s original version.
    During our experiments, we found inconsistencies between the HumanEval dataset
    on their GitHub and the original dataset. There were missing instances and modified
    or removed test cases compared to the original dataset. We reported this issue
    in their repository³³3[Link to Reflexion dataset issue](https://github.com/noahshinn/reflexion/issues/42).
    To ensure a valid assessment of their work, we used the original HumanEval dataset.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: Reflexion：我们将 Reflexion 的 `$max\_iters$` 设置为 “gpt4o” 进行了实验。注意，我们已调整 Reflexion
    的代码库以支持 “gpt4o” 模型和时间成本跟踪，因为这些特征在 Reflexion 的原版中不可用。在我们的实验过程中，我们发现了 HumanEval
    数据集与他们 GitHub 上的原始数据集之间的不一致。相比原始数据集，出现了缺失的实例和修改或删除的测试用例。我们在他们的仓库中报告了这个问题³³3[Link
    to Reflexion dataset issue](https://github.com/noahshinn/reflexion/issues/42)。为了确保对他们工作的有效评估，我们使用了原始的
    HumanEval 数据集。
- en: 'LATS: We ran the experiments with $max\_iters$ should be defined as passing
    the original test cases, not the generated ones. We corrected this issue in their
    code (and reported it to the authors) before running the experiments. While they
    have now resolved this issue in their codebase yet, the results in their paper
    had not been updated at the time of writing this paper⁴⁴4[LATS bug link](https://github.com/lapisrocks/LanguageAgentTreeSearch/commit/853d81614607dd27433faf17c7b0a7d660f95d22).
    Like Reflexion, they also used LLM-generated test cases for internal evaluations.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: LATS：我们进行了实验，其中 `$max\_iters$` 应定义为通过原始测试用例，而不是生成的测试用例。我们在运行实验之前在他们的代码中纠正了这个问题（并向作者报告了这个问题）。尽管他们现在已经在代码库中解决了这个问题，但在撰写本文时，他们论文中的结果尚未更新⁴⁴4[LATS
    bug link](https://github.com/lapisrocks/LanguageAgentTreeSearch/commit/853d81614607dd27433faf17c7b0a7d660f95d22)。与
    Reflexion 一样，他们也使用了 LLM 生成的测试用例进行内部评估。
- en: 'LDB: We set $pass@k$ to “ldb”, and the model to “gpt4o”. Consistent with previous
    works, we implemented the “gpt4o” model and tracking of time cost features as
    they were unavailable in the original version. The authors used seed programs
    in their code to enhance their results. The seed programs served as the initial
    implementation of the code. We removed these seed files to ensure fairness, as
    other baselines do not use any seeds (an initial given output code) to boost their
    results. Upon reviewing their dataset, we found no mismatches with the original
    dataset. However, we identified inconsistencies between the actual test cases
    in the function descriptions and the test cases they used as their visible test
    cases in the codebase, in HumanEval dataset. It appears that some test cases were
    manually modified. This poses an issue for a fair comparison. In instances where
    we identified such cases, we adjusted them to ensure no manual intervention. As
    a result, we replaced the original extracted test cases with the visible test
    cases to ensure a fair comparison before conducting our experiments and we reported
    this issue in their GitHub repository to be fixed⁵⁵5[LDB GitHub issue link](https://github.com/FloridSleeves/LLMDebugger/issues/12).'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: LDB：我们将 `$pass@k$` 设置为 “ldb”，模型设置为 “gpt4o”。与之前的工作一致，我们实现了 “gpt4o” 模型并跟踪时间成本特征，因为这些特征在原版中不可用。作者在代码中使用了种子程序来增强他们的结果。这些种子程序作为代码的初步实现。我们删除了这些种子文件以确保公平，因为其他基准不使用任何种子（初始给定输出代码）来提升他们的结果。在检查他们的数据集时，我们没有发现与原始数据集的不匹配。然而，我们发现了函数描述中的实际测试用例与他们在代码库中使用的可见测试用例之间的不一致，尤其是在
    HumanEval 数据集中。似乎一些测试用例被手动修改了。这对公平比较构成了问题。在我们识别出这种情况时，我们进行了调整，以确保没有手动干预。因此，在进行实验之前，我们用可见测试用例替换了原始提取的测试用例，并在他们的
    GitHub 仓库中报告了这个问题以供修复⁵⁵5[LDB GitHub issue link](https://github.com/FloridSleeves/LLMDebugger/issues/12)。
- en: For the MBPP experiments, LATS and Reflexion employed the same code generation
    approach to produce test cases, using these for internal evaluations. In contrast,
    LDB utilized the test cases within the MBPP dataset for the internal evaluation.
    We determined that using the original test cases for internal evaluation is unfair
    to other baselines and impractical, as these test cases are meant solely for final
    evaluation, not for use during the process (in practice, test cases are not typically
    available before the code is developed). Consequently, we modified their test
    cases to use the generated test cases produced by the method employed in LATS
    and Reflexion, so we can have a practical approach implemented across the baselines
    and a fair comparison.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 对于MBPP实验，LATS和Reflexion采用了相同的代码生成方法来生成测试用例，并将这些用例用于内部评估。相比之下，LDB使用了MBPP数据集中的测试用例进行内部评估。我们确定，使用原始测试用例进行内部评估对其他基准是不公平的且不切实际，因为这些测试用例仅用于最终评估，而在实际过程中（在实践中，测试用例通常在代码开发之前不可用）。因此，我们修改了测试用例，使用LATS和Reflexion中采用的方法生成的测试用例，以便在所有基准中实施一种实际的方法并进行公平比较。
- en: 4.6 Experimental Setup
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 实验设置
- en: 'TABLE II: Cost and effectiveness of LDB, LATS, Reflexion, and EPiC on HumanEval
    and MBPP datasets. The cells with green background color indicate the best scores
    and the red ones indicate the worst scores. EPiC results are the mean over 10
    runs.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 表II：LDB、LATS、Reflexion和EPiC在HumanEval和MBPP数据集上的成本和效果。背景色为绿色的单元格表示最佳得分，红色的单元格表示最差得分。EPiC的结果是10次运行的平均值。
- en: '| $Dataset$ |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| $Dataset$ |'
- en: '| Humaneval | Reflexion | $\%87$ |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| Humaneval | Reflexion | $\%87$ |'
- en: '| Humaneval | LDB | $\%92$ |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| Humaneval | LDB | $\%92$ |'
- en: '| Humaneval | LATS | $\%91$ |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| Humaneval | LATS | $\%91$ |'
- en: '| Humaneval | EPiC | $\%94$ | – |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| Humaneval | EPiC | $\%94$ | – |'
- en: '| MBPP | Reflexion | $\%71$ |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| MBPP | Reflexion | $\%71$ |'
- en: '| MBPP | LDB | $\%73$ |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| MBPP | LDB | $\%73$ |'
- en: '| MBPP | LATS | $\%76$ |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| MBPP | LATS | $\%76$ |'
- en: '| MBPP | EPiC | $\%79$ | – |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| MBPP | EPiC | $\%79$ | – |'
- en: '| $Tool$ |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| $Tool$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| $EPiC$ |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| $EPiC$ |'
- en: '| $EPiC$ |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| $EPiC$ |'
- en: 'TABLE III: EPiC’s statistical results from 10 runs on HumanEval and MBPP'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 表III：EPiC在HumanEval和MBPP上的10次运行的统计结果
- en: For all the experiments, we selected gpt-4o as the base LLM model, because it
    is OpenAI’s latest model, offering superior speed and cost-efficiency while maintaining
    equal intelligence compared to GPT-4. We conducted experiments on the HumanEval
    and MBPP datasets, utilizing the baseline methods in RQ1 and EPiC in RQ2.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有实验，我们选择了gpt-4o作为基础LLM模型，因为它是OpenAI最新的模型，提供了更高的速度和成本效益，同时在智能水平上与GPT-4相当。我们在HumanEval和MBPP数据集上进行了实验，利用了RQ1中的基准方法和RQ2中的EPiC。
- en: 'Our default configuration setups with EPiC are presented in Table [I](#S4.T1
    "TABLE I ‣ 4.2 Datasets and Models ‣ 4 Experiment Design ‣ EPiC: Cost-effective
    Search-based Prompt Engineering of LLMs for Code Generation"). The default configuration
    is chosen based on the experiments in Section [5.3](#S5.SS3 "5.3 RQ3: What are
    the impacts of different EA design choices on the cost-effectiveness of EPiC?
    ‣ 5 Experiment Results ‣ EPiC: Cost-effective Search-based Prompt Engineering
    of LLMs for Code Generation"). Fine-tuning EPiC’s hyper-parameters properly (e.g.,
    using a grid search over all combinations of input space in the hyper-parameters)
    would be extremely expensive. However, RQ3’s sensitivity analysis already provides
    an acceptable (not optimal) configuration to set as our default. We used the default
    configuration for RQ2.1 and changed it in the subsequent RQs, which we will explain
    later. In the default configuration, we utilized OpenAI’s GPT-4o as the code generator
    LLM agent. We evaluated our approach on both the HumanEval and MBPP datasets.
    Our mutation process employs the sim_words_as_mutator method. The Mutation_probability
    parameter specifies the likelihood of a word being substituted with a similar
    word. Population_size refers to the number of populations processed for each dataset
    instance, while Top_n indicates the top number of related words considered for
    the mutation process.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在EPiC的默认配置设置见表格[I](#S4.T1 "TABLE I ‣ 4.2 Datasets and Models ‣ 4 Experiment
    Design ‣ EPiC: Cost-effective Search-based Prompt Engineering of LLMs for Code
    Generation")。默认配置是基于[5.3](#S5.SS3 "5.3 RQ3: What are the impacts of different
    EA design choices on the cost-effectiveness of EPiC? ‣ 5 Experiment Results ‣
    EPiC: Cost-effective Search-based Prompt Engineering of LLMs for Code Generation")节中的实验选择的。适当地微调EPiC的超参数（例如，使用网格搜索所有超参数输入空间的组合）将非常昂贵。然而，RQ3的敏感性分析已经提供了一个可接受的（不是最优的）配置作为我们的默认配置。我们在RQ2.1中使用了默认配置，并在后续的RQ中进行了更改，我们会在后面解释。在默认配置中，我们使用了OpenAI的GPT-4o作为代码生成LLM代理。我们在HumanEval和MBPP数据集上评估了我们的方法。我们的变异过程采用了sim_words_as_mutator方法。Mutation_probability参数指定了一个词被替换为相似词的可能性。Population_size指的是处理每个数据集实例的人口数量，而Top_n表示变异过程中考虑的相关词的数量。'
- en: The experiments for RQ1, RQ2.1, and RQ3 were conducted on a device equipped
    with 16 GB of RAM and a 13th Gen Intel® Core™ i7-13700HX × 24 CPU. For RQ2.2,
    the experiments were performed on one node of the Alliance Canada server, utilizing
    two 32 GB Tesla V100 GPUs and 52 GB of RAM.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: RQ1、RQ2.1和RQ3的实验在配备了16 GB RAM和第13代Intel® Core™ i7-13700HX × 24 CPU的设备上进行。对于RQ2.2，实验是在Alliance
    Canada服务器的一个节点上进行的，使用了两个32 GB的Tesla V100 GPU和52 GB的RAM。
- en: 5 Experiment Results
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验结果
- en: '5.1 RQ1: How cost-effective are the SOTA LLM-based code generation tools?'
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 RQ1：SOTA LLM基础的代码生成工具的性价比如何？
- en: 'We evaluated LATS, LDB, and Reflexion on the entire HumanEval and Sanitized
    MBPP datasets. To monitor the monetary costs, we used the OpenAI dashboard. This
    cost is calculated based on the number of tokens exchanged with the OpenAI API.
    As the financial cost is calculated based on the number of exchanged tokens, we
    chose the dollar cost reported in the OpenAI dashboard as the metric to measure
    the cost of an approach. The results of this experiment are presented in Table
    [II](#S4.T2 "TABLE II ‣ 4.6 Experimental Setup ‣ 4 Experiment Design ‣ EPiC: Cost-effective
    Search-based Prompt Engineering of LLMs for Code Generation"), for HumanEval and
    MBPP.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在整个HumanEval和Sanitized MBPP数据集上评估了LATS、LDB和Reflexion。为了监控财务成本，我们使用了OpenAI仪表板。这个成本是基于与OpenAI
    API交换的令牌数量计算的。由于财务成本是基于交换的令牌数量计算的，我们选择了OpenAI仪表板中报告的美元成本作为衡量方法的成本指标。此实验的结果见表格[II](#S4.T2
    "TABLE II ‣ 4.6 Experimental Setup ‣ 4 Experiment Design ‣ EPiC: Cost-effective
    Search-based Prompt Engineering of LLMs for Code Generation")，适用于HumanEval和MBPP。'
- en: The first observation is that there is a notable discrepancy between our achieved
    $pass@1$ results and those reported in the original papers of the baselines. One
    possible reason is that we used GPT-4o for our experiments, whereas the original
    studies used other OpenAI models. Another plausible reason could be inconsistencies
    between the original dataset and the ones they used, as we utilized the original
    dataset while they used a modified version.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个观察是，我们的$pass@1$结果与基线原始论文中报告的结果存在显著差异。一个可能的原因是我们在实验中使用了GPT-4o，而原始研究使用了其他OpenAI模型。另一个可能的原因可能是原始数据集与他们使用的数据集之间的不一致，因为我们使用了原始数据集，而他们使用了修改版。
- en: Reflexion incurred the lowest cost in both datasets but also demonstrated the
    lowest performance, with an overall accuracy of %75 and a cost of ¢1.1 per instance.
    Conversely, LATS exhibited the highest time and dollar costs in both datasets,
    with a total cost of $\$96.71$ but ranked second on MBPP overall. LATS achieved
    the best performance overall but had the highest cost, considerably higher than
    both Reflexion and LDB. On the other hand, LDB provided better performance than
    Reflexion without significantly increasing the cost.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: Reflexion 在两个数据集中都产生了最低的成本，但也显示了最低的性能，总体准确率为 %75，每实例成本为 ¢1.1。相反，LATS 在两个数据集中都展示了最高的时间和金钱成本，总成本为
    $\$96.71$，但在 MBPP 数据集中总体排名第二。LATS 达到了最佳的总体性能，但成本最高，远高于 Reflexion 和 LDB。另一方面，LDB
    提供了比 Reflexion 更好的性能，但没有显著增加成本。
- en: A noteworthy observation is that the cost (measured by dollar amount) and effectiveness
    (measured by $pass@1$ increase in cost. This observation aligns with the understanding
    that some programming problems are inherently more challenging and require more
    time for both humans and LLMs to solve correctly. However, it is crucial to develop
    an approach that achieves higher performance without a substantial increase in
    cost, ensuring cost-effectiveness. This emphasizes that future studies in this
    field must always report their cost as part of the evaluation as well.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 一个值得注意的观察是，成本（以美元金额衡量）与效果（以 $pass@1$ 衡量）之间存在增加的关系。这一观察结果符合一些编程问题本质上更具挑战性，并且需要更多时间来正确解决的理解。然而，重要的是开发一种在不显著增加成本的情况下实现更高性能的方法，确保成本效益。这强调了未来该领域的研究必须始终将成本作为评估的一部分进行报告。
- en: '<svg id="S5.SS1.p5.pic1" class="ltx_picture" height="139.48" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,139.48) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="111.93" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Answer
    to RQ1: The SOTA code generation tools vary in cost-effectiveness, and the relationship
    between cost and effectiveness is not always linear. For instance, in MBPP dataset,
    LATS demonstrates the best performance (%76 pass@1) but incurs almost 6 times
    more cost compared to the second best baseline, LDB, with only %3 less pass@1\.
    On the other hand, Reflexion shows slightly lower performance (%71 pass@1), but
    with a very low cost ($4.9). Therefore, we recommend a cost analysis always be
    a part of the evaluation when studying LLMs for code generation.</foreignobject></g></g></svg><svg
    id="S5.F5.pic1" class="ltx_picture ltx_centering" height="167.42" overflow="visible"
    version="1.1" width="205.99"><g transform="translate(0,167.42) matrix(1 0 0 -1
    0 0) translate(31.76,0) translate(0,25.67) matrix(1.0 0.0 0.0 1.0 -31.76 -25.67)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g class="ltx_nestedsvg"
    transform="matrix(1 0 0 1 0 0) translate(31.76,0) translate(0,25.67)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g clip-path="url(#pgfcp1)"><g stroke-width="0.2pt"
    fill="#808080" stroke="#808080" color="#808080"><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '回答 RQ1: 最先进的代码生成工具在成本效益上有所不同，成本与效果之间的关系并不总是线性的。例如，在 MBPP 数据集中，LATS 展现了最佳的性能（%76
    pass@1），但其成本几乎是第二最佳基准 LDB 的 6 倍，虽然 pass@1 仅低了 %3。另一方面，Reflexion 显示了略低的性能（%71 pass@1），但成本非常低（$4.9）。因此，我们建议在研究用于代码生成的
    LLM 时，成本分析始终应成为评估的一部分。'
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g></g> <g clip-path="url(#pgfcp2)"><g stroke-width="0.2pt"
    fill="#808080" stroke="#808080" color="#808080"><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '请将修改后的描述放在 #Explanation 和 #End 之间。最多使用 600 个词。`</foreignobject></g></g> <g
    clip-path="url(#pgfcp2)"><g stroke-width="0.2pt" fill="#808080" stroke="#808080"
    color="#808080"><foreignobject width="600.47" height="602.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">你是一个变异工具。这是一个 Python 函数及其描述。请通过提高其清晰度和理解度来变异描述，以适应复杂的语言模型。`'
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g></g> <foreignobject width="600.47" height="602.45"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You are
    a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '请将修改后的描述放在 #Explanation 和 #End 之间。最多使用 600 个词。`</foreignobject></g></g> <foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">你是一个变异工具。这是一个 Python 函数及其描述。请通过提高其清晰度和理解度来变异描述，以适应复杂的语言模型。`'
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <foreignobject width="600.47" height="602.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You are a mutation tool. This
    is a Python function and its description. Please mutate the description by enhancing
    its clarity and comprehension for sophisticated language models.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '请将修改后的描述放在 #Explanation 和 #End 之间。最多使用 600 个词。`</foreignobject> <foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">你是一个变异工具。这是一个 Python 函数及其描述。请通过提高其清晰度和理解度来变异描述，以适应复杂的语言模型。`'
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 -2.42 -9.75)"
    fill="#000000" stroke="#000000"><foreignobject width="4.84" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$0$</foreignobject></g><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '请将修改后的描述放在 #Explanation 和 #End 之间。最多使用 600 个词。`</foreignobject> <g transform="matrix(1.0
    0.0 0.0 1.0 -2.42 -9.75)" fill="#000000" stroke="#000000"><foreignobject width="4.84"
    height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$0$</foreignobject></g><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">你是一个变异工具。这是一个 Python 函数及其描述。请通过提高其清晰度和理解度来变异描述，以适应复杂的语言模型。'
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 64.48 -9.75)"
    fill="#000000" stroke="#000000"><foreignobject width="4.84" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$5$</foreignobject></g><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '请将修改后的描述放在 #Explanation 和 #End 之间。最多使用 600 个词。`</foreignobject> <g transform="matrix(1.0
    0.0 0.0 1.0 64.48 -9.75)" fill="#000000" stroke="#000000"><foreignobject width="4.84"
    height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$5$</foreignobject></g><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">你是一个变异工具。这是一个 Python 函数及其描述。请通过提高其清晰度和理解度来变异描述，以适应复杂的语言模型。`'
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 128.97 -9.75)"
    fill="#000000" stroke="#000000"><foreignobject width="9.69" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$10$</foreignobject></g><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '请将更改后的描述放在 #Explanation 和 #End 之间。最多使用600个字。'
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 -13.19 11.03)"
    fill="#000000" stroke="#000000"><foreignobject width="9.69" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$86$</foreignobject></g><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '请将更改后的描述放在 #Explanation 和 #End 之间。最多使用600个字。'
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 -13.19 39.32)"
    fill="#000000" stroke="#000000"><foreignobject width="9.69" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$88$</foreignobject></g><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '请将更改后的描述放在 #Explanation 和 #End 之间。最多使用600个字。'
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 -13.19 67.62)"
    fill="#000000" stroke="#000000"><foreignobject width="9.69" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$90$</foreignobject></g><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '请将更改后的描述放在 #Explanation 和 #End 之间。最多使用600个字。'
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 -13.19 95.91)"
    fill="#000000" stroke="#000000"><foreignobject width="9.69" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$92$</foreignobject></g><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 请将更改后的描述放在#Explanation和#End之间。最多使用600个词。</foreignobject> <g transform="matrix(1.0
    0.0 0.0 1.0 -13.19 95.91)" fill="#000000" stroke="#000000"><foreignobject width="9.69"
    height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$92$</foreignobject></g><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">你是一个突变工具。这是一个Python函数及其描述。请通过增强其清晰度和理解度来突变描述，以适应复杂的语言模型。
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 -13.19 124.21)"
    fill="#000000" stroke="#000000"><foreignobject width="9.69" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$94$</foreignobject></g><g clip-path="url(#pgfcp3)"><g
    stroke="#FF0000" fill="#FF0000"><foreignobject width="600.47" height="602.45"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You are
    a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 请将更改后的描述放在#Explanation和#End之间。最多使用600个词。</foreignobject> <g transform="matrix(1.0
    0.0 0.0 1.0 -13.19 124.21)" fill="#000000" stroke="#000000"><foreignobject width="9.69"
    height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$94$</foreignobject></g><g
    clip-path="url(#pgfcp3)"><g stroke="#FF0000" fill="#FF0000"><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">你是一个突变工具。这是一个Python函数及其描述。请通过增强其清晰度和理解度来突变描述，以适应复杂的语言模型。
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g> <g stroke="#0000FF" fill="#0000FF"><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">You are a mutation tool. This is a Python function and its description.
    Please mutate the description by enhancing its clarity and comprehension for sophisticated
    language models.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 请将更改后的描述放在#Explanation和#End之间。最多使用600个词。</foreignobject></g> <g stroke="#0000FF"
    fill="#0000FF"><foreignobject width="600.47" height="602.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">你是一个突变工具。这是一个Python函数及其描述。请通过增强其清晰度和理解度来突变描述，以适应复杂的语言模型。
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g> <g stroke="#00FF00" fill="#00FF00"><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">You are a mutation tool. This is a Python function and its description.
    Please mutate the description by enhancing its clarity and comprehension for sophisticated
    language models.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 请将更改后的描述放在#Explanation和#End之间。最多使用600个词。</foreignobject></g> <g stroke="#00FF00"
    fill="#00FF00"><foreignobject width="600.47" height="602.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">你是一个突变工具。这是一个Python函数及其描述。请通过增强其清晰度和理解度来突变描述，以适应复杂的语言模型。
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g> <g stroke="#FFFF00" fill="#FFFF00"><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">You are a mutation tool. This is a Python function and its description.
    Please mutate the description by enhancing its clarity and comprehension for sophisticated
    language models.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 请将更改后的描述放在#Explanation和#End之间。最多使用600个词。</foreignobject></g> <g stroke="#FFFF00"
    fill="#FFFF00"><foreignobject width="600.47" height="602.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">你是一个突变工具。这是一个Python函数及其描述。请通过增强其清晰度和理解度来突变描述，以适应复杂的语言模型。
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g></g> <g stroke="#FF0000" fill="#FF0000" color="#FF0000"><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">You are a mutation tool. This is a Python function and its description.
    Please mutate the description by enhancing its clarity and comprehension for sophisticated
    language models.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 请将更改后的描述放在#Explanation和#End之间。最多使用600个单词。</foreignobject></g></g> <g stroke="#FF0000"
    fill="#FF0000" color="#FF0000"><foreignobject width="600.47" height="602.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">你是一个变异工具。这是一个Python函数及其描述。请通过提高其清晰度和对高级语言模型的理解，来变异描述。
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g> <g stroke="#0000FF" fill="#0000FF" color="#0000FF"><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">You are a mutation tool. This is a Python function and its description.
    Please mutate the description by enhancing its clarity and comprehension for sophisticated
    language models.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 请将更改后的描述放在#Explanation和#End之间。最多使用600个单词。</foreignobject></g> <g stroke="#0000FF"
    fill="#0000FF" color="#0000FF"><foreignobject width="600.47" height="602.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">你是一个变异工具。这是一个Python函数及其描述。请通过提高其清晰度和对高级语言模型的理解，来变异描述。
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g> <g stroke="#00FF00" fill="#00FF00" color="#00FF00"><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">You are a mutation tool. This is a Python function and its description.
    Please mutate the description by enhancing its clarity and comprehension for sophisticated
    language models.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 请将更改后的描述放在#Explanation和#End之间。最多使用600个单词。</foreignobject></g> <g stroke="#00FF00"
    fill="#00FF00" color="#00FF00"><foreignobject width="600.47" height="602.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">你是一个变异工具。这是一个Python函数及其描述。请通过提高其清晰度和对高级语言模型的理解，来变异描述。
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g> <g stroke="#FFFF00" fill="#FFFF00" color="#FFFF00"><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 请将更改后的描述放在#Explanation和#End之间。最多使用600个单词。</foreignobject></g> <g stroke="#FFFF00"
    fill="#FFFF00" color="#FFFF00"><foreignobject width="600.47" height="602.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">你是一个变异工具。这是一个Python函数及其描述。请通过提高其清晰度和对高级语言模型的理解，来变异描述。
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g> <foreignobject width="600.47" height="602.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You are a mutation tool. This
    is a Python function and its description. Please mutate the description by enhancing
    its clarity and comprehension for sophisticated language models.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 请将更改后的描述放在#Explanation和#End之间。最多使用600个单词。</foreignobject></g> <foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">你是一个变异工具。这是一个Python函数及其描述。请通过提高其清晰度和对高级语言模型的理解，来变异描述。
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 78.52 -22.44)"
    fill="#000000" stroke="#000000"><foreignobject width="16.92" height="5.96" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$cost$</foreignobject></g><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 请将更改后的描述放在#Explanation和#End之间。最多使用600个单词。</foreignobject> <g transform="matrix(1.0
    0.0 0.0 1.0 78.52 -22.44)" fill="#000000" stroke="#000000"><foreignobject width="16.92"
    height="5.96" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$cost$</foreignobject></g><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">你是一个变异工具。这是一个Python函数及其描述。请通过提高其清晰度和对高级语言模型的理解，来变异描述。
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(0.0 1.0 -1.0 0.0 -21.81 55.01)"
    fill="#000000" stroke="#000000"><foreignobject width="31.45" height="8.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$pass@1$</foreignobject></g><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">You are a mutation tool. This is a Python function and its description.
    Please mutate the description by enhancing its clarity and comprehension for sophisticated
    language models.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '请将修改后的描述放在 #Explanation 和 #End 之间。使用不超过 600 字。</foreignobject> <g transform="matrix(0.0
    1.0 -1.0 0.0 -21.81 55.01)" fill="#000000" stroke="#000000"><foreignobject width="31.45"
    height="8.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$pass@1$</foreignobject></g><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">您是一个变异工具。这是一个 Python 函数及其描述。请通过提高其清晰度和对复杂语言模型的理解来变异描述。'
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g fill="#FFFFFF" stroke="#000000" transform="matrix(1.0
    0.0 0.0 1.0 89.9 7.29)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0
    47.725)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 6.86)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 24.18 0) translate(25.12,0)
    matrix(1.0 0.0 0.0 1.0 -22.35 -2.64)" fill="#000000" stroke="#000000"><foreignobject
    width="44.7" height="8.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$Reflexion$</foreignobject></g></g></g></g></g></g></svg>'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '请将修改后的描述放在 #Explanation 和 #End 之间。使用不超过 600 字。</foreignobject> <g fill="#FFFFFF"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 89.9 7.29)"><g class="ltx_tikzmatrix"
    transform="matrix(1 0 0 -1 0 47.725)"><g class="ltx_tikzmatrix_row" transform="matrix(1
    0 0 1 0 6.86)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1
    0 0 -1 24.18 0) translate(25.12,0) matrix(1.0 0.0 0.0 1.0 -22.35 -2.64)" fill="#000000"
    stroke="#000000"><foreignobject width="44.7" height="8.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$Reflexion$</foreignobject></g></g></g></g></g></g></svg>'
- en: 'Figure 5: Comparison on HumanEval'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：在 HumanEval 上的比较
- en: <svg id="S5.F6.pic1" class="ltx_picture ltx_centering" height="173.49" overflow="visible"
    version="1.1" width="213.79"><g transform="translate(0,173.49) matrix(1 0 0 -1
    0 0) translate(31.76,0) translate(0,25.67) matrix(1.0 0.0 0.0 1.0 -31.76 -25.67)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g class="ltx_nestedsvg"
    transform="matrix(1 0 0 1 0 0) translate(31.76,0) translate(0,25.67)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g clip-path="url(#pgfcp4)"><g stroke-width="0.2pt"
    fill="#808080" stroke="#808080" color="#808080"><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: <svg id="S5.F6.pic1" class="ltx_picture ltx_centering" height="173.49" overflow="visible"
    version="1.1" width="213.79"><g transform="translate(0,173.49) matrix(1 0 0 -1
    0 0) translate(31.76,0) translate(0,25.67) matrix(1.0 0.0 0.0 1.0 -31.76 -25.67)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g class="ltx_nestedsvg"
    transform="matrix(1 0 0 1 0 0) translate(31.76,0) translate(0,25.67)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g clip-path="url(#pgfcp4)"><g stroke-width="0.2pt"
    fill="#808080" stroke="#808080" color="#808080"><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">您是一个变异工具。这是一个
    Python 函数及其描述。请通过提高其清晰度和对复杂语言模型的理解来变异描述。
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g></g> <g clip-path="url(#pgfcp5)"><g stroke-width="0.2pt"
    fill="#808080" stroke="#808080" color="#808080"><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '请将修改后的描述放在 #Explanation 和 #End 之间。使用不超过 600 字。</foreignobject></g></g> <g clip-path="url(#pgfcp5)"><g
    stroke-width="0.2pt" fill="#808080" stroke="#808080" color="#808080"><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">您是一个变异工具。这是一个 Python 函数及其描述。请通过提高其清晰度和对复杂语言模型的理解来变异描述。'
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g></g> <foreignobject width="600.47" height="602.45"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You are
    a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 请将更改后的描述放在#Explanation和#End之间。最多使用600字。</foreignobject></g></g> <foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">你是一个变异工具。这是一个Python函数及其描述。请通过提高描述的清晰度和理解度来变异该描述，以便适应复杂的语言模型。
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <foreignobject width="600.47" height="602.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You are a mutation tool. This
    is a Python function and its description. Please mutate the description by enhancing
    its clarity and comprehension for sophisticated language models.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 请将更改后的描述放在#Explanation和#End之间。最多使用600字。</foreignobject> <foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">你是一个变异工具。这是一个Python函数及其描述。请通过提高描述的清晰度和理解度来变异该描述，以便适应复杂的语言模型。
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 -2.42 -9.75)"
    fill="#000000" stroke="#000000"><foreignobject width="4.84" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$0$</foreignobject></g><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 请将更改后的描述放在#Explanation和#End之间。最多使用600字。</foreignobject> <g transform="matrix(1.0
    0.0 0.0 1.0 -2.42 -9.75)" fill="#000000" stroke="#000000"><foreignobject width="4.84"
    height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$0$</foreignobject></g><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">你是一个变异工具。这是一个Python函数及其描述。请通过提高描述的清晰度和理解度来变异该描述，以便适应复杂的语言模型。
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 41.07 -9.75)"
    fill="#000000" stroke="#000000"><foreignobject width="4.84" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$5$</foreignobject></g><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 请将更改后的描述放在#Explanation和#End之间。最多使用600字。</foreignobject> <g transform="matrix(1.0
    0.0 0.0 1.0 41.07 -9.75)" fill="#000000" stroke="#000000"><foreignobject width="4.84"
    height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$5$</foreignobject></g><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">你是一个变异工具。这是一个Python函数及其描述。请通过提高描述的清晰度和理解度来变异该描述，以便适应复杂的语言模型。
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 82.13 -9.75)"
    fill="#000000" stroke="#000000"><foreignobject width="9.69" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$10$</foreignobject></g><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 请将更改后的描述放在#Explanation和#End之间。最多使用600字。</foreignobject> <g transform="matrix(1.0
    0.0 0.0 1.0 82.13 -9.75)" fill="#000000" stroke="#000000"><foreignobject width="9.69"
    height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$10$</foreignobject></g><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">你是一个变异工具。这是一个Python函数及其描述。请通过提高描述的清晰度和理解度来变异该描述，以便适应复杂的语言模型。
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 125.62 -9.75)"
    fill="#000000" stroke="#000000"><foreignobject width="9.69" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$15$</foreignobject></g><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 请将修改后的描述放在#Explanation和#End之间。使用不超过600个单词。<foreignobject> <g transform="matrix(1.0
    0.0 0.0 1.0 125.62 -9.75)" fill="#000000" stroke="#000000"><foreignobject width="9.69"
    height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$15$</foreignobject></g><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">你是一个变异工具。这是一个Python函数及其描述。请通过提高描述的清晰度和理解性来变异描述，以适应复杂的语言模型。
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 169.11 -9.75)"
    fill="#000000" stroke="#000000"><foreignobject width="9.69" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$20$</foreignobject></g><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 请将修改后的描述放在#Explanation和#End之间。使用不超过600个单词。<foreignobject> <g transform="matrix(1.0
    0.0 0.0 1.0 169.11 -9.75)" fill="#000000" stroke="#000000"><foreignobject width="9.69"
    height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$20$</foreignobject></g><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">你是一个变异工具。这是一个Python函数及其描述。请通过提高描述的清晰度和理解性来变异描述，以适应复杂的语言模型。
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 -13.19 -3.12)"
    fill="#000000" stroke="#000000"><foreignobject width="9.69" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$70$</foreignobject></g><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 请将修改后的描述放在#Explanation和#End之间。使用不超过600个单词。<foreignobject> <g transform="matrix(1.0
    0.0 0.0 1.0 -13.19 -3.12)" fill="#000000" stroke="#000000"><foreignobject width="9.69"
    height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$70$</foreignobject></g><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">你是一个变异工具。这是一个Python函数及其描述。请通过提高描述的清晰度和理解性来变异描述，以适应复杂的语言模型。
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 -13.19 25.17)"
    fill="#000000" stroke="#000000"><foreignobject width="9.69" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$72$</foreignobject></g><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 请将修改后的描述放在#Explanation和#End之间。使用不超过600个单词。<foreignobject> <g transform="matrix(1.0
    0.0 0.0 1.0 -13.19 25.17)" fill="#000000" stroke="#000000"><foreignobject width="9.69"
    height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$72$</foreignobject></g><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">你是一个变异工具。这是一个Python函数及其描述。请通过提高描述的清晰度和理解性来变异描述，以适应复杂的语言模型。
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 -13.19 53.47)"
    fill="#000000" stroke="#000000"><foreignobject width="9.69" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$74$</foreignobject></g><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '请将更改后的描述放置在 #Explanation 和 #End 之间。最多使用 600 个单词。</foreignobject> <g transform="matrix(1.0
    0.0 0.0 1.0 -13.19 53.47)" fill="#000000" stroke="#000000"><foreignobject width="9.69"
    height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$74$</foreignobject></g><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">你是一个变异工具。这是一个 Python 函数及其描述。请通过提高描述的清晰度和理解度来变异它，以适应高级语言模型。'
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 -13.19 81.76)"
    fill="#000000" stroke="#000000"><foreignobject width="9.69" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$76$</foreignobject></g><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '请将更改后的描述放置在 #Explanation 和 #End 之间。最多使用 600 个单词。</foreignobject> <g transform="matrix(1.0
    0.0 0.0 1.0 -13.19 81.76)" fill="#000000" stroke="#000000"><foreignobject width="9.69"
    height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$76$</foreignobject></g><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">你是一个变异工具。这是一个 Python 函数及其描述。请通过提高描述的清晰度和理解度来变异它，以适应高级语言模型。'
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 -13.19 110.06)"
    fill="#000000" stroke="#000000"><foreignobject width="9.69" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$78$</foreignobject></g><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '请将更改后的描述放置在 #Explanation 和 #End 之间。最多使用 600 个单词。</foreignobject> <g transform="matrix(1.0
    0.0 0.0 1.0 -13.19 110.06)" fill="#000000" stroke="#000000"><foreignobject width="9.69"
    height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$78$</foreignobject></g><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">你是一个变异工具。这是一个 Python 函数及其描述。请通过提高描述的清晰度和理解度来变异它，以适应高级语言模型。'
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 -13.19 138.35)"
    fill="#000000" stroke="#000000"><foreignobject width="9.69" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$80$</foreignobject></g><g clip-path="url(#pgfcp6)"><g
    stroke="#FF0000" fill="#FF0000"><foreignobject width="600.47" height="602.45"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You are
    a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '请将更改后的描述放置在 #Explanation 和 #End 之间。最多使用 600 个单词。</foreignobject> <g transform="matrix(1.0
    0.0 0.0 1.0 -13.19 138.35)" fill="#000000" stroke="#000000"><foreignobject width="9.69"
    height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$80$</foreignobject></g><g
    clip-path="url(#pgfcp6)"><g stroke="#FF0000" fill="#FF0000"><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">你是一个变异工具。这是一个
    Python 函数及其描述。请通过提高描述的清晰度和理解度来变异它，以适应高级语言模型。'
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g> <g stroke="#0000FF" fill="#0000FF"><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">You are a mutation tool. This is a Python function and its description.
    Please mutate the description by enhancing its clarity and comprehension for sophisticated
    language models.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '请将更改后的描述放置在 #Explanation 和 #End 之间。最多使用 600 个单词。</foreignobject></g> <g stroke="#0000FF"
    fill="#0000FF"><foreignobject width="600.47" height="602.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">你是一个变异工具。这是一个 Python 函数及其描述。请通过提高描述的清晰度和理解度来变异它，以适应高级语言模型。'
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g> <g stroke="#00FF00" fill="#00FF00"><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">You are a mutation tool. This is a Python function and its description.
    Please mutate the description by enhancing its clarity and comprehension for sophisticated
    language models.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 请将修改后的描述放在#Explanation和#End之间。最多使用600个单词。</foreignobject></g> <g stroke="#00FF00"
    fill="#00FF00"><foreignobject width="600.47" height="602.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">你是一个变异工具。这是一个Python函数及其描述。请通过提高描述的清晰度和理解度来变异描述，以适应复杂的语言模型。
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g> <g stroke="#FFFF00" fill="#FFFF00"><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">You are a mutation tool. This is a Python function and its description.
    Please mutate the description by enhancing its clarity and comprehension for sophisticated
    language models.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 请将修改后的描述放在#Explanation和#End之间。最多使用600个单词。</foreignobject></g> <g stroke="#FFFF00"
    fill="#FFFF00"><foreignobject width="600.47" height="602.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">你是一个变异工具。这是一个Python函数及其描述。请通过提高描述的清晰度和理解度来变异描述，以适应复杂的语言模型。
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g></g> <g stroke="#FF0000" fill="#FF0000" color="#FF0000"><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">You are a mutation tool. This is a Python function and its description.
    Please mutate the description by enhancing its clarity and comprehension for sophisticated
    language models.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 请将修改后的描述放在#Explanation和#End之间。最多使用600个单词。</foreignobject></g></g> <g stroke="#FF0000"
    fill="#FF0000" color="#FF0000"><foreignobject width="600.47" height="602.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">你是一个变异工具。这是一个Python函数及其描述。请通过提高描述的清晰度和理解度来变异描述，以适应复杂的语言模型。
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g> <g stroke="#0000FF" fill="#0000FF" color="#0000FF"><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">You are a mutation tool. This is a Python function and its description.
    Please mutate the description by enhancing its clarity and comprehension for sophisticated
    language models.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 请将修改后的描述放在#Explanation和#End之间。最多使用600个单词。</foreignobject></g> <g stroke="#0000FF"
    fill="#0000FF" color="#0000FF"><foreignobject width="600.47" height="602.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">你是一个变异工具。这是一个Python函数及其描述。请通过提高描述的清晰度和理解度来变异描述，以适应复杂的语言模型。
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g> <g stroke="#00FF00" fill="#00FF00" color="#00FF00"><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">You are a mutation tool. This is a Python function and its description.
    Please mutate the description by enhancing its clarity and comprehension for sophisticated
    language models.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 请将修改后的描述放在#Explanation和#End之间。最多使用600个单词。</foreignobject></g> <g stroke="#00FF00"
    fill="#00FF00" color="#00FF00"><foreignobject width="600.47" height="602.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">你是一个变异工具。这是一个Python函数及其描述。请通过提高描述的清晰度和理解度来变异描述，以适应复杂的语言模型。
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g> <g stroke="#FFFF00" fill="#FFFF00" color="#FFFF00"><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 请将修改后的描述放在#Explanation和#End之间。最多使用600个单词。</foreignobject></g> <g stroke="#FFFF00"
    fill="#FFFF00" color="#FFFF00"><foreignobject width="600.47" height="602.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">你是一个变异工具。这是一个Python函数及其描述。请通过提高描述的清晰度和理解度来变异描述，以适应复杂的语言模型。
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g> <foreignobject width="600.47" height="602.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You are a mutation tool. This
    is a Python function and its description. Please mutate the description by enhancing
    its clarity and comprehension for sophisticated language models.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '请将更改后的描述放在 #Explanation 和 #End 之间。最多使用 600 个单词。</foreignobject></g> <foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">您是一个变异工具。这是一个 Python 函数及其描述。请通过提高描述的清晰度和理解力来变异该描述，以适应高级语言模型。'
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 78.52 -22.44)"
    fill="#000000" stroke="#000000"><foreignobject width="16.92" height="5.96" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$cost$</foreignobject></g><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '请将更改后的描述放在 #Explanation 和 #End 之间。最多使用 600 个单词。</foreignobject> <g transform="matrix(1.0
    0.0 0.0 1.0 78.52 -22.44)" fill="#000000" stroke="#000000"><foreignobject width="16.92"
    height="5.96" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$cost$</foreignobject></g><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">您是一个变异工具。这是一个 Python 函数及其描述。请通过提高描述的清晰度和理解力来变异该描述，以适应高级语言模型。'
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(0.0 1.0 -1.0 0.0 -21.81 55.01)"
    fill="#000000" stroke="#000000"><foreignobject width="31.45" height="8.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$pass@1$</foreignobject></g><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">You are a mutation tool. This is a Python function and its description.
    Please mutate the description by enhancing its clarity and comprehension for sophisticated
    language models.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '请将更改后的描述放在 #Explanation 和 #End 之间。最多使用 600 个单词。</foreignobject> <g transform="matrix(0.0
    1.0 -1.0 0.0 -21.81 55.01)" fill="#000000" stroke="#000000"><foreignobject width="31.45"
    height="8.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$pass@1$</foreignobject></g><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">您是一个变异工具。这是一个 Python 函数及其描述。请通过提高描述的清晰度和理解力来变异该描述，以适应高级语言模型。'
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g fill="#FFFFFF" stroke="#000000" transform="matrix(1.0
    0.0 0.0 1.0 89.9 7.29)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0
    47.725)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 6.86)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 24.18 0) translate(25.12,0)
    matrix(1.0 0.0 0.0 1.0 -22.35 -2.64)" fill="#000000" stroke="#000000"><foreignobject
    width="44.7" height="8.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$Reflexion$</foreignobject></g></g></g></g></g></g></svg>'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '请将更改后的描述放在 #Explanation 和 #End 之间。最多使用 600 个单词。</foreignobject> <g fill="#FFFFFF"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 89.9 7.29)"><g class="ltx_tikzmatrix"
    transform="matrix(1 0 0 -1 0 47.725)"><g class="ltx_tikzmatrix_row" transform="matrix(1
    0 0 1 0 6.86)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1
    0 0 -1 24.18 0) translate(25.12,0) matrix(1.0 0.0 0.0 1.0 -22.35 -2.64)" fill="#000000"
    stroke="#000000"><foreignobject width="44.7" height="8.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$Reflexion$</foreignobject></g></g></g></g></g></g></svg>'
- en: 'Figure 6: Comparison on MBPP'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：MBPP 比较
- en: '5.2 RQ2: How effective is EPiC in code generation?'
  id: totrans-327
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '5.2 RQ2: EPiC 在代码生成中效果如何？'
- en: 'In addressing RQ2.1, we accounted for the inherent randomness in our algorithm
    by testing EPiC with different seeds. The seed was utilized in Python’s random
    function, allowing us to control the algorithm’s randomness by setting the seed
    to a predefined value. Specifically, randomness was employed in lines 22 and 26
    of Algorithm [1](#alg1 "Algorithm 1 ‣ 3 Evolutionary Prompt Engineering for Code
    ‣ EPiC: Cost-effective Search-based Prompt Engineering of LLMs for Code Generation")
    and lines 15 and 18 of Algorithm [2](#alg2 "Algorithm 2 ‣ 3 Evolutionary Prompt
    Engineering for Code ‣ EPiC: Cost-effective Search-based Prompt Engineering of
    LLMs for Code Generation"). We run EPiC with 10 unique random seeds, and we report
    the median, minimum, and maximum pass@1 metrics across all runs. In this RQ, we
    empirically compare the results with those from RQ1 to assess the cost-effectiveness
    of EPiC compared to the SOTA.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '在处理 RQ2.1 时，我们通过使用不同的种子来考虑算法固有的随机性。种子被用于 Python 的随机函数，使我们能够通过将种子设置为预定义值来控制算法的随机性。具体而言，随机性被应用于算法
    [1](#alg1 "算法 1 ‣ 3 进化提示工程 ‣ EPiC: 成本效益的基于搜索的 LLM 提示工程") 的第 22 行和第 26 行，以及算法 [2](#alg2
    "算法 2 ‣ 3 进化提示工程 ‣ EPiC: 成本效益的基于搜索的 LLM 提示工程") 的第 15 行和第 18 行。我们使用 10 个独特的随机种子运行
    EPiC，并报告所有运行中的中位数、最小值和最大值 pass@1 指标。在此 RQ 中，我们通过与 RQ1 的结果进行实证比较，以评估 EPiC 相比于现有技术的成本效益。'
- en: '5.2.1 RQ2.1: How cost-effective is EPiC compared to the SOTA?'
  id: totrans-329
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '5.2.1 RQ2.1: EPiC 相比于现有技术，成本效益如何？'
- en: 'We evaluated EPiC on HumanEval and MBPP, each 10 times using 10 unique random
    seeds. The statistical results for this experiment are presented in Table [III](#S4.T3
    "TABLE III ‣ 4.6 Experimental Setup ‣ 4 Experiment Design ‣ EPiC: Cost-effective
    Search-based Prompt Engineering of LLMs for Code Generation"). The results on
    HumanEval indicate that the median $pass@1$) and the SOTA on Humaneval is presented
    in Figure [5](#S5.F5 "Figure 5 ‣ 5.1 RQ1: How cost-effective are the SOTA LLM-based
    code generation tools? ‣ 5 Experiment Results ‣ EPiC: Cost-effective Search-based
    Prompt Engineering of LLMs for Code Generation").'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在 HumanEval 和 MBPP 上评估了 EPiC，每个数据集使用 10 个独特的随机种子进行了 10 次测试。此实验的统计结果见表 [III](#S4.T3
    "表 III ‣ 4.6 实验设置 ‣ 4 实验设计 ‣ EPiC: 成本效益的基于搜索的 LLM 提示工程")。在 HumanEval 上的结果表明，中位数
    $pass@1$ 和现有技术的 SOTA 如图 [5](#S5.F5 "图 5 ‣ 5.1 RQ1: 现有技术的 LLM 基于代码生成工具的成本效益如何？
    ‣ 5 实验结果 ‣ EPiC: 成本效益的基于搜索的 LLM 提示工程") 所示。'
- en: 'In terms of cost, EPiC demonstrated a much lower cost than LATS, and almost
    the same cost as LDB, although they ranked after Reflexion. Nonetheless, EPiC
    displayed considerably better performance with a slightly higher cost compared
    to Reflexion. This cost is significantly lower than that of LATS and is comparable
    to LDB. As shown in Figure [5](#S5.F5 "Figure 5 ‣ 5.1 RQ1: How cost-effective
    are the SOTA LLM-based code generation tools? ‣ 5 Experiment Results ‣ EPiC: Cost-effective
    Search-based Prompt Engineering of LLMs for Code Generation"), EPiC achieves the
    best performance with a marginally higher cost than the lowest-cost baseline,
    making it the most cost-effective approach among the SOTA. We also conducted a
    one-sample Wilcoxon test over the 10 runs of EPiC. Our analysis of results on
    the HumanEval dataset revealed that the pass@1 differences reported for EPiC compared
    to Reflexion, LATS, and LDB are statistically significant, with a p-value of 0.0025.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '从成本角度来看，EPiC 显示出比 LATS 低得多的成本，几乎与 LDB 相同，尽管它们的排名在 Reflexion 之后。然而，EPiC 的表现明显优于
    Reflexion，尽管其成本略高。这一成本显著低于 LATS，并且与 LDB 相当。如图 [5](#S5.F5 "图 5 ‣ 5.1 RQ1: 现有技术的
    LLM 基于代码生成工具的成本效益如何？ ‣ 5 实验结果 ‣ EPiC: 成本效益的基于搜索的 LLM 提示工程") 所示，EPiC 在成本略高于最低成本基线的情况下实现了最佳性能，使其成为现有技术中最具成本效益的方法。我们还对
    EPiC 进行了 10 次运行的一样本 Wilcoxon 检验。对 HumanEval 数据集的结果分析表明，EPiC 与 Reflexion、LATS 和
    LDB 的 pass@1 差异具有统计学意义，p 值为 0.0025。'
- en: Similarly, on the MBPP dataset, EPiC’s median $pass@1$1). This finding is consistent
    with results from the HumanEval dataset, further underscoring the robustness of
    the EPiC. Our statistical analysis of the MBPP dataset demonstrates that the differences
    between EPiC’s pass@1 results over the 10 runs and the pass@1 results of the baselines
    are again statistically significant, with a p-value of 0.0025 for all three baselines.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在MBPP数据集上，EPiC的中位数$pass@1$1）。这一发现与HumanEval数据集的结果一致，进一步强调了EPiC的稳健性。我们对MBPP数据集的统计分析表明，EPiC的pass@1结果在10次运行中的差异与基线结果之间的差异再次具有统计显著性，对所有三个基线的p值为0.0025。
- en: In conclusion, the robustness of the EPiC is highlighted by its consistent performance
    on the HumanEval and MBPP datasets. On HumanEval, EPiC achieved a %93.5 $pass@1$
    was %79 with a variance of 0.55, showcasing its reliability.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，EPiC的稳健性通过其在HumanEval和MBPP数据集上的一致表现得到了突出。在HumanEval中，EPiC取得了93.5%的$pass@1$，而$pass@1$为79，方差为0.55，展示了其可靠性。
- en: 'TABLE IV: Performance of EPiC on Humaneval with an open-source LLM, i.e., magiccoder'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 表IV：EPiC在HumanEval上与开源LLM（即magiccoder）的性能
- en: '| $LLM$ |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| $LLM$ |'
- en: '| --- | --- | --- |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| $magiccoder$ |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| $magiccoder$ |'
- en: '| $magiccoder$ |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| $magiccoder$ |'
- en: 'TABLE V: The impact of different design choices of EPiC on HumanEval. The highlighted
    cells indicate the changes per approach relative to the base settings.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 表V：EPiC在HumanEval上的不同设计选择的影响。突出显示的单元格表示相对于基本设置的每种方法的变化
- en: '| $row$ |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| $row$ |'
- en: '| $1$ |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| $1$ |'
- en: '| $2$ |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| $2$ |'
- en: '| $3$ |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| $3$ |'
- en: '| $4$ |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| $4$ |'
- en: '| $5$ |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| $5$ |'
- en: '| $6$ |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| $6$ |'
- en: '| $7$ |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| $7$ |'
- en: '| $8$ |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| $8$ |'
- en: '5.2.2 RQ2.2: How effective is EPiC on smaller open-source LLMs compared to
    larger closed-source models?'
  id: totrans-349
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 RQ2.2：与较大的封闭源模型相比，EPiC在较小的开源LLM上的效果如何？
- en: In this RQ, we replaced EPiC’s LLM component, i.e., GPT4o, with a smaller open-source
    model, i.e., MagicCoder [[11](#bib.bib11)]. All other settings remain the same
    as in RQ2.1\. We chose MagicCoder because it has the highest performance, in the
    literature, among open-source LLMs at the time of designing these experiments.
    Note that given the low variance of the results in RQ2.1, over the 10 runs, we
    only ran this experiment once. Due to extensive cost, we also limit this experiment
    to the HumanEval dataset.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个RQ中，我们将EPiC的LLM组件，即GPT4o，替换为一个较小的开源模型，即MagicCoder [[11](#bib.bib11)]。所有其他设置与RQ2.1中保持一致。我们选择MagicCoder是因为它在设计这些实验时，在开源LLM中具有最高的性能。请注意，由于RQ2.1结果的方差较低，在10次运行中，我们只进行了此实验一次。由于高昂的成本，我们也将此实验限制在HumanEval数据集上。
- en: We evaluated the performance of $magiccoder$ has 7 billion parameters, making
    it significantly less complex than OpenAI’s model. However, this experiment demonstrates
    that the language understanding of smaller models is sufficiently sophisticated
    that a prompt mutation strategy can enhance its performance. According to the
    results, EPiC proves to be an effective approach even when applied to smaller
    LLMs.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 我们评估了拥有70亿参数的$magiccoder$的性能，这使得它在复杂性上明显低于OpenAI的模型。然而，这项实验表明，较小模型的语言理解足够复杂，提示变异策略可以提高其性能。根据结果，EPiC证明了即使应用于较小的LLM，仍然是一种有效的方法。
- en: '<svg id="S5.SS2.SSS2.p3.pic1" class="ltx_picture" height="103.59" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,103.59) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="76.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Answer
    to RQ2: EPiC outperforms the SOTA baselines by $\%1$) by %9 on HumanEval.</foreignobject></g></g></svg>'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg id="S5.SS2.SSS2.p3.pic1" class="ltx_picture" height="103.59" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,103.59) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="76.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Answer
    to RQ2: EPiC outperforms the SOTA baselines by $\%1$) by %9 on HumanEval.</foreignobject></g></g></svg>'
- en: '5.3 RQ3: What are the impacts of different EA design choices on the cost-effectiveness
    of EPiC?'
  id: totrans-353
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 RQ3：不同EA设计选择对EPiC成本效益的影响是什么？
- en: In this RQ, we measure the impact of different design choices (hyper-parameters)
    of EPiC on HumanEval. It is important to emphasize that the goal of this question
    is to analyze the sensitivity of the results over each hyper-parameter’s value.
    Thus we can independently analyze each hyper-parameter. In other words, we do
    NOT aim to optimize the hyper-parameters here, since that fine-tuning would require
    much more extensive study (e.g., a grid search over all combinations).
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个RQ中，我们测量了EPiC不同设计选择（超参数）对HumanEval的影响。重要的是强调，这个问题的目标是分析每个超参数值对结果的敏感性。因此，我们可以独立分析每个超参数。换句话说，我们在这里并不打算优化超参数，因为那样的微调需要更广泛的研究（例如，对所有组合进行网格搜索）。
- en: In addition, note that given the low variance of the results in RQ2.1, over
    the 10 runs, we only run the RQ3 experiment once, and due to its extensive cost
    (multiple runs per hyper-parameter), we also limit this RQ to the HumanEval dataset.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意，由于RQ2.1结果的方差较低，在10次运行中，我们只进行了RQ3实验一次，由于其高昂的成本（每个超参数多个运行），我们也将此RQ限制在HumanEval数据集上。
- en: 'The first row in Table [V](#S5.T5 "TABLE V ‣ 5.2.1 RQ2.1: How cost-effective
    is EPiC compared to the SOTA? ‣ 5.2 RQ2: How effective is EPiC in code generation?
    ‣ 5 Experiment Results ‣ EPiC: Cost-effective Search-based Prompt Engineering
    of LLMs for Code Generation") represents the base configuration selected for this
    RQ. We set $mutation\_tool$ was selected for the similarity word selection method
    as a robust option. We again explore this in more detail in RQ3.3.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 [V](#S5.T5 "TABLE V ‣ 5.2.1 RQ2.1: How cost-effective is EPiC compared to
    the SOTA? ‣ 5.2 RQ2: How effective is EPiC in code generation? ‣ 5 Experiment
    Results ‣ EPiC: Cost-effective Search-based Prompt Engineering of LLMs for Code
    Generation") 中的第一行代表了为这个RQ选择的基础配置。我们选择了$mutation\_tool$作为相似词选择方法的稳健选项。我们将在RQ3.3中更详细地探讨这一点。'
- en: '5.3.1 RQ3.1: How does the complexity of the mutation generator impact the cost-effectiveness
    of EPiC?'
  id: totrans-357
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1 RQ3.1：突变生成器的复杂性如何影响EPiC的性价比？
- en: 'We compared two mutation generator methods: (LLM_as_mutator and sim_words_as_mutator).
    LLM_as_mutator utilizes an LLM in the mutation process, whereas sim_words_as_mutator
    employs locally calculated similarity matrices for mutation. The first method
    is more advanced but more costly too, while the second approach, significantly
    simpler, incurs minimal cost.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 我们比较了两种突变生成器方法：（LLM_as_mutator 和 sim_words_as_mutator）。LLM_as_mutator在突变过程中利用LLM，而sim_words_as_mutator则使用局部计算的相似性矩阵进行突变。第一种方法更先进但成本更高，而第二种方法则明显更简单，成本较低。
- en: 'The results for this RQ are presented in rows 1 and 2 of Table [V](#S5.T5 "TABLE
    V ‣ 5.2.1 RQ2.1: How cost-effective is EPiC compared to the SOTA? ‣ 5.2 RQ2: How
    effective is EPiC in code generation? ‣ 5 Experiment Results ‣ EPiC: Cost-effective
    Search-based Prompt Engineering of LLMs for Code Generation"). We observed a $\%94$
    involves higher costs and is expected to produce better prompts. The selection
    of the prompt used in the mutation process also influences the results. The prompt
    may be an instruction to simply alter the words in the text or it can do more
    complex modifications, such as elaborating on or summarizing the original text.
    As explained in Section [3.2.2](#S3.SS2.SSS2 "3.2.2 Prompt Mutation ‣ 3.2 Evolutionary
    Prompt Engineering ‣ 3 Evolutionary Prompt Engineering for Code ‣ EPiC: Cost-effective
    Search-based Prompt Engineering of LLMs for Code Generation"), we used a prompt
    to elaborate on the original text for the mutation process. We leave the study
    on the choice of prompt for mutation to future research, as it falls outside the
    scope of this paper.'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '该RQ的结果见表格 [V](#S5.T5 "TABLE V ‣ 5.2.1 RQ2.1: How cost-effective is EPiC compared
    to the SOTA? ‣ 5.2 RQ2: How effective is EPiC in code generation? ‣ 5 Experiment
    Results ‣ EPiC: Cost-effective Search-based Prompt Engineering of LLMs for Code
    Generation") 的第1和第2行。我们观察到$\%94$涉及更高的成本，并且预期会产生更好的提示。突变过程中使用的提示也会影响结果。提示可能是简单地改变文本中的词，或者可以做更复杂的修改，例如详细说明或总结原始文本。如在[3.2.2节](#S3.SS2.SSS2
    "3.2.2 Prompt Mutation ‣ 3.2 Evolutionary Prompt Engineering ‣ 3 Evolutionary
    Prompt Engineering for Code ‣ EPiC: Cost-effective Search-based Prompt Engineering
    of LLMs for Code Generation")中所述，我们使用了一个提示来详细说明原始文本以进行突变。我们将突变提示的选择研究留待未来的研究，因为这超出了本文的范围。'
- en: '5.3.2 RQ3.2: How do the mutation probability and similar word selection method
    impact the performance of EPiC?'
  id: totrans-360
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.2 RQ3.2：突变概率和相似词选择方法如何影响EPiC的性能？
- en: The mutation probability defines the likelihood of a word being substituted
    with a similar word. We set the mutation_probability to $0.1$, we always select
    the word with the highest similarity, which avoids randomness when selecting the
    mutant word.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 突变概率定义了一个词被替换为相似词的可能性。我们将mutation_probability设置为$0.1$，始终选择最相似的词，这样在选择突变词时避免了随机性。
- en: 'The results for this research question are presented in Table [V](#S5.T5 "TABLE
    V ‣ 5.2.1 RQ2.1: How cost-effective is EPiC compared to the SOTA? ‣ 5.2 RQ2: How
    effective is EPiC in code generation? ‣ 5 Experiment Results ‣ EPiC: Cost-effective
    Search-based Prompt Engineering of LLMs for Code Generation") (rows 3, 4, 5, 6,
    and 7). We employed various mutation probabilities of $0.25$, indicating that
    the most similar word is the only selected word, which eliminates randomness in
    the word selection process.'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '该研究问题的结果见表格 [V](#S5.T5 "TABLE V ‣ 5.2.1 RQ2.1: How cost-effective is EPiC compared
    to the SOTA? ‣ 5.2 RQ2: How effective is EPiC in code generation? ‣ 5 Experiment
    Results ‣ EPiC: Cost-effective Search-based Prompt Engineering of LLMs for Code
    Generation")（第3、4、5、6和7行）。我们使用了各种突变概率$0.25$，这表明最相似的词是唯一选择的词，从而消除了选择过程中的随机性。'
- en: When the algorithm included randomness, the $pass@1$, indicating a decrease
    in performance.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 当算法包括随机性时，$pass@1$表明性能下降。
- en: These experiments suggest that incorporating randomness into EPiC may enhance
    its ability to identify correct solutions. This is likely because the algorithm
    can explore the search space more effectively, thereby avoiding local optima in
    certain instances.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 这些实验表明，将随机性引入EPiC可能增强其识别正确解决方案的能力。这可能是因为算法可以更有效地探索搜索空间，从而避免某些情况下的局部最优。
- en: Although our threshold-based $sim\_word\_selection$ word selection method is
    in theory a more robust solution. Since with the threshold-based method, there
    is a risk that no word may pass the threshold, or conversely, too many words may
    pass the threshold. In contrast, the fixed number method consistently ensures
    that the selection occurs among a predetermined number of similar words, providing
    greater reliability on the search space size.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的基于阈值的$sim\_word\_selection$词汇选择方法在理论上是一种更稳健的解决方案，但由于使用阈值方法可能没有任何词汇通过阈值，或者相反，太多词汇可能通过阈值。相比之下，固定数量方法始终确保选择发生在预定数量的相似词汇中，从而在搜索空间大小上提供更大的可靠性。
- en: '5.3.3 RQ3.3: How does population size impact the cost-effectiveness of EPiC?'
  id: totrans-366
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '5.3.3 RQ3.3: 人口规模如何影响EPiC的成本效益？'
- en: The population size defines the number of prompts in each generation. This plays
    an important role in the cost of EPiC. We tested different population sizes, specifically
    $5$, to evaluate the contribution of population size to the cost-effectiveness
    of EPiC.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 人口规模定义了每一代中的提示数量。这在EPiC的成本中发挥了重要作用。我们测试了不同的人口规模，特别是$5$，以评估人口规模对EPiC成本效益的贡献。
- en: 'The population size is a critical parameter in the EPiC algorithm, significantly
    influencing both its cost and performance. In our final experiment, as detailed
    in Table [V](#S5.T5 "TABLE V ‣ 5.2.1 RQ2.1: How cost-effective is EPiC compared
    to the SOTA? ‣ 5.2 RQ2: How effective is EPiC in code generation? ‣ 5 Experiment
    Results ‣ EPiC: Cost-effective Search-based Prompt Engineering of LLMs for Code
    Generation"), we set the population size to $10$ metric showed a modest improvement
    of %1\. This observation underscores a fundamental aspect of population-based
    optimization algorithms: larger population sizes generally enhance the likelihood
    of identifying the correct solution before the algorithm reaches its stopping
    criteria. However, this benefit comes with a trade-off. Increasing the population
    size also increases the associated computational cost.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '人口规模是EPiC算法中的一个关键参数，显著影响其成本和性能。在我们最终的实验中，如表[V](#S5.T5 "TABLE V ‣ 5.2.1 RQ2.1:
    How cost-effective is EPiC compared to the SOTA? ‣ 5.2 RQ2: How effective is EPiC
    in code generation? ‣ 5 Experiment Results ‣ EPiC: Cost-effective Search-based
    Prompt Engineering of LLMs for Code Generation")所示，我们将人口规模设置为$10$，该指标表现出%1的适度改善。这一观察突出了基于人口的优化算法的一个基本方面：较大的人口规模通常增加了在算法达到停止标准之前识别正确解决方案的可能性。然而，这种好处伴随着权衡。增加人口规模也会增加相关的计算成本。'
- en: In our specific experiment, a population size of $5$. It is essential to note,
    though, that this conclusion is dataset-dependent. The optimal population size
    for cost-effectiveness and performance may vary across different datasets, necessitating
    further empirical studies to generalize this finding.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的具体实验中，人口规模为$5$。然而，需要注意的是，这一结论是数据集依赖的。不同数据集中的成本效益和性能的最佳人口规模可能会有所不同，需进一步的实证研究以推广这一发现。
- en: Thus, when configuring the EPiC algorithm, one must carefully consider the trade-off
    between the desired accuracy and the computational cost, tailoring the population
    size to the specific characteristics of the dataset and of the task at hand.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在配置EPiC算法时，必须仔细考虑所需准确性与计算成本之间的权衡，将人口规模调整为数据集和任务特征的具体情况。
- en: 'In our case, we chose the most cost-effective configuration from Table [V](#S5.T5
    "TABLE V ‣ 5.2.1 RQ2.1: How cost-effective is EPiC compared to the SOTA? ‣ 5.2
    RQ2: How effective is EPiC in code generation? ‣ 5 Experiment Results ‣ EPiC:
    Cost-effective Search-based Prompt Engineering of LLMs for Code Generation") as
    the default configuration for experiments in RQ2\. But as explained in Section
    [6](#S6 "6 Threats to Validity ‣ EPiC: Cost-effective Search-based Prompt Engineering
    of LLMs for Code Generation") this might not be the most optimal configuration.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '在我们的案例中，我们选择了表 [V](#S5.T5 "TABLE V ‣ 5.2.1 RQ2.1: How cost-effective is EPiC
    compared to the SOTA? ‣ 5.2 RQ2: How effective is EPiC in code generation? ‣ 5
    Experiment Results ‣ EPiC: Cost-effective Search-based Prompt Engineering of LLMs
    for Code Generation") 中的最具成本效益的配置作为 RQ2 实验的默认配置。但正如第 [6](#S6 "6 Threats to Validity
    ‣ EPiC: Cost-effective Search-based Prompt Engineering of LLMs for Code Generation")
    节所解释的，这可能不是最优配置。'
- en: '<svg id="S5.SS3.SSS3.p6.pic1" class="ltx_picture" height="149.75" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,149.75) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="122.19" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Answer
    to RQ3: EPiC’s performance is slightly impacted by the choice of its hyper-parameters.
    Incorporating randomness in mutation probabilities and word selection methods
    enhances EPiC’s performance by $\%1$ on this dataset but also increases computational
    costs, highlighting the need to balance these factors based on specific datasets.
    Certain configuration choices involve a trade-off between cost and performance.
    These choices should be tailored to the particular task and dataset to ensure
    cost-effectiveness.</foreignobject></g></g></svg>'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg id="S5.SS3.SSS3.p6.pic1" class="ltx_picture" height="149.75" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,149.75) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="122.19" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Answer
    to RQ3: EPiC’s performance is slightly impacted by the choice of its hyper-parameters.
    Incorporating randomness in mutation probabilities and word selection methods
    enhances EPiC’s performance by $\%1$ on this dataset but also increases computational
    costs, highlighting the need to balance these factors based on specific datasets.
    Certain configuration choices involve a trade-off between cost and performance.
    These choices should be tailored to the particular task and dataset to ensure
    cost-effectiveness.</foreignobject></g></g></svg>'
- en: 5.4 Limitations
  id: totrans-373
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 限制
- en: This section discusses two potential limitations of this paper.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了本文的两个潜在限制。
- en: Test cases play an important role in code generation. The absence of accurate
    test cases can mislead the process, resulting in incorrect code implementations.
    In our study, we utilized LLM-generated test cases for intermediate evaluations,
    while reserving the original test cases from the dataset for the final evaluation.
    Our findings indicate that using the HumanEval dataset’s test cases for internal
    evaluations increases the $pass@1$ metric by more than %3-4\. This shows the impact
    of accurate test cases on the code generation process and partially explains why
    our results in RQ3 did not show substantial improvement despite changes in algorithm
    configurations.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 测试用例在代码生成中起着重要作用。准确测试用例的缺失可能会误导过程，导致不正确的代码实现。在我们的研究中，我们使用了 LLM 生成的测试用例进行中期评估，同时保留了数据集中原始的测试用例用于最终评估。我们的发现表明，使用
    HumanEval 数据集的测试用例进行内部评估可以将 $pass@1$ 指标提高超过 %3-4。这显示了准确测试用例对代码生成过程的影响，并部分解释了为什么尽管算法配置发生变化，我们在
    RQ3 中的结果没有显示出实质性改善。
- en: Understanding natural language by LLMs differs significantly from that of humans.
    It is well-established that modifying the original prompt can alter the output
    generated by an LLM. The mutation approach employed in EPiC sometimes produces
    prompts that are not easily understandable by humans and programmers to use for
    code generation. However, these mutated prompts can still lead to the generation
    of code with the correct behavior. This experience demonstrates that while mutations
    may render prompts less human-friendly, they do not become incomprehensible to
    LLMs. Instead, such mutations alter the vector representation of the prompts,
    enabling LLMs to generate alternative implementations of the code that may be
    correct. However, this limitation may negatively impact the developers’ understanding
    of the prompt, potentially leading to confusion or misinterpretation when using
    EPiC.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 对自然语言的理解与人类有显著不同。已知修改原始提示会改变 LLM 生成的输出。EPiC 中使用的变异方法有时会产生不易被人类和程序员理解的提示，用于代码生成。然而，这些变异的提示仍然能够生成具有正确行为的代码。这一经验表明，虽然变异可能使提示不那么适合人类，但它们并不会让
    LLM 无法理解。相反，这些变异改变了提示的向量表示，使 LLM 能生成可能正确的代码的替代实现。然而，这种限制可能会对开发者对提示的理解产生负面影响，可能导致在使用
    EPiC 时出现混淆或误解。
- en: 6 Threats to Validity
  id: totrans-377
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 可信度威胁
- en: 'External: One limitation of this research is that the datasets used (HumanEval
    and MBPP) may not fully represent the diversity of real-world coding tasks, which
    limits the generalizability of our findings. In addition, due to its prohibitive
    cost, RQ2.2 and RQ3 were only conducted on one dataset (HumanEval). Although these
    datasets are commonly used benchmarks for code generation with LLMs, there is
    a possibility that their data may have been leaked to both open-source and closed-source
    LLMs. This highlights the need for a new benchmark specifically for code generation
    tasks. Another potential threat is that our findings, based on specific LLMs such
    as GPT-4 and MagicCoder, may not generalize to other LLMs or future models with
    different architectures or training data. We chose an open-source small-size LLM
    and a closed-source large LLM to test our approach, aiming to ensure applicability
    to LLMs of different sizes. However, we cannot guarantee that GPT-4 and MagicCoder
    are the optimal candidates for this evaluation.'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 外部：这项研究的一个限制是使用的数据集（HumanEval和MBPP）可能无法完全代表现实世界编码任务的多样性，这限制了我们发现的普遍性。此外，由于高昂的成本，RQ2.2和RQ3仅在一个数据集（HumanEval）上进行。虽然这些数据集是LLMs代码生成的常用基准，但可能存在数据已泄露到开源和闭源LLMs中的可能性。这突显了需要为代码生成任务制定新的基准。另一个潜在威胁是我们基于特定LLMs（如GPT-4和MagicCoder）的发现可能无法推广到其他LLMs或具有不同架构或训练数据的未来模型。我们选择了一个开源的小型LLM和一个闭源的大型LLM来测试我们的方法，旨在确保对不同大小LLMs的适用性。然而，我们不能保证GPT-4和MagicCoder是此评估的最佳候选者。
- en: 'Conclusion: A potential threat here and in many other related studies using
    LLMs, is the non-deterministic nature of LLMs. Even setting the temperature to
    zero does not eliminate non-determinism, which could potentially invalidate the
    results and pose reproducibility issues. To alleviate this threat, in our study,
    we made sure we did not introduce further robustness issues by injecting more
    randomness through our algorithms. Thus we tested EPiC across multiple runs and
    empirically found our approach’s results to be robust, in that perspective. However,
    given the extensive cost of rerunning the experiments in RQ2.2 and RQ3 (where
    they already have many configurations to test), we skipped multiple runs with
    different seeds. Nonetheless, this might not be a serious threat to validity,
    given the low variance of the results in RQ2.1.'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 结论：这里及许多其他使用LLMs的相关研究中的潜在威胁是LLMs的非确定性特征。即使将温度设置为零，也无法消除非确定性，这可能会使结果无效并带来可重复性问题。为缓解这一威胁，在我们的研究中，我们确保没有通过算法注入更多随机性来引入额外的稳健性问题。因此，我们在多个运行中测试了EPiC，并在这方面实证发现我们的方法结果是稳健的。然而，考虑到在RQ2.2和RQ3中重新运行实验的高成本（这些实验已经有许多配置需要测试），我们跳过了不同种子的多次运行。然而，鉴于RQ2.1中结果的低方差，这可能对有效性不是严重威胁。
- en: 'Construct: One construct threat we face is that our evaluation metrics rely
    on LLM-generated test cases for intermediate evaluations which could lead to misleading
    results if these test cases are not representative of the intended tasks or suffer
    from hallucination issues. To alleviate the threat, we have shown that the results
    from the LLM-generated tests are not that far off from the results using the original
    (developer-written) tests.'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 构建：我们面临的一个构建威胁是我们的评估指标依赖于LLM生成的测试用例进行中间评估，如果这些测试用例不能代表预期任务或存在幻觉问题，可能会导致误导性结果。为缓解这一威胁，我们已经展示了LLM生成的测试结果与使用原始（开发者编写的）测试的结果差异并不大。
- en: 'Internal: In this research, to compare with the baselines, we used their publicly
    available code and default setup and assumed that these tools were configured
    for optimal performance. For our own implementation, we also used existing open-source
    packages for mutation to reduce any internal validity issues. However, we did
    not fine-tune our hyper-parameters. The default configurations are based on RQ3’s
    sensitivity analysis which is not a comprehensive hyper-parameter optimization.
    However, a fine-tuned EPiC could only be more outperforming the baselines. Thus
    there is no threats to the validity of general findings of the paper in this regard.'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 内部：在这项研究中，为了与基线进行比较，我们使用了其公开可用的代码和默认设置，并假设这些工具的配置已优化以获得最佳性能。对于我们自己的实现，我们也使用了现有的开源包进行变异，以减少任何内部有效性问题。然而，我们没有对超参数进行微调。默认配置基于RQ3的敏感性分析，这并不是全面的超参数优化。然而，经过微调的EPiC可能会比基线表现更好。因此，就这一点而言，论文的一般发现有效性没有威胁。
- en: 7 Conclusion and Future Work
  id: totrans-382
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论与未来工作
- en: In this study, we presented EPiC, a cost-effective search-based prompt engineering
    approach for improving code generation using LLMs. Our method leverages an evolutionary
    algorithm to refine prompts, resulting in enhanced code generation performance
    systematically. The experimental results demonstrate that EPiC not only outperforms
    SOTA methods in terms of accuracy but also maintains a competitive cost, making
    it a cost-effective solution for software engineering applications. EPiC achieved
    a 93.5% pass@1 rate on the HumanEval dataset and a 79% pass@1 rate on the MBPP
    dataset, outperforming other methods with a lower or comparable cost. Our experiments
    also showed that incorporating randomness in the mutation process and adjusting
    the population size can impact the performance and cost-effectiveness of the algorithm.
    Moreover, our evaluation using an open-source LLM, MagicCoder, revealed that EPiC
    could enhance the performance of smaller models, indicating its potential applicability
    across various LLMs.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项研究中，我们提出了 EPiC，一种成本效益高的基于搜索的提示工程方法，用于提升使用 LLMs 的代码生成。我们的方法利用进化算法来优化提示，从而系统地提高代码生成性能。实验结果表明，EPiC
    不仅在准确性方面超越了最前沿方法，而且保持了具有竞争力的成本，使其成为软件工程应用中的一种成本效益解决方案。EPiC 在 HumanEval 数据集上达到了
    93.5% 的 pass@1 率，在 MBPP 数据集上达到了 79% 的 pass@1 率，优于其他方法，并且成本更低或相当。我们的实验还表明，变异过程中引入随机性和调整种群大小会影响算法的性能和成本效益。此外，我们使用开源
    LLM MagicCoder 进行的评估显示，EPiC 可以提高较小模型的性能，表明其在各种 LLM 中的潜在适用性。
- en: In this paper, we employed EPiC for code generation. EPiC is task-agnostic and
    can be applied to other software engineering applications requiring prompt engineering,
    provided that an appropriate evaluation function is defined. We leave this part
    for future research. Additionally, we plan to explore more datasets, LLMs, mutation
    strategies, and software engineering tasks.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们使用了 EPiC 进行代码生成。EPiC 是任务无关的，可以应用于其他需要提示工程的软件工程应用，只要定义了适当的评估函数。我们将这部分留待未来研究。此外，我们计划探索更多的数据集、LLMs、变异策略和软件工程任务。
- en: References
  id: totrans-385
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Espejel, J. L., Alassan, M. S. Y., Chouham, E. M., & Ettifouri, E. H. (2023).
    A comprehensive review of State-of-The-Art methods for Java code generation from
    Natural Language Text. Natural Language Processing Journal, 100013.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Espejel, J. L., Alassan, M. S. Y., Chouham, E. M., & Ettifouri, E. H. (2023).
    关于从自然语言文本生成 Java 代码的最前沿方法的全面综述。自然语言处理期刊，100013。'
- en: '[2] Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., … & Zhou, M.
    (2020). Codebert: A pre-trained model for programming and natural languages. arXiv
    preprint arXiv:2002.08155.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., … & Zhou, M.
    (2020). Codebert：一个用于编程和自然语言的预训练模型。arXiv 预印本 arXiv:2002.08155。'
- en: '[3] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training
    of deep bidirectional transformers for language understanding. arXiv preprint
    arXiv:1810.04805.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert：用于语言理解的深度双向转换器的预训练。arXiv
    预印本 arXiv:1810.04805。'
- en: '[4] Husain, H., Wu, H. H., Gazit, T., Allamanis, M., & Brockschmidt, M. (2019).
    Codesearchnet challenge: Evaluating the state of semantic code search. arXiv preprint
    arXiv:1909.09436.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Husain, H., Wu, H. H., Gazit, T., Allamanis, M., & Brockschmidt, M. (2019).
    Codesearchnet 挑战：评估语义代码搜索的现状。arXiv 预印本 arXiv:1909.09436。'
- en: '[5] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019).
    Language models are unsupervised multitask learners. OpenAI blog, 1(8), 9.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019).
    语言模型是无监督的多任务学习者。OpenAI 博客，1(8)，9。'
- en: '[6] Lu, S., Guo, D., Ren, S., Huang, J., Svyatkovskiy, A., Blanco, A., … &
    Liu, S. (2021). Codexglue: A machine learning benchmark dataset for code understanding
    and generation. arXiv preprint arXiv:2102.04664.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Lu, S., Guo, D., Ren, S., Huang, J., Svyatkovskiy, A., Blanco, A., … &
    Liu, S. (2021). Codexglue: 一个用于代码理解和生成的机器学习基准数据集。arXiv 预印本 arXiv:2102.04664。'
- en: '[7] Perez, L., Ottens, L., & Viswanathan, S. (2021). Automatic code generation
    using pre-trained language models. arXiv preprint arXiv:2102.10535.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Perez, L., Ottens, L., & Viswanathan, S. (2021). 使用预训练语言模型的自动代码生成。arXiv
    预印本 arXiv:2102.10535。'
- en: '[8] Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. D. O., Kaplan, J.,
    … & Zaremba, W. (2021). Evaluating large language models trained on code. arXiv
    preprint arXiv:2107.03374.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. D. O., Kaplan, J.,
    … & Zaremba, W. (2021). 评估训练于代码的大型语言模型。arXiv 预印本 arXiv:2107.03374。'
- en: '[9] Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E.,
    … & Synnaeve, G. (2023). Code llama: Open foundation models for code. arXiv preprint
    arXiv:2308.12950.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E.,
    … & Synnaeve, G. (2023). Code llama: 用于代码的开放基础模型。arXiv 预印本 arXiv:2308.12950。'
- en: '[10] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei,
    Y., … & Scialom, T. (2023). Llama 2: Open foundation and fine-tuned chat models.
    arXiv preprint arXiv:2307.09288.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei,
    Y., … & Scialom, T. (2023). Llama 2: 开放基础和微调聊天模型。arXiv 预印本 arXiv:2307.09288。'
- en: '[11] Wei, Y., Wang, Z., Liu, J., Ding, Y., & Zhang, L. (2023). Magicoder: Source
    Code Is All You Need. arXiv preprint arXiv:2312.02120.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Wei, Y., Wang, Z., Liu, J., Ding, Y., & Zhang, L. (2023). Magicoder: 源代码是你所需的一切。arXiv
    预印本 arXiv:2312.02120。'
- en: '[12] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., … & Zhou,
    D. (2022). Chain-of-thought prompting elicits reasoning in large language models.
    Advances in Neural Information Processing Systems, 35, 24824-24837.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., … & Zhou,
    D. (2022). 链式思维提示引发大型语言模型的推理。神经信息处理系统进展，35，24824-24837。'
- en: '[13] Liu, J., Liu, A., Lu, X., Welleck, S., West, P., Bras, R. L., … & Hajishirzi,
    H. (2021). Generated knowledge prompting for commonsense reasoning. arXiv preprint
    arXiv:2110.08387.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Liu, J., Liu, A., Lu, X., Welleck, S., West, P., Bras, R. L., … & Hajishirzi,
    H. (2021). 生成知识提示用于常识推理。arXiv 预印本 arXiv:2110.08387。'
- en: '[14] Zhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S., Chan, H., &
    Ba, J. (2022). Large language models are human-level prompt engineers. arXiv preprint
    arXiv:2211.01910.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Zhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S., Chan, H., &
    Ba, J. (2022). 大型语言模型是人类水平的提示工程师。arXiv 预印本 arXiv:2211.01910。'
- en: '[15] Pryzant, R., Iter, D., Li, J., Lee, Y. T., Zhu, C., & Zeng, M. (2023).
    Automatic prompt optimization with” gradient descent” and beam search. arXiv preprint
    arXiv:2305.03495.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Pryzant, R., Iter, D., Li, J., Lee, Y. T., Zhu, C., & Zeng, M. (2023).
    使用“梯度下降”和束搜索的自动提示优化。arXiv 预印本 arXiv:2305.03495。'
- en: '[16] Guo, Q., Wang, R., Guo, J., Li, B., Song, K., Tan, X., … & Yang, Y. (2023).
    Connecting large language models with evolutionary algorithms yields powerful
    prompt optimizers. arXiv preprint arXiv:2309.08532.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Guo, Q., Wang, R., Guo, J., Li, B., Song, K., Tan, X., … & Yang, Y. (2023).
    将大型语言模型与进化算法连接产生强大的提示优化器。arXiv 预印本 arXiv:2309.08532。'
- en: '[17] Zelikman, E., Huang, Q., Poesia, G., Goodman, N., & Haber, N. (2023).
    Parsel: Algorithmic Reasoning with Language Models by Composing Decompositions.
    Advances in Neural Information Processing Systems, 36, 31466-31523.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Zelikman, E., Huang, Q., Poesia, G., Goodman, N., & Haber, N. (2023).
    Parsel: 通过组合分解进行语言模型的算法推理。神经信息处理系统进展，36，31466-31523。'
- en: '[18] Huang, D., Nan, Z., Hu, X., Jin, P., Peng, S., Wen, Y., … & Chen, Y. (2024).
    ANPL: Towards Natural Programming with Interactive Decomposition. Advances in
    Neural Information Processing Systems, 36.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Huang, D., Nan, Z., Hu, X., Jin, P., Peng, S., Wen, Y., … & Chen, Y. (2024).
    ANPL: 通过交互式分解迈向自然编程。神经信息处理系统进展，36。'
- en: '[19] Liu, Z., Zhang, Y., Li, P., Liu, Y., & Yang, D. (2023). Dynamic llm-agent
    network: An llm-agent collaboration framework with agent team optimization. arXiv
    preprint arXiv:2310.02170.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Liu, Z., Zhang, Y., Li, P., Liu, Y., & Yang, D. (2023). 动态 LLM-代理网络：一种具有代理团队优化的
    LLM-代理协作框架。arXiv 预印本 arXiv:2310.02170。'
- en: '[20] Shinn, N., Cassano, F., Labash, B., Gopinath, A., Narasimhan, K., & Yao,
    S. (2023). Reflexion: Language Agents with Verbal Reinforcement Learning.(2023).
    arXiv preprint cs.AI/2303.11366.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Shinn, N., Cassano, F., Labash, B., Gopinath, A., Narasimhan, K., & Yao,
    S. (2023). Reflexion: 具有语言强化学习的语言代理。arXiv 预印本 cs.AI/2303.11366。'
- en: '[21] Zhou, A., Yan, K., Shlapentokh-Rothman, M., Wang, H., & Wang, Y. X. (2023).
    Language agent tree search unifies reasoning acting and planning in language models.
    arXiv preprint arXiv:2310.04406.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Zhou, A., Yan, K., Shlapentokh-Rothman, M., Wang, H., & Wang, Y. X. (2023).
    语言代理树搜索统一了语言模型中的推理、行动和规划。arXiv 预印本 arXiv:2310.04406。'
- en: '[22] Huang, D., Bu, Q., Zhang, J. M., Luck, M., & Cui, H. (2023). AgentCoder:
    Multi-Agent-based Code Generation with Iterative Testing and Optimisation. arXiv
    preprint arXiv:2312.13010.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Huang, D., Bu, Q., Zhang, J. M., Luck, M., & Cui, H. (2023). AgentCoder:
    基于多代理的代码生成，采用迭代测试和优化。arXiv 预印本 arXiv:2312.13010。'
- en: '[23] Zhong, L., Wang, Z., & Shang, J. (2024). LDB: A Large Language Model Debugger
    via Verifying Runtime Execution Step-by-step. arXiv preprint arXiv:2402.16906.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Zhong, L., Wang, Z., & Shang, J. (2024). LDB: 一种通过逐步验证运行时执行的高级语言模型调试器。arXiv
    预印本 arXiv:2402.16906。'
- en: '[24] Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D.,
    Jiang, E., Cai, C., Terry, M., Le, Q. and Sutton, C., 2021\. Program synthesis
    with large language models. arXiv preprint arXiv:2108.07732.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D.,
    Jiang, E., Cai, C., Terry, M., Le, Q. 和 Sutton, C., 2021\. 使用大型语言模型进行程序合成。arXiv
    预印本 arXiv:2108.07732。'
- en: '[25] C. Fernando, D. Banarse, H. Michalewski, S. Osindero, and T. Rockt¨aschel,
    “Promptbreeder: Self-referential self-improvement via prompt evolution,” arXiv
    preprint arXiv:2309.16797, 2023.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] C. Fernando, D. Banarse, H. Michalewski, S. Osindero 和 T. Rocktäschel，“Promptbreeder：通过提示演变进行自我参照自我改进，”arXiv
    预印本 arXiv:2309.16797，2023年。'
- en: '[26] Y. B. Li and K. Wu, “Spell: Semantic prompt evolution based on a llm,”
    arXiv preprint arXiv:2310.01260, 2023.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Y. B. Li 和 K. Wu，“Spell：基于 LLM 的语义提示演变，”arXiv 预印本 arXiv:2310.01260，2023年。'
- en: '[27] H. Yang and K. Li, “Instoptima: Evolutionary multi-objective instruc-
    tion optimization via large language model-based instruction opera- tors,” arXiv
    preprint arXiv:2310.17630, 2023.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] H. Yang 和 K. Li，“Instoptima：通过大型语言模型基础的指令运算符进行进化多目标指令优化，”arXiv 预印本 arXiv:2310.17630，2023年。'
- en: '[28] Shin, J., Tang, C., Mohati, T., Nayebi, M., Wang, S. and Hemmati, H.,
    2023\. Prompt engineering or fine tuning: An empirical assessment of large language
    models in automated software engineering tasks. arXiv preprint arXiv:2310.10508.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Shin, J., Tang, C., Mohati, T., Nayebi, M., Wang, S. 和 Hemmati, H., 2023\.
    提示工程还是微调：大型语言模型在自动化软件工程任务中的实证评估。arXiv 预印本 arXiv:2310.10508。'
- en: '[29] Hong, S., Zheng, X., Chen, J., Cheng, Y., Wang, J., Zhang, C., Wang, Z.,
    Yau, S.K.S., Lin, Z., Zhou, L. and Ran, C., 2023\. Metagpt: Meta programming for
    multi-agent collaborative framework. arXiv preprint arXiv:2308.00352.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Hong, S., Zheng, X., Chen, J., Cheng, Y., Wang, J., Zhang, C., Wang, Z.,
    Yau, S.K.S., Lin, Z., Zhou, L. 和 Ran, C., 2023\. Metagpt：用于多代理协作框架的元编程。arXiv 预印本
    arXiv:2308.00352。'
- en: '[30] Fan, A., Gokkaya, B., Harman, M., Lyubarskiy, M., Sengupta, S., Yoo, S.
    and Zhang, J.M., 2023, May. Large language models for software engineering: Survey
    and open problems. In 2023 IEEE/ACM International Conference on Software Engineering:
    Future of Software Engineering (ICSE-FoSE) (pp. 31-53). IEEE.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Fan, A., Gokkaya, B., Harman, M., Lyubarskiy, M., Sengupta, S., Yoo, S.
    和 Zhang, J.M., 2023年5月。大型语言模型在软件工程中的应用：调查与开放问题。2023 IEEE/ACM 国际软件工程会议：软件工程的未来（ICSE-FoSE）（31-53页）。IEEE。'
- en: '[31] K. Jin, C. -Y. Wang, H. V. Pham and H. Hemmati, ”Can ChatGPT Support Developers?
    An Empirical Evaluation of Large Language Models for Code Generation,” 2024 IEEE/ACM
    21st International Conference on Mining Software Repositories (MSR), Lisbon, Portugal,
    2024, pp. 167-171.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] K. Jin, C. -Y. Wang, H. V. Pham 和 H. Hemmati，“ChatGPT 能支持开发者吗？大型语言模型在代码生成中的实证评估，”2024
    IEEE/ACM 第21届国际软件库挖掘会议（MSR），葡萄牙里斯本，2024年，167-171页。'
- en: '[32] Wang, Junjie, et al. ”Software testing with large language models: Survey,
    landscape, and vision.” IEEE Transactions on Software Engineering (2024).'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Wang, Junjie 等人。“使用大型语言模型进行软件测试：调查、现状和愿景。” IEEE 软件工程学报（2024）。'
- en: '[33] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P.,
    Neelakantan, A., Shyam, P., Sastry, G., Askell, A. and Agarwal, S., 2020\. Language
    models are few-shot learners. Advances in neural information processing systems,
    33, pp.1877-1901.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P.,
    Neelakantan, A., Shyam, P., Sastry, G., Askell, A. 和 Agarwal, S., 2020\. 语言模型是少样本学习者。神经信息处理系统进展，33，1877-1901页。'
- en: '[34] Liu, Jiachang, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin,
    and Weizhu Chen. ”What Makes Good In-Context Examples for GPT-$3$?.” arXiv preprint
    arXiv:2101.06804 (2021).'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Liu, Jiachang, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin 和
    Weizhu Chen。“什么使得 GPT-$3$ 的上下文示例好？”arXiv 预印本 arXiv:2101.06804（2021年）。'
- en: '[35] Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N.,
    Küttler, H., Lewis, M., Yih, W.T., Rocktäschel, T. and Riedel, S., 2020\. Retrieval-augmented
    generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing
    Systems, 33, pp.9459-9474.4'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N.,
    Küttler, H., Lewis, M., Yih, W.T., Rocktäschel, T. 和 Riedel, S., 2020\. 基于检索增强的生成用于知识密集型
    NLP 任务。神经信息处理系统进展，33，9459-9474页。'
- en: '[36] Yun He, Steven Zheng, Yi Tay, Jai Gupta, Yu Du, Vamsi Aribandi, Zhe Zhao,
    YaGuang Li, Zhao Chen, Donald Metzler, et al. 2022\. Hyperprompt: Prompt-based
    task-conditioning of transformers. In International Conference on Machine Learning.
    PMLR, 8678–8690.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Yun He, Steven Zheng, Yi Tay, Jai Gupta, Yu Du, Vamsi Aribandi, Zhe Zhao,
    YaGuang Li, Zhao Chen, Donald Metzler 等人，2022\. Hyperprompt：基于提示的任务条件化变换器。在国际机器学习会议。PMLR，8678–8690。'
- en: '[37] Salvatore Carta, Alessandro Giuliani, Leonardo Piano, Alessandro Sebastian
    Podda, Livio Pompianu, and Sandro Gabriele Tiddia. 2023\. Iterative Zero-Shot
    LLM Prompting for Knowledge Graph Construction. arXiv preprint arXiv:2307.01128
    (2023).'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] 萨尔瓦托雷·卡尔塔、亚历山德罗·朱利安尼、莱昂纳多·皮亚诺、亚历山德罗·塞巴斯蒂安·波达、利维奥·庞皮亚努和桑德罗·加布里埃尔·提迪亚。 2023。迭代零样本
    LLM 提示用于知识图谱构建。 arXiv 预印本 arXiv:2307.01128 (2023)。'
- en: '[38] Sahoo, Pranab, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal,
    and Aman Chadha. ”A systematic survey of prompt engineering in large language
    models: Techniques and applications.” arXiv preprint arXiv:2402.07927 (2024).'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] 萨胡、普拉纳布、阿尤什·库马尔·辛格、斯里帕尔娜·萨哈、维尼贾·贾因、萨姆拉特·蒙达尔和阿曼·查达。 “大型语言模型中的提示工程系统化调查：技术与应用。”
    arXiv 预印本 arXiv:2402.07927 (2024)。'
- en: '[39] Zhang, Zhuosheng, Aston Zhang, Mu Li, and Alex Smola. ”Automatic chain
    of thought prompting in large language models.” arXiv preprint arXiv:2210.03493
    (2022).'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] 张卓胜、阿斯顿·张、穆李和亚历克斯·斯莫拉。 “大型语言模型中的自动思维链提示。” arXiv 预印本 arXiv:2210.03493 (2022)。'
- en: '[40] Wang, Xuezhi, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang,
    Aakanksha Chowdhery, and Denny Zhou. ”Self-consistency improves chain of thought
    reasoning in language models.” arXiv preprint arXiv:2203.11171 (2022).'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] 王学智、杰森·韦、戴尔·舒尔曼斯、阮国立、艾德·奇、沙兰·纳朗、阿坎卡·乔德赫里和丹尼·周。 “自一致性改善语言模型中的思维链推理。” arXiv
    预印本 arXiv:2203.11171 (2022)。'
- en: '[41] Yao, Shunyu, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan,
    and Yuan Cao. ”React: Synergizing reasoning and acting in language models.” arXiv
    preprint arXiv:2210.03629 (2022).'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] 姚顺宇、杰弗里·赵、滕昱、段楠、伊扎克·沙夫兰、卡尔提克·纳拉辛汉和袁曹。 “React: 在语言模型中协同推理与行动。” arXiv 预印本
    arXiv:2210.03629 (2022)。'
- en: '[42] Li, Xingxuan, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq Joty, Soujanya
    Poria, and Lidong Bing. ”Chain-of-knowledge: Grounding large language models via
    dynamic knowledge adapting over heterogeneous sources.” arXiv preprint arXiv:2305.13269
    (2023).'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] 李星轩、赵若晨、叶肯·谢、丁博生、沙菲克·乔提、苏贞雅·波里亚和滨立东。 “Chain-of-knowledge: 通过动态知识适配于异质源来夯实大型语言模型。”
    arXiv 预印本 arXiv:2305.13269 (2023)。'
- en: '[43] Mu, Fangwen, et al. ”ClarifyGPT: Empowering LLM-based Code Generation
    with Intention Clarification.” arXiv preprint arXiv:2310.10996 (2023).'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] 穆芳文等。 “ClarifyGPT: 通过意图澄清赋能基于 LLM 的代码生成。” arXiv 预印本 arXiv:2310.10996 (2023)。'
- en: '[44] Zamani, Shayan, and Hadi Hemmati. ”A pragmatic approach for hyper-parameter
    tuning in search-based test case generation.” Empirical Software Engineering 26
    (2021): 1-35.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] 扎马尼、沙扬和赫尔赫·赫梅提。 “一种用于基于搜索的测试用例生成的超参数调整实用方法。” 实证软件工程 26 (2021): 1-35。'
