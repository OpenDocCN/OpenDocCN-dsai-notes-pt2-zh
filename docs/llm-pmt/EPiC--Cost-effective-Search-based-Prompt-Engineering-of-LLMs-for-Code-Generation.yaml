- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:40:41'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'EPiC: Cost-effective Search-based Prompt Engineering of LLMs for Code Generation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.11198](https://ar5iv.labs.arxiv.org/html/2408.11198)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Hamed Taherkhani, Melika Sepindband, Hung Viet Pham, Song Wang, Hadi Hemmati
    Hamed Taherkhani is with Lassonde School of Engineering, York University, Toronto,
    Ontario, Canada. Email:hamedth@yorku.ca Melika Sepidband is with Lassonde School
    of Engineering, York University, Toronto, Ontario, Canada. Email:melikasp@yorku.caHadi
    Hemmati is an associate professor at Lassonde School of Engineering, York University,
    Toronto, Ontario, Canada. Email:hemmati@yorku.caHung Viet Pham is an assistant
    professor at Lassonde School of Engineering, York University, Toronto, Ontario,
    Canada. Email:hvpham@yorku.caSong Wang is an associate professor at Lassonde School
    of Engineering, York University, Toronto, Ontario, Canada. Email:wangsong@yorku.ca
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) have seen increasing use in various software development
    tasks, especially in code generation. The most advanced recent methods attempt
    to incorporate feedback from code execution into prompts to help guide LLMs in
    generating correct code, in an iterative process. While effective, these methods
    could be costly and time-consuming due to numerous interactions with the LLM and
    the extensive token usage. To address this issue, we propose an alternative approach
    named Evolutionary Prompt Engineering for Code (EPiC), which leverages a lightweight
    evolutionary algorithm to evolve the original prompts toward better ones that
    produce high-quality code, with minimal interactions with LLM. Our evaluation
    against state-of-the-art (SOTA) LLM-based code generation models shows that EPiC
    outperforms all the baselines in terms of cost-effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Prompt Engineering, Code Generation, Large Language Models, Evolutionary Algorithm
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs have been used in many software development activities such as software
    testing, design, requirement engineering, code generation, maintenance, deployment,
    etc. [[30](#bib.bib30), [32](#bib.bib32)]. Among these activities, code generation
    using LLMs has demonstrated significant potential.
  prefs: []
  type: TYPE_NORMAL
- en: In LLM-based code generation, various prompt engineering techniques, including
    zero-shot [[5](#bib.bib5)], in-context learning [[33](#bib.bib33), [34](#bib.bib34)],
    RAG [[35](#bib.bib35)], and task-specific methods [[36](#bib.bib36), [37](#bib.bib37)],
    have been shown to outperform fine-tuned smaller models. The most advanced prompt
    engineering methods for code generation employ various agent-based approaches
    [[28](#bib.bib28)]. SOTA methods such as Reflexion [[20](#bib.bib20)], Language
    Agent Tree Search (LATS) [[21](#bib.bib21)], AgentCoder [[22](#bib.bib22)], LDB
    [[23](#bib.bib23)], and MetaGPT [[29](#bib.bib29)] are either planning-based or
    multi-collaborative agents. While effective, these methods can be costly and time-consuming
    due to numerous interactions with LLMs which results in extensive token usage,
    making them less attractive in practical settings. For instance, LATS requires
    on average 3 minutes to generate the implementation of a function with an average
    of only 6 lines of code, on the MBPP dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/675ea440798b2b7afdbd80694988355a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The initial failed prompt (left) and the mutated successful prompt
    (right)'
  prefs: []
  type: TYPE_NORMAL
- en: 'To tackle this limitation, in this paper, we explicitly focus on the cost of
    prompt engineering for code and propose a cost-effective approach that leverages
    a lightweight evolutionary search to optimize prompts for code generation. Our
    evolutionary-based prompt engineering tool for code generation, namely, EPiC,
    optimizes prompts for code generation by assessing the generated code against
    a fitness function, i.e., the pass rate of test cases. Our approach consists of
    two phases: Initial Evaluation (IE) and Evolutionary Prompt Engineering (EPE).
    The first phase involves primary code generation using an initial prompt and its
    evaluation using a set of test cases. If a correct solution is not generated,
    the process moves to the second (EPE) phase, where an initial population of prompts
    is generated using the initial prompt. Using each prompt in the initial population,
    we generate its corresponding code by the LLM under study. We then evaluate the
    generated code against test cases to calculate its fitness score. Candidate prompts
    are selected out of the population, using a weighted random selection approach.
    These candidates are then mutated to form the next generation of prompts. The
    mutation is carried out using two approaches: one utilizes an LLM guided by a
    prompt that specifies how to perform the mutation, and the other employs vector
    embeddings of words to find and replace similar words.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [1](#S1.F1 "Figure 1 ‣ EPiC: Cost-effective Search-based Prompt Engineering
    of LLMs for Code Generation") illustrates an example of a mutation applied to
    a given prompt. In this example, GPT-4o, with the initial prompt on the left,
    is confused by the word “print” and generates an incorrect print statement. However,
    the evolved prompt on the right uses the word “publish” which results in the expected
    return statement, thereby passing the provided test cases.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To evaluate the effectiveness of EPiC, we select two widely used code generation
    benchmark datasets, i.e., Humaneval [[8](#bib.bib8)] and MBPP [[24](#bib.bib24)].
    We compare EPiC against three SOTA LLM-based code generation tools i.e. Reflexion
    [[20](#bib.bib20)], LATS [[21](#bib.bib21)], and LDB [[23](#bib.bib23)]. We also
    implement EPiC leveraging two different LLMs: a large closed-source (GPT4-o) and
    a smaller open-source one (Magic Coder [[11](#bib.bib11)]). In addition, we have
    empirically investigated the impact of different components of our evolutionary
    algorithm on the cost and effectiveness of the results.'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, our results suggest that the cost and effectiveness of code-generation
    tools do not always increase proportionally. Specifically, EPiC outperforms all
    compared SOTA tools (measured by pass@1) by 1% to 3% on HumanEval and 2% to 7%
    on MBPP with either lower or comparable cost. In addition, EPiC is more effective
    than LATS (highest performing SOTA) but with 80% less cost and is almost as low
    cost as Reflexion (the lowest cost approach among SOTA), but with 8% better results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main contributions of this paper are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To the best of our knowledge, we are the first to explore the code generation
    task from the cost perspective.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We propose a novel framework EPiC that leverages a lightweight evolutionary
    algorithm to evolve the original prompts toward better ones that produce high-quality
    code.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We have demonstrated the cost-effectiveness of EPiC in code generation compared
    with baseline methods.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We release the data and source code of our experiments to enable other researchers
    to replicate and extend our study ([https://github.com/HamedTaherkhani/EPiC](https://github.com/HamedTaherkhani/EPiC))
    .
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Background and Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we briefly explain the background of LLMs for code generation
    and prompt engineering, then report the most related work in the context of prompt
    engineering of LLM for code.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 LLMs for Code Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Models for code generation can be divided into two categories, RNN-based models
    and transformer-based models [[1](#bib.bib1)]. RNN-based models are older models
    that are outperformed by transformers. Transformer-based code generation models
    are transformers pre-trained on code-related datasets. For instance, CodeBERT [[2](#bib.bib2)]
    is a transformer model based on BERT [[3](#bib.bib3)], specifically pre-trained
    on the CodeSearchNet dataset [[4](#bib.bib4)]. Another example is CodeGPT-2, which
    builds upon GPT-2 [[5](#bib.bib5)] and is pre-trained on the same dataset. Lu
    et al. [[6](#bib.bib6)] evaluated the performance of both GPT-2 and CodeGPT-2
    in the code generation task and later, Perez et al. [[7](#bib.bib7)], fine-tuned
    GPT-2 using their dataset for code generation.
  prefs: []
  type: TYPE_NORMAL
- en: Newer versions of LLMs, such as GPT-4, have demonstrated improved performance
    compared to their predecessors across diverse tasks, including code generation.
    For instance, Codex [[8](#bib.bib8)] is a language model developed by OpenAI based
    on the GPT architecture, similar to GPT-3\. However, it’s specifically designed
    for code generation and understanding. Codex is trained on a diverse range of
    publicly available code repositories and strongly focuses on understanding and
    generating programming code. Another model, CodeLlama [[9](#bib.bib9)], is a family
    of large language models for code based on Llama 2 [[10](#bib.bib10)] that is
    available in 7B, 13B, and 34B parameters. DeepSeek Coder is another state-of-the-art
    LLM that has been used in software tasks such as Code Generation. This LLM has
    various sizes, ranging from 1B to 33B. Magicoder [[11](#bib.bib11)] is another
    model that has been fine-tuned on both CodeLlama-7b-Python and deepseek-coder-6.7b.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this research, we employed two different types of LLMs: GPT-4o, as an example
    of closed-source large models, and MagicCoder as an example of open-source smaller
    models. We selected GPT-4o due to its high performance among the closed-source
    LLMs. MagicCoder was chosen as the most cost-effective small-size open-source
    model, at the time of designing our experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9b7aecf22b157056b7964926d04821c2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Diagram of EPiC. On the left, the initial evaluation for the initial
    prompt assessment is depicted and on the right, the evolutionary process of EPiC
    is illustrated.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Prompt Engineering for LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A prompt is an input or query provided to a model to generate a response or
    perform a task. Prompt engineering refers to the process of designing and refining
    prompts to achieve desired outcomes when using LLMs ¹¹1https://platform.openai.com/docs/guides/prompt-engineering.
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple categories of prompt engineering, including approaches without
    training, reasoning and logic, reducing hallucination, and evolutionary-based
    methods [[38](#bib.bib38)]. Zero-shot [[5](#bib.bib5)] and few-shot [[33](#bib.bib33)]
    prompting fall under the category of approaches without training. Techniques such
    as chain-of-thought (CoT) prompting [[12](#bib.bib12)], Automatic Chain-of-Thought
    (Auto-CoT) [[39](#bib.bib39)], Self-Consistency [[40](#bib.bib40)], and knowledge
    prompting [[13](#bib.bib13)] exemplify reasoning and logic-based methods. To reduce
    hallucination for prompt engineering, techniques such as Retrieval Augmented Generation
    (RAG) [[35](#bib.bib35)], ReAct Prompting [[41](#bib.bib41)], and Chain-of-Knowledge
    (CoK) Prompting [[42](#bib.bib42)] are employed. Evolutionary algorithms are utilized
    to optimize prompts, as demonstrated by EvoPrompt [[16](#bib.bib16)] and PromptBreeder
    [[25](#bib.bib25)]. There are other solutions such as Automated Prompt Engineering [[14](#bib.bib14)]
    and Prompt Optimization with Textual Gradients (ProTeGi) [[15](#bib.bib15)].
  prefs: []
  type: TYPE_NORMAL
- en: Zero-shot prompting, as described in [[5](#bib.bib5)], eliminates the necessity
    for extensive training data by employing accurately designed prompts to direct
    the model toward executing new tasks. The model receives a task description without
    labeled training data and employs its pre-existing knowledge to generate predictions
    based on the prompt. Few-shot prompting [[33](#bib.bib33)] uses a limited number
    of input-output examples to help models understand tasks, unlike zero-shot prompting
    which provides no examples. However, this method requires additional tokens, which
    can be impractical for longer texts. The selection and composition of examples
    can also significantly influence model behavior and introduce biases.
  prefs: []
  type: TYPE_NORMAL
- en: CoT [[12](#bib.bib12)] improves the reasoning abilities of large language models
    by incorporating intermediate reasoning steps within prompts. This technique breaks
    down complex tasks into smaller sub-tasks, mimicking human problem-solving. It
    significantly enhances performance in arithmetic, commonsense, and symbolic reasoning,
    especially with larger models. Auto-CoT  [[39](#bib.bib39)] is an approach designed
    to enhance the reasoning capabilities of LLMs by automating the generation of
    intermediate reasoning steps. It automates the creation of reasoning chains and
    demonstrations using diversity-based sampling to generate multiple reasoning paths.
  prefs: []
  type: TYPE_NORMAL
- en: Automated prompt engineering approaches use an agent or a similar automated
    engine to interact with LLM and typically get some feedback to improve the prompt.
    Zhou et al. [[14](#bib.bib14)] proposed APE, a framework designed for the automatic
    generation and selection of instructions. In this framework, they used an LLM
    to generate instruction candidates and then search for candidate solutions to
    maximize a selected score function, treating prompt engineering as an optimization
    problem. Pryzan et al. [[15](#bib.bib15)] proposed Prompt Optimization with Textual
    Gradients (ProTeGi), which improves prompts for LLMs using a non-parametric approach
    inspired by numerical gradient descent. It uses natural language gradients from
    training data to critique and edit prompts, employing a process guided by beam
    search and bandit selection. This approach improved the efficiency of previous
    prompt editing techniques across various NLP tasks, including the novel challenge
    of LLM jailbreak detection.
  prefs: []
  type: TYPE_NORMAL
- en: Evolutionary algorithms have been used in the past for prompt engineering of
    LLMs in generic NLP tasks. For example, EvoPrompt [[16](#bib.bib16)], a framework
    designed to automate the prompt engineering process for LLMs. EvoPrompt begins
    with an initial population of prompts and iteratively generates new ones using
    evolutionary operators such as mutation and crossover, which are performed by
    LLMs. Another similar approach is PromptBreeder [[25](#bib.bib25)], which employs
    task prompts to condition the LLM and mutation prompts to modify task prompts.
    The interplay between task and mutation prompts drives iterative improvement in
    PromptBreeder. Similar approaches for prompt enhancement using LLMs through EAs
    are also proposed in [[26](#bib.bib26)] and [[27](#bib.bib27)].
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to existing evolutionary prompt engineering techniques, EPiC is
    particularly tailored for prompt engineering in coding tasks, with a fitness function
    defined based on the pass rate of test cases. In addition, it focuses on cost-efficiency
    throughout the process. To achieve this, it minimizes the calls to external LLMs
    and implements the mutation operator using local lightweight word embeddings libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Prompt Engineering of LLMs for Code Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Zelikman et al. [[17](#bib.bib17)] introduced Parsel, a framework enabling automatic
    implementation and validation of complex algorithms with code LLMs. Using Parsel,
    they break down algorithmic tasks into structured descriptions written in natural
    language, then explore various combinations of function implementations using
    tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'DyLAN [[19](#bib.bib19)] is a framework designed to boost the performance of
    Large Language Model (LLM) agents by assembling them into a dynamic team that
    adapts to different tasks. Unlike traditional methods with fixed agent sets, DyLAN’s
    architecture adjusts dynamically to the task query, enhancing its versatility.
    It employs features like inference-time agent selection and early stopping to
    improve efficiency and effectiveness. Furthermore, DyLAN incorporates an automatic
    agent team optimization algorithm based on an unsupervised metric called Agent
    Importance Score, which selects the most effective agents for each task. Empirical
    results show DyLAN’s success in tasks like reasoning and code generation, achieving
    significant improvements over single LLM executions. Reflexion [[20](#bib.bib20)]
    is a reinforcement-based framework for this problem where language agents learn
    from linguistic feedback rather than weight updates. Agents reflect on task feedback
    verbally and maintain their own reflective text in memory, which helps them make
    better decisions in subsequent trials. Reflexion is adaptable to different types
    and sources of feedback signals and shows significant improvements over baseline
    agents across various tasks. LATS (Language Agent Tree Search) [[21](#bib.bib21)]
    is another search-based framework that combines LLMs’ abilities in planning, acting,
    and reasoning. LATS draws inspiration from the Monte Carlo tree search and repurposes
    LLMs’ strengths as agents, value functions, and optimizers. Crucially, LATS incorporates
    an environment for external feedback to enable more deliberate and adaptive problem-solving
    beyond existing techniques’ limitations. AgentCoder [[22](#bib.bib22)] utilizes
    a multi-agent framework with specialized agents: the programmer agent, the test
    designer agent, and the test executor agent. The programmer agent focuses on code
    generation and refinement based on feedback from the test executor agent, while
    the test designer agent generates test cases for the code. The test executor agent
    runs the code with the test cases and provides feedback to the programmer. This
    collaborative system aims to improve code generation efficiency, outperforming
    single-agent models and previous strategies. Zhong et al. [[23](#bib.bib23)] introduced
    Large Language Model Debugger (LDB), a framework that segments programs into basic
    blocks and tracks intermediate variable values during runtime. LDB enables LLMs
    to focus on simpler code units, verify their correctness block by block, and pinpoint
    errors effectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast to existing methods and to the best of our knowledge, EPiC is the
    first search-based prompt engineering method for code generation. It employs a
    lightweight process to identify the optimal solution in a cost-efficient manner.
    To achieve this, EPiC utilizes a local embedding function to implement mutation
    operators on text, to reduce the cost of iterative prompt engineering for code
    generation. It also guides the search over iterations using the fitness function
    in Section [4.4](#S4.SS4 "4.4 Objective Function ‣ 4 Experiment Design ‣ EPiC:
    Cost-effective Search-based Prompt Engineering of LLMs for Code Generation"),
    which helpsfindg the most effective prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Evolutionary Prompt Engineering for Code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Algorithm 1 Evolutionary algorithm
  prefs: []
  type: TYPE_NORMAL
- en: 1:procedure EvoALG($prompt,tests,popSize$ then15:               $return$30:end procedure
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 2 Mutation process in $sim\_words\_as\_mutator$
  prefs: []
  type: TYPE_NORMAL
- en: 1:procedure mutate($prompt,sim\_t,num\_t,mutation\_probability$ do15:        if $$random.random()></math>22:end procedure![Refer
    to caption](img/38b2b85f90c130e8ccb03f9a32c97dc2.png)
  prefs: []
  type: TYPE_NORMAL
- en: (a) Original prompt and its result
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b3cceccd291ebb305bf9ab2fcc1a2a83.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Transformed prompt and its result
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: An unsuccessful original prompt on the left and the transformed successful
    version of it on the right. The main issue with the original implementation is
    the incorrect initialization of the Eulerian number. The orange text in both figures
    highlights the code and text related to the initialization of the function.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This section provides a detailed elaboration of EPiC. As depicted in Figure
    [2](#S2.F2 "Figure 2 ‣ 2.1 LLMs for Code Generation ‣ 2 Background and Related
    Work ‣ EPiC: Cost-effective Search-based Prompt Engineering of LLMs for Code Generation"),
    our approach comprises two phases, i.e., the Initial Evaluation (IE) phase and
    the Evolutionary Prompt Engineering (EPE) phase. The process begins with a user
    providing an initial prompt that describes the intended functionality of the code
    to be generated. In the IE phase, we evaluate the prompt by generating the code
    based on the original prompt using an LLM. This evaluation determines whether
    the prompt is sufficient to generate the correct implementation or if it requires
    a further process in the EPE phase. EPiC’s algorithm is presented in Algorithm
    [1](#alg1 "Algorithm 1 ‣ 3 Evolutionary Prompt Engineering for Code ‣ EPiC: Cost-effective
    Search-based Prompt Engineering of LLMs for Code Generation"), which will be elaborated
    upon in the subsequent sections. Lines 2 to 6 in Algorithm [1](#alg1 "Algorithm
    1 ‣ 3 Evolutionary Prompt Engineering for Code ‣ EPiC: Cost-effective Search-based
    Prompt Engineering of LLMs for Code Generation") correspond to the IE phase of
    EPiC and lines 7 to 30 are the implementaion of the EPE phase.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Initial Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Step <svg id="S3.SS1.p1.1.pic1" class="ltx_picture" height="19.62" overflow="visible"
    version="1.1" width="19.62"><g transform="translate(0,19.62) matrix(1 0 0 -1 0
    0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g></svg>
    of the IE phase, the initial prompts for code and test case generation are provided
    to the LLM (the choice of LLM to be used in our case will be discussed in the
    Section [4](#S4 "4 Experiment Design ‣ EPiC: Cost-effective Search-based Prompt
    Engineering of LLMs for Code Generation")) to generate the corresponding code
    and test cases. Subsequently, in Step <svg id="S3.SS1.p1.2.pic2" class="ltx_picture"
    height="19.62" overflow="visible" version="1.1" width="19.62"><g transform="translate(0,19.62)
    matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000"
    stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">2</foreignobject></g></g></svg>, the generated
    code and test cases are sent to the Testcase Evaluation module. evaluatePrompt
    in Line 4 of Algorithm [1](#alg1 "Algorithm 1 ‣ 3 Evolutionary Prompt Engineering
    for Code ‣ EPiC: Cost-effective Search-based Prompt Engineering of LLMs for Code
    Generation") is responsible for generating the code and evaluating the test cases.
    If any test case fails on the code we continue with the EPE phase. If all the
    test cases pass, we stop and report the generated code as the final answer.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that test cases can be provided in various ways. One approach is to use
    developer-provided test cases for evaluation, while another is to generate test
    cases using LLMs. The challenge with developer-provided test cases is that typically
    users do not have test cases before implementation, making the approach impractical
    for a code generation task. On the other hand, the advantage of using LLM-generated
    test cases is that it makes the process fully automated. This method is currently
    used in most SOTA code generation tools [[20](#bib.bib20)], [[21](#bib.bib21)],
    [[22](#bib.bib22)], [[18](#bib.bib18)], and [[29](#bib.bib29)], as part of their
    internal evaluation. Consequently, we also opted to use LLMs for test case generation
    to ensure a fully automated approach, assuming no developer-provided test cases.
    Specifically, we employed OpenAI’s GPT-4o model to generate test cases with few-shot
    examples. To ensure the functional correctness of these test cases, we validated
    them by parsing their Abstract Syntax Trees (AST). A test case is considered valid
    if it successfully parses into a syntactically correct AST. This approach is the
    same approach used in Reflexion [[20](#bib.bib20)].
  prefs: []
  type: TYPE_NORMAL
- en: The prompts for code and test case generation which are used in our experiments
    are provided below. These prompts can be modified if necessary depending on the
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="S3.SS1.p4.pic1" class="ltx_picture" height="61.04" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,61.04) matrix(1 0 0 -1 0 0)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 8.99 8.99)"><foreignobject width="582.01" height="43.05" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Code Generation Prompt You
    are a Python developer that implements the correct code based on the function
    description provided. You are given one or more functions to implement. Don’t
    delete import statements in the code snippet. Use at most 1000 words.</foreignobject></g></g></svg><svg
    id="S3.SS1.p5.pic1" class="ltx_picture" height="228.31" overflow="visible" version="1.1"
    width="600"><g transform="translate(0,228.31) matrix(1 0 0 -1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 8.99 8.99)"><foreignobject width="582.01" height="210.32" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Test Case Generation Prompt
    You are an AI coding assistant who can write unique, diverse, and intuitive unit
    tests for functions given the signature and docstring. Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'def add3Numbers(x, y, z):'
  prefs: []
  type: TYPE_NORMAL
- en: ””” Add three numbers together. This function takes three numbers as input and
    returns the sum of the three numbers.”””
  prefs: []
  type: TYPE_NORMAL
- en: 'unit tests:'
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(1, 2, 3) == 6
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(-1, 2, 3) == 4
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(1, -2, 3) == 2
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(1, 2, -3) == 0
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(-3, -2, -1) == -6
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(0, 0, 0) == 0</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Evolutionary Prompt Engineering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the EPE phase, the first step (Step <svg id="S3.SS2.p1.1.pic1" class="ltx_picture"
    height="19.62" overflow="visible" version="1.1" width="19.62"><g transform="translate(0,19.62)
    matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><foreignobject width="181.55" height="24.17" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Test Case Generation Prompt
  prefs: []
  type: TYPE_NORMAL
- en: 'You are an AI coding assistant who can write unique, diverse, and intuitive
    unit tests for functions given the signature and docstring. Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'def add3Numbers(x, y, z):'
  prefs: []
  type: TYPE_NORMAL
- en: ””” Add three numbers together. This function takes three numbers as input and
    returns the sum of the three numbers.”””
  prefs: []
  type: TYPE_NORMAL
- en: 'unit tests:'
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(1, 2, 3) == 6
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(-1, 2, 3) == 4
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(1, -2, 3) == 2
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(1, 2, -3) == 0
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(-3, -2, -1) == -6
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(0, 0, 0) == 0</foreignobject> <g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g></svg>)
    involves populating the first generation of individuals, where each individual
    is a distinct prompt to generate the same code. In Step <svg id="S3.SS2.p1.2.pic2"
    class="ltx_picture" height="19.62" overflow="visible" version="1.1" width="19.62"><g
    transform="translate(0,19.62) matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><foreignobject width="181.55"
    height="24.17" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Test Case
    Generation Prompt
  prefs: []
  type: TYPE_NORMAL
- en: 'You are an AI coding assistant who can write unique, diverse, and intuitive
    unit tests for functions given the signature and docstring. Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'def add3Numbers(x, y, z):'
  prefs: []
  type: TYPE_NORMAL
- en: ””” Add three numbers together. This function takes three numbers as input and
    returns the sum of the three numbers.”””
  prefs: []
  type: TYPE_NORMAL
- en: 'unit tests:'
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(1, 2, 3) == 6
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(-1, 2, 3) == 4
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(1, -2, 3) == 2
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(1, 2, -3) == 0
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(-3, -2, -1) == -6
  prefs: []
  type: TYPE_NORMAL
- en: 'assert add3Numbers(0, 0, 0) == 0</foreignobject> <g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g></svg>,
    we generate multiple prompts by modifying the initial prompt using an LLM agent.
    Generating this prompt population forms an important part of our evolutionary
    algorithm. A more detailed description of this process will be provided in Section
    [3.2.1](#S3.SS2.SSS1 "3.2.1 Initial Population Builder ‣ 3.2 Evolutionary Prompt
    Engineering ‣ 3 Evolutionary Prompt Engineering for Code ‣ EPiC: Cost-effective
    Search-based Prompt Engineering of LLMs for Code Generation"). This step corresponds
    to line 8 of Algorithm [1](#alg1 "Algorithm 1 ‣ 3 Evolutionary Prompt Engineering
    for Code ‣ EPiC: Cost-effective Search-based Prompt Engineering of LLMs for Code
    Generation").'
  prefs: []
  type: TYPE_NORMAL
- en: In Step <svg id="S3.SS2.p2.1.pic1" class="ltx_picture" height="19.62" overflow="visible"
    version="1.1" width="19.62"><g transform="translate(0,19.62) matrix(1 0 0 -1 0
    0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><foreignobject
    width="181.55" height="24.17" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Test
    Case Generation Prompt
  prefs: []
  type: TYPE_NORMAL
- en: 'You are an AI coding assistant who can write unique, diverse, and intuitive
    unit tests for functions given the signature and docstring. Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'def add3Numbers(x, y, z):'
  prefs: []
  type: TYPE_NORMAL
- en: ””” Add three numbers together. This function takes three numbers as input and
    returns the sum of the three numbers.”””
  prefs: []
  type: TYPE_NORMAL
- en: 'unit tests:'
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(1, 2, 3) == 6
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(-1, 2, 3) == 4
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(1, -2, 3) == 2
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(1, 2, -3) == 0
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(-3, -2, -1) == -6
  prefs: []
  type: TYPE_NORMAL
- en: 'assert add3Numbers(0, 0, 0) == 0</foreignobject> <g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">4</foreignobject></g></g></svg>
    of EPE which corresponds to lines 12 to 20 in Algorithm [1](#alg1 "Algorithm 1
    ‣ 3 Evolutionary Prompt Engineering for Code ‣ EPiC: Cost-effective Search-based
    Prompt Engineering of LLMs for Code Generation"), we generate code for each prompt
    and evaluate each generated code sample using the same test cases. We define a
    fitness function based on the ratio of test cases passed. If any of the prompts
    achieves the maximum fitness score, the process stops. If not, the process continues
    until all prompts are validated and given a fitness score, which will guide the
    selection process. The fitness function is basically the passing rate of each
    prompt. The mathematical formula is explained in Section [4.4](#S4.SS4 "4.4 Objective
    Function ‣ 4 Experiment Design ‣ EPiC: Cost-effective Search-based Prompt Engineering
    of LLMs for Code Generation").'
  prefs: []
  type: TYPE_NORMAL
- en: This fitness function is used to select the candidate prompts for mutation in
    Step <svg id="S3.SS2.p3.1.pic1" class="ltx_picture" height="19.62" overflow="visible"
    version="1.1" width="19.62"><g transform="translate(0,19.62) matrix(1 0 0 -1 0
    0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><foreignobject
    width="181.55" height="24.17" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Test
    Case Generation Prompt
  prefs: []
  type: TYPE_NORMAL
- en: 'You are an AI coding assistant who can write unique, diverse, and intuitive
    unit tests for functions given the signature and docstring. Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'def add3Numbers(x, y, z):'
  prefs: []
  type: TYPE_NORMAL
- en: ””” Add three numbers together. This function takes three numbers as input and
    returns the sum of the three numbers.”””
  prefs: []
  type: TYPE_NORMAL
- en: 'unit tests:'
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(1, 2, 3) == 6
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(-1, 2, 3) == 4
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(1, -2, 3) == 2
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(1, 2, -3) == 0
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(-3, -2, -1) == -6
  prefs: []
  type: TYPE_NORMAL
- en: 'assert add3Numbers(0, 0, 0) == 0</foreignobject> <g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">5</foreignobject></g></g></svg>.
    This is implemented in chooseCandidate function in Algorithm [1](#alg1 "Algorithm
    1 ‣ 3 Evolutionary Prompt Engineering for Code ‣ EPiC: Cost-effective Search-based
    Prompt Engineering of LLMs for Code Generation") (Lines 22 and 26), where a weighted
    random selection algorithm selects candidates randomly, with the probability of
    selection being proportional to their respective fitness score (Section [4.4](#S4.SS4
    "4.4 Objective Function ‣ 4 Experiment Design ‣ EPiC: Cost-effective Search-based
    Prompt Engineering of LLMs for Code Generation")). This selection occurs with
    substitution, allowing the same prompt to be chosen multiple times. In Step <svg
    id="S3.SS2.p3.2.pic2" class="ltx_picture" height="19.62" overflow="visible" version="1.1"
    width="19.62"><g transform="translate(0,19.62) matrix(1 0 0 -1 0 0) translate(9.81,0)
    translate(0,9.81)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><foreignobject
    width="181.55" height="24.17" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Test
    Case Generation Prompt'
  prefs: []
  type: TYPE_NORMAL
- en: 'You are an AI coding assistant who can write unique, diverse, and intuitive
    unit tests for functions given the signature and docstring. Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'def add3Numbers(x, y, z):'
  prefs: []
  type: TYPE_NORMAL
- en: ””” Add three numbers together. This function takes three numbers as input and
    returns the sum of the three numbers.”””
  prefs: []
  type: TYPE_NORMAL
- en: 'unit tests:'
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(1, 2, 3) == 6
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(-1, 2, 3) == 4
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(1, -2, 3) == 2
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(1, 2, -3) == 0
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(-3, -2, -1) == -6
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(0, 0, 0) == 0</foreignobject> <g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">5</foreignobject></g></g></svg>,
    we select $N-1$ candidates for mutation and one prompt for “elitism”.
  prefs: []
  type: TYPE_NORMAL
- en: Following prompt selection, in Step <svg id="S3.SS2.p4.1.pic1" class="ltx_picture"
    height="19.62" overflow="visible" version="1.1" width="19.62"><g transform="translate(0,19.62)
    matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><foreignobject width="181.55" height="24.17" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Test Case Generation Prompt
  prefs: []
  type: TYPE_NORMAL
- en: 'You are an AI coding assistant who can write unique, diverse, and intuitive
    unit tests for functions given the signature and docstring. Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'def add3Numbers(x, y, z):'
  prefs: []
  type: TYPE_NORMAL
- en: ””” Add three numbers together. This function takes three numbers as input and
    returns the sum of the three numbers.”””
  prefs: []
  type: TYPE_NORMAL
- en: 'unit tests:'
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(1, 2, 3) == 6
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(-1, 2, 3) == 4
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(1, -2, 3) == 2
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(1, 2, -3) == 0
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(-3, -2, -1) == -6
  prefs: []
  type: TYPE_NORMAL
- en: 'assert add3Numbers(0, 0, 0) == 0</foreignobject> <g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">6</foreignobject></g></g></svg>,
    EPiC randomly mutates the selected prompt candidates to better explore the search
    space of potential prompts in the next generation. The prompt mutation which is
    detailed in Algorithm [2](#alg2 "Algorithm 2 ‣ 3 Evolutionary Prompt Engineering
    for Code ‣ EPiC: Cost-effective Search-based Prompt Engineering of LLMs for Code
    Generation"), is elaborated in Section [3.2.2](#S3.SS2.SSS2 "3.2.2 Prompt Mutation
    ‣ 3.2 Evolutionary Prompt Engineering ‣ 3 Evolutionary Prompt Engineering for
    Code ‣ EPiC: Cost-effective Search-based Prompt Engineering of LLMs for Code Generation").
    The next generation is formed by adding the elite prompt to the pool of mutated
    prompts. Lines 21 to 27 in Algorithm [1](#alg1 "Algorithm 1 ‣ 3 Evolutionary Prompt
    Engineering for Code ‣ EPiC: Cost-effective Search-based Prompt Engineering of
    LLMs for Code Generation") corresponds to Steps <svg id="S3.SS2.p4.2.pic2" class="ltx_picture"
    height="19.62" overflow="visible" version="1.1" width="19.62"><g transform="translate(0,19.62)
    matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><foreignobject width="181.55" height="24.17" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Test Case Generation Prompt'
  prefs: []
  type: TYPE_NORMAL
- en: 'You are an AI coding assistant who can write unique, diverse, and intuitive
    unit tests for functions given the signature and docstring. Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'def add3Numbers(x, y, z):'
  prefs: []
  type: TYPE_NORMAL
- en: ””” Add three numbers together. This function takes three numbers as input and
    returns the sum of the three numbers.”””
  prefs: []
  type: TYPE_NORMAL
- en: 'unit tests:'
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(1, 2, 3) == 6
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(-1, 2, 3) == 4
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(1, -2, 3) == 2
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(1, 2, -3) == 0
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(-3, -2, -1) == -6
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(0, 0, 0) == 0</foreignobject> <g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">5</foreignobject></g></g></svg>
    and <svg id="S3.SS2.p4.3.pic3" class="ltx_picture" height="19.62" overflow="visible"
    version="1.1" width="19.62"><g transform="translate(0,19.62) matrix(1 0 0 -1 0
    0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><foreignobject
    width="181.55" height="24.17" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Test
    Case Generation Prompt
  prefs: []
  type: TYPE_NORMAL
- en: 'You are an AI coding assistant who can write unique, diverse, and intuitive
    unit tests for functions given the signature and docstring. Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'def add3Numbers(x, y, z):'
  prefs: []
  type: TYPE_NORMAL
- en: ””” Add three numbers together. This function takes three numbers as input and
    returns the sum of the three numbers.”””
  prefs: []
  type: TYPE_NORMAL
- en: 'unit tests:'
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(1, 2, 3) == 6
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(-1, 2, 3) == 4
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(1, -2, 3) == 2
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(1, 2, -3) == 0
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(-3, -2, -1) == -6
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(0, 0, 0) == 0</foreignobject> <g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">6</foreignobject></g></g></svg>.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of EPE is repeated iteratively until a solution achieves maximum
    fitness score or until predefined stopping criteria, i.e., reaching the maximum
    number of iterations or observing no improvement in the fitness scores, are met.
    In such cases, the best-generated code, determined by its fitness, is chosen and
    returned (line 29 of Algorithm [1](#alg1 "Algorithm 1 ‣ 3 Evolutionary Prompt
    Engineering for Code ‣ EPiC: Cost-effective Search-based Prompt Engineering of
    LLMs for Code Generation")).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6c102c97ebac2821cd4d964b1de7cdf0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: An example of a mutation generated by $sim\_words\_as\_mutator$.
    The orange words highlight the selected and mutated words in this example.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next two subsections, we will explain more details about Step <svg id="S3.SS2.p6.1.pic1"
    class="ltx_picture" height="19.62" overflow="visible" version="1.1" width="19.62"><g
    transform="translate(0,19.62) matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><foreignobject width="181.55"
    height="24.17" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Test Case
    Generation Prompt
  prefs: []
  type: TYPE_NORMAL
- en: 'You are an AI coding assistant who can write unique, diverse, and intuitive
    unit tests for functions given the signature and docstring. Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'def add3Numbers(x, y, z):'
  prefs: []
  type: TYPE_NORMAL
- en: ””” Add three numbers together. This function takes three numbers as input and
    returns the sum of the three numbers.”””
  prefs: []
  type: TYPE_NORMAL
- en: 'unit tests:'
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(1, 2, 3) == 6
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(-1, 2, 3) == 4
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(1, -2, 3) == 2
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(1, 2, -3) == 0
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(-3, -2, -1) == -6
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(0, 0, 0) == 0</foreignobject> <g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g></svg>
    (Initial Population Builder), and Step <svg id="S3.SS2.p6.2.pic2" class="ltx_picture"
    height="19.62" overflow="visible" version="1.1" width="19.62"><g transform="translate(0,19.62)
    matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><foreignobject width="181.55" height="24.17" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Test Case Generation Prompt
  prefs: []
  type: TYPE_NORMAL
- en: 'You are an AI coding assistant who can write unique, diverse, and intuitive
    unit tests for functions given the signature and docstring. Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'def add3Numbers(x, y, z):'
  prefs: []
  type: TYPE_NORMAL
- en: ””” Add three numbers together. This function takes three numbers as input and
    returns the sum of the three numbers.”””
  prefs: []
  type: TYPE_NORMAL
- en: 'unit tests:'
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(1, 2, 3) == 6
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(-1, 2, 3) == 4
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(1, -2, 3) == 2
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(1, 2, -3) == 0
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(-3, -2, -1) == -6
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(0, 0, 0) == 0</foreignobject> <g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">6</foreignobject></g></g></svg>
    (Prompt Mutation.)
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 Initial Population Builder
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This subsection provides a detailed explanation of the Step <svg id="S3.SS2.SSS1.p1.1.pic1"
    class="ltx_picture" height="19.62" overflow="visible" version="1.1" width="19.62"><g
    transform="translate(0,19.62) matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><foreignobject width="181.55"
    height="24.17" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Test Case
    Generation Prompt
  prefs: []
  type: TYPE_NORMAL
- en: 'You are an AI coding assistant who can write unique, diverse, and intuitive
    unit tests for functions given the signature and docstring. Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'def add3Numbers(x, y, z):'
  prefs: []
  type: TYPE_NORMAL
- en: ””” Add three numbers together. This function takes three numbers as input and
    returns the sum of the three numbers.”””
  prefs: []
  type: TYPE_NORMAL
- en: 'unit tests:'
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(1, 2, 3) == 6
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(-1, 2, 3) == 4
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(1, -2, 3) == 2
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(1, 2, -3) == 0
  prefs: []
  type: TYPE_NORMAL
- en: assert add3Numbers(-3, -2, -1) == -6
  prefs: []
  type: TYPE_NORMAL
- en: 'assert add3Numbers(0, 0, 0) == 0</foreignobject> <g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g></svg>
    in Figure [2](#S2.F2 "Figure 2 ‣ 2.1 LLMs for Code Generation ‣ 2 Background and
    Related Work ‣ EPiC: Cost-effective Search-based Prompt Engineering of LLMs for
    Code Generation"). Human-written prompts are often very brief and lack the necessary
    information or context. This results in insufficient descriptions and a failure
    to adhere to a well-defined format [[43](#bib.bib43)]. Thus manual prompting typically
    results in several back-and-forth interactions with LLM until all necessary details
    are given [[28](#bib.bib28)]. Providing the initial prompt in a structured and
    elaborate format will guide the LLM to generate better results with fewer interactions [[31](#bib.bib31)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'In our evolutionary algorithm (Algorithm [1](#alg1 "Algorithm 1 ‣ 3 Evolutionary
    Prompt Engineering for Code ‣ EPiC: Cost-effective Search-based Prompt Engineering
    of LLMs for Code Generation")), we require an initial population of prompts. To
    create it, we utilize OpenAI’s GPT-4o to generate multiple prompts based on the
    initial given prompt. The prompt we use is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="S3.SS2.SSS1.p3.pic1" class="ltx_picture" height="94.25" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,94.25) matrix(1 0 0 -1 0 0)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 8.99 8.99)"><foreignobject width="582.01" height="76.26" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Please rewrite the function
    description based on these instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: 1- Add input and output types of the function to the description. 2- Elaborate
    the description so that it is understandable for large language models. 3- Keep
    the original test cases and add three test cases to the description to cover the
    edge cases. Do not separate the generated test cases and the original ones. 4-
    Keep the structure of the function and add the description as a comment in the
    function. Use at most 500 words.</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'We employ a high-temperature setting (0.9) for the LLMto stimulate creativity
    in prompt generation. Elaborate prompts, which include explicit input-output types
    and test cases, more effectively direct the language model to produce the desired
    behavior compared to the original prompts. Figures [3(a)](#S3.F3.sf1 "In Figure
    3 ‣ 3 Evolutionary Prompt Engineering for Code ‣ EPiC: Cost-effective Search-based
    Prompt Engineering of LLMs for Code Generation") and [3(b)](#S3.F3.sf2 "In Figure
    3 ‣ 3 Evolutionary Prompt Engineering for Code ‣ EPiC: Cost-effective Search-based
    Prompt Engineering of LLMs for Code Generation") show an example of such an elaborate
    prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [3(a)](#S3.F3.sf1 "In Figure 3 ‣ 3 Evolutionary Prompt Engineering for
    Code ‣ EPiC: Cost-effective Search-based Prompt Engineering of LLMs for Code Generation")
    illustrates the original prompt provided to an LLM, i.e., GPT-4o, which results
    in an implementation that fails to pass the test cases. Conversely, Figure [3(b)](#S3.F3.sf2
    "In Figure 3 ‣ 3 Evolutionary Prompt Engineering for Code ‣ EPiC: Cost-effective
    Search-based Prompt Engineering of LLMs for Code Generation") depicts a transformed
    version of the prompt that successfully guides the LLM towards a correct implementation.
    The primary issue with the original implementation is the incorrect initialization
    of the Eulerian number. In contrast, the transformed prompt includes specific
    instructions on how to properly initialize the Eulerian number, leading to a correct
    implementation. The orange text in both figures highlights the code and text related
    to the initialization.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After generating the elaborate prompt, we add the previously generated test
    cases (Step <svg id="S3.SS2.SSS1.p6.1.pic1" class="ltx_picture" height="19.62"
    overflow="visible" version="1.1" width="19.62"><g transform="translate(0,19.62)
    matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><foreignobject width="601.3" height="1440.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Please rewrite the function description based
    on these instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '1- Add input and output types of the function to the description. 2- Elaborate
    the description so that it is understandable for large language models. 3- Keep
    the original test cases and add three test cases to the description to cover the
    edge cases. Do not separate the generated test cases and the original ones. 4-
    Keep the structure of the function and add the description as a comment in the
    function. Use at most 500 words.</foreignobject> <g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g></svg>
    in Figure [2](#S2.F2 "Figure 2 ‣ 2.1 LLMs for Code Generation ‣ 2 Background and
    Related Work ‣ EPiC: Cost-effective Search-based Prompt Engineering of LLMs for
    Code Generation")) to each prompt. These test cases will also be used during the
    evolutionary algorithm’s fitness evaluation in Step <svg id="S3.SS2.SSS1.p6.2.pic2"
    class="ltx_picture" height="19.62" overflow="visible" version="1.1" width="19.62"><g
    transform="translate(0,19.62) matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><foreignobject width="601.3"
    height="1440.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Please
    rewrite the function description based on these instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: 1- Add input and output types of the function to the description. 2- Elaborate
    the description so that it is understandable for large language models. 3- Keep
    the original test cases and add three test cases to the description to cover the
    edge cases. Do not separate the generated test cases and the original ones. 4-
    Keep the structure of the function and add the description as a comment in the
    function. Use at most 500 words.</foreignobject> <g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">4</foreignobject></g></g></svg>.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Prompt Mutation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This subsection provides a detailed explanation of the Step <svg id="S3.SS2.SSS2.p1.1.pic1"
    class="ltx_picture" height="19.62" overflow="visible" version="1.1" width="19.62"><g
    transform="translate(0,19.62) matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><foreignobject width="601.3"
    height="1440.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Please
    rewrite the function description based on these instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '1- Add input and output types of the function to the description. 2- Elaborate
    the description so that it is understandable for large language models. 3- Keep
    the original test cases and add three test cases to the description to cover the
    edge cases. Do not separate the generated test cases and the original ones. 4-
    Keep the structure of the function and add the description as a comment in the
    function. Use at most 500 words.</foreignobject> <g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">6</foreignobject></g></g></svg>
    in Figure [2](#S2.F2 "Figure 2 ‣ 2.1 LLMs for Code Generation ‣ 2 Background and
    Related Work ‣ EPiC: Cost-effective Search-based Prompt Engineering of LLMs for
    Code Generation"). We employed two kinds of prompt mutation approaches. In the
    first approach, EPiC uses an LLM to facilitate the mutation process ($LLM\_as\_mutator$)
    is described in Algorithm [2](#alg2 "Algorithm 2 ‣ 3 Evolutionary Prompt Engineering
    for Code ‣ EPiC: Cost-effective Search-based Prompt Engineering of LLMs for Code
    Generation"). We will compare the performance of both approaches in the following
    sections. We use the following prompt for mutation using LLMs:'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S3.SS2.SSS2.p2.pic1" class="ltx_picture" height="61.04" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,61.04) matrix(1 0 0 -1 0 0)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 8.99 8.99)"><foreignobject width="582.01" height="43.05" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">You are a mutation tool. This
    is a Python function and its description. Please mutate the description by enhancing
    its clarity and comprehension for sophisticated language models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the $sim\_words\_as\_mutator$. In this example, selected words are highlighted
    and replaced with their related synonyms by this algorithm. For instance, the
    word “takes” is randomly chosen for mutation. The algorithm first identifies similar
    words for this word, resulting in the following list: [“take”, “make”, “require”,
    “have”, “carry”, “get”, “bring”, “accept”, “lead”, “hold”]. The algorithm then
    randomly selects the word “accept” and replaces it with “take”. This process is
    similarly applied to other randomly selected words, such as “iterates”, “returns”,
    “containing”, “collection”, and “elements”.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiment Design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Research Questions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We define the following research questions to explore the performance of EPiC:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RQ1: How cost-effective are the SOTA LLM-based code generation approaches?
    Motivation: Most of the SOTA LLM-based code generation studies do not include
    a comprehensive approach to evaluate cost-effectiveness. In this RQ, we set out
    to evaluate these tools from the cost-effectiveness perspective.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RQ2: How effective is EPiC in code generation? Motivation: In this RQ, we explore
    the performance of EPiC against SOTA LLM-based code generation tools in terms
    of both cost and effectiveness. We experiment with a large closed-source LLM (RQ2.1)
    and a smaller open-source model(RQ2.2)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RQ2.1: How cost-effective is EPiC compared to the SOTA?'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RQ2.2: How effective is EPiC on smaller open-source LLMs compared to larger
    closed-source models?'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RQ3:What are the impacts of different EA design choices on the cost-effectiveness
    of EPiC? Motivation: The goal of this RQ is to explore the impact of different
    hyperparameters in our approach. We define three sub-RQs to address the impact
    of the mutation generator (RQ3.1), mutation probability and similar word selection
    (RQ3.2), and population size (RQ3.3), as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RQ3.1: How does the complexity of the mutation generator impact the cost-effectiveness
    of EPiC?'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RQ3.2: How does the mutation probability and similar word selection impact
    the performance of EPiC?'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RQ3.3: How does population size impact the cost-effectiveness of EPiC?'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.2 Datasets and Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'TABLE I: Default configuration parameters'
  prefs: []
  type: TYPE_NORMAL
- en: '| $mutation\_probability$ |'
  prefs: []
  type: TYPE_TB
- en: '| $0.4$ |'
  prefs: []
  type: TYPE_TB
- en: 'We used two datasets for our experiments: HumanEval [[8](#bib.bib8)] and MBPP
    [[24](#bib.bib24)].'
  prefs: []
  type: TYPE_NORMAL
- en: The HumanEval is a collection of 164 programming problems designed to evaluate
    the functional correctness of code generated by AI models. Each problem includes
    a function signature, a detailed description of the task (docstring), the function
    body, and multiple unit tests to verify the solution. One major obstacle in making
    fair judgments on the performance of the related works was that some papers did
    not use the HumanEval dataset as is. For instance, there are cases where the samples
    are removed or changed. To have a fair comparison among all methods we transformed
    the original dataset into each tool’s required format (if needed) to re-run their
    code using the original dataset. We will explain this in more detail as we discuss
    the configuration setup for assessing each related work.
  prefs: []
  type: TYPE_NORMAL
- en: The Mostly Basic Programming Problems (MBPP) comprises 974 short Python programming
    tasks designed to be solvable by entry-level programmers. The tasks were crowd-sourced
    from individuals with basic Python knowledge, who provided a problem statement,
    a self-contained Python function solving the problem, and three test cases to
    check for correctness. A subset of 427 tasks was manually inspected and edited
    for consistency and accuracy, referred to as mbpp-sanitized. In this paper, we
    used mbpp-sanitized for our experiments. While experimenting with the SOTA, we
    discovered that they have used different subsets of the MBPP dataset. For this
    reason, their reported results are not comparable. Consequently, we chose to use
    the mbpp-sanitized subset in all our experiments.
  prefs: []
  type: TYPE_NORMAL
- en: For RQ1 and RQ2.1, we used both datasets. For RQ2.2 and RQ3, we used only HumanEval.
    We will explain this in more detail in the corresponding RQs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We employed two LLMs in our study: OpenAI’s GPT-4o²²2https://openai.com/index/hello-gpt-4o/
    and Magicoder-S-DS-6.7B [[11](#bib.bib11)]. The latter model was specifically
    utilized for RQ 2.2, while OpenAI’s GPT-4o model was used for all other experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Evaluation Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To evaluate the quality of the generated code by LLMs, we utilized the pass@k
    metric. This metric, introduced in [[8](#bib.bib8)], is advantageous over metrics
    such as CodeBLEU and ROUGE because it evaluates code behavior rather than text
    semantics, which are the focus of the latter metrics. The formula for pass@k is
    presented in Equation [1](#S4.E1 "In 4.3 Evaluation Metrics ‣ 4 Experiment Design
    ‣ EPiC: Cost-effective Search-based Prompt Engineering of LLMs for Code Generation").
    In this equation, $E$ samples is correct. When applied to multiple problems, it
    evaluates the expectation across all problems.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id="S4.E1.m1.3" class="ltx_Math" alttext="pass@k:=\underset{\text{
    Problems }}{\mathbb{E}}\left[1-\frac{\left(\begin{array}[]{c}n-c\\ k\end{array}\right)}{\left(\begin{array}[]{c}n\\'
  prefs: []
  type: TYPE_NORMAL
- en: k\end{array}\right)}\right]$$ |  | (1) |
  prefs: []
  type: TYPE_NORMAL
- en: When comparing EPiC with baselines, we also report Mann-Whitney U-test results,
    which is a non-parametric significant test, to consider the randomness introduced
    by our algorithm. The tests compare the distribution of 10 pass@1 results for
    EPiC (using 10 random seeds) with the pass@1 of each baseline, separately. Note
    that here we only rerun EPiC with 10 random seeds to consider its internal randomness
    introduced in the algorithm and do not rerun the baselines as they do not have
    internal randomness. In other words, we do not study the potential randomness
    of LLMs results and their effect on each baseline, as this is not reported in
    the original papers and would go beyond the scope of this paper.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Objective Function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We use $\mathcal{X}$. The fitness function is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{F}(x,\mathcal{T})=\frac{\sum_{i=1}^{i=n}t_{i}(\mathcal{S}(x))}{n}$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: The goal of the EPiC framework is to optimize the fitness function $\mathcal{F}$.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Baselines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To evaluate the performance of EPiC, we first identified the most relevant
    papers in code generation that utilized a common benchmark for comparison, i.e.,
    papers that used HumanEval or MBPP, either exclusively or as part of their benchmarks.
    These two benchmarks were chosen due to their popularity in the field. Further
    narrowing down the selection, we applied three additional criteria: (1) whether
    their results were competitive (more than %90 pass@1 on HumanEval), (2) whether
    their GitHub repository is available online, and (3) their cost is not disproportionately
    high. The third criterion was specifically defined to exclude SOTA tools with
    excessively high costs, ensuring that the selected tools remained competitive
    in terms of cost-effectiveness. For comparison, we used the pass@1 metric. As
    a result, four papers are selected as our baselines, i.e., Reflexion[[20](#bib.bib20)],
    LATS[[21](#bib.bib21)], LDB[[23](#bib.bib23)], and AgentCoder[[22](#bib.bib22)]
    (details are in Section [2](#S2 "2 Background and Related Work ‣ EPiC: Cost-effective
    Search-based Prompt Engineering of LLMs for Code Generation")). For each baseline,
    we ran the experiments with the recommended configuration provided in its GitHub
    repository or provided in its paper. After initial evaluation, AgentCoder[[22](#bib.bib22)]
    was excluded from further analysis as we found out that it’s 6 times more expensive
    than the second most expensive alternative. The baselines’ configuration details
    used in our experiments are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Reflexion: We ran Reflexion with $max\_iters$ set to “gpt4o”. Note that, we
    have adapted Reflexion’s code base to support the “gpt4o” model and tracking of
    time cost as these features were unavailable in Reflexion’s original version.
    During our experiments, we found inconsistencies between the HumanEval dataset
    on their GitHub and the original dataset. There were missing instances and modified
    or removed test cases compared to the original dataset. We reported this issue
    in their repository³³3[Link to Reflexion dataset issue](https://github.com/noahshinn/reflexion/issues/42).
    To ensure a valid assessment of their work, we used the original HumanEval dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'LATS: We ran the experiments with $max\_iters$ should be defined as passing
    the original test cases, not the generated ones. We corrected this issue in their
    code (and reported it to the authors) before running the experiments. While they
    have now resolved this issue in their codebase yet, the results in their paper
    had not been updated at the time of writing this paper⁴⁴4[LATS bug link](https://github.com/lapisrocks/LanguageAgentTreeSearch/commit/853d81614607dd27433faf17c7b0a7d660f95d22).
    Like Reflexion, they also used LLM-generated test cases for internal evaluations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'LDB: We set $pass@k$ to “ldb”, and the model to “gpt4o”. Consistent with previous
    works, we implemented the “gpt4o” model and tracking of time cost features as
    they were unavailable in the original version. The authors used seed programs
    in their code to enhance their results. The seed programs served as the initial
    implementation of the code. We removed these seed files to ensure fairness, as
    other baselines do not use any seeds (an initial given output code) to boost their
    results. Upon reviewing their dataset, we found no mismatches with the original
    dataset. However, we identified inconsistencies between the actual test cases
    in the function descriptions and the test cases they used as their visible test
    cases in the codebase, in HumanEval dataset. It appears that some test cases were
    manually modified. This poses an issue for a fair comparison. In instances where
    we identified such cases, we adjusted them to ensure no manual intervention. As
    a result, we replaced the original extracted test cases with the visible test
    cases to ensure a fair comparison before conducting our experiments and we reported
    this issue in their GitHub repository to be fixed⁵⁵5[LDB GitHub issue link](https://github.com/FloridSleeves/LLMDebugger/issues/12).'
  prefs: []
  type: TYPE_NORMAL
- en: For the MBPP experiments, LATS and Reflexion employed the same code generation
    approach to produce test cases, using these for internal evaluations. In contrast,
    LDB utilized the test cases within the MBPP dataset for the internal evaluation.
    We determined that using the original test cases for internal evaluation is unfair
    to other baselines and impractical, as these test cases are meant solely for final
    evaluation, not for use during the process (in practice, test cases are not typically
    available before the code is developed). Consequently, we modified their test
    cases to use the generated test cases produced by the method employed in LATS
    and Reflexion, so we can have a practical approach implemented across the baselines
    and a fair comparison.
  prefs: []
  type: TYPE_NORMAL
- en: 4.6 Experimental Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'TABLE II: Cost and effectiveness of LDB, LATS, Reflexion, and EPiC on HumanEval
    and MBPP datasets. The cells with green background color indicate the best scores
    and the red ones indicate the worst scores. EPiC results are the mean over 10
    runs.'
  prefs: []
  type: TYPE_NORMAL
- en: '| $Dataset$ |'
  prefs: []
  type: TYPE_TB
- en: '| Humaneval | Reflexion | $\%87$ |'
  prefs: []
  type: TYPE_TB
- en: '| Humaneval | LDB | $\%92$ |'
  prefs: []
  type: TYPE_TB
- en: '| Humaneval | LATS | $\%91$ |'
  prefs: []
  type: TYPE_TB
- en: '| Humaneval | EPiC | $\%94$ | – |'
  prefs: []
  type: TYPE_TB
- en: '| MBPP | Reflexion | $\%71$ |'
  prefs: []
  type: TYPE_TB
- en: '| MBPP | LDB | $\%73$ |'
  prefs: []
  type: TYPE_TB
- en: '| MBPP | LATS | $\%76$ |'
  prefs: []
  type: TYPE_TB
- en: '| MBPP | EPiC | $\%79$ | – |'
  prefs: []
  type: TYPE_TB
- en: '| $Tool$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $EPiC$ |'
  prefs: []
  type: TYPE_TB
- en: '| $EPiC$ |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE III: EPiC’s statistical results from 10 runs on HumanEval and MBPP'
  prefs: []
  type: TYPE_NORMAL
- en: For all the experiments, we selected gpt-4o as the base LLM model, because it
    is OpenAI’s latest model, offering superior speed and cost-efficiency while maintaining
    equal intelligence compared to GPT-4. We conducted experiments on the HumanEval
    and MBPP datasets, utilizing the baseline methods in RQ1 and EPiC in RQ2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our default configuration setups with EPiC are presented in Table [I](#S4.T1
    "TABLE I ‣ 4.2 Datasets and Models ‣ 4 Experiment Design ‣ EPiC: Cost-effective
    Search-based Prompt Engineering of LLMs for Code Generation"). The default configuration
    is chosen based on the experiments in Section [5.3](#S5.SS3 "5.3 RQ3: What are
    the impacts of different EA design choices on the cost-effectiveness of EPiC?
    ‣ 5 Experiment Results ‣ EPiC: Cost-effective Search-based Prompt Engineering
    of LLMs for Code Generation"). Fine-tuning EPiC’s hyper-parameters properly (e.g.,
    using a grid search over all combinations of input space in the hyper-parameters)
    would be extremely expensive. However, RQ3’s sensitivity analysis already provides
    an acceptable (not optimal) configuration to set as our default. We used the default
    configuration for RQ2.1 and changed it in the subsequent RQs, which we will explain
    later. In the default configuration, we utilized OpenAI’s GPT-4o as the code generator
    LLM agent. We evaluated our approach on both the HumanEval and MBPP datasets.
    Our mutation process employs the sim_words_as_mutator method. The Mutation_probability
    parameter specifies the likelihood of a word being substituted with a similar
    word. Population_size refers to the number of populations processed for each dataset
    instance, while Top_n indicates the top number of related words considered for
    the mutation process.'
  prefs: []
  type: TYPE_NORMAL
- en: The experiments for RQ1, RQ2.1, and RQ3 were conducted on a device equipped
    with 16 GB of RAM and a 13th Gen Intel® Core™ i7-13700HX × 24 CPU. For RQ2.2,
    the experiments were performed on one node of the Alliance Canada server, utilizing
    two 32 GB Tesla V100 GPUs and 52 GB of RAM.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experiment Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '5.1 RQ1: How cost-effective are the SOTA LLM-based code generation tools?'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We evaluated LATS, LDB, and Reflexion on the entire HumanEval and Sanitized
    MBPP datasets. To monitor the monetary costs, we used the OpenAI dashboard. This
    cost is calculated based on the number of tokens exchanged with the OpenAI API.
    As the financial cost is calculated based on the number of exchanged tokens, we
    chose the dollar cost reported in the OpenAI dashboard as the metric to measure
    the cost of an approach. The results of this experiment are presented in Table
    [II](#S4.T2 "TABLE II ‣ 4.6 Experimental Setup ‣ 4 Experiment Design ‣ EPiC: Cost-effective
    Search-based Prompt Engineering of LLMs for Code Generation"), for HumanEval and
    MBPP.'
  prefs: []
  type: TYPE_NORMAL
- en: The first observation is that there is a notable discrepancy between our achieved
    $pass@1$ results and those reported in the original papers of the baselines. One
    possible reason is that we used GPT-4o for our experiments, whereas the original
    studies used other OpenAI models. Another plausible reason could be inconsistencies
    between the original dataset and the ones they used, as we utilized the original
    dataset while they used a modified version.
  prefs: []
  type: TYPE_NORMAL
- en: Reflexion incurred the lowest cost in both datasets but also demonstrated the
    lowest performance, with an overall accuracy of %75 and a cost of ¢1.1 per instance.
    Conversely, LATS exhibited the highest time and dollar costs in both datasets,
    with a total cost of $\$96.71$ but ranked second on MBPP overall. LATS achieved
    the best performance overall but had the highest cost, considerably higher than
    both Reflexion and LDB. On the other hand, LDB provided better performance than
    Reflexion without significantly increasing the cost.
  prefs: []
  type: TYPE_NORMAL
- en: A noteworthy observation is that the cost (measured by dollar amount) and effectiveness
    (measured by $pass@1$ increase in cost. This observation aligns with the understanding
    that some programming problems are inherently more challenging and require more
    time for both humans and LLMs to solve correctly. However, it is crucial to develop
    an approach that achieves higher performance without a substantial increase in
    cost, ensuring cost-effectiveness. This emphasizes that future studies in this
    field must always report their cost as part of the evaluation as well.
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="S5.SS1.p5.pic1" class="ltx_picture" height="139.48" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,139.48) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="111.93" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Answer
    to RQ1: The SOTA code generation tools vary in cost-effectiveness, and the relationship
    between cost and effectiveness is not always linear. For instance, in MBPP dataset,
    LATS demonstrates the best performance (%76 pass@1) but incurs almost 6 times
    more cost compared to the second best baseline, LDB, with only %3 less pass@1\.
    On the other hand, Reflexion shows slightly lower performance (%71 pass@1), but
    with a very low cost ($4.9). Therefore, we recommend a cost analysis always be
    a part of the evaluation when studying LLMs for code generation.</foreignobject></g></g></svg><svg
    id="S5.F5.pic1" class="ltx_picture ltx_centering" height="167.42" overflow="visible"
    version="1.1" width="205.99"><g transform="translate(0,167.42) matrix(1 0 0 -1
    0 0) translate(31.76,0) translate(0,25.67) matrix(1.0 0.0 0.0 1.0 -31.76 -25.67)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g class="ltx_nestedsvg"
    transform="matrix(1 0 0 1 0 0) translate(31.76,0) translate(0,25.67)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g clip-path="url(#pgfcp1)"><g stroke-width="0.2pt"
    fill="#808080" stroke="#808080" color="#808080"><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g></g> <g clip-path="url(#pgfcp2)"><g stroke-width="0.2pt"
    fill="#808080" stroke="#808080" color="#808080"><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g></g> <foreignobject width="600.47" height="602.45"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You are
    a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <foreignobject width="600.47" height="602.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You are a mutation tool. This
    is a Python function and its description. Please mutate the description by enhancing
    its clarity and comprehension for sophisticated language models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 -2.42 -9.75)"
    fill="#000000" stroke="#000000"><foreignobject width="4.84" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$0$</foreignobject></g><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 64.48 -9.75)"
    fill="#000000" stroke="#000000"><foreignobject width="4.84" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$5$</foreignobject></g><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 128.97 -9.75)"
    fill="#000000" stroke="#000000"><foreignobject width="9.69" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$10$</foreignobject></g><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 -13.19 11.03)"
    fill="#000000" stroke="#000000"><foreignobject width="9.69" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$86$</foreignobject></g><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 -13.19 39.32)"
    fill="#000000" stroke="#000000"><foreignobject width="9.69" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$88$</foreignobject></g><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 -13.19 67.62)"
    fill="#000000" stroke="#000000"><foreignobject width="9.69" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$90$</foreignobject></g><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 -13.19 95.91)"
    fill="#000000" stroke="#000000"><foreignobject width="9.69" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$92$</foreignobject></g><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 -13.19 124.21)"
    fill="#000000" stroke="#000000"><foreignobject width="9.69" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$94$</foreignobject></g><g clip-path="url(#pgfcp3)"><g
    stroke="#FF0000" fill="#FF0000"><foreignobject width="600.47" height="602.45"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You are
    a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g> <g stroke="#0000FF" fill="#0000FF"><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">You are a mutation tool. This is a Python function and its description.
    Please mutate the description by enhancing its clarity and comprehension for sophisticated
    language models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g> <g stroke="#00FF00" fill="#00FF00"><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">You are a mutation tool. This is a Python function and its description.
    Please mutate the description by enhancing its clarity and comprehension for sophisticated
    language models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g> <g stroke="#FFFF00" fill="#FFFF00"><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">You are a mutation tool. This is a Python function and its description.
    Please mutate the description by enhancing its clarity and comprehension for sophisticated
    language models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g></g> <g stroke="#FF0000" fill="#FF0000" color="#FF0000"><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">You are a mutation tool. This is a Python function and its description.
    Please mutate the description by enhancing its clarity and comprehension for sophisticated
    language models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g> <g stroke="#0000FF" fill="#0000FF" color="#0000FF"><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">You are a mutation tool. This is a Python function and its description.
    Please mutate the description by enhancing its clarity and comprehension for sophisticated
    language models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g> <g stroke="#00FF00" fill="#00FF00" color="#00FF00"><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">You are a mutation tool. This is a Python function and its description.
    Please mutate the description by enhancing its clarity and comprehension for sophisticated
    language models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g> <g stroke="#FFFF00" fill="#FFFF00" color="#FFFF00"><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g> <foreignobject width="600.47" height="602.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You are a mutation tool. This
    is a Python function and its description. Please mutate the description by enhancing
    its clarity and comprehension for sophisticated language models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 78.52 -22.44)"
    fill="#000000" stroke="#000000"><foreignobject width="16.92" height="5.96" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$cost$</foreignobject></g><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(0.0 1.0 -1.0 0.0 -21.81 55.01)"
    fill="#000000" stroke="#000000"><foreignobject width="31.45" height="8.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$pass@1$</foreignobject></g><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">You are a mutation tool. This is a Python function and its description.
    Please mutate the description by enhancing its clarity and comprehension for sophisticated
    language models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g fill="#FFFFFF" stroke="#000000" transform="matrix(1.0
    0.0 0.0 1.0 89.9 7.29)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0
    47.725)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 6.86)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 24.18 0) translate(25.12,0)
    matrix(1.0 0.0 0.0 1.0 -22.35 -2.64)" fill="#000000" stroke="#000000"><foreignobject
    width="44.7" height="8.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$Reflexion$</foreignobject></g></g></g></g></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: Comparison on HumanEval'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S5.F6.pic1" class="ltx_picture ltx_centering" height="173.49" overflow="visible"
    version="1.1" width="213.79"><g transform="translate(0,173.49) matrix(1 0 0 -1
    0 0) translate(31.76,0) translate(0,25.67) matrix(1.0 0.0 0.0 1.0 -31.76 -25.67)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g class="ltx_nestedsvg"
    transform="matrix(1 0 0 1 0 0) translate(31.76,0) translate(0,25.67)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g clip-path="url(#pgfcp4)"><g stroke-width="0.2pt"
    fill="#808080" stroke="#808080" color="#808080"><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g></g> <g clip-path="url(#pgfcp5)"><g stroke-width="0.2pt"
    fill="#808080" stroke="#808080" color="#808080"><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g></g> <foreignobject width="600.47" height="602.45"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You are
    a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <foreignobject width="600.47" height="602.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You are a mutation tool. This
    is a Python function and its description. Please mutate the description by enhancing
    its clarity and comprehension for sophisticated language models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 -2.42 -9.75)"
    fill="#000000" stroke="#000000"><foreignobject width="4.84" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$0$</foreignobject></g><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 41.07 -9.75)"
    fill="#000000" stroke="#000000"><foreignobject width="4.84" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$5$</foreignobject></g><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 82.13 -9.75)"
    fill="#000000" stroke="#000000"><foreignobject width="9.69" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$10$</foreignobject></g><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 125.62 -9.75)"
    fill="#000000" stroke="#000000"><foreignobject width="9.69" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$15$</foreignobject></g><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 169.11 -9.75)"
    fill="#000000" stroke="#000000"><foreignobject width="9.69" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$20$</foreignobject></g><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 -13.19 -3.12)"
    fill="#000000" stroke="#000000"><foreignobject width="9.69" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$70$</foreignobject></g><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 -13.19 25.17)"
    fill="#000000" stroke="#000000"><foreignobject width="9.69" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$72$</foreignobject></g><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 -13.19 53.47)"
    fill="#000000" stroke="#000000"><foreignobject width="9.69" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$74$</foreignobject></g><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 -13.19 81.76)"
    fill="#000000" stroke="#000000"><foreignobject width="9.69" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$76$</foreignobject></g><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 -13.19 110.06)"
    fill="#000000" stroke="#000000"><foreignobject width="9.69" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$78$</foreignobject></g><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 -13.19 138.35)"
    fill="#000000" stroke="#000000"><foreignobject width="9.69" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$80$</foreignobject></g><g clip-path="url(#pgfcp6)"><g
    stroke="#FF0000" fill="#FF0000"><foreignobject width="600.47" height="602.45"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You are
    a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g> <g stroke="#0000FF" fill="#0000FF"><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">You are a mutation tool. This is a Python function and its description.
    Please mutate the description by enhancing its clarity and comprehension for sophisticated
    language models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g> <g stroke="#00FF00" fill="#00FF00"><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">You are a mutation tool. This is a Python function and its description.
    Please mutate the description by enhancing its clarity and comprehension for sophisticated
    language models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g> <g stroke="#FFFF00" fill="#FFFF00"><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">You are a mutation tool. This is a Python function and its description.
    Please mutate the description by enhancing its clarity and comprehension for sophisticated
    language models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g></g> <g stroke="#FF0000" fill="#FF0000" color="#FF0000"><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">You are a mutation tool. This is a Python function and its description.
    Please mutate the description by enhancing its clarity and comprehension for sophisticated
    language models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g> <g stroke="#0000FF" fill="#0000FF" color="#0000FF"><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">You are a mutation tool. This is a Python function and its description.
    Please mutate the description by enhancing its clarity and comprehension for sophisticated
    language models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g> <g stroke="#00FF00" fill="#00FF00" color="#00FF00"><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">You are a mutation tool. This is a Python function and its description.
    Please mutate the description by enhancing its clarity and comprehension for sophisticated
    language models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g> <g stroke="#FFFF00" fill="#FFFF00" color="#FFFF00"><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject></g> <foreignobject width="600.47" height="602.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You are a mutation tool. This
    is a Python function and its description. Please mutate the description by enhancing
    its clarity and comprehension for sophisticated language models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(1.0 0.0 0.0 1.0 78.52 -22.44)"
    fill="#000000" stroke="#000000"><foreignobject width="16.92" height="5.96" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$cost$</foreignobject></g><foreignobject width="600.47"
    height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFF00">You
    are a mutation tool. This is a Python function and its description. Please mutate
    the description by enhancing its clarity and comprehension for sophisticated language
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g transform="matrix(0.0 1.0 -1.0 0.0 -21.81 55.01)"
    fill="#000000" stroke="#000000"><foreignobject width="31.45" height="8.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$pass@1$</foreignobject></g><foreignobject
    width="600.47" height="602.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#FFFF00">You are a mutation tool. This is a Python function and its description.
    Please mutate the description by enhancing its clarity and comprehension for sophisticated
    language models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please put the changed description between #Explanation and #End. Use at most
    600 words.</foreignobject> <g fill="#FFFFFF" stroke="#000000" transform="matrix(1.0
    0.0 0.0 1.0 89.9 7.29)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0
    47.725)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 6.86)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 24.18 0) translate(25.12,0)
    matrix(1.0 0.0 0.0 1.0 -22.35 -2.64)" fill="#000000" stroke="#000000"><foreignobject
    width="44.7" height="8.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$Reflexion$</foreignobject></g></g></g></g></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Comparison on MBPP'
  prefs: []
  type: TYPE_NORMAL
- en: '5.2 RQ2: How effective is EPiC in code generation?'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In addressing RQ2.1, we accounted for the inherent randomness in our algorithm
    by testing EPiC with different seeds. The seed was utilized in Python’s random
    function, allowing us to control the algorithm’s randomness by setting the seed
    to a predefined value. Specifically, randomness was employed in lines 22 and 26
    of Algorithm [1](#alg1 "Algorithm 1 ‣ 3 Evolutionary Prompt Engineering for Code
    ‣ EPiC: Cost-effective Search-based Prompt Engineering of LLMs for Code Generation")
    and lines 15 and 18 of Algorithm [2](#alg2 "Algorithm 2 ‣ 3 Evolutionary Prompt
    Engineering for Code ‣ EPiC: Cost-effective Search-based Prompt Engineering of
    LLMs for Code Generation"). We run EPiC with 10 unique random seeds, and we report
    the median, minimum, and maximum pass@1 metrics across all runs. In this RQ, we
    empirically compare the results with those from RQ1 to assess the cost-effectiveness
    of EPiC compared to the SOTA.'
  prefs: []
  type: TYPE_NORMAL
- en: '5.2.1 RQ2.1: How cost-effective is EPiC compared to the SOTA?'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We evaluated EPiC on HumanEval and MBPP, each 10 times using 10 unique random
    seeds. The statistical results for this experiment are presented in Table [III](#S4.T3
    "TABLE III ‣ 4.6 Experimental Setup ‣ 4 Experiment Design ‣ EPiC: Cost-effective
    Search-based Prompt Engineering of LLMs for Code Generation"). The results on
    HumanEval indicate that the median $pass@1$) and the SOTA on Humaneval is presented
    in Figure [5](#S5.F5 "Figure 5 ‣ 5.1 RQ1: How cost-effective are the SOTA LLM-based
    code generation tools? ‣ 5 Experiment Results ‣ EPiC: Cost-effective Search-based
    Prompt Engineering of LLMs for Code Generation").'
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of cost, EPiC demonstrated a much lower cost than LATS, and almost
    the same cost as LDB, although they ranked after Reflexion. Nonetheless, EPiC
    displayed considerably better performance with a slightly higher cost compared
    to Reflexion. This cost is significantly lower than that of LATS and is comparable
    to LDB. As shown in Figure [5](#S5.F5 "Figure 5 ‣ 5.1 RQ1: How cost-effective
    are the SOTA LLM-based code generation tools? ‣ 5 Experiment Results ‣ EPiC: Cost-effective
    Search-based Prompt Engineering of LLMs for Code Generation"), EPiC achieves the
    best performance with a marginally higher cost than the lowest-cost baseline,
    making it the most cost-effective approach among the SOTA. We also conducted a
    one-sample Wilcoxon test over the 10 runs of EPiC. Our analysis of results on
    the HumanEval dataset revealed that the pass@1 differences reported for EPiC compared
    to Reflexion, LATS, and LDB are statistically significant, with a p-value of 0.0025.'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, on the MBPP dataset, EPiC’s median $pass@1$1). This finding is consistent
    with results from the HumanEval dataset, further underscoring the robustness of
    the EPiC. Our statistical analysis of the MBPP dataset demonstrates that the differences
    between EPiC’s pass@1 results over the 10 runs and the pass@1 results of the baselines
    are again statistically significant, with a p-value of 0.0025 for all three baselines.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, the robustness of the EPiC is highlighted by its consistent performance
    on the HumanEval and MBPP datasets. On HumanEval, EPiC achieved a %93.5 $pass@1$
    was %79 with a variance of 0.55, showcasing its reliability.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IV: Performance of EPiC on Humaneval with an open-source LLM, i.e., magiccoder'
  prefs: []
  type: TYPE_NORMAL
- en: '| $LLM$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $magiccoder$ |'
  prefs: []
  type: TYPE_TB
- en: '| $magiccoder$ |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE V: The impact of different design choices of EPiC on HumanEval. The highlighted
    cells indicate the changes per approach relative to the base settings.'
  prefs: []
  type: TYPE_NORMAL
- en: '| $row$ |'
  prefs: []
  type: TYPE_TB
- en: '| $1$ |'
  prefs: []
  type: TYPE_TB
- en: '| $2$ |'
  prefs: []
  type: TYPE_TB
- en: '| $3$ |'
  prefs: []
  type: TYPE_TB
- en: '| $4$ |'
  prefs: []
  type: TYPE_TB
- en: '| $5$ |'
  prefs: []
  type: TYPE_TB
- en: '| $6$ |'
  prefs: []
  type: TYPE_TB
- en: '| $7$ |'
  prefs: []
  type: TYPE_TB
- en: '| $8$ |'
  prefs: []
  type: TYPE_TB
- en: '5.2.2 RQ2.2: How effective is EPiC on smaller open-source LLMs compared to
    larger closed-source models?'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this RQ, we replaced EPiC’s LLM component, i.e., GPT4o, with a smaller open-source
    model, i.e., MagicCoder [[11](#bib.bib11)]. All other settings remain the same
    as in RQ2.1\. We chose MagicCoder because it has the highest performance, in the
    literature, among open-source LLMs at the time of designing these experiments.
    Note that given the low variance of the results in RQ2.1, over the 10 runs, we
    only ran this experiment once. Due to extensive cost, we also limit this experiment
    to the HumanEval dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We evaluated the performance of $magiccoder$ has 7 billion parameters, making
    it significantly less complex than OpenAI’s model. However, this experiment demonstrates
    that the language understanding of smaller models is sufficiently sophisticated
    that a prompt mutation strategy can enhance its performance. According to the
    results, EPiC proves to be an effective approach even when applied to smaller
    LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="S5.SS2.SSS2.p3.pic1" class="ltx_picture" height="103.59" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,103.59) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="76.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Answer
    to RQ2: EPiC outperforms the SOTA baselines by $\%1$) by %9 on HumanEval.</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: '5.3 RQ3: What are the impacts of different EA design choices on the cost-effectiveness
    of EPiC?'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this RQ, we measure the impact of different design choices (hyper-parameters)
    of EPiC on HumanEval. It is important to emphasize that the goal of this question
    is to analyze the sensitivity of the results over each hyper-parameter’s value.
    Thus we can independently analyze each hyper-parameter. In other words, we do
    NOT aim to optimize the hyper-parameters here, since that fine-tuning would require
    much more extensive study (e.g., a grid search over all combinations).
  prefs: []
  type: TYPE_NORMAL
- en: In addition, note that given the low variance of the results in RQ2.1, over
    the 10 runs, we only run the RQ3 experiment once, and due to its extensive cost
    (multiple runs per hyper-parameter), we also limit this RQ to the HumanEval dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first row in Table [V](#S5.T5 "TABLE V ‣ 5.2.1 RQ2.1: How cost-effective
    is EPiC compared to the SOTA? ‣ 5.2 RQ2: How effective is EPiC in code generation?
    ‣ 5 Experiment Results ‣ EPiC: Cost-effective Search-based Prompt Engineering
    of LLMs for Code Generation") represents the base configuration selected for this
    RQ. We set $mutation\_tool$ was selected for the similarity word selection method
    as a robust option. We again explore this in more detail in RQ3.3.'
  prefs: []
  type: TYPE_NORMAL
- en: '5.3.1 RQ3.1: How does the complexity of the mutation generator impact the cost-effectiveness
    of EPiC?'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We compared two mutation generator methods: (LLM_as_mutator and sim_words_as_mutator).
    LLM_as_mutator utilizes an LLM in the mutation process, whereas sim_words_as_mutator
    employs locally calculated similarity matrices for mutation. The first method
    is more advanced but more costly too, while the second approach, significantly
    simpler, incurs minimal cost.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The results for this RQ are presented in rows 1 and 2 of Table [V](#S5.T5 "TABLE
    V ‣ 5.2.1 RQ2.1: How cost-effective is EPiC compared to the SOTA? ‣ 5.2 RQ2: How
    effective is EPiC in code generation? ‣ 5 Experiment Results ‣ EPiC: Cost-effective
    Search-based Prompt Engineering of LLMs for Code Generation"). We observed a $\%94$
    involves higher costs and is expected to produce better prompts. The selection
    of the prompt used in the mutation process also influences the results. The prompt
    may be an instruction to simply alter the words in the text or it can do more
    complex modifications, such as elaborating on or summarizing the original text.
    As explained in Section [3.2.2](#S3.SS2.SSS2 "3.2.2 Prompt Mutation ‣ 3.2 Evolutionary
    Prompt Engineering ‣ 3 Evolutionary Prompt Engineering for Code ‣ EPiC: Cost-effective
    Search-based Prompt Engineering of LLMs for Code Generation"), we used a prompt
    to elaborate on the original text for the mutation process. We leave the study
    on the choice of prompt for mutation to future research, as it falls outside the
    scope of this paper.'
  prefs: []
  type: TYPE_NORMAL
- en: '5.3.2 RQ3.2: How do the mutation probability and similar word selection method
    impact the performance of EPiC?'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The mutation probability defines the likelihood of a word being substituted
    with a similar word. We set the mutation_probability to $0.1$, we always select
    the word with the highest similarity, which avoids randomness when selecting the
    mutant word.
  prefs: []
  type: TYPE_NORMAL
- en: 'The results for this research question are presented in Table [V](#S5.T5 "TABLE
    V ‣ 5.2.1 RQ2.1: How cost-effective is EPiC compared to the SOTA? ‣ 5.2 RQ2: How
    effective is EPiC in code generation? ‣ 5 Experiment Results ‣ EPiC: Cost-effective
    Search-based Prompt Engineering of LLMs for Code Generation") (rows 3, 4, 5, 6,
    and 7). We employed various mutation probabilities of $0.25$, indicating that
    the most similar word is the only selected word, which eliminates randomness in
    the word selection process.'
  prefs: []
  type: TYPE_NORMAL
- en: When the algorithm included randomness, the $pass@1$, indicating a decrease
    in performance.
  prefs: []
  type: TYPE_NORMAL
- en: These experiments suggest that incorporating randomness into EPiC may enhance
    its ability to identify correct solutions. This is likely because the algorithm
    can explore the search space more effectively, thereby avoiding local optima in
    certain instances.
  prefs: []
  type: TYPE_NORMAL
- en: Although our threshold-based $sim\_word\_selection$ word selection method is
    in theory a more robust solution. Since with the threshold-based method, there
    is a risk that no word may pass the threshold, or conversely, too many words may
    pass the threshold. In contrast, the fixed number method consistently ensures
    that the selection occurs among a predetermined number of similar words, providing
    greater reliability on the search space size.
  prefs: []
  type: TYPE_NORMAL
- en: '5.3.3 RQ3.3: How does population size impact the cost-effectiveness of EPiC?'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The population size defines the number of prompts in each generation. This plays
    an important role in the cost of EPiC. We tested different population sizes, specifically
    $5$, to evaluate the contribution of population size to the cost-effectiveness
    of EPiC.
  prefs: []
  type: TYPE_NORMAL
- en: 'The population size is a critical parameter in the EPiC algorithm, significantly
    influencing both its cost and performance. In our final experiment, as detailed
    in Table [V](#S5.T5 "TABLE V ‣ 5.2.1 RQ2.1: How cost-effective is EPiC compared
    to the SOTA? ‣ 5.2 RQ2: How effective is EPiC in code generation? ‣ 5 Experiment
    Results ‣ EPiC: Cost-effective Search-based Prompt Engineering of LLMs for Code
    Generation"), we set the population size to $10$ metric showed a modest improvement
    of %1\. This observation underscores a fundamental aspect of population-based
    optimization algorithms: larger population sizes generally enhance the likelihood
    of identifying the correct solution before the algorithm reaches its stopping
    criteria. However, this benefit comes with a trade-off. Increasing the population
    size also increases the associated computational cost.'
  prefs: []
  type: TYPE_NORMAL
- en: In our specific experiment, a population size of $5$. It is essential to note,
    though, that this conclusion is dataset-dependent. The optimal population size
    for cost-effectiveness and performance may vary across different datasets, necessitating
    further empirical studies to generalize this finding.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, when configuring the EPiC algorithm, one must carefully consider the trade-off
    between the desired accuracy and the computational cost, tailoring the population
    size to the specific characteristics of the dataset and of the task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, we chose the most cost-effective configuration from Table [V](#S5.T5
    "TABLE V ‣ 5.2.1 RQ2.1: How cost-effective is EPiC compared to the SOTA? ‣ 5.2
    RQ2: How effective is EPiC in code generation? ‣ 5 Experiment Results ‣ EPiC:
    Cost-effective Search-based Prompt Engineering of LLMs for Code Generation") as
    the default configuration for experiments in RQ2\. But as explained in Section
    [6](#S6 "6 Threats to Validity ‣ EPiC: Cost-effective Search-based Prompt Engineering
    of LLMs for Code Generation") this might not be the most optimal configuration.'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="S5.SS3.SSS3.p6.pic1" class="ltx_picture" height="149.75" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,149.75) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="122.19" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Answer
    to RQ3: EPiC’s performance is slightly impacted by the choice of its hyper-parameters.
    Incorporating randomness in mutation probabilities and word selection methods
    enhances EPiC’s performance by $\%1$ on this dataset but also increases computational
    costs, highlighting the need to balance these factors based on specific datasets.
    Certain configuration choices involve a trade-off between cost and performance.
    These choices should be tailored to the particular task and dataset to ensure
    cost-effectiveness.</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Limitations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section discusses two potential limitations of this paper.
  prefs: []
  type: TYPE_NORMAL
- en: Test cases play an important role in code generation. The absence of accurate
    test cases can mislead the process, resulting in incorrect code implementations.
    In our study, we utilized LLM-generated test cases for intermediate evaluations,
    while reserving the original test cases from the dataset for the final evaluation.
    Our findings indicate that using the HumanEval dataset’s test cases for internal
    evaluations increases the $pass@1$ metric by more than %3-4\. This shows the impact
    of accurate test cases on the code generation process and partially explains why
    our results in RQ3 did not show substantial improvement despite changes in algorithm
    configurations.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding natural language by LLMs differs significantly from that of humans.
    It is well-established that modifying the original prompt can alter the output
    generated by an LLM. The mutation approach employed in EPiC sometimes produces
    prompts that are not easily understandable by humans and programmers to use for
    code generation. However, these mutated prompts can still lead to the generation
    of code with the correct behavior. This experience demonstrates that while mutations
    may render prompts less human-friendly, they do not become incomprehensible to
    LLMs. Instead, such mutations alter the vector representation of the prompts,
    enabling LLMs to generate alternative implementations of the code that may be
    correct. However, this limitation may negatively impact the developers’ understanding
    of the prompt, potentially leading to confusion or misinterpretation when using
    EPiC.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Threats to Validity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'External: One limitation of this research is that the datasets used (HumanEval
    and MBPP) may not fully represent the diversity of real-world coding tasks, which
    limits the generalizability of our findings. In addition, due to its prohibitive
    cost, RQ2.2 and RQ3 were only conducted on one dataset (HumanEval). Although these
    datasets are commonly used benchmarks for code generation with LLMs, there is
    a possibility that their data may have been leaked to both open-source and closed-source
    LLMs. This highlights the need for a new benchmark specifically for code generation
    tasks. Another potential threat is that our findings, based on specific LLMs such
    as GPT-4 and MagicCoder, may not generalize to other LLMs or future models with
    different architectures or training data. We chose an open-source small-size LLM
    and a closed-source large LLM to test our approach, aiming to ensure applicability
    to LLMs of different sizes. However, we cannot guarantee that GPT-4 and MagicCoder
    are the optimal candidates for this evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Conclusion: A potential threat here and in many other related studies using
    LLMs, is the non-deterministic nature of LLMs. Even setting the temperature to
    zero does not eliminate non-determinism, which could potentially invalidate the
    results and pose reproducibility issues. To alleviate this threat, in our study,
    we made sure we did not introduce further robustness issues by injecting more
    randomness through our algorithms. Thus we tested EPiC across multiple runs and
    empirically found our approach’s results to be robust, in that perspective. However,
    given the extensive cost of rerunning the experiments in RQ2.2 and RQ3 (where
    they already have many configurations to test), we skipped multiple runs with
    different seeds. Nonetheless, this might not be a serious threat to validity,
    given the low variance of the results in RQ2.1.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Construct: One construct threat we face is that our evaluation metrics rely
    on LLM-generated test cases for intermediate evaluations which could lead to misleading
    results if these test cases are not representative of the intended tasks or suffer
    from hallucination issues. To alleviate the threat, we have shown that the results
    from the LLM-generated tests are not that far off from the results using the original
    (developer-written) tests.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Internal: In this research, to compare with the baselines, we used their publicly
    available code and default setup and assumed that these tools were configured
    for optimal performance. For our own implementation, we also used existing open-source
    packages for mutation to reduce any internal validity issues. However, we did
    not fine-tune our hyper-parameters. The default configurations are based on RQ3’s
    sensitivity analysis which is not a comprehensive hyper-parameter optimization.
    However, a fine-tuned EPiC could only be more outperforming the baselines. Thus
    there is no threats to the validity of general findings of the paper in this regard.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion and Future Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this study, we presented EPiC, a cost-effective search-based prompt engineering
    approach for improving code generation using LLMs. Our method leverages an evolutionary
    algorithm to refine prompts, resulting in enhanced code generation performance
    systematically. The experimental results demonstrate that EPiC not only outperforms
    SOTA methods in terms of accuracy but also maintains a competitive cost, making
    it a cost-effective solution for software engineering applications. EPiC achieved
    a 93.5% pass@1 rate on the HumanEval dataset and a 79% pass@1 rate on the MBPP
    dataset, outperforming other methods with a lower or comparable cost. Our experiments
    also showed that incorporating randomness in the mutation process and adjusting
    the population size can impact the performance and cost-effectiveness of the algorithm.
    Moreover, our evaluation using an open-source LLM, MagicCoder, revealed that EPiC
    could enhance the performance of smaller models, indicating its potential applicability
    across various LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we employed EPiC for code generation. EPiC is task-agnostic and
    can be applied to other software engineering applications requiring prompt engineering,
    provided that an appropriate evaluation function is defined. We leave this part
    for future research. Additionally, we plan to explore more datasets, LLMs, mutation
    strategies, and software engineering tasks.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Espejel, J. L., Alassan, M. S. Y., Chouham, E. M., & Ettifouri, E. H. (2023).
    A comprehensive review of State-of-The-Art methods for Java code generation from
    Natural Language Text. Natural Language Processing Journal, 100013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., … & Zhou, M.
    (2020). Codebert: A pre-trained model for programming and natural languages. arXiv
    preprint arXiv:2002.08155.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training
    of deep bidirectional transformers for language understanding. arXiv preprint
    arXiv:1810.04805.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Husain, H., Wu, H. H., Gazit, T., Allamanis, M., & Brockschmidt, M. (2019).
    Codesearchnet challenge: Evaluating the state of semantic code search. arXiv preprint
    arXiv:1909.09436.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019).
    Language models are unsupervised multitask learners. OpenAI blog, 1(8), 9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Lu, S., Guo, D., Ren, S., Huang, J., Svyatkovskiy, A., Blanco, A., … &
    Liu, S. (2021). Codexglue: A machine learning benchmark dataset for code understanding
    and generation. arXiv preprint arXiv:2102.04664.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Perez, L., Ottens, L., & Viswanathan, S. (2021). Automatic code generation
    using pre-trained language models. arXiv preprint arXiv:2102.10535.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. D. O., Kaplan, J.,
    … & Zaremba, W. (2021). Evaluating large language models trained on code. arXiv
    preprint arXiv:2107.03374.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E.,
    … & Synnaeve, G. (2023). Code llama: Open foundation models for code. arXiv preprint
    arXiv:2308.12950.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei,
    Y., … & Scialom, T. (2023). Llama 2: Open foundation and fine-tuned chat models.
    arXiv preprint arXiv:2307.09288.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Wei, Y., Wang, Z., Liu, J., Ding, Y., & Zhang, L. (2023). Magicoder: Source
    Code Is All You Need. arXiv preprint arXiv:2312.02120.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., … & Zhou,
    D. (2022). Chain-of-thought prompting elicits reasoning in large language models.
    Advances in Neural Information Processing Systems, 35, 24824-24837.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Liu, J., Liu, A., Lu, X., Welleck, S., West, P., Bras, R. L., … & Hajishirzi,
    H. (2021). Generated knowledge prompting for commonsense reasoning. arXiv preprint
    arXiv:2110.08387.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Zhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S., Chan, H., &
    Ba, J. (2022). Large language models are human-level prompt engineers. arXiv preprint
    arXiv:2211.01910.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Pryzant, R., Iter, D., Li, J., Lee, Y. T., Zhu, C., & Zeng, M. (2023).
    Automatic prompt optimization with” gradient descent” and beam search. arXiv preprint
    arXiv:2305.03495.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Guo, Q., Wang, R., Guo, J., Li, B., Song, K., Tan, X., … & Yang, Y. (2023).
    Connecting large language models with evolutionary algorithms yields powerful
    prompt optimizers. arXiv preprint arXiv:2309.08532.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Zelikman, E., Huang, Q., Poesia, G., Goodman, N., & Haber, N. (2023).
    Parsel: Algorithmic Reasoning with Language Models by Composing Decompositions.
    Advances in Neural Information Processing Systems, 36, 31466-31523.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Huang, D., Nan, Z., Hu, X., Jin, P., Peng, S., Wen, Y., … & Chen, Y. (2024).
    ANPL: Towards Natural Programming with Interactive Decomposition. Advances in
    Neural Information Processing Systems, 36.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Liu, Z., Zhang, Y., Li, P., Liu, Y., & Yang, D. (2023). Dynamic llm-agent
    network: An llm-agent collaboration framework with agent team optimization. arXiv
    preprint arXiv:2310.02170.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Shinn, N., Cassano, F., Labash, B., Gopinath, A., Narasimhan, K., & Yao,
    S. (2023). Reflexion: Language Agents with Verbal Reinforcement Learning.(2023).
    arXiv preprint cs.AI/2303.11366.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Zhou, A., Yan, K., Shlapentokh-Rothman, M., Wang, H., & Wang, Y. X. (2023).
    Language agent tree search unifies reasoning acting and planning in language models.
    arXiv preprint arXiv:2310.04406.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Huang, D., Bu, Q., Zhang, J. M., Luck, M., & Cui, H. (2023). AgentCoder:
    Multi-Agent-based Code Generation with Iterative Testing and Optimisation. arXiv
    preprint arXiv:2312.13010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Zhong, L., Wang, Z., & Shang, J. (2024). LDB: A Large Language Model Debugger
    via Verifying Runtime Execution Step-by-step. arXiv preprint arXiv:2402.16906.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D.,
    Jiang, E., Cai, C., Terry, M., Le, Q. and Sutton, C., 2021\. Program synthesis
    with large language models. arXiv preprint arXiv:2108.07732.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] C. Fernando, D. Banarse, H. Michalewski, S. Osindero, and T. Rockt¨aschel,
    “Promptbreeder: Self-referential self-improvement via prompt evolution,” arXiv
    preprint arXiv:2309.16797, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Y. B. Li and K. Wu, “Spell: Semantic prompt evolution based on a llm,”
    arXiv preprint arXiv:2310.01260, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] H. Yang and K. Li, “Instoptima: Evolutionary multi-objective instruc-
    tion optimization via large language model-based instruction opera- tors,” arXiv
    preprint arXiv:2310.17630, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Shin, J., Tang, C., Mohati, T., Nayebi, M., Wang, S. and Hemmati, H.,
    2023\. Prompt engineering or fine tuning: An empirical assessment of large language
    models in automated software engineering tasks. arXiv preprint arXiv:2310.10508.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Hong, S., Zheng, X., Chen, J., Cheng, Y., Wang, J., Zhang, C., Wang, Z.,
    Yau, S.K.S., Lin, Z., Zhou, L. and Ran, C., 2023\. Metagpt: Meta programming for
    multi-agent collaborative framework. arXiv preprint arXiv:2308.00352.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Fan, A., Gokkaya, B., Harman, M., Lyubarskiy, M., Sengupta, S., Yoo, S.
    and Zhang, J.M., 2023, May. Large language models for software engineering: Survey
    and open problems. In 2023 IEEE/ACM International Conference on Software Engineering:
    Future of Software Engineering (ICSE-FoSE) (pp. 31-53). IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] K. Jin, C. -Y. Wang, H. V. Pham and H. Hemmati, ”Can ChatGPT Support Developers?
    An Empirical Evaluation of Large Language Models for Code Generation,” 2024 IEEE/ACM
    21st International Conference on Mining Software Repositories (MSR), Lisbon, Portugal,
    2024, pp. 167-171.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Wang, Junjie, et al. ”Software testing with large language models: Survey,
    landscape, and vision.” IEEE Transactions on Software Engineering (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P.,
    Neelakantan, A., Shyam, P., Sastry, G., Askell, A. and Agarwal, S., 2020\. Language
    models are few-shot learners. Advances in neural information processing systems,
    33, pp.1877-1901.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Liu, Jiachang, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin,
    and Weizhu Chen. ”What Makes Good In-Context Examples for GPT-$3$?.” arXiv preprint
    arXiv:2101.06804 (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N.,
    Küttler, H., Lewis, M., Yih, W.T., Rocktäschel, T. and Riedel, S., 2020\. Retrieval-augmented
    generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing
    Systems, 33, pp.9459-9474.4'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Yun He, Steven Zheng, Yi Tay, Jai Gupta, Yu Du, Vamsi Aribandi, Zhe Zhao,
    YaGuang Li, Zhao Chen, Donald Metzler, et al. 2022\. Hyperprompt: Prompt-based
    task-conditioning of transformers. In International Conference on Machine Learning.
    PMLR, 8678–8690.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Salvatore Carta, Alessandro Giuliani, Leonardo Piano, Alessandro Sebastian
    Podda, Livio Pompianu, and Sandro Gabriele Tiddia. 2023\. Iterative Zero-Shot
    LLM Prompting for Knowledge Graph Construction. arXiv preprint arXiv:2307.01128
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Sahoo, Pranab, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal,
    and Aman Chadha. ”A systematic survey of prompt engineering in large language
    models: Techniques and applications.” arXiv preprint arXiv:2402.07927 (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Zhang, Zhuosheng, Aston Zhang, Mu Li, and Alex Smola. ”Automatic chain
    of thought prompting in large language models.” arXiv preprint arXiv:2210.03493
    (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Wang, Xuezhi, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang,
    Aakanksha Chowdhery, and Denny Zhou. ”Self-consistency improves chain of thought
    reasoning in language models.” arXiv preprint arXiv:2203.11171 (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Yao, Shunyu, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan,
    and Yuan Cao. ”React: Synergizing reasoning and acting in language models.” arXiv
    preprint arXiv:2210.03629 (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Li, Xingxuan, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq Joty, Soujanya
    Poria, and Lidong Bing. ”Chain-of-knowledge: Grounding large language models via
    dynamic knowledge adapting over heterogeneous sources.” arXiv preprint arXiv:2305.13269
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Mu, Fangwen, et al. ”ClarifyGPT: Empowering LLM-based Code Generation
    with Intention Clarification.” arXiv preprint arXiv:2310.10996 (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Zamani, Shayan, and Hadi Hemmati. ”A pragmatic approach for hyper-parameter
    tuning in search-based test case generation.” Empirical Software Engineering 26
    (2021): 1-35.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
