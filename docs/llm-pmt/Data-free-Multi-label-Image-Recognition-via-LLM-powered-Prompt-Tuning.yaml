- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:45:48'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Data-free Multi-label Image Recognition via LLM-powered Prompt Tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.01209](https://ar5iv.labs.arxiv.org/html/2403.01209)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Shuo Yang^(1,2)  Zirui Shang²  Yongqi Wang²
  prefs: []
  type: TYPE_NORMAL
- en: Derong Deng¹  Hongwei Chen¹  Qiyuan Cheng¹  Xinxiao Wu^(1,2)
  prefs: []
  type: TYPE_NORMAL
- en: ¹Guangdong Laboratory of Machine Perception and Intelligent Computing
  prefs: []
  type: TYPE_NORMAL
- en: Shenzhen MSU-BIT University, China
  prefs: []
  type: TYPE_NORMAL
- en: ²Beijing Key Laboratory of Intelligent Information Technology
  prefs: []
  type: TYPE_NORMAL
- en: School of Computer Science & Technology, Beijing Institute of Technology, China
  prefs: []
  type: TYPE_NORMAL
- en: '{shuoyang,shangzirui,wuxinxiao}@bit.edu.cn'
  prefs: []
  type: TYPE_NORMAL
- en: '{1285441164yq, derongdeng.dero, chwr0001}@gmail.com  chengqiyuan@smbu.edu.cn
    Corresponding author.'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This paper proposes a novel framework for multi-label image recognition without
    any training data, called data-free framework, which uses knowledge of pre-trained
    Large Language Model (LLM) to learn prompts to adapt pre-trained Vision-Language
    Model (VLM) like CLIP to multi-label classification. Through asking LLM by well-designed
    questions, we acquire comprehensive knowledge about characteristics and contexts
    of objects, which provides valuable text descriptions for learning prompts. Then
    we propose a hierarchical prompt learning method by taking the multi-label dependency
    into consideration, wherein a subset of category-specific prompt tokens are shared
    when the corresponding objects exhibit similar attributes or are more likely to
    co-occur. Benefiting from the remarkable alignment between visual and linguistic
    semantics of CLIP, the hierarchical prompts learned from text descriptions are
    applied to perform classification of images during inference. Our framework presents
    a new way to explore the synergies between multiple pre-trained models for novel
    category recognition. Extensive experiments on three public datasets (MS-COCO,
    VOC2007, and NUS-WIDE) demonstrate that our method achieves better results than
    the state-of-the-art methods, especially outperforming the zero-shot multi-label
    recognition methods by 4.7% in mAP on MS-COCO.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Multi-label image recognition aims to recognize all objects present in an image.
    This task is challenging due to the emergence of novel objects and scenes [[5](#bib.bib5)]
    during inference in real-world scenarios, as shown in Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Data-free Multi-label Image Recognition via LLM-powered Prompt
    Tuning")(a). Recent large-scale pre-trained Vision-Language Models (VLMs) like
    CLIP [[37](#bib.bib37)] spawn the training-free zero-shot methods [[21](#bib.bib21)],
    which can handle new categories by calculating similarities between images and
    texts in a well-aligned embedding space. To further effectively adapt VLMs to
    enhance the performance of novel categories, several methods have been proposed
    to learn adapter [[3](#bib.bib3)] or prompts [[43](#bib.bib43)] using sufficient
    annotated images, as shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Data-free
    Multi-label Image Recognition via LLM-powered Prompt Tuning")(b). However, the
    performance of these prompt learning methods may be limited when it is infeasible
    to collect sufficient fully annotated images.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/781020070e0718dc65760714287fa2d4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Illustration of different ways to handle novel categories. (a) Traditional
    methods train on base categories but fail on novel categories. (b) Recent prompting
    methods successfully adapt VLM to novel categories but need annotated data for
    prompt tuning. (c) Our data-free framework only performs prompt tuning to adapt
    VLM to novel categories by LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: To address this issue, Sun et al. [[43](#bib.bib43)] propose dual context optimization
    to quickly adapt CLIP to multi-label recognition using partially labeled images,
    where only a few categories for each training image are annotated, significantly
    reducing the annotation burden. Guo et al. [[20](#bib.bib20)] propose texts as
    images in prompt tuning to adapt CLIP, where the text descriptions are human-written
    image captions from existing datasets and serve as alternatives to images. This
    method presents a more practical and efficient way for prompting as text descriptions
    are more easily accessible than images.
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we propose a data-free framework for multi-label image recognition
    without any data for training. It leverages knowledge of objects from pre-trained
    Large Language Model (LLM) to adapt CLIP to multi-label classification by textual
    prompt tuning, as shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Data-free
    Multi-label Image Recognition via LLM-powered Prompt Tuning")(c). Specifically,
    we propose to collect comprehensive information of objects by designing different
    types of questions posed to LLM. Starting with asking LLM category-agnostic questions
    like [object lists], please summarize 90 attributes that may be common to the
    above 80 words to acquire common attributes, such as shape, color, and material,
    shared by all categories, similarly we then acquire particular attributes for
    each category by category-specific questions like please summarize 30 attributes
    of [object]. Finally, we acquire text descriptions of the attributes by category-description
    questions like please help me generate 100 different sentences about [category]
    from the angle of the [attribute]. Moreover, we design scene-related questions
    like generate ten sentences to describe different scenes involving [category1]
    and [category2] to acquire text descriptions of contextual relationships between
    multiple object categories in real-world scenes, namely relationship knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: 'Along with category labels, the acquired text descriptions of attribute and
    relationship knowledge from LLM are used as images for prompt tuning CLIP to multi-label
    recognition. To incorporate the relationship information between multiple objects
    into prompt learning to further improve the performance, we propose a hierarchical
    prompt learning method, which categorizes the prompt tokens into three types:
    (1) shared tokens shared by all object categories; (2) partial-shared tokens shared
    by the object categories of the same subgroups with co-occurrence relationship
    or similar attributes; (3) category-specific tokens specific to each individual
    object category. Through designing these hierarchical tokens, we learn prompts
    that absorb both task-specific knowledge and object-specific knowledge, as well
    as the relationship knowledge between objects. Benefiting from the remarkable
    alignment between visual and linguistic semantics of CLIP, the hierarchical prompts
    learned from text description are applied to perform classification of images
    during inference.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, the contributions of our work are three-fold:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose a data-free framework for multi-label image recognition without any
    training data, which leverages rich knowledge in LLM to prompt tune CLIP. Our
    framework introduces a promising avenue for handling new objects in visual recognition,
    relying solely on pre-trained models, and also paves an effective way to explore
    the synergies between multiple pre-trained models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose a hierarchical prompt learning method to adapt CLIP by using the
    acquired knowledge of objects from LLM. It incorporates relationships between
    different categories into learnable prompts, thus further improving the multi-label
    recognition performance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose to collect comprehensive information about object attributes and
    relationships from LLM by designing different types of questions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Multi-Label Image Recognition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Early multi-label image recognition methods [[51](#bib.bib51)] naively treat
    this task as a multiple independent binary classification problem, which trains
    a binary classifier for each category [[33](#bib.bib33), [35](#bib.bib35)]. However,
    these methods do not consider correlations among labels, and recent works have
    focused on incorporating semantic dependencies among labels via graph modeling [[13](#bib.bib13),
    [29](#bib.bib29), [12](#bib.bib12), [10](#bib.bib10), [48](#bib.bib48)] or sequential
    perdition [[47](#bib.bib47), [32](#bib.bib32), [49](#bib.bib49), [23](#bib.bib23),
    [58](#bib.bib58), [56](#bib.bib56)]. There are also works focused on the attention
    mechanism [[39](#bib.bib39), [64](#bib.bib64), [17](#bib.bib17), [57](#bib.bib57)]
    or loss functions [[19](#bib.bib19), [30](#bib.bib30), [38](#bib.bib38)].
  prefs: []
  type: TYPE_NORMAL
- en: Despite remarkable progress that has been made, these methods heavily on large-scale
    annotated images for training, which limits their capabilities in data-limited
    or label-limited scenarios. In recent years, several methods have emerged to address
    the few-shot/zero-shot multi-label image recognition  [[29](#bib.bib29), [4](#bib.bib4),
    [25](#bib.bib25), [40](#bib.bib40), [24](#bib.bib24), [5](#bib.bib5)] and partial-labeled
    image recognition [[15](#bib.bib15), [2](#bib.bib2), [11](#bib.bib11), [36](#bib.bib36),
    [60](#bib.bib60)]. By exploring the synergies between LLM and VLM, we take a significant
    step forward in multi-label image recognition by introducing a data-free framework
    where no training data is provided.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c52e6ad3c405692e3aedb697abd7b9fa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Overview of our framework.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Adapting CLIP to Visual Tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Vision-Language Models (VLMs) have demonstrated impressive capabilities on learning
    generic representations, such as CLIP [[37](#bib.bib37)]. In order to adapt VLMs
    to specific downstream tasks, many prompt tuning methods [[55](#bib.bib55), [26](#bib.bib26),
    [62](#bib.bib62), [61](#bib.bib61), [42](#bib.bib42), [63](#bib.bib63), [41](#bib.bib41)]
    have been proposed to learn task-specific prompts, which gains significant attention
    for both excellent performance and parameter-efficient characteristic. To further
    to bridge the domain gap between the data used to train VLMs and that of specific
    tasks, dedicated adapters [[59](#bib.bib59), [44](#bib.bib44), [46](#bib.bib46),
    [18](#bib.bib18), [52](#bib.bib52), [9](#bib.bib9), [3](#bib.bib3)] have been
    designed and integrated into CLIP, avoiding fine-tuning the entire model.
  prefs: []
  type: TYPE_NORMAL
- en: The works most relevant to our method are DualCoOp [[43](#bib.bib43)] and TaI-DPT [[20](#bib.bib20)].
    DualCoOp learns a pair of differentiable prompts using partial-annotated images,
    and TaI-DPT uses image captioning collected from existing datasets as images to
    learn prompts. In contrast, our method inquires LLMs to acquire comprehensive
    knowledge of object categories as text description for prompt learning. Moreover,
    our method learns relationship-aware hierarchical prompts which are tailored to
    multi-label image recognition.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 LLM-enhanced Visual Understanding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LLMs have been allied to help visual understanding tasks [[53](#bib.bib53),
    [6](#bib.bib6), [8](#bib.bib8)] due to their “emergent abilities” of learning
    how to answer such questions from the in-context examples [[50](#bib.bib50)].
    Visual information is represented as text descriptions and then fed into the LLMs
    together with the target question and in-context examples to generate the desired
    results [[54](#bib.bib54), [22](#bib.bib22)]. A recent study employs ChatGPT [[7](#bib.bib7)]
    to generate a comparison tree, which enhances CLIP’s zero-shot performance on
    image classification. In this paper, we focus on using LLM to generate text descriptions
    to facilitate prompt tuning of CLIP to multi-label image recognition.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Our Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We propose a data-free framework for multi-label image recognition without any
    training data. A large language model (ChatGLM) serves as a repository of encyclopedic
    knowledge, and we propose to acquire comprehensive knowledge of object categories
    by designing different types of questions posed to ChatGLM. This is motivated
    by the fact that we can effectively identify an object in a picture if provided
    with a linguistic description. Then a pre-trained vision-language model (CLIP)
    is prompt tuned using the acquired knowledge to enhance multi-label classification,
    based on the aligned visual and linguistic embedding space. We propose a hierarchical
    prompt learning method to incorporate relationships between objects into the learnable
    prompts. Figure [2](#S2.F2 "Figure 2 ‣ 2.1 Multi-Label Image Recognition ‣ 2 Related
    Work ‣ Data-free Multi-label Image Recognition via LLM-powered Prompt Tuning")
    shows an overview of our framework.
  prefs: []
  type: TYPE_NORMAL
- en: Given an input image x, multi-label image recognition aims to identify all object
    categories in it, formulated as $\mathbf{S}=f_{\Phi,\Psi}(\mathbf{x})$.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Knowledge Acquirement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To describe an object, it is crucial to have detailed information about its
    color, shape, texture, and other attributes. To obtain this information, we engage
    with ChatGLM, a highly knowledgeable language model that functions as a chatbot
    and responds to carefully crafted questions. Its extensive encyclopedic knowledge
    allows us to extract the necessary details for adequate object description.
  prefs: []
  type: TYPE_NORMAL
- en: Coarse Attribute Description. To capture diverse aspects of objects, we begin
    by extracting common attributes shared by all categories using category-agnostic
    questions and particular attributions for individual categories using category-specific
    questions, formulated as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $(\mathbb{A}_{c},\mathbb{A}_{s})=\Phi(\Pi_{1}(\mathbb{Y})),$ |  | (1)
    |'
  prefs: []
  type: TYPE_TB
- en: where $\Pi_{1}(\cdot)$-th category.
  prefs: []
  type: TYPE_NORMAL
- en: Then we obtain the text descriptions of each category by asking additional questions.
    Note that these text descriptions of attributes may contain noise, we call them
    coarse attribute descriptions. This process is formulated by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbb{D}_{i}^{c}=\Phi(\Pi_{2}(\mathbb{A}_{c}\cup\mathbb{A}_{s,i},Y_{i})),$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where $\Pi_{2}(\cdot)$ denote the coarse attribute description sets.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-grained Attribute Description. We design several questions to remove the
    noisy attributes that are irrelevant to the specific category, resulting in a
    fine-grained attribute set for each category. We then inquire ChatGLM to acquire
    the fine-grained attribute descriptions $\mathbb{D}^{f}=\{\mathbb{D}^{f}_{1},\cdots,\mathbb{D}^{f}_{N}\}$
    of Eq.([2](#S3.E2 "Equation 2 ‣ 3.2 Knowledge Acquirement ‣ 3 Our Method ‣ Data-free
    Multi-label Image Recognition via LLM-powered Prompt Tuning")). This process is
    formulated by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $\Pi_{3}(\cdot)$ denotes the questions about how to remove irrelevant
    attributes, like [attribute list], please delete the above attribute words given
    that are not very relevant to [category]. Finally, 70 attribute words remain.
  prefs: []
  type: TYPE_NORMAL
- en: 'Relationship Description. In multi-label image recognition, the co-occurrence
    relationships between different categories contribute significantly to the performance [[12](#bib.bib12),
    [48](#bib.bib48)]. To simulate this scenario, we first split all categories into
    multiple scene-related subgroups by ChatGLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbb{G}=\{\mathbb{G}_{i}\}_{i=1}^{n_{3}}=\Phi(\Pi_{4}(\mathbb{Y})),$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\Pi_{4}(\cdot)$, like generate 100 different descriptive sentences for
    a scene containing [category1] and [category2], and fed into ChatGLM to obtain
    relationship descriptions:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbb{D}_{i}^{r}=\Phi(\Pi_{5}(\mathbb{G}_{i})),$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbb{D}_{i}^{r}$ denote the fine-grained attribute description sets.
  prefs: []
  type: TYPE_NORMAL
- en: Figure [3](#S3.F3 "Figure 3 ‣ 3.2 Knowledge Acquirement ‣ 3 Our Method ‣ Data-free
    Multi-label Image Recognition via LLM-powered Prompt Tuning") illustrates an example
    of the designed questions and their corresponding answers from ChatGLM.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/43c06393ada1dc8cd26bb1f59effcd8c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: An example of the designed questions and their corresponding answers
    from ChatGLM. More detailed examples can be found in the supplementary materials.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Hierarchical Prompt Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Based on the previous generated text descriptions $\mathbb{D}=\mathbb{D}^{k},k\in\{c,f,r\}$,
    we propose hierarchical prompt learning to adapt CLIP to multi-label recognition,
    where hierarchical prompts are designed to model relationships between categories,
    and both global and local prompt learning are introduced to grasp the discriminability
    of features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hierarchical Prompts. A learnable prompt usually consists of several learnable
    tokens and a placeholder to put the category label, denoted as $\mathbf{p}_{i}=[\mathbf{t}_{1}^{i},\mathbf{t}_{2}^{i},\cdots,\mathbf{t}_{M}^{i},Y_{i}]$-th
    category label. For different categories, there are two types of prompts: shared
    prompts, where tokens are shared across different categories, and category-specific
    prompts, where tokens are distinct for each category. Both types of prompts have
    been demonstrated effective in recent works [[62](#bib.bib62), [63](#bib.bib63)],
    but they neglect the relationships of different categories, leading to sub-optimal
    performance.'
  prefs: []
  type: TYPE_NORMAL
- en: To capture the relationships between different categories, we propose hierarchical
    prompts $\mathbf{P}^{h}$, a mixed version of both shared tokens and category-specific
    tokens, and additional partial-shared tokens only between categories that most
    likely co-occur in a scene or have similar attributes, denote as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\mathbf{P}^{h}=\begin{bmatrix}\mathbf{t}_{s},&amp;\mathbf{t}_{p,1},&amp;\mathbf{t}_{p,2},&amp;\mathbf{t}_{p,3},&amp;\cdots&amp;\mathbf{t}_{c,1},&amp;Y_{1}\\
    \mathbf{t}_{s},&amp;\mathbf{t}_{c,a},&amp;\mathbf{t}_{p,2},&amp;\mathbf{t}_{p,4},&amp;\cdots&amp;\mathbf{t}_{c,2},&amp;Y_{2}\\'
  prefs: []
  type: TYPE_NORMAL
- en: \mathbf{t}_{s},&amp;\mathbf{t}_{p,1},&amp;\mathbf{t}_{p,2},&amp;\mathbf{t}_{p,3},&amp;\cdots&amp;\mathbf{t}_{c,3},&amp;Y_{3}\\
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;\ddots&amp;&amp;&amp;\ddots&amp;\\'
  prefs: []
  type: TYPE_NORMAL
- en: \mathbf{t}_{s},&amp;\mathbf{t}_{p,1},&amp;\mathbf{t}_{c,b},&amp;\mathbf{t}_{p,4},&amp;\cdots&amp;\mathbf{t}_{c,N},&amp;Y_{N}\end{bmatrix},$$
    |  |
  prefs: []
  type: TYPE_NORMAL
- en: 'where all tokens are divided into three types: (1) shared tokens, denoted as
    $\mathbf{t}_{s}$) due to no relationships in that circumstance. For example, in
    the kitchenware scene, the knife and oven have a shared token, but the corresponding
    tokens of the sofa and book may be specific.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Global Learning. Global learning aims to learn global hierarchical prompts
    to grasp the discriminative ability of global features. Let $\mathbf{P}_{g,i}^{h}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{G}_{i}=\Psi_{t}(\mathbf{P}_{g,i}^{h})\in\mathbb{R}^{d},\quad
    i\in\{1,2,\cdots,N\},$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: 'where $d=512$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{T}^{G}=\Psi_{t}(\mathbf{r})\in\mathbb{R}^{d},\quad\mathbf{r}\in\mathbb{D}.$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: The cosine similarity between the global feature of text description and the
    global category embedding, denoted as $S_{i}^{G}$, is calculated by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$S_{i}^{G}=\left<\mathbf{T}^{G},\mathbf{G}_{i}\right></math> |  | (8)
    |'
  prefs: []
  type: TYPE_TB
- en: 'Local Learning. Local learning aims to learn local hierarchical prompts to
    grasp the discriminative ability of fine-grained features. Let $\mathbf{P}_{l,i}^{h}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{L}_{i}=\Psi_{t}(\mathbf{P}_{l,i}^{h})\in\mathbb{R}^{d},\quad
    i\in\{1,2,\cdots,N\}.$ |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: 'For the text description $\mathbf{r}\in\mathbb{D}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{T}^{L}=\widetilde{\Psi}_{t}(\mathbf{r})\in\mathbb{R}^{N_{r}\times
    d},\quad\mathbf{r}\in\mathbb{D},$ |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: 'where $N_{r}$ token features (global features). The category-aware similarity
    between the sequential local features of text description and the local category
    embedding is calculated in a weighted manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: where <math id=$$-th token (column) of local features.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Training Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the global similarity $\mathbf{S}^{G}=[S_{1}^{G},\cdots,S_{N}^{G}]$ of each
    text description, we adopt two loss functions to optimize the corresponding learnable
    prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ranking Loss. We utilize the ranking loss to assess the disparity between classification
    scores and ground-truth labels. Specifically, the ranking loss for the global
    and local learning is calculated separately:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\mathcal{L}_{rank}=\mathcal{L}_{global}+\mathcal{L}_{local},$
    |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 | , |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 | , |  |'
  prefs: []
  type: TYPE_TB
- en: where $m=1$.
  prefs: []
  type: TYPE_NORMAL
- en: Order Loss. Due to the potential noise introduced by text generated from ChatGLM,
    which could mislead the optimization process, we introduce an anchor to the learned
    prompts. Specifically, we anchor the learned prompts using hand-craft prompts,
    such as a photo of [category]. The rationale behind this is that human-defined
    prompts have demonstrated mediocre zero-shot performance, thus the resulting order
    of all given categories can be considered reasonable to some extent. We posit
    that maintaining this order in the learned prompts helps mitigate the impact of
    noisy inputs, thereby enhancing the overall effectiveness of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Results of different text descriptions on MS-COCO, VOC2007 and NUS-WIDE.
    $\dagger$ means that results are from TaI-DPT [[20](#bib.bib20)]. Note that the
    hand-craft prompts used in this paper are designed specifically for each category.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Knowledge | Training | MS-COCO | VOC2007 | NUS-WIDE |'
  prefs: []
  type: TYPE_TB
- en: '| F1 | mAP | F1 | mAP | F1 | mAP |'
  prefs: []
  type: TYPE_TB
- en: '| Hand-craft prompts^† (zero-shot) | $\times$ | - | 49.7 | - | 77.3 | - | 37.4
    |'
  prefs: []
  type: TYPE_TB
- en: '| Hand-craft prompts (zero-shot) | $\times$ | 47.7 | 62.1 | 58.5 | 86.6 | 30.4
    | 43.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Captions | $\checkmark$ | 48.8 | 67.5 | 56.1 | 86.8 | 43.4 | 44.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Coarse Attribute | $\checkmark$ | 51.9 | 64.2 | 57.1 | 88.0 | 36.6 | 46.0
    |'
  prefs: []
  type: TYPE_TB
- en: '| Fine-grained Attribute | $\checkmark$ | 52.1 | 64.4 | 58.1 | 88.2 | 36.8
    | 46.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours | $\checkmark$ | 57.3 | 66.8 | 60.0 | 88.7 | 40.0 | 47.0 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Results of different prompts on MS-COCO.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompts | F1 | mAP |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Hand-craft | 47.7 | 62.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Category-specific | 52.7 | 64.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Shared | 54.7 | 66.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Hierarchical (Ours) | 57.3 | 66.8 |'
  prefs: []
  type: TYPE_TB
- en: 'For learnable global and local prompts and hand-craft prompts, denoted as $G,L,H$.
    The order loss is then calculated by the Kullback-Leibler (KL) divergence :'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{order}=\text{KL}(\mathbf{D}^{G},\mathbf{D}^{H})+\text{KL}(\mathbf{D}^{L},\mathbf{D}^{H}),$
    |  | (13) |'
  prefs: []
  type: TYPE_TB
- en: Finally, the overall loss is given by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}=\mathcal{L}_{rank}+\lambda_{1}\cdot\mathcal{L}_{order}$ |  |
    (14) |'
  prefs: []
  type: TYPE_TB
- en: 4.1 Inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Thanks to large-scale image-text contrastive pre-training of CLIP, text features
    have been well-aligned to the image features of the same semantic meanings. As
    a result, our prompts learned from text descriptions can be applied to images
    during inference.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, with the learned hierarchical prompts, we extract the global and
    local class embedding using Eq.([6](#S3.E6 "Equation 6 ‣ 3.3 Hierarchical Prompt
    Learning ‣ 3 Our Method ‣ Data-free Multi-label Image Recognition via LLM-powered
    Prompt Tuning")) and Eq.([9](#S3.E9 "Equation 9 ‣ 3.3 Hierarchical Prompt Learning
    ‣ 3 Our Method ‣ Data-free Multi-label Image Recognition via LLM-powered Prompt
    Tuning")) repeatly, denoted as $\mathbf{T}^{G}=[\mathbf{T}_{1}^{G},\cdots,\mathbf{T}_{N}^{G}]$,
    we extract the global and local image features by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{I}^{G}=\Psi_{i}(\mathbf{x})\in\mathbb{R}^{d},\quad\quad\mathbf{I}^{L}=\widetilde{\Psi}_{i}(\mathbf{x})\in\mathbb{R}^{N_{I}\times
    d},$ |  | (15) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{I}^{G}$ is the length of flatted dense image features. Finally,
    we calculate the similarities between category embeddings and image features by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{S}=\lambda_{2}\cdot\mathbf{S}^{G}+(1-\lambda_{2})\cdot\mathbf{S}^{L},$
    |  | (16) |'
  prefs: []
  type: TYPE_TB
- en: where $\lambda_{2}$ is the local similarity, where
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: and <math id="S4.SS1.p2.11.m1.4" class="ltx_Math" alttext="\mathbf{s}_{i,j}=\left<\mathbf{T}_{i}^{L},\mathbf{I}_{j}^{L}\right></math>-th
    token (column) of local image features.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1 Datasets and Evaluation Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Datasets. We conduct experiments on the MS-COCO [[31](#bib.bib31)], VOC2007 [[16](#bib.bib16)]
    and NUS-WIDE [[13](#bib.bib13)] datasets for evaluation. For all three datasets,
    no training data is used and the testing is performed on the testing or validation
    sets. MS-COCO is a widely used multi-label dataset for image recognition, which
    contains 80 categories with 82,081 training images and 40,504 validation images.
    VOC2007 contains 20 object categories with a total of 5,011 images for training
    and validation and 4,952 images for testing. NUS-WIDE contains 81 categories with
    161,789 images for training and 107,859 images for testing.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics. We use the conventional evaluation metrics, including the mean of class-average
    precision (mAP) and the overall F1 score at Top-3 predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Implementation Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We employ ResNet-50 as the visual encoder of CLIP with an input resolution of
    224 $\times$ in Eq.([16](#S4.E16 "Equation 16 ‣ 4.1 Inference ‣ 4 Training Objectives
    ‣ Data-free Multi-label Image Recognition via LLM-powered Prompt Tuning")) are
    set to 0.2 and 0.65, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Ablation Studies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Effectiveness of different text descriptions. To evaluate the acquired text
    descriptions from ChatGLM by our method, we employ different inquiry strategies
    to obtain text descriptions for comparison, including (1) “Hand-craft prompt”:
    the inference is directly performed using hand-craft prompts without prompt tuning;
    (2) “Image Captions”: the human-written image captions from existing datasets
    are used for prompt tuning; (3) “Coarse Attribute”: the text descriptions of object
    attributes with noise are used for prompt tuning, generated by Eq.([1](#S3.E1
    "Equation 1 ‣ 3.2 Knowledge Acquirement ‣ 3 Our Method ‣ Data-free Multi-label
    Image Recognition via LLM-powered Prompt Tuning")); (4) “Fine-grained Attribute”:
    the text descriptions of filtered object attributes are used for prompt tuning,
    generated by Eq.([3](#S3.E3 "Equation 3 ‣ 3.2 Knowledge Acquirement ‣ 3 Our Method
    ‣ Data-free Multi-label Image Recognition via LLM-powered Prompt Tuning")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [1](#S4.T1 "Table 1 ‣ 4 Training Objectives ‣ Data-free Multi-label Image
    Recognition via LLM-powered Prompt Tuning") shows the results of different text
    descriptions on the MS-COCO, VOC2007 and NUS-WIDE datasets. We have the following
    observations: (1) Our method achieves substantial improvements over “Hand-crafted
    prompts”, with F1 score gains of 7.64%, 1.55%, and 9.6% on the three datasets,
    respectively. This highlights the remarkable contribution of knowledge extracted
    from ChatGLM in enhancing the zero-shot performance of CLIP on multi-label image
    recognition; (2) Our method outperforms “Fine-grained Attribute” descriptions
    by approximately 5.2%, 1.9% and 3.2% in F1 score on the three datasets, respectively.
    This superiority emphasizes that considering the relationships between objects
    captures more discriminative information to enhance multi-label recognition; (3)
    Our method performs better than “Image Captions” on most metrics, suggesting that
    ChatGLM provides more comprehensive knowledge than human-written image captions;
    (4) The performance of “Coarse Attribute” is lower than that of “Fine-grained
    Attribute”, confirming that the presence of noise within the coarse attributes
    of objects degrades the performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Effectiveness of different prompts. To evaluate the proposed hierarchical prompts,
    we employ different types of prompts for comparison, including (1) “Hand-craft”:
    the human-defined prompts are customized for each category like an image of [category];
    (2) “Category-specific”: the learnable tokens of prompts are specific for each
    category; (3) “Shared”: the learnable tokens of prompts are shared across all
    categories.'
  prefs: []
  type: TYPE_NORMAL
- en: Table [2](#S4.T2 "Table 2 ‣ 4 Training Objectives ‣ Data-free Multi-label Image
    Recognition via LLM-powered Prompt Tuning") shows the results of different types
    of prompts on MS-COCO. Our hierarchical prompts outperform all other methods,
    demonstrating the effectiveness of incorporating inter-category relationships
    into prompts. Moreover, compared to the performance of hand-crafted prompts, that
    of learnable prompts (i.e., category-specific, shared and hierarchical prompts)
    is much higher. Interestingly, “Shared prompts” outperforms “Category-specific”,
    indicating the better generalization ability of shared prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Results of different components on MS-COCO.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Local learning | Order loss | F1 | mAP |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $\times$ | 52.4 | 62.2 |'
  prefs: []
  type: TYPE_TB
- en: '| ✓ | $\times$ | 56.3 | 66.0 |'
  prefs: []
  type: TYPE_TB
- en: '| $\times$ | ✓ | 53.2 | 63.2 |'
  prefs: []
  type: TYPE_TB
- en: '| ✓ | ✓ | 57.3 | 66.8 |'
  prefs: []
  type: TYPE_TB
- en: Effectiveness of order loss. To evaluate the effectiveness of the order loss
    in Eq.([13](#S4.E13 "Equation 13 ‣ 4 Training Objectives ‣ Data-free Multi-label
    Image Recognition via LLM-powered Prompt Tuning")), we remove it for comparison.
    The results on MS-COCO are shown in Table [3](#S5.T3 "Table 3 ‣ 5.3 Ablation Studies
    ‣ 5 Experiments ‣ Data-free Multi-label Image Recognition via LLM-powered Prompt
    Tuning"), verifying the advantage of the order loss in mitigating the noisy of
    text descriptions acquired from ChatGLM.
  prefs: []
  type: TYPE_NORMAL
- en: Effectiveness of local learning. To evaluate the effectiveness of the local
    learning of hierarchical prompts, we remove it for comparison. The results on
    MS-COCO are shown in Table [3](#S5.T3 "Table 3 ‣ 5.3 Ablation Studies ‣ 5 Experiments
    ‣ Data-free Multi-label Image Recognition via LLM-powered Prompt Tuning"), highlighting
    the significant impact of focusing on image sub-regions in multi-label recognition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Results of different numbers of different token types in hierarchical
    prompts on MS-COCO. S: shared token; P-S #1: partial-shared token over more categories
    (within coarse subgroups); P-S #2: partial-shared token over fewer categories
    (within more fined subgroups); C-S: category-specific token.'
  prefs: []
  type: TYPE_NORMAL
- en: '| S | P-S #1 | P-S #2 | C-S | F1 | mAP |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 8 | 8 | 8 | 56.4 | 66.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 8 | 6 | 6 | 57.2 | 66.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | 8 | 4 | 4 | 57.3 | 66.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 20 | 4 | 4 | 4 | 57.1 | 66.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 20 | 6 | 4 | 2 | 56.0 | 66.5 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Comparison results (mAP) with the state-of-the-art methods on MS-COCO,
    VOC2007 and NUS-WIDE.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Venue | Training source | Annotation | MS-COCO | VOC2007 | NUS-WIDE
    |'
  prefs: []
  type: TYPE_TB
- en: '| SRN [[64](#bib.bib64)] | CVPR 2017 | Image | Fully labeled | 77.1 | - | 62.0
    |'
  prefs: []
  type: TYPE_TB
- en: '| ML-GCN [[12](#bib.bib12)] | CVPR 2019 | 83.0 | 94.0 | - |'
  prefs: []
  type: TYPE_TB
- en: '| ASL [[38](#bib.bib38)] | ICCV 2021 | 86.6 | 94.6 | 65.2 |'
  prefs: []
  type: TYPE_TB
- en: '| SARB [[36](#bib.bib36)] | AAAI 2022 | Image | Partially labeled | 71.2 |
    83.5 | - |'
  prefs: []
  type: TYPE_TB
- en: '| SST [[11](#bib.bib11)] | AAAI 2022 | 68.1 | 81.5 | - |'
  prefs: []
  type: TYPE_TB
- en: '| DualCoOp [[43](#bib.bib43)] | NeurIPS 2022 | 78.7 | 90.3 | - |'
  prefs: []
  type: TYPE_TB
- en: '| LL-R [[27](#bib.bib27)] | CVPR 2022 | Image | One labeled | 72.6 | 90.6 |
    47.4 |'
  prefs: []
  type: TYPE_TB
- en: '| G²NetPL [[1](#bib.bib1)] | BMVC 2022 | 72.5 | 89.9 | 48.5 |'
  prefs: []
  type: TYPE_TB
- en: '| LSAN [[45](#bib.bib45)] | CVPR 2016 | Image | Unlabeled | 65.5 | 87.9 | 41.3
    |'
  prefs: []
  type: TYPE_TB
- en: '| WAN [[34](#bib.bib34)] | ICCV 2019 | 63.9 | 86.2 | 40.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Curriculum [[15](#bib.bib15)] | CVPR 2019 | 63.2 | 83.1 | 39.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Naive AN [[28](#bib.bib28)] | NeurIPS 2020 | 65.1 | 86.5 | 40.8 |'
  prefs: []
  type: TYPE_TB
- en: '| TaI-DPT [[20](#bib.bib20)] | CVPR 2023 | Image Caption | Unlabeled | 65.1
    | 88.3 | 46.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours | - | ChatGLM | 66.8 | 88.7 | 47.0 |'
  prefs: []
  type: TYPE_TB
- en: 5.4 Parameter Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Number of different types of token. The performances of different numbers of
    different token types in hierarchical prompts on MS-COCO are shown in Figure [4](#S5.F4
    "Figure 4 ‣ 5.4 Parameter Analysis ‣ 5 Experiments ‣ Data-free Multi-label Image
    Recognition via LLM-powered Prompt Tuning") (a). We observe that the performance
    increases as the token number increases, but a number larger than 32 brings negative
    impact. Moreover, we also analyze the composition of different types of tokens
    in Table [4](#S5.T4 "Table 4 ‣ 5.3 Ablation Studies ‣ 5 Experiments ‣ Data-free
    Multi-label Image Recognition via LLM-powered Prompt Tuning"). Note that the partial-shared
    tokens are split into two parts: “P-S #1” are coarse parts that tokens are shared
    over more categories than that of “P-S #2”. We observe similar mAP scores for
    different configurations, indicating that our method is robust to the numbers
    of different token types.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2c492d51aff1ca5535272fff1585eca8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Results on MS-COCO. (a) Analysis of the effect of number of tokens.
    (b) Analysis of the effect of weight between global and local prompts, i.e. $\lambda_{2}$
    in Eq.([16](#S4.E16 "Equation 16 ‣ 4.1 Inference ‣ 4 Training Objectives ‣ Data-free
    Multi-label Image Recognition via LLM-powered Prompt Tuning")).'
  prefs: []
  type: TYPE_NORMAL
- en: Weights of global and local prompts. To evaluate the contributions of the global
    and local prompts to the final performance, we conducted experiments with varied
    weights assigned to each branch, as depicted in Figure [4](#S5.F4 "Figure 4 ‣
    5.4 Parameter Analysis ‣ 5 Experiments ‣ Data-free Multi-label Image Recognition
    via LLM-powered Prompt Tuning") (b). We observe that as the weight allocated to
    the global prompts increases, the performance initially rises, peaking at a weight
    of 0.65, then gradually declines. This trend demonstrates that the global branch
    plays a more critical role, but the local branch is also necessary.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/72aed0ff7e51b2f3773084f3d2c569b4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Visualization of top-3 predicated categories by different prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Comparison with State-of-the-art Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We compare the proposed method with several state-of-the-art methods at different
    annotations levels of training images, including fully labeled methods (SRN [[64](#bib.bib64)],
    ML-GCN [[12](#bib.bib12)], and ASL [[38](#bib.bib38)]), partially labeled methods
    (SARB [[36](#bib.bib36)], SST [[11](#bib.bib11)], and DualCoOp [[43](#bib.bib43)]),
    one labeled methods (LL-R [[27](#bib.bib27)] and G²NetPL  [[1](#bib.bib1)]), and
    unlabeled methods (LSAN [[45](#bib.bib45)], WAN [[34](#bib.bib34)], Curriculum [[15](#bib.bib15)],
    and Naive AN [[28](#bib.bib28)]). We also compare our method with a recent method
    TaI-DPT [[20](#bib.bib20)] that uses image captions for training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [5](#S5.T5 "Table 5 ‣ 5.3 Ablation Studies ‣ 5 Experiments ‣ Data-free
    Multi-label Image Recognition via LLM-powered Prompt Tuning") shows the comparison
    results on VOC2007, MS-COCO, and NUS-WIDE. We have observations as follows: (1)
    Our method outperforms all the unsupervised methods using unlabeled training images,
    underscoring the superiority of comprehensive knowledge of objects stored in ChatGLM;
    (2) Our method exhibits a slight performance advantage over TaI-DPT, which is
    trained on human-written image captions. This result suggests that ChatGLM has
    the ability to emulate human understanding, further validating the effectiveness
    of our method; (3) The performance of our method significantly drops compared
    to the fully labeled methods, probably due to the domain gap between the training
    data of CLIP and the target data of specific task.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.6 Qualitative Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Figure [5](#S5.F5 "Figure 5 ‣ 5.4 Parameter Analysis ‣ 5 Experiments ‣ Data-free
    Multi-label Image Recognition via LLM-powered Prompt Tuning") illustrates the
    top-3 category predictions of different prompts. Notably, the hierarchical prompts
    achieve better performance, especially on smaller objects, as shown in (a), (b),
    and (c). However, as depicted in (b), an instance of incorrect top-1 prediction
    arises in the category labeled “train”, likely due to an excessive emphasis on
    global image features, resembling a train station. Conversely, as shown in (d),
    the hand-craft prompts demonstrate superior performance, probably due to the meticulous
    design of hand-craft prompts integrating certain human prior knowledge. For instance,
    when designing the prompt for the “mouse” category, we use the term “computer
    mouse”, aligning more closely with its contextual usage to improve the performance.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have presented a novel data-free framework for multi-label image recognition,
    which leverages enriched text descriptions powered by LLMs such as ChatGLM to
    well adapt VLMs like CLIP through prompt tuning. By first querying ChatGLM with
    well-designed questions and then learning hierarchical prompts with contextual
    relationships between categories, our method successfully achieves promising results
    without any training data, which is evaluated by extensive experiments on three
    benchmark datasets. Our method provides an effective way to explore the synergies
    between multiple pre-trained models for visual recognition under data scarcity.
    In the future, we are going to apply the proposed data-free framework to more
    computer vision tasks such as action recognition in videos.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Abdelfattah et al. [2022a] Rabab Abdelfattah, Xin Zhang, Mostafa M Fouda, Xiaofeng
    Wang, and Song Wang. G2netpl: Generic game-theoretic network for partial-label
    image classification. *BMVC*, 2022a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Abdelfattah et al. [2022b] Rabab Abdelfattah, Xin Zhang, Zhenyao Wu, Xinyi
    Wu, Xiaofeng Wang, and Song Wang. Plmcl: Partial-label momentum curriculum learning
    for multi-label image classification. In *ECCV*, pages 39–55\. Springer, 2022b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Abdelfattah et al. [2023] Rabab Abdelfattah, Qing Guo, Xiaoguang Li, Xiaofeng
    Wang, and Song Wang. Cdul: Clip-driven unsupervised learning for multi-label image
    classification. In *ICCV*, pages 1348–1357, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alfassy et al. [2019] Amit Alfassy, Leonid Karlinsky, Amit Aides, Joseph Shtok,
    Sivan Harary, Rogerio Feris, Raja Giryes, and Alex M Bronstein. Laso: Label-set
    operations networks for multi-label few-shot learning. In *CVPR*, pages 6548–6557,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ben-Cohen et al. [2021] Avi Ben-Cohen, Nadav Zamir, Emanuel Ben-Baruch, Itamar
    Friedman, and Lihi Zelnik-Manor. Semantic diversity learning for zero-shot multi-label
    classification. In *ICCV*, pages 640–650, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *NeurIPS*, 33:1877–1901,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bubeck et al. [2023] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes
    Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,
    et al. Sparks of artificial general intelligence: Early experiments with gpt-4.
    *arXiv preprint arXiv:2303.12712*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cai et al. [2023] Yuzhe Cai, Shaoguang Mao, Wenshan Wu, Zehua Wang, Yaobo Liang,
    Tao Ge, Chenfei Wu, Wang You, Ting Song, Yan Xia, et al. Low-code llm: Visual
    programming over llms. *arXiv preprint arXiv:2304.08103*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2023] Guangyi Chen, Xiao Liu, Guangrun Wang, Kun Zhang, Philip HS
    Torr, Xiao-Ping Zhang, and Yansong Tang. Tem-adapter: Adapting image-text pretraining
    for video question answer. In *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*, pages 13945–13955, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2019a] Tianshui Chen, Muxin Xu, Xiaolu Hui, Hefeng Wu, and Liang
    Lin. Learning semantic-specific graph representation for multi-label image recognition.
    In *ICCV*, pages 522–531, 2019a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2022] Tianshui Chen, Tao Pu, Hefeng Wu, Yuan Xie, and Liang Lin.
    Structured semantic transfer for multi-label recognition with partial labels.
    In *AAAI*, pages 339–346, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2019b] Zhao-Min Chen, Xiu-Shen Wei, Peng Wang, and Yanwen Guo.
    Multi-label image recognition with graph convolutional networks. In *CVPR*, pages
    5177–5186, 2019b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chua et al. [2009] Tat-Seng Chua, Jinhui Tang, Richang Hong, Haojie Li, Zhiping
    Luo, and Yantao Zheng. Nus-wide: a real-world web image database from national
    university of singapore. In *Proceedings of the ACM international conference on
    image and video retrieval*, pages 1–9, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Du et al. [2022] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu,
    Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive
    blank infilling. In *ACL*, pages 320–335, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Durand et al. [2019] Thibaut Durand, Nazanin Mehrasa, and Greg Mori. Learning
    a deep convnet for multi-label classification with partial labels. In *CVPR*,
    pages 647–657, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Everingham et al. [2010] Mark Everingham, Luc Van Gool, Christopher KI Williams,
    John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge.
    *IJCV*, 88:303–338, 2010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao and Zhou [2020] Bin-Bin Gao and Hong-Yu Zhou. Multi-label image recognition
    with multi-class attentional regions. *arXiv e-prints*, pages arXiv–2007, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. [2023] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang,
    Yongfeng Zhang, Hongsheng Li, and Yu Qiao. Clip-adapter: Better vision-language
    models with feature adapters. *IJCV*, pages 1–15, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gong et al. [2013] Yunchao Gong, Yangqing Jia, Thomas Leung, Alexander Toshev,
    and Sergey Ioffe. Deep convolutional ranking for multilabel image annotation.
    *arXiv preprint arXiv:1312.4894*, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. [2023a] Zixian Guo, Bowen Dong, Zhilong Ji, Jinfeng Bai, Yiwen Guo,
    and Wangmeng Zuo. Texts as images in prompt tuning for multi-label image recognition.
    In *CVPR*, pages 2808–2817, 2023a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. [2023b] Ziyu Guo, Renrui Zhang, Longtian Qiu, Xianzheng Ma, Xupeng
    Miao, Xuming He, and Bin Cui. Calip: Zero-shot enhancement of clip with parameter-free
    attention. In *AAAI*, pages 746–754, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gupta and Kembhavi [2023] Tanmay Gupta and Aniruddha Kembhavi. Visual programming:
    Compositional visual reasoning without training. In *CVPR*, pages 14953–14962,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. [2018] Shiyi He, Chang Xu, Tianyu Guo, Chao Xu, and Dacheng Tao. Reinforced
    multi-label image classification by exploring curriculum. In *AAAI*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huynh and Elhamifar [2020] Dat Huynh and Ehsan Elhamifar. A shared multi-attention
    framework for multi-label zero-shot learning. In *CVPR*, pages 8776–8786, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ji et al. [2020] Zhong Ji, Biying Cui, Huihui Li, Yu-Gang Jiang, Tao Xiang,
    Timothy Hospedales, and Yanwei Fu. Deep ranking for image zero-shot multi-label
    classification. *IEEE TIP*, 29:6549–6560, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jia et al. [2022] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge
    Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In *ECCV*,
    pages 709–727\. Springer, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim et al. [2022] Youngwook Kim, Jae Myung Kim, Zeynep Akata, and Jungwoo Lee.
    Large loss matters in weakly supervised multi-label classification. In *CVPR*,
    pages 14156–14165, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kundu and Tighe [2020] Kaustav Kundu and Joseph Tighe. Exploiting weakly supervised
    visual patterns to learn from partial annotations. *NeurIPS*, 33:561–572, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. [2018] Chung-Wei Lee, Wei Fang, Chih-Kuan Yeh, and Yu-Chiang Frank
    Wang. Multi-label zero-shot learning with structured knowledge graphs. In *CVPR*,
    pages 1576–1585, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2017] Yuncheng Li, Yale Song, and Jiebo Luo. Improving pairwise ranking
    for multi-label image classification. In *CVPR*, pages 3617–3625, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. [2014] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
    Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco:
    Common objects in context. In *ECCV*, pages 740–755\. Springer, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2017] Feng Liu, Tao Xiang, Timothy M Hospedales, Wankou Yang, and
    Changyin Sun. Semantic regularisation for recurrent image annotation. In *CVPR*,
    pages 2872–2880, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu and Tsang [2015] Weiwei Liu and Ivor Tsang. On the optimality of classifier
    chain for multi-label classification. *NeurIPS*, 28, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mac Aodha et al. [2019] Oisin Mac Aodha, Elijah Cole, and Pietro Perona. Presence-only
    geographical priors for fine-grained image classification. In *ICCV*, pages 9596–9606,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Misra et al. [2016] Ishan Misra, C Lawrence Zitnick, Margaret Mitchell, and
    Ross Girshick. Seeing through the human reporting bias: Visual classifiers from
    noisy human-centric labels. In *CVPR*, pages 2930–2939, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pu et al. [2022] Tao Pu, Tianshui Chen, Hefeng Wu, and Liang Lin. Semantic-aware
    representation blending for multi-label image recognition with partial labels.
    In *AAAI*, pages 2091–2098, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, et al. Learning transferable visual models from natural language supervision.
    In *International conference on machine learning*, pages 8748–8763\. PMLR, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ridnik et al. [2021] Tal Ridnik, Emanuel Ben-Baruch, Nadav Zamir, Asaf Noy,
    Itamar Friedman, Matan Protter, and Lihi Zelnik-Manor. Asymmetric loss for multi-label
    classification. In *ICCV*, pages 82–91, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sarafianos et al. [2018] Nikolaos Sarafianos, Xiang Xu, and Ioannis A Kakadiaris.
    Deep imbalanced attribute classification using visual attention aggregation. In
    *ECCV*, pages 680–697, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simon et al. [2022] Christian Simon, Piotr Koniusz, and Mehrtash Harandi. Meta-learning
    for multi-label few-shot classification. In *WACV*, pages 3951–3960, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Singha et al. [2023] Mainak Singha, Harsh Pal, Ankit Jha, and Biplab Banerjee.
    Ad-clip: Adapting domains in prompt space using clip. In *ICCV*, pages 4355–4364,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sohn et al. [2023] Kihyuk Sohn, Huiwen Chang, José Lezama, Luisa Polania, Han
    Zhang, Yuan Hao, Irfan Essa, and Lu Jiang. Visual prompt tuning for generative
    transfer learning. In *CVPR*, pages 19840–19851, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. [2022] Ximeng Sun, Ping Hu, and Kate Saenko. Dualcoop: Fast adaptation
    to multi-label recognition with limited annotations. *NeurIPS*, 35:30569–30582,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sung et al. [2022] Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Vl-adapter: Parameter-efficient
    transfer learning for vision-and-language tasks. In *CVPR*, pages 5227–5237, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy et al. [2016] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon
    Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer
    vision. In *CVPR*, pages 2818–2826, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Upadhyay et al. [2023] Uddeshya Upadhyay, Shyamgopal Karthik, Massimiliano
    Mancini, and Zeynep Akata. Probvlm: Probabilistic adapter for frozen vison-language
    models. In *ICCV*, pages 1899–1910, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2016] Jiang Wang, Yi Yang, Junhua Mao, Zhiheng Huang, Chang Huang,
    and Wei Xu. Cnn-rnn: A unified framework for multi-label image classification.
    In *CVPR*, pages 2285–2294, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2020] Ya Wang, Dongliang He, Fu Li, Xiang Long, Zhichao Zhou, Jinwen
    Ma, and Shilei Wen. Multi-label classification with label graph superimposing.
    In *AAAI*, pages 12265–12272, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2017] Zhouxia Wang, Tianshui Chen, Guanbin Li, Ruijia Xu, and Liang
    Lin. Multi-label image recognition by recurrently discovering attentional regions.
    In *ICCV*, pages 464–472, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. [2022] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph,
    Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler,
    Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William
    Fedus. Emergent abilities of large language models. *Transactions on Machine Learning
    Research*, 2022. Survey Certification.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. [2015] Yunchao Wei, Wei Xia, Min Lin, Junshi Huang, Bingbing Ni,
    Jian Dong, Yao Zhao, and Shuicheng Yan. Hcp: A flexible cnn framework for multi-label
    image classification. *IEEE TPAMI*, 38(9):1901–1907, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. [2023] Mengde Xu, Zheng Zhang, Fangyun Wei, Han Hu, and Xiang Bai.
    Side adapter network for open-vocabulary semantic segmentation. In *CVPR*, pages
    2945–2954, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. [2023] Jianing Yang, Xuweiyi Chen, Shengyi Qian, Nikhil Madaan,
    Madhavan Iyengar, David F Fouhey, and Joyce Chai. Llm-grounder: Open-vocabulary
    3d visual grounding with large language model as an agent. *arXiv preprint arXiv:2309.12311*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. [2022] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao
    Lu, Zicheng Liu, and Lijuan Wang. An empirical study of gpt-3 for few-shot knowledge-based
    vqa. In *AAAI*, pages 3081–3089, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. [2021] Yuan Yao, Ao Zhang, Zhengyan Zhang, Zhiyuan Liu, Tat-Seng
    Chua, and Maosong Sun. Cpt: Colorful prompt tuning for pre-trained vision-language
    models. *arXiv preprint arXiv:2109.11797*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yazici et al. [2020] Vacit Oguz Yazici, Abel Gonzalez-Garcia, Arnau Ramisa,
    Bartlomiej Twardowski, and Joost van de Weijer. Orderless recurrent models for
    multi-label classification. In *CVPR*, pages 13440–13449, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ye et al. [2020] Jin Ye, Junjun He, Xiaojiang Peng, Wenhao Wu, and Yu Qiao.
    Attention-driven dynamic graph convolutional network for multi-label image recognition.
    In *ECCV*, pages 649–665\. Springer, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2018] Junjie Zhang, Qi Wu, Chunhua Shen, Jian Zhang, and Jianfeng
    Lu. Multilabel image classification with regional latent semantic dependencies.
    *IEEE TMM*, 20(10):2801–2813, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2022] Renrui Zhang, Wei Zhang, Rongyao Fang, Peng Gao, Kunchang
    Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip-adapter: Training-free adaption
    of clip for few-shot classification. In *ECCV*, pages 493–510\. Springer, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2023] Wenqiao Zhang, Changshuo Liu, Lingze Zeng, Bengchin Ooi,
    Siliang Tang, and Yueting Zhuang. Learning in imperfect environment: Multi-label
    classification with long-tailed distribution and partial labels. In *ICCV*, pages
    1423–1432, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. [2022a] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei
    Liu. Conditional prompt learning for vision-language models. In *CVPR*, pages
    16816–16825, 2022a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. [2022b] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei
    Liu. Learning to prompt for vision-language models. *IJCV*, 130(9):2337–2348,
    2022b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. [2023] Beier Zhu, Yulei Niu, Yucheng Han, Yue Wu, and Hanwang Zhang.
    Prompt-aligned gradient for prompt tuning. In *ICCV*, pages 15659–15669, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. [2017] Feng Zhu, Hongsheng Li, Wanli Ouyang, Nenghai Yu, and Xiaogang
    Wang. Learning spatial regularization with image-level supervisions for multi-label
    image classification. In *CVPR*, pages 5513–5522, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
