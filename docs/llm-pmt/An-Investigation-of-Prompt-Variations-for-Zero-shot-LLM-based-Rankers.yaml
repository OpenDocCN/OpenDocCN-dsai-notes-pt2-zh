- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:42:30'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: An Investigation of Prompt Variations for Zero-shot LLM-based Rankers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.14117](https://ar5iv.labs.arxiv.org/html/2406.14117)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Shuoqi Sun¹, Shengyao Zhuang^(1,2), Shuai Wang¹, Guido Zuccon¹
  prefs: []
  type: TYPE_NORMAL
- en: ¹The University of Queensland,
  prefs: []
  type: TYPE_NORMAL
- en: ²CSIRO
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We provide a systematic understanding of the impact of specific components and
    wordings used in prompts on the effectiveness of rankers based on zero-shot Large
    Language Models (LLMs). Several zero-shot ranking methods based on LLMs have recently
    been proposed. Among many aspects, methods differ across (1) the ranking algorithm
    they implement, e.g., pointwise vs. listwise, (2) the backbone LLMs used, e.g.,
    GPT3.5 vs. FLAN-T5, (3) the components and wording used in prompts, e.g., the
    use or not of role-definition (role-playing) and the actual words used to express
    this. It is currently unclear whether performance differences are due to the underlying
    ranking algorithm, or because of spurious factors such as better choice of words
    used in prompts. This confusion risks to undermine future research. Through our
    large-scale experimentation and analysis, we find that ranking algorithms do contribute
    to differences between methods for zero-shot LLM ranking. However, so do the LLM
    backbones – but even more importantly, the choice of prompt components and wordings
    affect the ranking. In fact, in our experiments, we find that, at times, these
    latter elements have more impact on the ranker’s effectiveness than the actual
    ranking algorithms, and that differences among ranking methods become more blurred
    when prompt variations are considered.
  prefs: []
  type: TYPE_NORMAL
- en: An Investigation of Prompt Variations for Zero-shot LLM-based Rankers
  prefs: []
  type: TYPE_NORMAL
- en: Shuoqi Sun¹, Shengyao Zhuang^(1,2), Shuai Wang¹, Guido Zuccon¹ ¹The University
    of Queensland, ²CSIRO
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) are massively parametrised transformer models that
    have been undergone training on a large extent of text Zhao et al. ([2023](#bib.bib32)).
    Generative LLMs are capable of generating text in response to some textual input,
    called prompt Brown et al. ([2020](#bib.bib3)), or context. This prompting facility
    has been used to instruct LLMs to perform specific tasks Brown et al. ([2020](#bib.bib3));
    Wang et al. ([2023](#bib.bib29)); White et al. ([2023](#bib.bib30)); Zhuang et al.
    ([2023b](#bib.bib36)); Fan et al. ([2023](#bib.bib5)).
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we investigate the use of LLMs to create zero-shot rankers¹¹1We
    specifically focus on re-rankers, where an initial set of documents is retrieved
    from the index using a first-stage retriever, and a subset is provided to the
    re-ranker for producing the final search engine results. For ease of reading,
    we use rankers, in place of re-rankers throughout the paper. Ma et al. ([2023](#bib.bib14));
    Sun et al. ([2023](#bib.bib22)); Tang et al. ([2023](#bib.bib23)); Zhuang et al.
    ([2023a](#bib.bib33)); Qin et al. ([2023](#bib.bib18)); Zhuang et al. ([2023b](#bib.bib36)).
    With zero-shot, we mean ranking methods that do not require to be specifically
    trained for ranking tasks (beyond the pretraining executed to create the backbone
    LLM). These rankers operate by following the instructions provided in the prompt,
    which include the query for which the ranking should be produced, and the $k$
    documents that should be considered for ranking. Using the LLM, rankers then generate
    an answer to comply with the ranking instruction. Finally, either the generated
    answer contains the ranking provided by the method, or the logits of the answer
    are examined to infer a ranking.
  prefs: []
  type: TYPE_NORMAL
- en: 'Four families of zero-shot LLM rankers have been proposed: pointwise Zhuang
    et al. ([2023a](#bib.bib33)), pairwise Qin et al. ([2023](#bib.bib18)), listwise Ma
    et al. ([2023](#bib.bib14)); Sun et al. ([2023](#bib.bib22)); Tang et al. ([2023](#bib.bib23)),
    and setwise Zhuang et al. ([2023b](#bib.bib36)). They differ because of the ranking
    algorithm (or mechanism) implemented in the instructions described in the prompt.
    For example, in pointwise the LLM is instructed to determine the relevance of
    a document, while in pairwise the LLM is instructed to determine which of two
    documents is more relevant. Within each family, one or more methods have been
    proposed. They typically differ in the backbone LLM, the wording of the prompts,
    and in how the ranking is formed once the answer is generated by the LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Prompt |'
  prefs: []
  type: TYPE_TB
- en: '| PRP, [Qin et al.](#bib.bib18) | Passage: {text} Query: {query} Does the passage
    answer the query? |'
  prefs: []
  type: TYPE_TB
- en: '| RankGPT, [Sun et al.](#bib.bib22) | You are RankGPT, an intelligent assistant
    that can rank passages based on their relevancy to the query. I will provide you
    with num passages, each indicated by number identifier []. Rank the passages based
    on their relevance to query: {query}. {PASSAGES} Search Query: {query}. Rank the
    num passages above based on their relevance to the search query. The passages
    should be listed in descending order using identifiers. The most relevant passages
    should be listed first. The output format should be [] > [], e.g., [1] > [2].
    Only response the ranking results, do not say any word or explain. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Comparison between the prompts used by PRP Qin et al. ([2023](#bib.bib18))
    and RankGPT Sun et al. ([2023](#bib.bib22)): they do not simply differ in the
    ranking approach used (PRP: pointwise; RankGPT: listwise), but also in the accessory
    wording present in the prompt, e.g., the role-playing present in RankGPT (first
    line of the prompt).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recent work by Zhuang et al. ([2023b](#bib.bib36)) has shown that, once these
    zero-shot LLM rankers are compared using the same backbone LLM and fixing how
    the ranking is derived (i.e. generation vs. logits), setwise methods are the most
    effective. Depending on the dataset, pairwise and listwise methods are similarly
    effective, with pointwise methods providing lower effectiveness overall. While
    they did recognise that the use of different backbone LLMs in previous work biased
    the comparison between methods, they did not identify that the actual prompts
    used by the different rankers differed not just in terms of the words used to
    describe the ranking algorithm, i.e. the instruction associated with how scoring
    should be performed, but also in terms of “accessory wording” used. For example,
    consider the original prompt used by [Qin et al.](#bib.bib18) for their PRP pairwise
    approach Qin et al. ([2023](#bib.bib18)) and that used by [Sun et al.](#bib.bib22)
    for their RankGPT listwise approach Sun et al. ([2023](#bib.bib22)) – see Table [1](#S1.T1
    "Table 1 ‣ 1 Introduction ‣ An Investigation of Prompt Variations for Zero-shot
    LLM-based Rankers"). One can notice that RankGPT’s prompt includes also a “role-playing”
    component (in the system part of the prompt: “You are RankGPT […]”), absent in
    PRP.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With this respect, then, we ask ourself: What is the effect of such differences
    in wording used in the prompts? And, more broadly: Are differences in effectiveness
    due to the actual ranking algorithm, or they are due to the choice of words used
    in the prompts? Are differences due to LLM characteristics such as backbone and
    size?'
  prefs: []
  type: TYPE_NORMAL
- en: These are important questions because answering them will give us an understanding
    of what impacts the effectiveness of LLM rankers, and will influence how methods
    should be compared in future. We explore these directions of enquiry through a
    wide array of experiments in which we fix the backbone LLMs of the rankers, and
    we vary prompts in a controlled manner, categorising different prompt components
    and investigating their effects.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Sensibility of LLMs to Prompt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Previous work has shown that LLMs are sensitive to the prompt formulation.
    For example, Kim et al. ([2023](#bib.bib10)) explored the effect of various prompts
    and prompting technology on deploying LLMs across Natural Language Generation
    tasks, showing dependency between prompt and effectiveness. Similar findings were
    reported by Thomas et al. ([2023](#bib.bib25)), who explored different prompt
    wordings in the context of relevance labelling. Kamruzzaman and Kim ([2024](#bib.bib9))
    explored how different prompting strategies could influence the presence of social
    biases in LLM outputs: prompts designed to engage higher cognitive processes could
    reduce biases compared to simpler prompts. These previous work, among others,
    have demonstrated that the structure and subtle nuances of prompt phrasing can
    dramatically impact the performance of the LLMs, across a wide range of tasks
    and contexts.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Prompt Optimisation and Self-Optimisers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Strategic prompt design is not only beneficial but necessary to harness the
    full capabilities of LLMs. Recent studies like that by Yang et al. ([2023](#bib.bib31))
    have taken this further by investigating LLMs as self-optimisers. These models
    utilize their generative capabilities to iteratively refine prompts, thereby enhancing
    their performance on downstream tasks. Guo et al. ([2023](#bib.bib7)) integrated
    evolutionary algorithms with LLMs so that prompts can be dynamically adapted without
    needing gradient information. Sabbatella et al. ([2024](#bib.bib19))’s method
    involves a continuous relaxation of the search space, allowing for efficient optimization
    of prompts; this is suitable for scenarios where only black-box access to LLMs
    is provided. In our work we do not explore the adaptation of self-optimisers to
    prompts for zero-shot LLM rankers: but our study motivates pursuing this as a
    direction for future work.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Zero-Shot LLM Rankers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We next review generative LLM-based rankers; four ranker families are discussed:
    pointwise Zhuang et al. ([2023a](#bib.bib33)), pairwise Qin et al. ([2023](#bib.bib18)),
    listwise Ma et al. ([2023](#bib.bib14)); Sun et al. ([2023](#bib.bib22)); Tang
    et al. ([2023](#bib.bib23)), and setwise Zhuang et al. ([2023b](#bib.bib36)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In pointwise, two approaches can be followed: generation Liang et al. ([2022](#bib.bib12));
    Nogueira et al. ([2020](#bib.bib16)) and likelihood Zhuang et al. ([2021](#bib.bib34));
    Zhuang and Zuccon ([2021](#bib.bib37)). In generation, LLMs are prompted with
    a query-document pair and asked to generate a binary answer (“yes"/“no") to the
    question of whether the document is relevant to the query, and the likelihood
    of generating “yes" (extracted from the associated token logits) decides the ranking.
    In likelihood, a query likelihood model is involved in ranking. The LLM is prompted
    with the document and asked to produce a relevant query. Then, the raking of documents
    is based on the likelihood of generating the provided query Sachan et al. ([2022](#bib.bib20)),
    which is obtained from the associated token logits. In our experiments with pointwise
    we implemented the generation approach, which is more commonly used in previous
    work.'
  prefs: []
  type: TYPE_NORMAL
- en: Pairwise ranking instead compares the relevance of two documents to a single
    query; these are passed onto the model via the prompt. LLMs are prompted to answer
    which document is more relevant to the query. The ranking of documents is based
    on relative relevance Qin et al. ([2023](#bib.bib18)).
  prefs: []
  type: TYPE_NORMAL
- en: For listwise ranking a list of documents and a query is passed to the LLM via
    the prompt. LLMs are driven to generate the document labels of relevant documents
    in a certain order; the ranking then depends on this Ma et al. ([2023](#bib.bib14));
    Sun et al. ([2023](#bib.bib22)); Pradeep et al. ([2023](#bib.bib17)).
  prefs: []
  type: TYPE_NORMAL
- en: In setwise, a set of documents is provided with one query. LLMs are prompted
    to select the most relevant document, in an iterative way. This allows sorting
    algorithms to give the ranking results based on the preference to documents, and
    to stop ranking after $k$ were given as input) Zhuang et al. ([2023b](#bib.bib36)).
  prefs: []
  type: TYPE_NORMAL
- en: In common across all these rankers is their use of generative LLM backbones
    in a zero-shot manner, i.e. without the need for further training the LLM for
    the specific ranking approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'We finally note a recent, different trend in information retrieval where generative
    LLMs are used to obtain dense representations of documents and queries (separately,
    i.e. in a bi-encoder manner), which are then in turn used for dense retrieval:
    while most require fine-tuning Lee et al. ([2024](#bib.bib11)); Wang et al. ([2024a](#bib.bib27),
    [b](#bib.bib28)); BehnamGhader et al. ([2024](#bib.bib2)), zero-shot methods are
    also emerging Zhuang et al. ([2024](#bib.bib35)). In this paper we focus on zero-shot
    LLM-based re-rankers, and thus do not consider these dense retrievers; however
    we note they are likely affected in the same way from the issues we investigate Zhuang
    et al. ([2024](#bib.bib35)).'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Different strategies for using LLMs as rankers have been proposed; they not
    only differ in the ranking algorithm and backbone used, but also in the specific
    wording provided to the LLMs to perform the task. Next, we aim to collect the
    original prompts used in the literature, which will serve as the foundational
    prompt setting for our experiments. We then analyse the individual characteristics
    of each prompt, building a taxonomy of the components used in the prompts along
    with a list of instantiations used for each component across the different prompts
    (Section [3.1](#S3.SS1 "3.1 Prompts Collection and Taxonomy of Ranking Prompt
    Components ‣ 3 Methodology ‣ An Investigation of Prompt Variations for Zero-shot
    LLM-based Rankers")). With this information, we aim then to design experiments
    where we can systematically analyse the impact of components and specific wordings
    associated with components across different ranking algorithms and backbones.
    We do this by assembling prompt variations (Section [3.2](#S3.SS2 "3.2 Building
    Prompt Variations ‣ 3 Methodology ‣ An Investigation of Prompt Variations for
    Zero-shot LLM-based Rankers")). Experiment configurations are then outlined in
    Section [3.3](#S3.SS3 "3.3 Experiments Settings ‣ 3 Methodology ‣ An Investigation
    of Prompt Variations for Zero-shot LLM-based Rankers").
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Prompts Collection and Taxonomy of Ranking Prompt Components
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We started by collecting the original prompts used in current zero-shot LLM-based
    rankers Zhuang et al. ([2023a](#bib.bib33)); Qin et al. ([2023](#bib.bib18));
    Ma et al. ([2023](#bib.bib14)); Sun et al. ([2023](#bib.bib22)); Tang et al. ([2023](#bib.bib23));
    Zhuang et al. ([2023b](#bib.bib36)); these are reported in Appendix [A](#A1 "Appendix
    A Original Prompts of LLM rankers ‣ An Investigation of Prompt Variations for
    Zero-shot LLM-based Rankers").
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Type and wording alternatives of prompt components. None (Wording
    Alternative 0) refers to a prompt that does not use that component. A dash ($-$)
    means all prompts require that component and so the Wording Alternative 0 cannot
    be used (i.e. that component cannot be left empty). A checkmark means the prompt
    can be created without that component.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Wording Alternatives |'
  prefs: []
  type: TYPE_TB
- en: '| Component | Ranker | None (0) | 1 | 2 | 3 | 4 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| Task Instruction (TI) | pointwise | - | Does the passage answer the query?
    | Is this passage relevant to the query? | For the following query and document,
    judge whether they are relevant. | Judge the relevance between the query and the
    document. | - |'
  prefs: []
  type: TYPE_TB
- en: '| pairwise | - | Given a query, which of the following two passages is more
    relevant to the query? | - | - | - |  |'
  prefs: []
  type: TYPE_TB
- en: '| listwise | - | Rank the {num} passages based on their relevance to the search
    query. | Sort the Passages by their relevance to the Query. | I will provide you
    with {num} passages, each indicated by number identifier []. Rank the passages
    based on their relevance to query. | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| setwise | - | Which one is the most relevant to the query. | - | - | - |
    - |'
  prefs: []
  type: TYPE_TB
- en: '| Output Type (OT) | pointwise |  | Judge whether they are "Highly Relevant",
    "Somewhat Relevant", or "Not Relevant”. | From a scale of 0 to 4, judge the relevance.
    | Answer ’Yes’ or ’No’. | Answer True/False. | - |'
  prefs: []
  type: TYPE_TB
- en: '| pairwise | - | Output Passage A or Passage B. | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| listwise | - | Sorted Passages = [ | The passages should be listed in descending
    order using identifiers. The most relevant passages should be listed first. The
    output format should be [] >[], e.g., [1] >[2]. | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| setwise | - | Output the passage label of the most relevant passage. | Generate
    the passage label. | Generate the passage label that is the most relevant to the
    query, then explain why you think this passage is the most relevant. | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Tone Words (TW) | All |  | You better get this right or you will be punished.
    | Only output the ranking results, do not say any word or explanation. | Please
    | Only | Must |'
  prefs: []
  type: TYPE_TB
- en: '| Role Playing (RP) | All |  | You are RankGPT, an intelligent assistant that
    can rank passages based on their relevancy to the query. | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: After the original prompts were collected, we analysed the prompts to identify
    high level components that are present in at least one original prompt, along
    with the associated variants. We also augmented the list of components and variants
    with other wordings we devised to explore specific categories further, e.g., tone
    words (see below).
  prefs: []
  type: TYPE_NORMAL
- en: 'The analysis revealed five components:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Evidence (EV): these are the query and the associated passages to rank.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Task Instruction (TI): the instructions associated to the specific ranking
    strategy: these outline to the LLM the algorithmic steps to follow to produce
    a ranking. Example wordings include “which passage is more relevant” (pairwise)
    and “is this passage relevant to the query” (pointwise).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Output Type (OT): the instructions that specify the format of the output the
    LLM needs to generate. For example, for pointwise ranking the LLM could be instructed
    to generated a Yes/No or a True/False answer.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tone Words (TW): words that express a positive, negative, or neutral connotation
    and that help express the attitude of the prompt author towards the ranking instruction,
    e.g., “please” or “you better get this right or you will be punished”.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Role Playing (RP): a description of the tool implemented by the LLM, used to
    make the LLM “impersonate” that role.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note that TI and OT largely determine the ranker family that is implemented,
    while TW and RP can be generally applied to any ranker family and EV are always
    present. Table [2](#S3.T2 "Table 2 ‣ 3.1 Prompts Collection and Taxonomy of Ranking
    Prompt Components ‣ 3 Methodology ‣ An Investigation of Prompt Variations for
    Zero-shot LLM-based Rankers") lists the options we consider in our experiments
    for each of these components (but EV, since it is always the same). Some options
    refer to variations found in an original ranking prompt formulation; we augmented
    these with wordings we devised to explore additional alternatives for some of
    the components.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to these components, our analysis of existing ranking prompts identified
    different approaches in the ordering of some of the components within the prompts;
    in particular:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Evidence Ordering (EO): the relative ordering of the query and passage(s) provided
    to the LLM – whether the query is given first, followed by the passage(s), which
    we label as QF, or vice versa, passage(s) followed by the query (labelled PF).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Position of Evidence (PE): instruction to specify the position of the evidence
    in the prompt – at the beginning (B) or at the end of the prompt (E).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.2 Building Prompt Variations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To explore the role of the identified components, their interactions, and the
    effect of specific wordings used to instantiate each of the components, we setup
    a large scale experiment where prompts with unique combinations of these aspects
    are built.
  prefs: []
  type: TYPE_NORMAL
- en: 'To build prompts, we first consider the ordering options we identified, and
    build a prompt template for each combination: this gives rise to four prompt templates,
    shown in Table [3](#S3.T3 "Table 3 ‣ 3.2 Building Prompt Variations ‣ 3 Methodology
    ‣ An Investigation of Prompt Variations for Zero-shot LLM-based Rankers").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Prompt templates that combine the five components with the four ordering
    options available. Q and P denote query text passage(s) text, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '| EO/PE | B | E |'
  prefs: []
  type: TYPE_TB
- en: '| QF | RP+ TI (Q) + P + TW+ TO | RP+ TW+ TO + TI (Q) + P |'
  prefs: []
  type: TYPE_TB
- en: '| PF | RP+ P + TI (Q) + TW+ TO | RP+ TW+ TO + P + TI (Q) |'
  prefs: []
  type: TYPE_TB
- en: Then, for each template, we consider all possible instantiations. The number
    of variations of wordings for the task instruction (TI) and output type (OT) components
    differ across families of rankers – thus giving rise to different number of prompt
    instantiations across each family. In particular, for pointwise we consider 768
    unique prompts; for pairwise 48; for listwise 288; and for setwise 144.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally note that we did minor modifications to the original prompts from previous
    works to improve their consistency: for example, some prompts enclosed the query
    in quotes, while others appended the query after a colon; other differences included
    the presence of multiple line breaks. We settled on adapting a unique format for
    these aspects. These differences in prompt formatting resulted in non statistically
    significant differences in effectiveness (e.g., [Qin et al.](#bib.bib18)’s original
    prompt on COVID and Llama 3-8B backbone obtained an nDCG@10 of 0.8014, while our
    adjusted prompt lead to an nDCG@10 of 0.7966).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Experiments Settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To experiment with the LLM rankers and the prompt variations, we setup a two
    stage ranking pipeline in line with previous work. For the first stage, we use
    the BM25 implementation from Pyserini Lin et al. ([2021](#bib.bib13)) to retrieve
    the top 100 documents for a query. For the second stage, we use the LLM ranker
    to re-rank these 100 documents.
  prefs: []
  type: TYPE_NORMAL
- en: As LLMs backbones, we selected instruction-tuned checkpoints from the Flan-T5
    family Chung et al. ([2024](#bib.bib4)), Mistral 7B Jiang et al. ([2023](#bib.bib8)),
    and Llama3 8B AI@Meta ([2024](#bib.bib1)). These are popular and highly-performant
    open LLMs; we excluded close-source LLMs like GPT4 in our analysis because of
    the high costs associated with running our experiments on commercial APIs.
  prefs: []
  type: TYPE_NORMAL
- en: 'For Flan-T5 we considered checkpoints of different sizes: Large (783M), XL
    (2.85B) and XXL (11.3B). This allowed us to explore the role of backbone size
    in instruction following ability and ranking. We did not considered extending
    the experiments to larger sizes of the other backbones, e.g., Llama3 70B, because
    of the high computational costs associated with doing this systematically.'
  prefs: []
  type: TYPE_NORMAL
- en: Flan-T5 models have a maximum input length of 512 tokens. Upon analysis of queries
    and passages lengths used in our evaluation, methods such as setwise and listwise
    will exceed the length limits as they have multiple passages in the prompt, and
    amount all methods we investigated, listwise requires longest input length. Hence,
    to make comparisons between the different experimental conditions fair,we truncated
    inputs for all methods and LLM backbones to fit the 512 tokens limit. Based on
    our calculation of the longest non-query-and-passage prompt instructions, we set
    the query and passage lengths at 20 and 80 words, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'We used three datasets from the ir_datasets MacAvaney et al. ([2021](#bib.bib15))
    python library: TREC Deep Learning (DL) 2019 and 2020 (43 and 48 queries respectively),
    and BeIR TREC COVID (50 queries) Voorhees et al. ([2021](#bib.bib26)); Thakur
    et al. ([2021](#bib.bib24)). We performed our analysis using nDCG@10, the primary
    metric across these datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Results Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We base our empirical analysis along six main lines of enquiry that help us
    investigate the impact prompts have on LLM rankers, including what makes ranking
    prompts effective, how rankers respond to different prompts, how ranking methods
    truly compare at the net of variations in prompt wording, and the impact of LLM
    backbones.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Are there better prompts?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For each family of rankers, we compare the effectiveness of the original prompts
    with all other prompt variations. Results are displayed in Figure [1](#S4.F1 "Figure
    1 ‣ 4.1 Are there better prompts? ‣ 4 Results Analysis ‣ An Investigation of Prompt
    Variations for Zero-shot LLM-based Rankers"): for each family of rankers, the
    star symbols identify the original prompts, while the boxplot shows the distribution
    of effectiveness across all prompt variations. In all cases we find better prompts
    that can achieve statistically higher effectiveness than the original prompts,
    with the exception of listwise and pairwise on the COVID dataset when the Llama
    3 backbone is used. We also identify cases where the original prompt was the worst
    among those considered for the specific ranking family: this is the case for example
    for the listwise prompt evaluated on DL19 and DL 20 with the FlanT5-Large backbone.
    We present the actual nDCG@10 scores of original prompts and the best prompts
    in Appendix [C](#A3 "Appendix C Effectiveness of Original and Best Prompts ‣ An
    Investigation of Prompt Variations for Zero-shot LLM-based Rankers").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: Effectiveness (nDCG@10) of zero-shot LLM-based rankers across ranking
    methods, prompt variations, LLM backbones and datasets. Effectiveness achieved
    by original prompts for each method is marked with a star, and annotated with
    value.'
  prefs: []
  type: TYPE_NORMAL
- en: (a) DL 19
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bed431941901b3899adc3b38dd1dde8b.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) DL 20
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ba1d692fd0ce141d01d6800b255a0e90.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) COVID
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c7818e9fdb9fdbef49391788cacfa07b.png)![Refer to caption](img/aa073e9a999b471ff28f427809fdeb3c.png)![Refer
    to caption](img/3851d873819c1b64e739352149927efe.png)![Refer to caption](img/bc0cc7a884231c1ff4ec5009f652f436.png)![Refer
    to caption](img/ff326f77291ea6d0b9b6116ac7f364f8.png)![Refer to caption](img/8462b330ce5697d584c6f3aebab43c9a.png)![Refer
    to caption](img/25b43892fdf762883422e09c3aa35f1e.png)![Refer to caption](img/4908776a2387cd9a279170ae8d62a497.png)![Refer
    to caption](img/fdba0576755a84e2b6f0ec3cb9e3bd0b.png)![Refer to caption](img/1674e1643cf2e5cc0bf6d49766d118f3.png)![Refer
    to caption](img/677b2500e8eea4fbf519f4f06ffa66ea.png)![Refer to caption](img/096235fdf56a2330319b030d08282620.png)![Refer
    to caption](img/d8aaeb1a4e3827cc9d66f4ad5a641ed1.png)'
  prefs: []
  type: TYPE_IMG
- en: 4.2 What are the characteristic of the best prompts?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Table [5](#A2.T5 "Table 5 ‣ Appendix B Best Prompt Variations ‣ An Investigation
    of Prompt Variations for Zero-shot LLM-based Rankers") in Appendix [B](#A2 "Appendix
    B Best Prompt Variations ‣ An Investigation of Prompt Variations for Zero-shot
    LLM-based Rankers") reports the optimal prompt components for various ranking
    methods and datasets. We identify notable patterns as below.
  prefs: []
  type: TYPE_NORMAL
- en: 'Task Instruction and Output Type: We analyse task instructions and output types
    together due to their inherent interrelation; the type of output largely depends
    on the ranking algorithm the task instruction implements. We analyse these separately
    for each ranking family because instructions and output types vary across ranking
    families. We do not analyse pairwise here as for this method there are no alternative
    choices for these wordings.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pointwise ranking does not show a consistent optimal choice; however, Task
    Instruction #3 is most prevalent, while Output Type #2 (judging relevance on a
    numerical scale) is seldom optimal.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Listwise ranking shows that the best task instruction varies by dataset and
    LLM backbone. Output Type #2 appears in 80% of the most effective configurations.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Setwise ranking studied with a single task instruction, finds Output Type #3
    (label and explain the most relevant passage) as the most common among top-performing
    prompts, appearing in 53% of cases.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Tone Words: The inclusion of tone words does not present a clear trend, with
    the percentage of prompts achieving a higher effectiveness with no tone words
    or each of the five tone words distributed as follows: 18% (no tone words), 17%,
    18%, 20%, 15%, and 12%. This indicates a relatively uniform influence of tone
    words choice on prompt effectiveness. Furthermore, no consistent patterns were
    observed regarding the influence of LLM backbones or datasets on the effectiveness
    related to tone words. Notably, including a tone word in the prompt led to increased
    effectiveness in 82% of the cases, underscoring the potential benefit of tone
    words in enhancing prompt performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Role Playing: Role playing often leads to the best effectiveness for pointwise
    and pairwise prompts (80% and 66% of the cases, respectively), it has mixed effects
    for setwise (ipresent in the best prompt about half of the times), while it is
    not associated with the best effectiveness for listwise (13%). Overall, 55% of
    the prompts with highest effectiveness include role playing wording. Role playing
    was originally only used by one ranking method, RankGPT Sun et al. ([2023](#bib.bib22)),
    highlighting the difficulty in comparing different ranking methods if their prompt
    variations are not fairly explored.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Evidence Ordering: For pointwise ranking, presenting passage text before the
    query text is preferred in 86% of top-performing prompts. The preference is less
    clear in other ranking types: 40% for pairwise; 40% for listwise; 46% for setwise
    where the best prompts have the passage text before the query text. Considering
    model backbones, Flan-T5 tends to perform best when presented with passage text
    before query text (66% of cases); while results are mixed for the other backbone
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Position of Evidence: Among the best prompts, there tend to be an overall preference
    for prompts that provide the evidence at the beginning (before any other instruction):
    this is the case in 63% of the best prompts, with pointwise and listwise prompts
    exhibiting more often this pattern (73% and 67% respectively). Across all datasets,
    most best prompts for the FlanT5-XXL backbone have evidence at the beginning.
    This is also the case for Llama3-8B for DL19 and COVID and for Mistral-7B for
    COVID.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary. The analysis of the prompt templates associated with the prompts leading
    to the best effectiveness across all ranking families did not highlight any specific
    prompt wording combination that is more conducive of best effectiveness than others
    across all datasets and backbones – though we found that tone words and role playing
    are frequent among most of the best prompts. This analysis is further extended
    in Section [4.4](#S4.SS4 "4.4 Are ranking methods stable? ‣ 4 Results Analysis
    ‣ An Investigation of Prompt Variations for Zero-shot LLM-based Rankers") where
    the stability of rankers across prompts variations is considered.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Which ranking method is most effective?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before our analysis of prompt variations, the only comparison of ranking methods
    across the four families within a consistent setting of datasets and backbone
    was provided by Zhuang et al. ([2023b](#bib.bib36)). According to their results,
    the best performing rankers were setwise and pairwise (depending on dataset and
    backbone), followed by listwise and then pointwise, which were distinguishably
    worse.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our analysis of prompt variations reveals a somewhat different picture. Consider
    the top results for each ranking family across datasets and LLM backbone reported
    in Figure [1](#S4.F1 "Figure 1 ‣ 4.1 Are there better prompts? ‣ 4 Results Analysis
    ‣ An Investigation of Prompt Variations for Zero-shot LLM-based Rankers"). Similarly
    to previous findings, we also observe that overall pairwise and setwise methods
    deliver the best results. However, we find that pointwise can be as competitive
    as these previous methods if instructed with specific prompts. This is the case
    throughout all datasets and LLM backbones, with the exception of Llama 3\. For
    Llama 3 on DL datasets, in fact, we observe that pointwise significantly underperforms
    other methods. At the same time, we also observe that there are instances in which
    pointwise ranking can far exceed other methods: this is the case on DL19 when
    using the FLanT5-Large and XL backbones (best pointwise nDCG@10 respectively 0.6918
    and 0.7010, statistically significantly outperforming the other methods in most
    cases), and on DL20 when using Mistral-7B (best pointwise 0.6486).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Are ranking methods stable?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We consider the four ranking families independently, and study the variance
    of their effectiveness across all prompt variations for that family. Results are
    displayed in Figure [1](#S4.F1 "Figure 1 ‣ 4.1 Are there better prompts? ‣ 4 Results
    Analysis ‣ An Investigation of Prompt Variations for Zero-shot LLM-based Rankers").
    We observe that pointwise methods display the largest variability in effectiveness
    due to prompt variations, with some prompt variations delivering poor effectiveness.
    Setwise and pairwise instead do better, displaying lower variability: setwise
    achieves this across all backbones and datasets investigated, while pairwise displays
    larger variability in specific conditions, e.g., when using Mistral, and for the
    COVID dataset when using Llama3.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Two key insights arise: 1) ranking algorithms are susceptible to prompt variations
    and may exhibit wide-ranging performance variations, and 2) different ranking
    algorithms have varying sensitivity to prompts, indicating the potential for some
    to mitigate the impact of these variations. This suggests more comprehensive prompting
    optimisation, e.g., via self-prompting, may further improve effectiveness across
    all rankers.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Does the LLM size matter?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We answer this question by focusing on the FlanT5 models only, which come in
    three different sizes (first three rows of plots in Figure [1](#S4.F1 "Figure
    1 ‣ 4.1 Are there better prompts? ‣ 4 Results Analysis ‣ An Investigation of Prompt
    Variations for Zero-shot LLM-based Rankers")). We observe that in general larger
    models deliver higher effectiveness and reduced variations across the board. However,
    the pointwise family represents an exception to this. Improvements are observed
    when passing from FlanT5-large to FlanT5-XL. However when using FlanT5-XXL we
    observe both decreased effectiveness and large variance in effectiveness across
    the prompt variations.
  prefs: []
  type: TYPE_NORMAL
- en: 4.6 Does the LLM backbone matter?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We answer this question by comparing three backbones across the results from
    Figure [1](#S4.F1 "Figure 1 ‣ 4.1 Are there better prompts? ‣ 4 Results Analysis
    ‣ An Investigation of Prompt Variations for Zero-shot LLM-based Rankers")): FlanT5-XXL
    (11.3B parameters), Mistral-7B and Llama3-8B: the last two are of comparable size,
    while the first has approximately 40-60% more parameters. We observe that generally
    Llama3 and FlanT5 outperform Mistral-based rankers. Llama3 and FlanT5 have overall
    similar effectiveness, though on the COVID dataset Llama3-based rankers consistently
    outperform those with FlanT5\. We also observe that Mistral-based rankers exhibit
    larger variance in effectiveness due to prompt variations than rankers based on
    the other backbones.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Recent works have shown the promise of using generative LLMs to implement effective
    zero-shot re-rankers. Although distinctions among various methods primarily arise
    from the underlying ranking algorithms, further differences emerge based on the
    characteristics of the prompts employed to implement these algorithms. These characteristics
    are not associated with the actual ranking algorithm: e.g., they may be wordings
    related to a role-playing strategy, or tone words. In this paper, we analysed
    these prompts and mapped prompt wordings into a set of components to form prompt
    templates that allowed us to better understand the content of these prompts. We
    then performed a systematic analysis of prompt variations across different types
    of zero-shot LLM rankers. Our analysis revealed that ranking effectiveness varies
    considerably across different implementations of prompt components. Optimal prompt
    wording showed variability depending on the ranking method, dataset, and LLM backbone
    employed, suggesting that automatic prompt optimization, tailored to specific
    ranking methods and datasets, may be more effective than manual prompt engineering
    for optimizing ranking performance.'
  prefs: []
  type: TYPE_NORMAL
- en: We make code, runs and analysis available at [https://github.com/ielab/zeroshot-rankers-prompt-variations](https://github.com/ielab/zeroshot-rankers-prompt-variations).
  prefs: []
  type: TYPE_NORMAL
- en: 6 Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Latency is an important factor affective the deployment of rankers in production.
    In this paper, we did not consider query latency as one of the dimensions of analysis
    and comparison because our focus was on varying the prompts to measure ranking
    effectiveness. [Zhuang et al.](#bib.bib36) provided a first insight into comparing
    latency among the zero-shot LLM rankers we considered. Latency of the method and
    prompts we considered in our analysis would be similar to that performed there;
    however we note that some of the prompts we consider are longer than others (e.g.
    if the role playing component is added to prompts) – longer prompts result in
    increased query latency.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, our experiments considered 1,248 prompt variations. While this is a
    large number, many more prompt variations could have been designed and investigated.
    However, sensibly increasing the number of prompt variations in our experiments
    would have been infeasible as it would have exceeded our compute budget. The current
    set of experiments took in excess of 12,400 GPU-hours to execute. In particular,
    varying prompt for the pairwise ranking methods results in large computational
    requirements compared to other methods because of the extensive number of pairwise
    computations (and thus LLM inferences) required by this method to answer a query.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our analysis we considered instruction-tuned checkpoints from three LLM
    backbones: FlanT5 (in three different sizes), Mistral and Llama 3\. A wider range
    of LLMs could have been considered, importantly including OpenAI’s GPT models
    such as GPT3.5 and GPT4\. GPT3.5 for example was found more effective than Llama
    2 and Vicuna based LLM rankers in a limited set of experiments in previous work Zhuang
    et al. ([2023b](#bib.bib36)). We were however restricted to using non-commercial
    models because of the high costs involved in performing the large number of experiments
    we considered with commercial APIs. For example, using the cost estimates from
    [Zhuang et al.](#bib.bib36), executing our experiments for pairwise, listwise
    and setwise across all considered prompts using GPT3.5 alone would have costed
    approximately USD $5,000.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Ethical considerations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In our experiments we considered a large number of prompt variations: for pointwise
    we consider 768 unique prompts; for pairwise 48; for listwise 288; and for setwise
    144 – for a total of 1,248 prompts. We ran these prompts across three datasets,
    and using five different LLMs with up to 11B parameters. The execution of these
    experiments required a large amount of compute power; we ran experiments across
    three clusters: one with Nvidia A100 GPUs, another with Nvidia H100 GPUs, and
    a smaller one with Nvidia A6000 GPUs. Although we only considered zero-shot LLM
    ranking approaches and thus did not conduct any LLM training, we recognize that
    our experiments may have still consumed substantial energy, thereby contributing
    to CO2 emissions Scells et al. ([2022](#bib.bib21)) and water consumption Zuccon
    et al. ([2023](#bib.bib38)).'
  prefs: []
  type: TYPE_NORMAL
- en: The publicly available LLMs we used may have been trained with content that
    contains several types of societal biases Gallegos et al. ([2024](#bib.bib6))
    – and these biases may permeate in the rankings our zero-shot LLM rankers produced.
    Future research could explore ways to mitigate these biases prompt engineering.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AI@Meta (2024) AI@Meta. 2024. [Llama 3 model card](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BehnamGhader et al. (2024) Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach,
    Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy. 2024. [Llm2vec: Large language
    models are secretly powerful text encoders](https://arxiv.org/abs/2404.05961).
    *Preprint*, arXiv:2404.05961.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chung et al. (2024) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay,
    William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
    2024. Scaling instruction-finetuned language models. *Journal of Machine Learning
    Research*, 25(70):1–53.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan et al. (2023) Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei,
    Yiqi Wang, Jiliang Tang, and Qing Li. 2023. Recommender systems in the era of
    large language models (llms). *arXiv preprint arXiv:2307.02046*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gallegos et al. (2024) Isabel O Gallegos, Ryan A Rossi, Joe Barrow, Md Mehrab
    Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K
    Ahmed. 2024. Bias and fairness in large language models: A survey. *Computational
    Linguistics*, pages 1–79.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2023) Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song,
    Xu Tan, Guoqing Liu, Jiang Bian, and Yujiu Yang. 2023. Connecting large language
    models with evolutionary algorithms yields powerful prompt optimizers. *arXiv
    preprint arXiv:2309.08532*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. *arXiv preprint
    arXiv:2310.06825*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kamruzzaman and Kim (2024) Mahammed Kamruzzaman and Gene Louis Kim. 2024. Prompting
    techniques for reducing social bias in llms through system 1 and system 2 cognitive
    processes. *arXiv preprint arXiv:2404.17218*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim et al. (2023) Joonghoon Kim, Saeran Park, Kiyoon Jeong, Sangmin Lee, Seung Hun
    Han, Jiyoon Lee, and Pilsung Kang. 2023. Which is better? exploring prompting
    strategy for llm-based metrics. *arXiv preprint arXiv:2311.03754*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. (2024) Jinhyuk Lee, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer,
    Jeremy R. Cole, Kai Hui, Michael Boratko, Rajvi Kapadia, Wen Ding, Yi Luan, Sai
    Meher Karthik Duddu, Gustavo Hernandez Abrego, Weiqiang Shi, Nithi Gupta, Aditya
    Kusupati, Prateek Jain, Siddhartha Reddy Jonnalagadda, Ming-Wei Chang, and Iftekhar
    Naim. 2024. [Gecko: Versatile text embeddings distilled from large language models](https://arxiv.org/abs/2403.20327).
    *Preprint*, arXiv:2403.20327.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang et al. (2022) Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras,
    Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya
    Kumar, et al. 2022. Holistic evaluation of language models. *arXiv preprint arXiv:2211.09110*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2021) Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang,
    Ronak Pradeep, and Rodrigo Nogueira. 2021. Pyserini: A python toolkit for reproducible
    information retrieval research with sparse and dense representations. In *Proceedings
    of the 44th International ACM SIGIR Conference on Research and Development in
    Information Retrieval*, pages 2356–2362.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. (2023) Xueguang Ma, Xinyu Zhang, Ronak Pradeep, and Jimmy Lin. 2023.
    Zero-shot listwise document reranking with a large language model. *arXiv preprint
    arXiv:2305.02156*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MacAvaney et al. (2021) Sean MacAvaney, Andrew Yates, Sergey Feldman, Doug Downey,
    Arman Cohan, and Nazli Goharian. 2021. Simplified data wrangling with ir_datasets.
    In *SIGIR*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nogueira et al. (2020) Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. 2020.
    Document ranking with a pretrained sequence-to-sequence model. *arXiv preprint
    arXiv:2003.06713*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pradeep et al. (2023) Ronak Pradeep, Sahel Sharifymoghaddam, and Jimmy Lin.
    2023. Rankvicuna: Zero-shot listwise document reranking with open-source large
    language models. *arXiv preprint arXiv:2309.15088*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qin et al. (2023) Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu,
    Jiaming Shen, Tianqi Liu, Jialu Liu, Donald Metzler, Xuanhui Wang, et al. 2023.
    Large language models are effective text rankers with pairwise ranking prompting.
    *arXiv preprint arXiv:2306.17563*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sabbatella et al. (2024) Antonio Sabbatella, Andrea Ponti, Ilaria Giordani,
    Antonio Candelieri, and Francesco Archetti. 2024. Prompt optimization in large
    language models. *Mathematics*, 12(6):929.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sachan et al. (2022) Devendra Singh Sachan, Mike Lewis, Mandar Joshi, Armen
    Aghajanyan, Wen-tau Yih, Joelle Pineau, and Luke Zettlemoyer. 2022. Improving
    passage retrieval with zero-shot question generation. *arXiv preprint arXiv:2204.07496*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scells et al. (2022) Harrisen Scells, Shengyao Zhuang, and Guido Zuccon. 2022.
    Reduce, reuse, recycle: Green information retrieval research. In *Proceedings
    of the 45th International ACM SIGIR Conference on Research and Development in
    Information Retrieval*, pages 2825–2837.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2023) Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin,
    and Zhaochun Ren. 2023. Is chatgpt good at search? investigating large language
    models as re-ranking agent. *arXiv preprint arXiv:2304.09542*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tang et al. (2023) Raphael Tang, Xinyu Zhang, Xueguang Ma, Jimmy Lin, and Ferhan
    Ture. 2023. Found in the middle: Permutation self-consistency improves listwise
    ranking in large language models. *arXiv preprint arXiv:2310.07712*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thakur et al. (2021) Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek
    Srivastava, and Iryna Gurevych. 2021. Beir: A heterogenous benchmark for zero-shot
    evaluation of information retrieval models. *arXiv preprint arXiv:2104.08663*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thomas et al. (2023) Paul Thomas, Seth Spielman, Nick Craswell, and Bhaskar
    Mitra. 2023. Large language models can accurately predict searcher preferences.
    *arXiv preprint arXiv:2309.10621*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Voorhees et al. (2021) Ellen Voorhees, Tasmeer Alam, Steven Bedrick, Dina Demner-Fushman,
    William R Hersh, Kyle Lo, Kirk Roberts, Ian Soboroff, and Lucy Lu Wang. 2021.
    Trec-covid: constructing a pandemic information retrieval test collection. In
    *ACM SIGIR Forum*, volume 54, pages 1–12\. ACM New York, NY, USA.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2024a) Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun
    Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2024a. [Text embeddings by weakly-supervised
    contrastive pre-training](https://arxiv.org/abs/2212.03533). *Preprint*, arXiv:2212.03533.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2024b) Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan
    Majumder, and Furu Wei. 2024b. [Improving text embeddings with large language
    models](https://arxiv.org/abs/2401.00368). *Preprint*, arXiv:2401.00368.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023) Shuai Wang, Harrisen Scells, Bevan Koopman, and Guido Zuccon.
    2023. Can chatgpt write a good boolean query for systematic review literature
    search? In *Proceedings of the 46th International ACM SIGIR Conference on Research
    and Development in Information Retrieval*, pages 1426–1436.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: White et al. (2023) Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos
    Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, and Douglas C Schmidt.
    2023. A prompt pattern catalog to enhance prompt engineering with chatgpt. *arXiv
    preprint arXiv:2302.11382*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2023) Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V
    Le, Denny Zhou, and Xinyun Chen. 2023. Large language models as optimizers. *arXiv
    preprint arXiv:2309.03409*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2023) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei
    Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al.
    2023. A survey of large language models. *arXiv preprint arXiv:2303.18223*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhuang et al. (2023a) Honglei Zhuang, Zhen Qin, Kai Hui, Junru Wu, Le Yan,
    Xuanhui Wang, and Michael Berdersky. 2023a. Beyond yes and no: Improving zero-shot
    llm rankers via scoring fine-grained relevance labels. *arXiv preprint arXiv:2310.14122*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhuang et al. (2021) Shengyao Zhuang, Hang Li, and Guido Zuccon. 2021. Deep
    query likelihood model for information retrieval. In *Advances in Information
    Retrieval: 43rd European Conference on IR Research, ECIR 2021, Virtual Event,
    March 28–April 1, 2021, Proceedings, Part II 43*, pages 463–470\. Springer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhuang et al. (2024) Shengyao Zhuang, Xueguang Ma, Bevan Koopman, Jimmy Lin,
    and Guido Zuccon. 2024. Promptreps: Prompting large language models to generate
    dense and sparse representations for zero-shot document retrieval. *arXiv preprint
    arXiv:2404.18424*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhuang et al. (2023b) Shengyao Zhuang, Honglei Zhuang, Bevan Koopman, and Guido
    Zuccon. 2023b. A setwise approach for effective and highly efficient zero-shot
    ranking with large language models. *arXiv preprint arXiv:2310.09497*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhuang and Zuccon (2021) Shengyao Zhuang and Guido Zuccon. 2021. Tilde: Term
    independent likelihood model for passage re-ranking. In *Proceedings of the 44th
    International ACM SIGIR Conference on Research and Development in Information
    Retrieval*, pages 1483–1492.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zuccon et al. (2023) Guido Zuccon, Harrisen Scells, and Shengyao Zhuang. 2023.
    Beyond co2 emissions: The overlooked impact of water consumption of information
    retrieval models. In *Proceedings of the 2023 ACM SIGIR International Conference
    on Theory of Information Retrieval*, pages 283–289.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Original Prompts of LLM rankers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table 4: The content of each original prompt is structured with specific placeholders
    enclosed in braces “{}”. For instance, “{query}” represents the query text.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Rankers | Original Prompt 1 | Original Prompt 2 | Original Prompt 3 | Original
    Prompt 4 |'
  prefs: []
  type: TYPE_TB
- en: '| Pointwise |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Query: {query} &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Passage: {text} &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Does the passage answer the query? &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Answer ’Yes’ or ’No’ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Passage: {text} &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Query: {query} &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Is this passage relevant to the query? &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Please answer True/False. Answer: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; For the following query and document, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; judge whether they are ’Highly Relevant’, ’Somewhat Relevant’, or ’Not
    Relevant’. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Query: {query} &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Document:{text} &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Output: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; From a scale of 0 to 4, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; judge the relevance between the query and the document. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Query: {query} &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Document:{text} &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Output: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Pairwise |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Given a query: {query}, which of the following two passages is more
    relevant to the query? &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Passage A: {doc1} &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Passage B: {doc2} &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Output Passage A or Passage B: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Listwise |'
  prefs: []
  type: TYPE_TB
- en: '&#124; {Several Passages} &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Query = {query} &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Passages = [Passage 1, Passage2, Passage3, Passage4] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Sort the Passages by their relevance to the Query. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Sorted Passages = [ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; You are RankGPT, an intelligent assistant that can rank passages based
    on their relevancy to the query. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; I will provide you with {num} passages, each indicated by number identifier
    []. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Rank the passages based on their relevance to query: {query} &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; {Several Passages} &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Search Query: {query}. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Rank the {num} passages above based on their relevance to the search
    query. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; The passages should be listed in descending order using identifiers.
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; The most relevant passages should be listed first. The output format
    should be [] <math id="A1.T4.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="></math>
    [2]. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Only response the ranking results, do not say any word or explain. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Setwise |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Given a query "{query}", which of the following passages is the most
    relevant one to the query? &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Passages [Passage A, Passage B, Passage C] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Output only the passage label of the most relevant passage: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: The original prompts associated to each LLM ranking method are reported in Table [4](#A1.T4
    "Table 4 ‣ Appendix A Original Prompts of LLM rankers ‣ An Investigation of Prompt
    Variations for Zero-shot LLM-based Rankers"); these prompts were collected from
    the works of Zhuang et al. ([2023a](#bib.bib33)); Qin et al. ([2023](#bib.bib18));
    Ma et al. ([2023](#bib.bib14)); Sun et al. ([2023](#bib.bib22)); Tang et al. ([2023](#bib.bib23));
    Zhuang et al. ([2023b](#bib.bib36)),.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the original prompts for listwise the number of passages included in a single
    prompt is a parameter of the method. All passages included are denoted by “{Several
    Passages}”, and “{num}” indicates the actual number of passages. Furthermore,
    in the first original listwise prompt, there is an instruction specifying the
    format as follows: “Passages = [Passage 1, Passage 2, Passage 3, Passage 4]”.
    It is important to note that these entries are placeholders, not actual passages,
    and their count is determined by “{num}”.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Best Prompt Variations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Table [5](#A2.T5 "Table 5 ‣ Appendix B Best Prompt Variations ‣ An Investigation
    of Prompt Variations for Zero-shot LLM-based Rankers") reports the template combinations
    that lead to the best performing prompts across combinations of ranking method,
    LLM backbones, and dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Prompt templates containing the components that lead to the most effective
    results across combinations of ranking method, LLM backbones, and dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '(a) Dataset: DL19'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Pointwise | Pairwise | Listwise | Setwise |'
  prefs: []
  type: TYPE_TB
- en: '| FlanT5-large | TI_3, OT_1, TW_0, PF, B, RP_1 | TI_1, OT_1, TW_4, QF, B, RP_None
    | TI_3, OT_2, TW_2, PF, E, RP_None | TI_1, OT_3, TW_0, QF, B, RP_None |'
  prefs: []
  type: TYPE_TB
- en: '| FlanT5-xl | TI_2, OT_3, TW_4, PF, E, RP_1 | TI_1, OT_1, TW_2, QF, B, RP_1
    | TI_3, OT_2, TW_3, QF, B, RP_None | TI_1, OT_3, TW_3, QF, E, RP_1 |'
  prefs: []
  type: TYPE_TB
- en: '| FlanT5-xxl | TI_4, OT_3, TW_1, PF, B, RP_None | TI_1, OT_1, TW_0, QF, E,
    RP_1 | TI_1, OT_2, TW_2, QF, B, RP_None | TI_1, OT_1, TW_4, QF, B, RP_1 |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-7B | TI_1, OT_4, TW_3, PF, E, RP_1 | TI_1, OT_1, TW_0, QF, E, RP_1
    | TI_1, OT_1, TW_2, QF, B, RP_None | TI_1, OT_3, TW_0, QF, E, RP_1 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama3-8B | TI_2, OT_3, TW_2, PF, B, RP_1 | TI_1, OT_1, TW_0, QF, B, RP_None
    | TI_1, OT_2, TW_0, QF, B, RP_None | TI_1, OT_1, TW_3, PF, E, RP_None |'
  prefs: []
  type: TYPE_TB
- en: '(b) Dataset: DL20'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Pointwise | Pairwise | Listwise | Setwise |'
  prefs: []
  type: TYPE_TB
- en: '| FlanT5-large | TI_3, OT_3, TW_3, PF, B, RP_1 | TI_1, OT_1, TW_2, PF, E, RP_None
    | TI_3, OT_2, TW_1, PF, E, RP_None | TI_1, OT_1, TW_2, PF, E, RP_1 |'
  prefs: []
  type: TYPE_TB
- en: '| FlanT5-xl | TI_3, OT_3, TW_4, QF, B, RP_1 | TI_1, OT_1, TW_5, QF, B, RP_1
    | TI_2, OT_2, TW_3, PF, E, RP_1 | TI_1, OT_2, TW_5, PF, B, RP_None |'
  prefs: []
  type: TYPE_TB
- en: '| FlanT5-xxl | TI_3, OT_1, TW_3, PF, B, RP_None | TI_1, OT_1, TW_2, PF, E,
    RP_1 | TI_2, OT_2, TW_3, PF, B, RP_None | TI_1, OT_3, TW_3, PF, B, RP_1 |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-7B | TI_1, OT_4, TW_2, PF, E, RP_1 | TI_1, OT_1, TW_1, QF, E, RP_None
    | TI_3, OT_1, TW_0, QF, B, RP_1 | TI_1, OT_3, TW_2, QF, E, RP_1 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama3-8B | TI_1, OT_3, TW_1, PF, B, RP_1 | TI_1, OT_1, TW_3, PF, E, RP_1
    | TI_3, OT_2, TW_0, QF, B, RP_None | TI_1, OT_2, TW_4, PF, E, RP_None |'
  prefs: []
  type: TYPE_TB
- en: '(c) Dataset: COVID'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Pointwise | Pairwise | Listwise | Setwise |'
  prefs: []
  type: TYPE_TB
- en: '| FlanT5-large | TI_3, OT_1, TW_1, PF, B, RP_1 | TI_1, OT_1, TW_5, PF, B, RP_1
    | TI_3, OT_2, TW_4, PF, E, RP_None | TI_1, OT_1, TW_1, PF, B, RP_None |'
  prefs: []
  type: TYPE_TB
- en: '| FlanT5-xl | TI_3, OT_1, TW_1, PF, B, RP_1 | TI_1, OT_1, TW_5, PF, B, RP_1
    | TI_2, OT_2, TW_1, QF, E, RP_None | TI_1, OT_3, TW_5, QF, E, RP_None |'
  prefs: []
  type: TYPE_TB
- en: '| FlanT5-xxl | TI_3, OT_1, TW_1, PF, B, RP_None | TI_1, OT_1, TW_0, PF, B,
    RP_1 | TI_1, OT_2, TW_3, PF, B, RP_None | TI_1, OT_3, TW_5, PF, B, RP_1 |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-7B | TI_2, OT_2, TW_4, PF, B, RP_1 | TI_1, OT_1, TW_3, QF, B, RP_1
    | TI_1, OT_1, TW_4, QF, B, RP_None | TI_1, OT_1, TW_2, QF, B, RP_None |'
  prefs: []
  type: TYPE_TB
- en: '| Llama3-8B | TI_2, OT_3, TW_5, QF, E, RP_1 | TI_1, OT_1, TW_4, QF, B, RP_None
    | TI_1, OT_2, TW_0, QF, B, RP_None | TI_1, OT_3, TW_1, QF, B, RP_1 |'
  prefs: []
  type: TYPE_TB
- en: Appendix C Effectiveness of Original and Best Prompts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Table [6](#A3.T6 "Table 6 ‣ Appendix C Effectiveness of Original and Best Prompts
    ‣ An Investigation of Prompt Variations for Zero-shot LLM-based Rankers") reports
    the nDCG@10 obtained by the original prompts for each ranking method and the best
    prompt variation for each ranking family.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Comparison of nDCG@10 across different rankers, LLMs and datasets.
    Po: pointwise, Pa: pairwise, Li: listwise, Se: setwise. Original: the original
    (adapted) prompt. Best: the best prompt found in our experiments. For each dataset
    and backbone, we highlighted in italics the best performing method that used the
    original prompt, and in bold the best performing method overall.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | Mistral-7B | Llama 3-8B |'
  prefs: []
  type: TYPE_TB
- en: '| DL19 | DL20 | COVID | DL19 | DL20 | COVID |'
  prefs: []
  type: TYPE_TB
- en: '| Ranker | Original | Best | Original | Best | Original | Best | Original |
    Best | Original | Best | Original | Best |'
  prefs: []
  type: TYPE_TB
- en: '| Po | Qin et al. ([2023](#bib.bib18)) | 0.5851 | 0.6699 | 0.5237 | 0.6486
    | 0.6897 | 0.7988 | 0.4176 | 0.5328 | 0.3383 | 0.4950 | 0.6678 | 0.7916 |'
  prefs: []
  type: TYPE_TB
- en: '| Po | Ma et al. ([2023](#bib.bib14)) | 0.6254 | 0.5948 | 0.7379 | 0.4808 |
    0.3794 | 0.7527 |'
  prefs: []
  type: TYPE_TB
- en: '| Po | Zhuang et al. ([2023a](#bib.bib33)).1 | 0.6414 | 0.5951 | 0.7758 | 0.3764
    | 0.3088 | 0.7485 |'
  prefs: []
  type: TYPE_TB
- en: '| Po | Zhuang et al. ([2023a](#bib.bib33)).2 | 0.5577 | 0.4896 | 0.6967 | 0.3890
    | 0.3466 | 0.7242 |'
  prefs: []
  type: TYPE_TB
- en: '| Pa | Qin et al. ([2023](#bib.bib18)) | 0.6263 | 0.6450 | 0.6036 | 0.6059
    | 0.7724 | 0.7738 | 0.6738 | 0.6738 | 0.6163 | 0.6340 | 0.7966 | 0.8012 |'
  prefs: []
  type: TYPE_TB
- en: '| Li | Ma et al. ([2023](#bib.bib14)) | 0.5863 | 0.6686 | 0.5492 | 0.6348 |
    0.7101 | 0.7758 | 0.6364 | 0.7004 | 0.6088 | 0.6619 | 0.7393 | 0.8123 |'
  prefs: []
  type: TYPE_TB
- en: '| Li | Sun et al. ([2023](#bib.bib22)) | 0.5402 | 0.5184 | 0.6204 | 0.5926
    | 0.5377 | 0.6589 |'
  prefs: []
  type: TYPE_TB
- en: '| Se | Zhuang et al. ([2023b](#bib.bib36)) | 0.6567 | 0.6811 | 0.6180 | 0.6256
    | 0.7846 | 0.8053 | 0.6733 | 0.7000 | 0.5990 | 0.6486 | 0.7897 | 0.8035 |  |  |  |
    FlanT5-Large | FlanT5-XL | FlanT5-XXL |'
  prefs: []
  type: TYPE_TB
- en: '| DL19 | DL20 | COVID | DL19 | DL20 | COVID | DL19 | DL20 | COVID |'
  prefs: []
  type: TYPE_TB
- en: '| Ranker | Original | Best | Original | Best | Original | Best | Original |
    Best | Original | Best | Original | Best | Original | Best | Original | Best |
    Original | Best |'
  prefs: []
  type: TYPE_TB
- en: '| Po | Qin et al. ([2023](#bib.bib18)) | 0.6180 | 0.6918 | 0.5984 | 0.6266
    | 0.6490 | 0.7570 | 0.6157 | 0.7010 | 0.6439 | 0.6727 | 0.6786 | 0.7742 | 0.6289
    | 0.6860 | 0.6514 | 0.6733 | 0.6642 | 0.7784 |'
  prefs: []
  type: TYPE_TB
- en: '| Po | Ma et al. ([2023](#bib.bib14)) | 0.6483 | 0.6128 | 0.6971 | 0.6690 |
    0.6426 | 0.7301 | 0.6563 | 0.6533 | 0.7321 |'
  prefs: []
  type: TYPE_TB
- en: '| Po | Zhuang et al. ([2023a](#bib.bib33)).1 | 0.6468 | 0.5698 | 0.6672 | 0.6300
    | 0.6444 | 0.7023 | 0.6451 | 0.6274 | 0.6983 |'
  prefs: []
  type: TYPE_TB
- en: '| Po | Zhuang et al. ([2023a](#bib.bib33)).2 | 0.5164 | 0.4387 | 0.6206 | 0.6054
    | 0.5971 | 0.7095 | 0.4068 | 0.3301 | 0.4335 |'
  prefs: []
  type: TYPE_TB
- en: '| Pa | Qin et al. ([2023](#bib.bib18)) | 0.6677 | 0.6724 | 0.6237 | 0.6382
    | 0.7558 | 0.7788 | 0.6845 | 0.6986 | 0.6766 | 0.6823 | 0.7536 | 0.7750 | 0.6915
    | 0.7135 | 0.6992 | 0.7126 | 0.7452 | 0.7917 |'
  prefs: []
  type: TYPE_TB
- en: '| Li | Ma et al. ([2023](#bib.bib14)) | 0.5465 | 0.6443 | 0.5081 | 0.6125 |
    0.6067 | 0.7521 | 0.5688 | 0.6441 | 0.5576 | 0.6300 | 0.6312 | 0.7438 | 0.5915
    | 0.7011 | 0.5852 | 0.6897 | 0.6955 | 0.7834 |'
  prefs: []
  type: TYPE_TB
- en: '| Li | Sun et al. ([2023](#bib.bib22)) | 0.6199 | 0.5552 | 0.7445 | 0.6129
    | 0.5966 | 0.6938 | 0.6920 | 0.6672 | 0.7736 |'
  prefs: []
  type: TYPE_TB
- en: '| Se | Zhuang et al. ([2023b](#bib.bib36)) | 0.6503 | 0.6693 | 0.5754 | 0.6525
    | 0.7440 | 0.7932 | 0.6812 | 0.6959 | 0.6747 | 0.6855 | 0.7540 | 0.7745 | 0.6925
    | 0.7047 | 0.6776 | 0.7036 | 0.7617 | 0.7890 |'
  prefs: []
  type: TYPE_TB
