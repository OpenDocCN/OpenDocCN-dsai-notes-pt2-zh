- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:46:40'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.11683](https://ar5iv.labs.arxiv.org/html/2402.11683)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: First Author
  prefs: []
  type: TYPE_NORMAL
- en: Affiliation / Address line 1
  prefs: []
  type: TYPE_NORMAL
- en: Affiliation / Address line 2
  prefs: []
  type: TYPE_NORMAL
- en: Affiliation / Address line 3
  prefs: []
  type: TYPE_NORMAL
- en: email@domain
  prefs: []
  type: TYPE_NORMAL
- en: '&Second Author'
  prefs: []
  type: TYPE_NORMAL
- en: Affiliation / Address line 1
  prefs: []
  type: TYPE_NORMAL
- en: Affiliation / Address line 2
  prefs: []
  type: TYPE_NORMAL
- en: Affiliation / Address line 3
  prefs: []
  type: TYPE_NORMAL
- en: email@domain    Tejpalsingh Siledar^∗^♣, Swaroop Nath^∗^♣, Sri Raghava^∗^♣,
    Rupasai Rangaraju^∗^♣,
  prefs: []
  type: TYPE_NORMAL
- en: Swaprava Nath^♣, Pushpak Bhattacharyya^♣,
  prefs: []
  type: TYPE_NORMAL
- en: Suman Banerjee^♠, Amey Patil^♠, Sudhanshu Shekhar Singh^♠,
  prefs: []
  type: TYPE_NORMAL
- en: Muthusamy Chelliah^♠, Nikesh Garera^♠
  prefs: []
  type: TYPE_NORMAL
- en: ^♣Computer Science and Engineering, IIT Bombay, India,
  prefs: []
  type: TYPE_NORMAL
- en: ^♠Flipkart, India
  prefs: []
  type: TYPE_NORMAL
- en: '{tejpalsingh, swaroopnath, sriraghava, rupasai, swaprava, pb}@cse.iitb.ac.in'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Evaluation of opinion summaries using conventional reference-based metrics rarely
    provides a holistic evaluation and has been shown to have a relatively low correlation
    with human judgments. Recent studies suggest using Large Language Models (LLMs)
    as reference-free metrics for NLG evaluation, however, they remain unexplored
    for opinion summary evaluation. Moreover, limited opinion summary evaluation datasets
    inhibit progress. To address this, we release the SummEval-Op dataset covering
    $7$ with humans, outperforming all previous approaches. To the best of our knowledge,
    we are the first to investigate LLMs as evaluators on both closed-source and open-source
    models in the opinion summarization domain.
  prefs: []
  type: TYPE_NORMAL
- en: 'One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation'
  prefs: []
  type: TYPE_NORMAL
- en: Tejpalsingh Siledar^∗^♣, Swaroop Nath^∗^♣, Sri Raghava^∗^♣, Rupasai Rangaraju^∗^♣,
    Swaprava Nath^♣, Pushpak Bhattacharyya^♣, Suman Banerjee^♠, Amey Patil^♠, Sudhanshu
    Shekhar Singh^♠, Muthusamy Chelliah^♠, Nikesh Garera^♠ ^♣Computer Science and
    Engineering, IIT Bombay, India, ^♠Flipkart, India {tejpalsingh, swaroopnath, sriraghava,
    rupasai, swaprava, pb}@cse.iitb.ac.in
  prefs: []
  type: TYPE_NORMAL
- en: '^*^*footnotetext: Equal contribution'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/946a269c9efd0257e08b37c71c3148db.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: G-Eval vs. Op-I-Prompt. On closed-source model (ChatGPT-$3.5$ dimensions:
    fluency (FA), coherence (CO), relevance (RE), faithfulness (FA), aspect coverage (AC),
    sentiment consistency (SC), and specificity (SP). Check Figure [4](#S5.F4 "Figure
    4 ‣ 5.1 Summarization Models ‣ 5 Experiments ‣ One Prompt To Rule Them All: LLMs
    for Opinion Summary Evaluation") for more details.'
  prefs: []
  type: TYPE_NORMAL
- en: Opinion summarization systems predominantly use traditional metrics such as
    Rouge (Lin, [2004](#bib.bib20)) and BertScore (Zhang et al., [2019](#bib.bib35))
    for automatic evaluation, however, they have been shown to have poor correlations
    with human judgments (Shen and Wan, [2023](#bib.bib26)). Moreover, these metrics
    fall short of comprehensively evaluating opinion summaries. Additionally, obtaining
    reference-based datasets at a large scale is an expensive process.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, Large Language Models (LLMs) have been utilized as reference-free
    evaluators for Natural Language Generation (NLG) outputs (Fu et al., [2023](#bib.bib10);
    Chiang and Lee, [2023a](#bib.bib5), [b](#bib.bib6); Wang et al., [2023](#bib.bib30);
    Liu et al., [2023](#bib.bib21)). The idea is to prompt a powerful LLM such as
    ChatGPT-$3.5$) primarily because of the limitations of the open-source models
    in following instructions and producing the desired output (Chiang and Lee, [2023b](#bib.bib6)).
  prefs: []
  type: TYPE_NORMAL
- en: To this end, we first create SummEval-OP, a reference-free opinion summarization
    dataset covering $7$ dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our contributions are:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'SummEval-Op^*^**[https://github.com/tjsiledar/SummEval-OP](https://github.com/tjsiledar/SummEval-OP),
    an opinion summarization benchmark dataset, consisting of a total of $2,912$ dimensions-
    fluency, coherence, relevance, faithfulness, aspect coverage, sentiment consistency,
    and specificity related to the evaluation of opinion summaries (Section [4](#S4
    "4 SummEval-Op Benchmark Dataset ‣ One Prompt To Rule Them All: LLMs for Opinion
    Summary Evaluation")).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Op-I-Prompt, a dimension-independent prompt and Op-Prompts, a dimension-dependent
    set of prompts, enabling opinion summary evaluation for all the $7$ on average
    in correlation with human judgments (Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation"), Section
    [3](#S3 "3 Methodology ‣ One Prompt To Rule Them All: LLMs for Opinion Summary
    Evaluation")). To the best of our knowledge we are the first to test the applicability
    of different prompt approaches on open-source LLMs.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Benchmarking of recent LLMs (closed and open-source) on the aforementioned
    $7$ dimensions for the task of opinion summarization, which to the best of our
    knowledge is first of its kind (Section [6](#S6 "6 Results and Analysis ‣ One
    Prompt To Rule Them All: LLMs for Opinion Summary Evaluation")).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Detailed analysis, comparing an open-source LLM against a closed-source LLM
    acting as evaluators for automatic evaluation of opinion summaries on $7$ dimensions.
    Analysis indicates that Op-I-Prompt emerges as a good alternative for evaluating
    opinion summaries showing a high correlation with humans when compared with alternatives
    (Section [6](#S6 "6 Results and Analysis ‣ One Prompt To Rule Them All: LLMs for
    Opinion Summary Evaluation")).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLM-based Evaluators  Fu et al. ([2023](#bib.bib10)) introduced GPTScore that
    operates on the premise that a generative pre-training model (e.g. GPT-3) is likely
    to assign a higher probability to the generation of high-quality text in line
    with provided instructions and context. Chiang and Lee ([2023a](#bib.bib5)) were
    the first to explore LLMs for evaluation. Chiang and Lee ([2023b](#bib.bib6))
    provide concrete guidelines that improve ChatGPT’s correlation with humans. Wang
    et al. ([2023](#bib.bib30)) conducted an initial survey exploring the utilization
    of ChatGPT as an NLG evaluator. Kocmi and Federmann ([2023](#bib.bib16)) used
    GPT models for evaluating machine learning tasks. Liu et al. ([2023](#bib.bib21))
    introduced G-Eval, a framework for evaluation of NLG outputs using Chain of Thought
    (CoT) (Wei et al., [2023](#bib.bib32)) and assigning weights to a predetermined
    set of integer scores based on their generation probabilities from GPT-3/4\. Chen
    et al. ([2023](#bib.bib4)) were the first to investigate approaches to reference-free
    NLG evaluation using LLMs, finding that an explicit score generated by ChatGPT
    is the most effective and stable approach. Zheng et al. ([2023](#bib.bib36)) show
    that strong LLMs such as GPT-4 achieve a similar level of agreement to that of
    humans and hence can be used to approximate human preferences. Our work investigates
    two prompt strategies and tests the applicability of different prompt approaches
    on closed-source and open-source LLMs for opinion summary evaluation for $7$ dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Opinion Summary Evaluation Benchmark Shen and Wan ([2023](#bib.bib26)) created
    the OpinSummEval dataset, utilizing the Yelp test set (Chu and Liu, [2019](#bib.bib8)),
    annotating for $4$ dimensions on the recent LLM summaries, subsequently establishing
    benchmarks for comparison.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/61a6ba7b173e4df8d0dfd719659d13d9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Comparison of Prompt Approaches. G-Eval Prompts first generates the
    Evaluation Steps using Task Description and Evaluation Criteria in Chain-of-Thought
    fashion. Finally the full prompt is used to evaluate the opinion summaries. In
    contrast, our Op-I-Prompt is simpler and has Task Description, Evaluation Criteria,
    and Evaluation Steps fixed for a dimension/metric independent evaluation. Here,
    only the Metric part needs to be changed for evaluating any dimension/metric.
    Finally Op-Prompts are dimension/metric dependent prompts that needs to be specifically
    crafted for each dimension/metric.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We describe our dimension independent and dependent prompts and the model scoring
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Prompt Approaches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Figure [2](#S2.F2 "Figure 2 ‣ 2 Related Work ‣ One Prompt To Rule Them All:
    LLMs for Opinion Summary Evaluation") shows the different prompt approaches for
    evaluating opinion summaries. In general, the prompts include the following $3$
    components-'
  prefs: []
  type: TYPE_NORMAL
- en: 'Task Description: Defines the task that the LLM will be performing. In our
    case, the task is to evaluate a summary corresponding to a set of reviews on a
    given metric/dimension.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation Criteria: Defines the criteria that will be used to perform the
    task. In our case, the task being opinion summary evaluation, the criteria is
    to assign a score ($1-5$) for a certain metric/dimension depending on the extent
    to which the summary adheres to it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation Steps: This comprises the steps that the LLM must take to correctly
    perform the described task. In our case, it contains the steps that the LLM should
    follow to evaluate a certain metric/dimension.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We propose two prompt approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Op-I-Prompt  is a metric-independent opinion summary evaluation prompt. Here
    we split the Evaluation Criteria to create a new component Metric consisting only
    the evaluation dimension. All the remaining components i.e. Task Description,
    Evaluation Criteria, and Evaluation Steps are crafted in such a way that they
    are applicable in general to any opinion summary evaluation dimension. This benefits
    us in the following way: (a) we have a metric independent prompt that can now
    evaluate any metric/dimension just by replacing with the desired definition of
    the dimension within the Metric block (b) the remaining components, crafted specifically
    keeping the task in mind, ensures that the evaluation by LLM takes place as defined
    by us.'
  prefs: []
  type: TYPE_NORMAL
- en: Op-Prompts is a set of metric-dependent prompts. We specifically handcrafted
    these prompts for each of the $7$ evaluation dimensions. Although this ensures
    that the evaluation happens exactly in the way we define, this requires a certain
    level of expertise in the evaluation domain and prompting. This could be seen
    as a much stricter version of the prompt compared to Op-I-Prompt where the prompt
    is suited to any evaluation dimension which is not the case here. A prompt defined
    for a certain dimension could not be utilized for any other dimension.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, G-Eval (Liu et al., [2023](#bib.bib21)) used auto chain-of-thoughts
    Wei et al. ([2022](#bib.bib31)) by using Task Description and Evaluation Criteria
    to automatically generate the Evaluation Steps. Finally, all the components together
    constitute the G-Eval prompt that is used by an LLM to evaluate summaries. Our
    work investigates the applicability of all these prompts to both closed-source
    and open-source models for evaluating opinion summaries.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Scoring Function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Liu et al. ([2023](#bib.bib21)) pointed out the limitation of LLM outputting
    an integer score and proposed using a weighted average of the scores as the LLMs
    output, where the weights are the probabilities of the corresponding score. Formally,
    say, the scoring is scheme is from $\{s_{1},...,s_{j}\}$ is computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle o=\sum_{k=1}^{j}p(s_{k})\times s_{k}$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: $p(s_{k})$) to get a reliable estimate of the probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 4 SummEval-Op Benchmark Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We created the SummEval-Op benchmark dataset for evaluating the opinion summaries
    on $7$ dimensions. In this section, we discuss the dataset used, opinion summary
    evaluation metrics, annotation details, and its analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We utilized the Amazon test set (He and McAuley, [2016](#bib.bib12); Bražinskas
    et al., [2020](#bib.bib3)), comprising of reviews from $4$ for each product. We
    do not directly consider only one of the human summaries as this would bias the
    summaries to a single person.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Opinion Summarization Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The evaluation of opinion summaries focused on the following $7$ dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: fluency (FL)- The quality of summary in terms of grammar, spelling, punctuation,
    capitalization, word choice, and sentence structure and should contain no errors.
    The summary should be easy to read, follow, comprehend and should contain no errors.
    Annotators received specific guidelines on how to penalize summaries based on
    fluency levels.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: coherence (CO)- The collective quality of all sentences. The summary should
    be well-structured and well-organized. The summary should not just be a heap of
    related information, but should build from sentence to a coherent body of information.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: relevance (RE)- The summary should not contain opinions that are either not
    consensus or important. The summary should include only important opinions from
    the reviews. Annotators were instructed to penalize summaries if they contained
    redundancies and excess/unimportant information.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: faithfulness (FA)- Every piece of information mentioned in the summary should
    be verifiable/supported/inferred from the reviews only. Summaries should be penalized
    if any piece of information is not verifiable/supported/inferred from the reviews
    or if the summary overgeneralizes something.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: aspect coverage (AC)- The summary should cover all the aspects that are majorly
    being discussed in the reviews. Summaries should be penalized if they miss out
    on an aspect that was majorly being discussed in the reviews and awarded if it
    covers all.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '6.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: sentiment consistency (SC)- All the aspects being discussed in the summary should
    accurately reflect the consensus sentiment of the corresponding aspects from the
    reviews. Summaries should be penalized if they do not cover accurately the sentiment
    regarding any aspect within the summary.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '7.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: specificity (SP)- The summary should avoid containing generic opinions. All
    the opinions within the summary should contain detailed and specific information
    about the consensus opinions. Summaries should be penalized for missing out details
    and should be awarded if they are specific.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.3 Annotation Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For creating the SummEval-Op dataset, annotations were collected for a total
    of $13$ (# of summaries) x $7$ ratings.
  prefs: []
  type: TYPE_NORMAL
- en: 'We chose to hire $3$. We asked the raters to be critical and discuss the ratings
    during re-evaluation. Check Appendix [B](#A2 "Appendix B Rater Agreement ‣ One
    Prompt To Rule Them All: LLMs for Opinion Summary Evaluation")'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Annotation Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '|  | Round-I $\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| fluency | 0.55 | 0.84 |'
  prefs: []
  type: TYPE_TB
- en: '| coherence | 0.43 | 0.73 |'
  prefs: []
  type: TYPE_TB
- en: '| relevance | 0.50 | 0.79 |'
  prefs: []
  type: TYPE_TB
- en: '| faithfulness | 0.63 | 0.86 |'
  prefs: []
  type: TYPE_TB
- en: '| aspect coverage | 0.64 | 0.82 |'
  prefs: []
  type: TYPE_TB
- en: '| sentiment consistency | 0.41 | 0.78 |'
  prefs: []
  type: TYPE_TB
- en: '| specificity | 0.34 | 0.76 |'
  prefs: []
  type: TYPE_TB
- en: '| AVG | 0.50 | 0.80 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Krippendorff’s alpha coefficient (${\alpha}$ dimensions. As expected,
    we see an improvement in Round-II coefficient scores.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/afe574687dad8b6ba7a097dec55dbc95.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Ratings Distribution. We plot the average frequency of scores obtained
    by human raters across $7$ is mostly preferred.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We evaluated the inter-rater agreement for the $3$). We report the dimension-wise
    agreement scores for both rounds in Table [1](#S4.T1 "Table 1 ‣ 4.4 Annotation
    Analysis ‣ 4 SummEval-Op Benchmark Dataset ‣ One Prompt To Rule Them All: LLMs
    for Opinion Summary Evaluation"). We observe that for both Round-I and Round-II,
    faithfulness and aspect coverage score higher than others. This is mostly because
    faithfulness and aspect coverage could be identified by cross-examining with the
    reviews. After Round-II, coherence and specificity are the most disagreed upon
    between raters. This could be attributed to their subjective nature (Kryściński
    et al., [2018](#bib.bib18)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [3](#S4.F3 "Figure 3 ‣ 4.4 Annotation Analysis ‣ 4 SummEval-Op Benchmark
    Dataset ‣ One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation") shows
    the average frequency of assigning a particular score by human raters for $7$.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We discuss the available benchmark dataset for opinion summary evaluation, the
    summarization models used for opinion summary generation, baseline metrics, and
    the implementation details.
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | FL $\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\rho$ |'
  prefs: []
  type: TYPE_TB
- en: '| SummEval-Op (Ours) | Humans | 0.80 | 0.77 | 0.81 | 0.76 | 0.91 | 0.86 | 0.89
    | 0.85 | 0.93 | 0.87 | 0.91 | 0.85 | 0.92 | 0.87 |'
  prefs: []
  type: TYPE_TB
- en: '| Rouge-1 | -0.36 | -0.28 | -0.30 | -0.24 | -0.31 | -0.23 | -0.35 | -0.26 |
    -0.44 | -0.32 | -0.38 | -0.29 | -0.30 | -0.23 |'
  prefs: []
  type: TYPE_TB
- en: '| Rouge-2 | -0.23 | -0.18 | -0.14 | -0.10 | -0.17 | -0.12 | -0.21 | -0.16 |
    -0.26 | -0.19 | -0.24 | -0.18 | -0.14 | -0.09 |'
  prefs: []
  type: TYPE_TB
- en: '| Rouge-L | -0.39 | -0.32 | -0.30 | -0.23 | -0.34 | -0.25 | -0.40 | -0.30 |
    -0.51 | -0.37 | -0.45 | -0.33 | -0.38 | -0.27 |'
  prefs: []
  type: TYPE_TB
- en: '| BERTScore | -0.32 | -0.27 | -0.28 | -0.22 | -0.29 | -0.22 | -0.34 | -0.26
    | -0.51 | -0.43 | -0.41 | -0.33 | -0.37 | -0.28 |'
  prefs: []
  type: TYPE_TB
- en: '| BARTScore | -0.19 | -0.15 | -0.19 | -0.14 | -0.29 | -0.22 | -0.33 | -0.25
    | -0.45 | -0.35 | -0.37 | -0.28 | -0.36 | -0.27 |'
  prefs: []
  type: TYPE_TB
- en: '| SummaC | 0.23 | 0.20 | 0.18 | 0.14 | 0.30 | 0.25 | 0.25 | 0.21 | 0.24 | 0.19
    | 0.25 | 0.20 | 0.26 | 0.21 |'
  prefs: []
  type: TYPE_TB
- en: '| UniEval | 0.36 | 0.28 | 0.52 | 0.42 | 0.33 | 0.25 | 0.17 | 0.14 | - | - |
    - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| G-Eval-3.5 | 0.63 | 0.55 | 0.59 | 0.49 | 0.68 | 0.56 | 0.70 | 0.58 | 0.79
    | 0.67 | 0.73 | 0.61 | 0.75 | 0.63 |'
  prefs: []
  type: TYPE_TB
- en: '| OP-I-GPT-3.5 | 0.60 | 0.51 | 0.61 | 0.51 | 0.69 | 0.56 | 0.71 | 0.59 | 0.80
    | 0.68 | 0.73 | 0.61 | 0.74 | 0.61 |'
  prefs: []
  type: TYPE_TB
- en: '| G-Eval-Mistral | 0.50 | 0.43 | 0.54 | 0.45 | 0.52 | 0.42 | 0.54 | 0.44 |
    0.61 | 0.49 | 0.55 | 0.46 | 0.62 | 0.50 |'
  prefs: []
  type: TYPE_TB
- en: '| OP-Mistral | 0.38 | 0.32 | 0.58 | 0.47 | 0.56 | 0.45 | 0.57 | 0.46 | 0.80
    | 0.67 | 0.60 | 0.49 | 0.75 | 0.62 |'
  prefs: []
  type: TYPE_TB
- en: '| OP-I-Mistral | 0.54 | 0.45 | 0.58 | 0.47 | 0.59 | 0.47 | 0.63^∗ | 0.51^∗
    | 0.82^∗ | 0.70^∗ | 0.73^∗ | 0.61^∗ | 0.71^∗ | 0.58^∗ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Spearman ($\rho$) to G-Eval-Mistral computed using Mann-Whitney U
    Test. Humans- averaged correlation of each annotator with the overall averaged
    ratings.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Summarization Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Pre-LLMs: For the Pre-LLMs, we obtain the publicly available summaries for
    the Amazon test set of these models. These models were trained in a self-supervised
    manner using only reviews data. (1) PlanSum (Amplayo and Lapata, [2020](#bib.bib1))
    uses content plans to create relevant review-summary pairs. The content plans
    take the form of aspect and sentiment distributions which are used along with
    input reviews for generating summaries. (2) MultimodalSum (Im et al., [2021](#bib.bib13))
    uses non-text data such as image and metadata along with reviews to generate opinion
    summaries. It uses a separate encoder for each modality and uses synthetic datasets
    to train the model in an end-to-end fashion. (3) Siledar et al. ([2023](#bib.bib27))
    uses lexical and semantic similarities to create a highly relevant synthetic dataset
    of review-summary pairs. This is then used to fine-tune any pre-trained language
    model for generating opinion summaries. (hereby referred to as LS-Sum-G).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d4beaaccfe6bbcbf707abc4fc29fa016.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Spearman correlation scores at different number of output generations
    (n) for the $7$B as their LLM. Generally, Op-I-Prompt shows better relative performance
    on both closed-source and open-source models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'LLMs: For the LLMs, we use simple prompts ^*^**Check Appendix [D.4](#A4.SS4
    "D.4 Summarization Prompt ‣ Appendix D Prompts ‣ One Prompt To Rule Them All:
    LLMs for Opinion Summary Evaluation") for the prompt to generate opinion summaries.
    These models were not specifically fine-tuned for opinion summarization. We use
    the HuggingFace library (Wolf et al., [2020](#bib.bib33)) to access these models.
    (1) ChatGPT-$\mathbf{3.5}$K user-shared conversations collected from ShareGPT
    [ShareGPT](#bib.bib25) . We use the: lmsys/vicuna-7b-v1.5 model and lmsys/vicuna-13b-v1.5
    model. (5) Solar-$\mathbf{10.7}$B (Tunstall et al., [2023](#bib.bib29)) is an
    open-sourced fine-tuned version of mistralai/Mistral-7B-v0.1 that was trained
    on a mix of publicly available, synthetic datasets using Direct Preference Optimization
    (DPO) (Rafailov et al., [2023](#bib.bib24)). We use the beta version: HuggingFaceH4/zephyr-7b-beta
    model.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Baselines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Following baseline metrics are used: Rouge-{1,2,L} score (Lin, [2004](#bib.bib20)),
    BERTScore (Zhang et al., [2019](#bib.bib35)), BARTScore (Yuan et al., [2021](#bib.bib34)),
    SummaC (Laban et al., [2022](#bib.bib19)), UniEval (Zhong et al., [2022](#bib.bib37)).
    We include G-Eval (Liu et al., [2023](#bib.bib21)) as our prompt-based baseline.
    G-Eval-3.5 and G-Eval-Mistral use ChatGPT-$3.5$B as their LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Implementation Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For evaluation, we used Mistral-$7$B size ensures easy replication. We set
    the hyperparameters to `n=100, temperature=0.7` to sample multiple generations.
    Example prompts are in Appendix [D](#A4 "Appendix D Prompts ‣ One Prompt To Rule
    Them All: LLMs for Opinion Summary Evaluation").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | FL $\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| Human Summaries | 4.39 | 4.41 | 3.78 | 3.98 | 3.54 | 3.71 | 3.66 |'
  prefs: []
  type: TYPE_TB
- en: '| Pre-LLMs |'
  prefs: []
  type: TYPE_TB
- en: '| PlanSum | 1.86 | 1.94 | 1.60 | 1.38 | 1.52 | 1.59 | 1.56 |'
  prefs: []
  type: TYPE_TB
- en: '| MultimodalSum | 4.62 | 4.09 | 2.63 | 2.27 | 2.18 | 2.76 | 2.43 |'
  prefs: []
  type: TYPE_TB
- en: '| LS-Sum-G | 4.76 | 4.40 | 2.87 | 2.74 | 2.32 | 3.03 | 2.69 |'
  prefs: []
  type: TYPE_TB
- en: '| LLMs |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGPT-$3.5$ | 4.89 | 4.58 | 4.25 | 4.71 | 4.22 | 4.16 | 3.96 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-$4$ | 5.00 | 4.91 | 3.52 | 4.96 | 4.93 | 4.83 | 4.57 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA$2$B | 4.79 | 4.34 | 3.77 | 4.49 | 3.67 | 3.79 | 3.46 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA$2$B | 4.87 | 4.49 | 4.25 | 4.62 | 4.02 | 4.00 | 3.94 |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-$7$B | 4.86 | 4.60 | 4.33 | 4.66 | 4.56 | 4.35 | 4.25 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-$7$B | 4.83 | 4.23 | 3.92 | 4.35 | 3.96 | 3.92 | 3.67 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-$13$B | 4.87 | 4.41 | 4.09 | 4.43 | 4.03 | 4.00 | 3.77 |'
  prefs: []
  type: TYPE_TB
- en: '| Solar-$10.7$B | 4.89 | 4.73 | 4.20 | 4.72 | 4.50 | 4.56 | 4.35 |'
  prefs: []
  type: TYPE_TB
- en: '| Zephyr-$7$B | 4.89 | 4.36 | 4.08 | 4.54 | 4.18 | 3.95 | 3.83 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Model-wise averaged annotator ratings of opinion summaries along $7$
    dimensions for the Amazon test set. Best scores are in bold, second-best are underlined.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Results and Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'G-Eval vs. Op-I-Prompt vs. Op-Prompts. Table [2](#S5.T2 "Table 2 ‣ 5 Experiments
    ‣ One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation") and Table
    [5](#A1.T5 "Table 5 ‣ Appendix A Available Benchmark Dataset ‣ One Prompt To Rule
    Them All: LLMs for Opinion Summary Evaluation") report the summary-level ^*^**Check
    Appendix [C](#A3 "Appendix C Opinion Summary Evaluation ‣ One Prompt To Rule Them
    All: LLMs for Opinion Summary Evaluation") for definition. correlation scores
    on the SummEval-OP and OpinSummEval dataset. In the case of closed-source models,
    we observe that our Op-I-GPT-3.5 outperforms or performs comparably to G-Eval-3.5
    across all dimensions on both datasets. Specifically, our Op-I-GPT-3.5 outperforms
    G-Eval-3.5 on all $4$ dimensions for the OpinSummEval dataset, whereas for the
    SummEval-Op dataset, outperforms on coherence, faithfulness, and aspect coverage,
    performs comparably on relevance and sentiment consistency, underperforms slightly
    on fluency  and specificity.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For open-source models, overall, we observe that Op-I-Mistral performs the
    best, followed by Op-Mistral and then G-Eval-Mistral. Figure [4](#S5.F4 "Figure
    4 ‣ 5.1 Summarization Models ‣ 5 Experiments ‣ One Prompt To Rule Them All: LLMs
    for Opinion Summary Evaluation") shows the performance of different prompt approaches
    over n=100 generations for $7$ dimensions and by a large margin specifically for
    aspect coverage, sentiment consistency, and specificity.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | AVG-S | MW $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| FL | G-Eval-Mistral^∗ | 0.48 | $\mathbf{2.9\times 10^{-4}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Op-I-Mistral | 0.38 |'
  prefs: []
  type: TYPE_TB
- en: '| CO | G-Eval-Mistral^∗ | 0.52 | $\mathbf{2.1\times 10^{-4}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Op-I-Mistral | 0.47 |'
  prefs: []
  type: TYPE_TB
- en: '| RE | G-Eval-Mistral | 0.51 | $6.7\times 10^{-2}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Op-I-Mistral | 0.49 |'
  prefs: []
  type: TYPE_TB
- en: '| FA | G-Eval-Mistral | 0.53 | $\mathbf{1.9\times 10^{-1}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Op-I-Mistral | 0.54 |'
  prefs: []
  type: TYPE_TB
- en: '| AC | G-Eval-Mistral | 0.58 | $\mathbf{2.1\times 10^{-4}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Op-I-Mistral^∗ | 0.74 |'
  prefs: []
  type: TYPE_TB
- en: '| SC | G-Eval-Mistral | 0.54 | $\mathbf{2.1\times 10^{-4}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Op-I-Mistral^∗ | 0.63 |'
  prefs: []
  type: TYPE_TB
- en: '| SP | G-Eval-Mistral | 0.59 | $\mathbf{7.4\times 10^{-4}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Op-I-Mistral^∗ | 0.63 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Significance Test. P-values computed using Mann-Whitney U Test (MW)
    and T-Test (TT) between the average Spearman correlation scores (AVG-S) taken
    over $10$ represents significant performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Significance Testing. We perform significance testing using the Mann-Whitney
    U Test (McKnight and Najab, [2010](#bib.bib22)) for comparison between Op-I-Mistral
    and G-Eval-Mistral. Table [2](#S5.T2 "Table 2 ‣ 5 Experiments ‣ One Prompt To
    Rule Them All: LLMs for Opinion Summary Evaluation") report results for Spearman
    and Kendall Tau scores computed by using the scoring function with n=100. Op-I-Mistral
    significantly (p-value $<0.05$ independent groups and compute Spearman correlations
    for each group. Table [4](#S6.T4 "Table 4 ‣ 6 Results and Analysis ‣ One Prompt
    To Rule Them All: LLMs for Opinion Summary Evaluation") reports the Mann-Whitney
    U Test and T-Test p-values and arrives at a similar observation of Op-I-Mistral
    significantly outperforming on aforementioned dimensions except faithfulness.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Models for Opinion Summarization. Table [3](#S5.T3 "Table 3 ‣ 5.3 Implementation
    Details ‣ 5 Experiments ‣ One Prompt To Rule Them All: LLMs for Opinion Summary
    Evaluation") reports averaged annotator ratings for the $7$.'
  prefs: []
  type: TYPE_NORMAL
- en: Metric Evaluation. Reference-based metrics (Rouge 1,2,L, BERTScore) as expected
    show weak correlation with human judgments. Reference-free metrics such as BARTScore
    does very poorly, however, SummaC performs moderately well. UniEval does well
    in coherence but still trails behind prompt-based approaches. To summarize, reference-based
    metrics are inadequate for assessing model performances in the LLMs era.
  prefs: []
  type: TYPE_NORMAL
- en: How sensitive is Op-I-Prompt? We test Op-I-Prompt for $3$, indicating that Op-I-Prompt
    is indifferent to the variations of dimensions’ definition.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this work, we present the SummEval-OP dataset, Op-I-Prompt and Op-Prompts
    for opinion summary evaluation on $7$ dimensions. Experimentally, we observe Op-I-Prompt
    outperforms alternatives on open-source models and performs comparably better
    on closed-source models showing good correlations with human judgements. Some
    key takeaways are: (a) Prompts that do well for powerful closed-source LLMs may
    not work well for open-source LLMs; (b) Opinion summaries by LLMs are preferred
    by humans compared to reference and previous model summaries; (c) Reference-based
    summaries and metrics are inadequate in assessing LLM-based outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We do not use GPT-$4$ as the closed-source model to perform our experiments.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Our Op-I-Prompt was specifically designed to evaluate any dimension of the opinion
    summaries where Op-Prompts are dimension-dependent. However, their applicability
    to other tasks needs further investigation and appropriate changes to the prompt.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Due to the nature of the available test sets and for benchmarking the already
    available models, SummEval-OP considers only 8 input reviews following the literature.
    This we believe is a major limitation in the opinion summarization field. Datasets
    with a larger number of reviews need to be considered for the creation of future
    benchmark datasets.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The assessment quality of all the prompt approaches needs to be investigated
    for a larger amount of reviews as well.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Ethical Considerations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The SummEval-OP dataset was created using the already available Amazon test
    set. We hired $3$ raters who have written papers on opinion summarization (1)
    or are working in the opinion summarization domain (2). These were male Masters’
    students aged 21-30\. All the raters received stipends suitable for the tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The Op-I-Prompt and Op-Prompts are designed to offer automatic evaluation of
    opinion summaries for multiple dimensions. Its primary aim is to assist researchers,
    developers, and other stakeholders in accurately assessing summaries generated
    by NLG systems. However, there are potential risks associated with these prompts
    if they fail to accurately evaluate the quality of opinion summaries or exhibit
    a bias towards LLM-created content. We urge the research community to use these
    prompts with caution and check their reliability for their use cases.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Amplayo and Lapata (2020) Reinald Kim Amplayo and Mirella Lapata. 2020. [Unsupervised
    opinion summarization with noising and denoising](https://doi.org/10.18653/v1/2020.acl-main.175).
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pages 1934–1945, Online. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bhandari et al. (2020) Manik Bhandari, Pranav Narayan Gour, Atabak Ashfaq, Pengfei
    Liu, and Graham Neubig. 2020. [Re-evaluating evaluation in text summarization](https://doi.org/10.18653/v1/2020.emnlp-main.751).
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*, pages 9347–9359, Online. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bražinskas et al. (2020) Arthur Bražinskas, Mirella Lapata, and Ivan Titov.
    2020. [Unsupervised opinion summarization as copycat-review generation](https://doi.org/10.18653/v1/2020.acl-main.461).
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pages 5151–5169, Online. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2023) Yi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, and Ruifeng
    Xu. 2023. [Exploring the use of large language models for reference-free text
    quality evaluation: An empirical study](http://arxiv.org/abs/2304.00723).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chiang and Lee (2023a) Cheng-Han Chiang and Hung-yi Lee. 2023a. [Can large
    language models be an alternative to human evaluations?](https://doi.org/10.18653/v1/2023.acl-long.870)
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 15607–15631, Toronto, Canada. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chiang and Lee (2023b) Cheng-Han Chiang and Hung-yi Lee. 2023b. [A closer look
    into using large language models for automatic evaluation](https://doi.org/10.18653/v1/2023.findings-emnlp.599).
    In *Findings of the Association for Computational Linguistics: EMNLP 2023*, pages
    8928–8942, Singapore. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica, and Eric P. Xing. 2023. [Vicuna: An open-source chatbot impressing
    gpt-4 with 90%* chatgpt quality](https://lmsys.org/blog/2023-03-30-vicuna/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chu and Liu (2019) Eric Chu and Peter Liu. 2019. [MeanSum: A neural model for
    unsupervised multi-document abstractive summarization](https://proceedings.mlr.press/v97/chu19b.html).
    In *Proceedings of the 36th International Conference on Machine Learning*, volume 97
    of *Proceedings of Machine Learning Research*, pages 1223–1232\. PMLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fabbri et al. (2021) Alexander R. Fabbri, Wojciech Kryściński, Bryan McCann,
    Caiming Xiong, Richard Socher, and Dragomir Radev. 2021. [Summeval: Re-evaluating
    summarization evaluation](http://arxiv.org/abs/2007.12626).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu et al. (2023) Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu.
    2023. [Gptscore: Evaluate as you desire](http://arxiv.org/abs/2302.04166).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gillick and Liu (2010) Dan Gillick and Yang Liu. 2010. [Non-expert evaluation
    of summarization systems is risky](https://aclanthology.org/W10-0722). In *Proceedings
    of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s
    Mechanical Turk*, pages 148–151, Los Angeles. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He and McAuley (2016) Ruining He and Julian McAuley. 2016. [Ups and downs:
    Modeling the visual evolution of fashion trends with one-class collaborative filtering](https://api.semanticscholar.org/CorpusID:1964279).
    *Proceedings of the 25th International Conference on World Wide Web*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Im et al. (2021) Jinbae Im, Moonki Kim, Hoyeop Lee, Hyunsouk Cho, and Sehee
    Chung. 2021. [Self-supervised multimodal opinion summarization](https://doi.org/10.18653/v1/2021.acl-long.33).
    In *Proceedings of the 59th Annual Meeting of the Association for Computational
    Linguistics and the 11th International Joint Conference on Natural Language Processing
    (Volume 1: Long Papers)*, pages 388–403, Online. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,
    Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and
    William El Sayed. 2023. [Mistral 7b](http://arxiv.org/abs/2310.06825).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2023) Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho
    Song, Yunsu Kim, Hyeonwoo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, Changbae Ahn,
    Seonghoon Yang, Sukyung Lee, Hyunbyung Park, Gyoungjin Gim, Mikyoung Cha, Hwalsuk
    Lee, and Sunghun Kim. 2023. [Solar 10.7b: Scaling large language models with simple
    yet effective depth up-scaling](http://arxiv.org/abs/2312.15166).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kocmi and Federmann (2023) Tom Kocmi and Christian Federmann. 2023. [Large language
    models are state-of-the-art evaluators of translation quality](https://aclanthology.org/2023.eamt-1.19).
    In *Proceedings of the 24th Annual Conference of the European Association for
    Machine Translation*, pages 193–203, Tampere, Finland. European Association for
    Machine Translation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krippendorff (2011) Klaus Krippendorff. 2011. [Computing krippendorff’s alpha-reliability](https://api.semanticscholar.org/CorpusID:59901023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kryściński et al. (2018) Wojciech Kryściński, Romain Paulus, Caiming Xiong,
    and Richard Socher. 2018. [Improving abstraction in text summarization](https://doi.org/10.18653/v1/D18-1207).
    In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing*, pages 1808–1817, Brussels, Belgium. Association for Computational
    Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Laban et al. (2022) Philippe Laban, Tobias Schnabel, Paul N. Bennett, and Marti A.
    Hearst. 2022. [SummaC: Re-visiting NLI-based models for inconsistency detection
    in summarization](https://doi.org/10.1162/tacl_a_00453). *Transactions of the
    Association for Computational Linguistics*, 10:163–177.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin (2004) Chin-Yew Lin. 2004. [ROUGE: A package for automatic evaluation of
    summaries](https://aclanthology.org/W04-1013). In *Text Summarization Branches
    Out*, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023) Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu,
    and Chenguang Zhu. 2023. [G-eval: NLG evaluation using gpt-4 with better human
    alignment](https://doi.org/10.18653/v1/2023.emnlp-main.153). In *Proceedings of
    the 2023 Conference on Empirical Methods in Natural Language Processing*, pages
    2511–2522, Singapore. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McKnight and Najab (2010) Patrick E McKnight and Julius Najab. 2010. Mann-whitney
    u test. *The Corsini encyclopedia of psychology*, pages 1–1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. 2023. ChatGPT (August 3 Version). [https://chat.openai.com](https://chat.openai.com).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rafailov et al. (2023) Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano
    Ermon, Christopher D. Manning, and Chelsea Finn. 2023. [Direct preference optimization:
    Your language model is secretly a reward model](http://arxiv.org/abs/2305.18290).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (25) ShareGPT. Sharegpt. [https://sharegpt.com/](https://sharegpt.com/). Accessed
    on February 15, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen and Wan (2023) Yuchen Shen and Xiaojun Wan. 2023. Opinsummeval: Revisiting
    automated evaluation for opinion summarization. *arXiv preprint arXiv:2310.18122*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Siledar et al. (2023) Tejpalsingh Siledar, Suman Banerjee, Amey Patil, Sudhanshu
    Singh, Muthusamy Chelliah, Nikesh Garera, and Pushpak Bhattacharyya. 2023. [Synthesize,
    if you do not have: Effective synthetic dataset creation strategies for self-supervised
    opinion summarization in E-commerce](https://doi.org/10.18653/v1/2023.findings-emnlp.899).
    In *Findings of the Association for Computational Linguistics: EMNLP 2023*, pages
    13480–13491, Singapore. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. [Llama
    2: Open foundation and fine-tuned chat models](http://arxiv.org/abs/2307.09288).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tunstall et al. (2023) Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen
    Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine
    Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and
    Thomas Wolf. 2023. [Zephyr: Direct distillation of lm alignment](http://arxiv.org/abs/2310.16944).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023) Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang
    Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023. [Is ChatGPT a good NLG
    evaluator? a preliminary study](https://doi.org/10.18653/v1/2023.newsum-1.1).
    In *Proceedings of the 4th New Frontiers in Summarization Workshop*, pages 1–11,
    Singapore. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai
    hsin Chi, F. Xia, Quoc Le, and Denny Zhou. 2022. [Chain of thought prompting elicits
    reasoning in large language models](https://api.semanticscholar.org/CorpusID:246411621).
    *ArXiv*, abs/2201.11903.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2023) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. [Chain-of-thought prompting
    elicits reasoning in large language models](http://arxiv.org/abs/2201.11903).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wolf et al. (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
    Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
    and Alexander M. Rush. 2020. [Huggingface’s transformers: State-of-the-art natural
    language processing](http://arxiv.org/abs/1910.03771).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yuan et al. (2021) Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. [Bartscore:
    Evaluating generated text as text generation](https://proceedings.neurips.cc/paper_files/paper/2021/file/e4d2b6e6fdeca3e60e0f1a62fee3d9dd-Paper.pdf).
    In *Advances in Neural Information Processing Systems*, volume 34, pages 27263–27277\.
    Curran Associates, Inc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2019) Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger,
    and Yoav Artzi. 2019. [Bertscore: Evaluating text generation with bert](https://api.semanticscholar.org/CorpusID:127986044).
    *ArXiv*, abs/1904.09675.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao
    Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. [Judging llm-as-a-judge with
    mt-bench and chatbot arena](http://arxiv.org/abs/2306.05685).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhong et al. (2022) Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei
    Liu, Chenguang Zhu, Heng Ji, and Jiawei Han. 2022. [Towards a unified multi-dimensional
    evaluator for text generation](https://doi.org/10.18653/v1/2022.emnlp-main.131).
    In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language
    Processing*, pages 2023–2038, Abu Dhabi, United Arab Emirates. Association for
    Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Available Benchmark Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'OpinSummEval: Shen and Wan ([2023](#bib.bib26)) used the Yelp test set (Chu
    and Liu, [2019](#bib.bib8)) to annotate for $4$ dimensions. For consistency, we
    hereby refer to the above-mentioned dimensions as fluency, coherence, aspect coverage,
    and sentiment consistency  respectively, in line with our definitions.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | FL $\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\rho$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| OpinSummEval | Rouge-1^† | 0.08 | 0.06 | 0.11 | 0.09 | 0.14 | 0.11 | 0.00
    | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '| Rouge-2^† | 0.13 | 0.10 | 0.13 | 0.11 | 0.15 | 0.11 | 0.04 | 0.04 |'
  prefs: []
  type: TYPE_TB
- en: '| Rouge-L^† | 0.13 | 0.10 | 0.18 | 0.15 | 0.18 | 0.14 | 0.07 | 0.05 |'
  prefs: []
  type: TYPE_TB
- en: '| BERTScore^† | 0.38 | 0.30 | 0.20 | 0.17 | 0.20 | 0.16 | 0.08 | 0.06 |'
  prefs: []
  type: TYPE_TB
- en: '| BARTScore^† | 0.42 | 0.33 | 0.35 | 0.29 | 0.28 | 0.22 | 0.41 | 0.34 |'
  prefs: []
  type: TYPE_TB
- en: '| SummaC^† | 0.06 | 0.05 | 0.02 | 0.01 | 0.07 | 0.06 | 0.20 | 0.17 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGPT-3.5^† | 0.47 | 0.42 | 0.28 | 0.25 | 0.34 | 0.30 | 0.37 | 0.33 |'
  prefs: []
  type: TYPE_TB
- en: '| G-Eval-3.5^† | 0.41 | 0.36 | 0.29 | 0.26 | 0.27 | 0.23 | 0.38 | 0.34 |'
  prefs: []
  type: TYPE_TB
- en: '| Op-I-GPT-3.5 | 0.51 | 0.46 | 0.36 | 0.32 | 0.33 | 0.29 | 0.43 | 0.39 |'
  prefs: []
  type: TYPE_TB
- en: '| G-Eval-Mistral | 0.46 | 0.41 | 0.41 | 0.38 | 0.36 | 0.32 | 0.49 | 0.45 |'
  prefs: []
  type: TYPE_TB
- en: '| OP-Mistral | 0.35 | 0.33 | 0.45 | 0.41 | 0.34 | 0.30 | 0.45 | 0.41 |'
  prefs: []
  type: TYPE_TB
- en: '| OP-I-Mistral | 0.46 | 0.41 | 0.37 | 0.35 | 0.38 | 0.34 | 0.49 | 0.45 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Spearman ($\rho$ represents results as reported in Shen and Wan ([2023](#bib.bib26))'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Rater Agreement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We hired $3$ raters received stipends suitable for the tasks. The annotation
    interface provided raters with the reviews and associated summaries product-wise.
    Models associated with summaries were not revealed to the raters to remove any
    bias.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [6](#A2.T6 "Table 6 ‣ Appendix B Rater Agreement ‣ One Prompt To Rule
    Them All: LLMs for Opinion Summary Evaluation") reports pairwise root mean squared
    error scores for the $3$. Table [7](#A2.T7 "Table 7 ‣ Appendix B Rater Agreement
    ‣ One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation") reports the
    pairwise correlations between raters as well as the correlation between each rater
    and average ratings for both Round-I and Round-II.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | FL $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Round-I |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| A1-A2 | 0.95 | 1.06 | 1.01 | 1.09 | 0.91 | 1.08 | 0.95 |'
  prefs: []
  type: TYPE_TB
- en: '| A2-A3 | 0.44 | 0.86 | 1.09 | 1.05 | 0.84 | 1.19 | 1.42 |'
  prefs: []
  type: TYPE_TB
- en: '| A1-A3 | 1.00 | 1.23 | 1.16 | 1.24 | 1.15 | 1.47 | 1.55 |'
  prefs: []
  type: TYPE_TB
- en: '| AVG-I | 0.80 | 1.05 | 1.09 | 1.13 | 0.97 | 1.25 | 1.31 |'
  prefs: []
  type: TYPE_TB
- en: '| Round-II |'
  prefs: []
  type: TYPE_TB
- en: '| A1-A2 | 0.55 | 0.66 | 0.65 | 0.60 | 0.64 | 0.64 | 0.60 |'
  prefs: []
  type: TYPE_TB
- en: '| A2-A3 | 0.31 | 0.62 | 0.67 | 0.67 | 0.68 | 0.71 | 0.79 |'
  prefs: []
  type: TYPE_TB
- en: '| A1-A3 | 0.53 | 0.73 | 0.67 | 0.68 | 0.76 | 0.76 | 0.73 |'
  prefs: []
  type: TYPE_TB
- en: '| AVG-II | 0.47 | 0.67 | 0.66 | 0.65 | 0.69 | 0.70 | 0.71 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Round-I and Round-II Ratings: Pairwise Root Mean Squared Error scores
    for $3$ raters A1, A2, and A3.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | FL $\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\rho$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Pairwise correlation among raters |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Round-I | A1-A2 | 0.58 | 0.56 | 0.54 | 0.50 | 0.65 | 0.60 | 0.73 | 0.68 |
    0.78 | 0.72 | 0.60 | 0.53 | 0.65 | 0.59 |'
  prefs: []
  type: TYPE_TB
- en: '| A2-A3 | 0.79 | 0.78 | 0.40 | 0.38 | 0.52 | 0.47 | 0.63 | 0.58 | 0.77 | 0.71
    | 0.56 | 0.51 | 0.58 | 0.53 |'
  prefs: []
  type: TYPE_TB
- en: '| A1-A3 | 0.55 | 0.53 | 0.34 | 0.31 | 0.40 | 0.36 | 0.60 | 0.54 | 0.74 | 0.68
    | 0.57 | 0.51 | 0.57 | 0.51 |'
  prefs: []
  type: TYPE_TB
- en: '| AVG-I | 0.64 | 0.62 | 0.43 | 0.40 | 0.52 | 0.48 | 0.65 | 0.60 | 0.76 | 0.70
    | 0.58 | 0.52 | 0.60 | 0.54 |'
  prefs: []
  type: TYPE_TB
- en: '| Pairwise correlation between raters and the overall average ratings |'
  prefs: []
  type: TYPE_TB
- en: '|  | A-A1 | 0.95 | 0.93 | 0.86 | 0.82 | 0.81 | 0.74 | 0.86 | 0.80 | 0.91 |
    0.85 | 0.85 | 0.77 | 0.87 | 0.80 |'
  prefs: []
  type: TYPE_TB
- en: '|  | A-A2 | 0.70 | 0.67 | 0.72 | 0.67 | 0.82 | 0.75 | 0.81 | 0.75 | 0.91 |
    0.85 | 0.85 | 0.78 | 0.87 | 0.80 |'
  prefs: []
  type: TYPE_TB
- en: '|  | A-A3 | 0.57 | 0.54 | 0.56 | 0.51 | 0.74 | 0.67 | 0.81 | 0.76 | 0.88 |
    0.81 | 0.76 | 0.69 | 0.76 | 0.69 |'
  prefs: []
  type: TYPE_TB
- en: '|  | AVG-II | 0.74 | 0.71 | 0.71 | 0.67 | 0.79 | 0.72 | 0.83 | 0.77 | 0.90
    | 0.84 | 0.82 | 0.75 | 0.83 | 0.76 |'
  prefs: []
  type: TYPE_TB
- en: '| Pairwise correlation among raters |'
  prefs: []
  type: TYPE_TB
- en: '| Round-II | A1-A2 | 0.63 | 0.61 | 0.64 | 0.61 | 0.80 | 0.75 | 0.83 | 0.79
    | 0.85 | 0.80 | 0.77 | 0.72 | 0.81 | 0.76 |'
  prefs: []
  type: TYPE_TB
- en: '| A2-A3 | 0.85 | 0.84 | 0.59 | 0.56 | 0.78 | 0.73 | 0.77 | 0.73 | 0.83 | 0.78
    | 0.77 | 0.73 | 0.81 | 0.77 |'
  prefs: []
  type: TYPE_TB
- en: '| A1-A3 | 0.66 | 0.65 | 0.59 | 0.56 | 0.77 | 0.72 | 0.78 | 0.73 | 0.84 | 0.79
    | 0.79 | 0.74 | 0.82 | 0.78 |'
  prefs: []
  type: TYPE_TB
- en: '| AVG-I | 0.71 | 0.70 | 0.61 | 0.58 | 0.78 | 0.73 | 0.79 | 0.75 | 0.84 | 0.79
    | 0.78 | 0.73 | 0.81 | 0.77 |'
  prefs: []
  type: TYPE_TB
- en: '| Pairwise correlation between raters and the overall average ratings |'
  prefs: []
  type: TYPE_TB
- en: '|  | A-A1 | 0.94 | 0.92 | 0.87 | 0.83 | 0.91 | 0.85 | 0.89 | 0.84 | 0.94 |
    0.88 | 0.92 | 0.86 | 0.92 | 0.87 |'
  prefs: []
  type: TYPE_TB
- en: '|  | A-A2 | 0.76 | 0.74 | 0.80 | 0.75 | 0.91 | 0.86 | 0.87 | 0.82 | 0.93 |
    0.88 | 0.91 | 0.85 | 0.92 | 0.87 |'
  prefs: []
  type: TYPE_TB
- en: '|  | A-A2 | 0.69 | 0.67 | 0.76 | 0.71 | 0.91 | 0.85 | 0.92 | 0.88 | 0.93 |
    0.86 | 0.91 | 0.85 | 0.92 | 0.87 |'
  prefs: []
  type: TYPE_TB
- en: '|  | AVG-II | 0.80 | 0.77 | 0.81 | 0.76 | 0.91 | 0.86 | 0.89 | 0.85 | 0.93
    | 0.87 | 0.91 | 0.85 | 0.92 | 0.87 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Rater Correlations: Pairwise Spearman ($\rho$ raters A1, A2, and A3
    along with the average of their ratings A.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/75680dc7d1ae2550de6284c23f603383.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Performance of different models as rated by human annotators. We
    observe that GPT-$4$B. Self-supervised models perform worse. In general, all the
    LLMs perform better than human annotated summaries.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Opinion Summary Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For each product $d_{i}$ denote the correlation measure. Bhandari et al. ([2020](#bib.bib2))
    defines the summary-level correlation as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{R}(a,b)=\frac{1}{\mathcal{Z}}\sum_{i}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle[\mathcal{M}_{b}(s_{i1}),...,\mathcal{M}_{b}(s_{i\mathcal{N}})])$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: Appendix D Prompts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For brevity, we provide different prompts for only a single dimension- Aspect
    Coverage. We will release prompts for all the dimensions across different approaches
    publicly.
  prefs: []
  type: TYPE_NORMAL
- en: D.1 Op-I-Prompt for Aspect Coverage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Task Description:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You will be given a set of reviews using which a summary has been generated.
    Your task is to evaluate the summary based on the given metric. Evaluate to which
    extent does the summary follows the given metric considering the reviews as the
    input. Use the following evaluation criteria to judge the extent to which the
    metric is followed. Make sure you understand the task and the following evaluation
    metric very clearly. Evaluation Criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: The task is to judge the extent to which the metric is followed by the summary.
    Following are the scores and the evaluation criteria according to which scores
    must be assigned.
  prefs: []
  type: TYPE_NORMAL
- en: <score>1</score> - The metric is not followed at all while generating the summary
    from the reviews.
  prefs: []
  type: TYPE_NORMAL
- en: <score>2</score> - The metric is followed only to a limited extent while generating
    the summary from the reviews.
  prefs: []
  type: TYPE_NORMAL
- en: <score>3</score> - The metric is followed to a good extent while generating
    the summary from the reviews.
  prefs: []
  type: TYPE_NORMAL
- en: <score>4</score> - The metric is followed mostly while generating the summary
    from the reviews.
  prefs: []
  type: TYPE_NORMAL
- en: '<score>5</score> - The metric is followed completely while generating the summary
    from the reviews. Metric:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Aspect Coverage - The summary should cover all the aspects that are majorly
    being discussed in the reviews. Summaries should be penalized if they miss out
    on an aspect that was majorly being discussed in the reviews and awarded if it
    covers all. Reviews:'
  prefs: []
  type: TYPE_NORMAL
- en: '{} Summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '{} Evaluation Steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow the following steps strictly while giving the response:'
  prefs: []
  type: TYPE_NORMAL
- en: 1.First write down the steps that are needed to evaluate the summary as per
    the metric. Reiterate what metric you will be using to evaluate the summary.
  prefs: []
  type: TYPE_NORMAL
- en: 2.Give a step-by-step explanation if the summary adheres to the metric considering
    the reviews as the input. Stick to the metric only for evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: 3.Next, evaluate the extent to which the metric is followed.
  prefs: []
  type: TYPE_NORMAL
- en: 4.Use the previous information to rate the summary using the evaluation criteria
    and assign a score within the <score></score> tags.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First give a detailed explanation and then finally give a single score following
    the format: Score- <score>5</score>'
  prefs: []
  type: TYPE_NORMAL
- en: 'THE EVALUATION AND SCORE MUST BE ASSIGNED STRICTLY ACCORDING TO THE METRIC
    ONLY AND NOTHING ELSE! Response:'
  prefs: []
  type: TYPE_NORMAL
- en: D.2 Op-Prompts for Aspect Coverage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Task Description:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You will be given a set of reviews. You will then be given one summary written
    for the set of reviews. Your task is to rate the summary on one metric. Make sure
    you understand the following evaluation metric very clearly. Your task is to rate
    the summary corresponding to the given reviews on the evaluation criteria. Evaluation
    Criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: Aspect Coverage - The summary should cover all the aspects that are majorly
    being discussed in the reviews. Summaries should be penalized if they miss out
    on an aspect that was majorly being discussed in the reviews and awarded if it
    covers all.
  prefs: []
  type: TYPE_NORMAL
- en: <score>1</score> - Summary does not cover any important aspects present in the
    reviews.
  prefs: []
  type: TYPE_NORMAL
- en: <score>2</score> - Summary does not cover most of the important aspects present
    in the reviews.
  prefs: []
  type: TYPE_NORMAL
- en: <score>3</score> - Summary covers around half of the important aspects present
    in the reviews.
  prefs: []
  type: TYPE_NORMAL
- en: <score>4</score> - Summary covers most of the important aspects present in reviews.
  prefs: []
  type: TYPE_NORMAL
- en: '<score>5</score> - Summary covers all the important aspects discussed in reviews.
    Metric:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Aspect Coverage - The summary should cover all the aspects that are majorly
    being discussed in the reviews. Summaries should be penalized if they miss out
    on an aspect that was majorly being discussed in the reviews and awarded if it
    covers all. Reviews:'
  prefs: []
  type: TYPE_NORMAL
- en: '{} Summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '{} Evaluation Steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go step-by-step. Follow the following steps strictly while giving the
    response:'
  prefs: []
  type: TYPE_NORMAL
- en: 1.Identify the important aspects present in the reviews and list them with numbering
  prefs: []
  type: TYPE_NORMAL
- en: 2.Identify the important aspects present in the summary and list them with numbering
  prefs: []
  type: TYPE_NORMAL
- en: 3.Identify the important aspects covered by the summary that are present in
    the reviews and list them with numbering
  prefs: []
  type: TYPE_NORMAL
- en: 4.Calculate the total number of important aspects covered by the summary that
    are present in the reviews
  prefs: []
  type: TYPE_NORMAL
- en: 5.Calculate the total number of important aspects present in the reviews
  prefs: []
  type: TYPE_NORMAL
- en: 6.Finally use the evaluation criteria to output only a single score within <score></score>
    tags.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First give a detailed explanation of how much is the coverage and then finally
    give a single score following the format: Score- <score>5</score> Response:'
  prefs: []
  type: TYPE_NORMAL
- en: D.3 G-Eval for Aspect Coverage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Task Description:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You will be given a set of reviews and a corresponding summary. Make sure you
    understand the following evaluation metric very clearly. Your task is to rate
    the summary corresponding to the given reviews on the evaluation criteria. Evaluation
    Criteria: Aspect Coverage (1-5) - The summary should cover all the aspects that
    are majorly being discussed in the reviews. Summaries should be penalized if they
    miss out on an aspect that was majorly being discussed in the reviews and awarded
    if it covers all. Reviews:'
  prefs: []
  type: TYPE_NORMAL
- en: '{} Summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '{} Evaluation Steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 1.Read through the given set of reviews carefully.
  prefs: []
  type: TYPE_NORMAL
- en: 2.Compare the content of the reviews to the provided summary.
  prefs: []
  type: TYPE_NORMAL
- en: 3.Evaluate whether the summary covers all the major aspects that are being discussed
    in the reviews.
  prefs: []
  type: TYPE_NORMAL
- en: 4.Rate the summary on a scale of 1-5 based on how well it covers the aspects
    discussed in the reviews.
  prefs: []
  type: TYPE_NORMAL
- en: 5.Provide a brief explanation for your rating, citing specific examples from
    the reviews and summary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: Strictly give the score within <score></score> tags only e.g Score: <score>5</score>.
    Response:'
  prefs: []
  type: TYPE_NORMAL
- en: D.4 Summarization Prompt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Generate a summary for the following set of reviews. Generate the summary in
    a paragraph format. No bulletpoints or explanations needed. Just output the summary
    text. Reviews:'
  prefs: []
  type: TYPE_NORMAL
- en: '{} Summary:'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Dimension Definitions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For ablation, we try out three different definition variations of aspect coverage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Definition 1: The summary should cover all the aspects that are majorly being
    discussed in the reviews. Summaries should be penalized if they miss out on an
    aspect that was majorly being discussed in the reviews and awarded if it covers
    all.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Definition 2: This refers to the comprehensiveness of a summary in capturing
    all significant aspects discussed in reviews. A summary is evaluated based on
    its ability to include major topics of discussion; it is deemed deficient if it
    overlooks any crucial aspect and commendable if it encompasses them all.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Definition 3: Aspect coverage pertains to the extent to which a summary encapsulates
    the key facets discussed in reviews. Summaries are evaluated based on their ability
    to incorporate major discussion points. They are considered deficient if they
    omit any critical aspect and commendable if they address them all comprehensively.'
  prefs: []
  type: TYPE_NORMAL
