- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:51:38'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Federated Prompting and Chain-of-Thought Reasoning for Improving LLMs Answering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2304.13911](https://ar5iv.labs.arxiv.org/html/2304.13911)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '¹¹institutetext: South China Normal University, Guangdong, China'
  prefs: []
  type: TYPE_NORMAL
- en: '¹¹email: {2022024952,2022024954}@m.scnu.edu.cn, fanchenyou@scnu.edu.cnXiangyang
    Liu    Tianqi Pang    Chenyou Fan'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We investigate how to enhance answer precision in frequently asked questions
    posed by distributed users using cloud-based Large Language Models (LLMs). Our
    study focuses on a typical situations where users ask similar queries that involve
    identical mathematical reasoning steps and problem-solving procedures. Due to
    the unsatisfactory accuracy of LLMs’ zero-shot prompting with standalone questions,
    we propose to improve the distributed synonymous questions using Self-Consistency
    (SC) and Chain-of-Thought (CoT) techniques. Specifically, we first retrieve synonymous questions
    from a crowd-sourced database and create a federated question pool. We call these
    federated synonymous questions with the same or different parameters SP-questions
    or DP-questions, respectively. We refer to our methods as Fed-SP-SC and Fed-DP-CoT,
    which can generate significantly more accurate answers for all user queries without
    requiring sophisticated model-tuning. Through extensive experiments, we demonstrate
    that our proposed methods can significantly enhance question accuracy by fully
    exploring the synonymous nature of the questions and the consistency of the answers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Synonymous Question-answering Federated Learning Large Language Model Prompt
    Learning Chain-of-Thought.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3ac7d5a88869d1abe904a699830c7fff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: A general overview of our approach to dealing with federated synonymous
    question-answering. Our approach is categorized into two user scenarios: synonymous
    questions that share the same parameters, and those that have different parameters.
    When the parameters are the same, we utilize self-consistency to select the most
    commonly voted answer as the consistent response. However, for cases where the
    parameters are different, we amalgamate each question’s consistent answer to create
    a Chain-of-Thought, which makes it easier for the LLM to respond to new queries.'
  prefs: []
  type: TYPE_NORMAL
- en: Recently, Large Language Models (LLMs) such as PaLM [[2](#bib.bib2)] and GPT
    family [[1](#bib.bib1), [13](#bib.bib13)] have revolutionized the methodology
    of tackling natural language processing (NLP) tasks such as sentiment analysis [[20](#bib.bib20)],
    question answering [[24](#bib.bib24)], text summarization [[23](#bib.bib23)],
    and reasoning on arithmetic and common sense questions [[25](#bib.bib25)].
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) are highly over-parameterized with millions or
    billions of parameters, e.g., the GPT-3 model has about 175 Billion parameters.
    Due to this redundancy in model design, LLMs can represent language in a highly
    flexible and expressive manner by capturing the complex and structured patterns
    in human languages. In addition, LLMs can generate remarkably natural dialogues
    and accurate answers with contextual understanding, sometimes even surpassing
    human experts in certain tasks. For example, in arithmetic reasoning, GPT-4 achieved
    an accuracy rate of 92% on the GSM8K dataset [[12](#bib.bib12)]; in common sense
    reasoning, KEAR achieved an accuracy rate of 89.4% on the CSQA dataset [[22](#bib.bib22)].
  prefs: []
  type: TYPE_NORMAL
- en: We consider a practical user scenario in which a large number of users can access
    a cloud-deployed LLM for solving personal tasks from all places over the world.
    For example, more and more primary school students and their parents rely on the
    capability of LLMs for solving mathematical problems. The users often access a
    LLM and ask common realistic questions. For example, primary school children might
    ask “Chickens and rabbits are in the same cage, a total of 35 heads, 94 feet,
    how many chickens and rabbits are there?”, while computer science students often
    ask “How to write a QuickSort in Python?”.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the complexity in task understanding and reasoning, the LLMs often return
    the wrong answers even given seemingly simple questions. For example, on the GSM8K
    dataset, the fine-tuned GPT-3 (175B) with verifier only achieves an accuracy of
    55.0%. Meanwhile, the PaLM-540B (Few-shot-CoT) only achieves an accuracy of 58.1%. [[8](#bib.bib8)]
    *How to improve the question answering accuracy has become a serious challenge
    which decides whether LLMs can be accepted as a robust and reliable part in realistic
    applications.*
  prefs: []
  type: TYPE_NORMAL
- en: One commonsense is that can we crowd-source many questions and aggregate those
    questions to better understand some common questions. A common question might
    be asked frequently as its variants in the concrete parameters or rephrased formulations.
    For example, the Chickens-and-rabbits questions can be asked with different number
    of heads and feet. Now we want to ask *Can we fully utilize those similar questions
    to improve the question answering of the LLMs without tuning the model parameters
    or infringing user privacy?*
  prefs: []
  type: TYPE_NORMAL
- en: Recent progressives in federated learning (FL) [[10](#bib.bib10)] have proved
    that utilizing distributed data sources can both preserve data privacy and enhance
    model training. In the FL paradigm, each client trains a local learning model
    with *own data*, while a central server regularly communicates with all agents
    to generate a better global model through the aggregation of the local models.
  prefs: []
  type: TYPE_NORMAL
- en: In this study, we consider improving the reasoning capacity of LLMs by better
    understanding crowd-sourced similar questions, from which we can explore the contextual
    information and improve the LLM answers substantially. Inspired by FL, we propose
    two typical scenarios when a user sends a QA request to the LLM and the LLM tries
    to answer with a collected question database.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synonymous Questions with Same Parameters (SP-questions). The cloud-deployed
    system retrieves from the database and finds several synonymous but rephrased
    questions with exactly the same parameters. For example,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Q1:“If a farmer has a certain number of chickens and rabbits in a barn, and
    there are a total of 32 heads and 100 feet, how many chickens and how many rabbits
    does the farmer have?”'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Q2:“In a barn, there are a certain number of chickens and rabbits that have
    a total of 32 heads and 100 feet. how many of each animal are in the barn?”*
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synonymous Questions with Different Parameters (DP-questions). This situation
    is harder as the question parameters mined in the database are different from
    each other. For example,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Q1:“If a farmer has a certain number of chickens and rabbits in a barn, and
    there are a total of 32 heads and 100 feet, how many chickens and how many rabbits
    does the farmer have?”'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Q2:“A farmer has a total of 20 chickens and rabbits in his barn. If the total
    number of legs in the barn is 56, how many chickens and how many rabbits are in
    the barn?”*
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For SP-questions, we propose to leverage LLMs to directly generate answers first.
    Then we federate the answers and apply the self-consistency [[19](#bib.bib19)]
    technique to obtain the most voted answer for all synonymous questions in the
    federation. We call this method Fed-SP-SC (Fed-SP with Self-Consistency).
  prefs: []
  type: TYPE_NORMAL
- en: For DP-questions, we propose to leverage LLMs to generate consistent answers
    for each DP-questions first. Different from procedures of dealing SP-questions,
    we cannot directly agglomerate the answers since they are for different parameters.
    Instead, we federate them by forming the Chain-of-Thought (CoT) to provide hints
    to the LLMs. We append the original query to the CoT as the full prompt to obtain
    improved final answer. We call this technique Fed-DP-CoT.
  prefs: []
  type: TYPE_NORMAL
- en: Once the LLM has finished generating the answer using either Fed-SP-SC or Fed-DP-CoT,
    the system will store both the questions and answers into the database. This enables
    the system to collect all records and leverage past records to produce re-fined
    answers to new queries. For questions that have been asked before with wrong answers,
    the system can evolve itself by correcting the answers with self-consistency mechanism
    or with more comprehensive CoT prompts.
  prefs: []
  type: TYPE_NORMAL
- en: We extensively evaluate our methods on the GSM8K and SVAMP datasets and demonstrate
    that the Fed-SP-SC method achieves a notable improvement in accuracy of 14-18%
    over the standalone LLMs with Zero-Shot-CoT (“Let’s think step by step”). Additionally,
    our Fed-DP-CoT method delivers an impressive increase of 10-15% over the standalone
    LLMs with Zero-Shot-CoT.
  prefs: []
  type: TYPE_NORMAL
- en: We summarize our contributions in this study as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We consider a practical but under-studied scenario, which is the cloud-based
    LLMs are frequently asked similar and even synonymous common questions from large
    number distributed users.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We abstract two main user scenarios: distributed users are querying synonymous
    questions that share the same parameters (SP-questions), and those that have different
    parameters (DP-questions).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We design the system to firstly federate those SP- and DP-questions first by
    retrieving the database. Then we propose to utilize self-consistency methodology
    to select the most commonly voted answer to improve SP-question answering. All
    consistent answers and CoTs will be stored back into database for further reuse.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We also amalgamate consistent answers to create a chain-of-thought prompt that
    significantly improves DP-questions answering quality. We also design a simple
    disclaimer to handle noisy CoT generated from LLM answers better.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Inherited from Federated Learning, our Fed-SP-SC and Fed-DP-COT methods can
    collaboratively enhance the question-answering process of the LLM while preserving
    their anonymity. There would be no data exchange or leakage among distributed
    users.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pre-trained Language Models (PLMs). Recent studies in Transformer-based language
    models such as ELMo [[15](#bib.bib15)] and BERT [[4](#bib.bib4)] have shown their
    capabilities in scaling up model sizes with pre-training methodology such as Masked
    Language Modeling [[4](#bib.bib4)]. Shortly after, several Large Language Models (LLMs),
    e.g., the GPT family [[1](#bib.bib1), [13](#bib.bib13)], PaLM [[2](#bib.bib2)],
    Jurassic-X [[9](#bib.bib9)], Megatron-Turing [[16](#bib.bib16)] , LaMDA [[17](#bib.bib17)],LLaMA [[18](#bib.bib18)],
    have been emerging with huge amount of parameters of up to 100B-5000B parameters.
    They have shown great advantages in language modeling tasks, such as arithmetic
    reasoning, commonsense reasoning, symbolic reasoning and natural language inference.
  prefs: []
  type: TYPE_NORMAL
- en: However, PLMs are still like black boxes which lack of explanation. Some recent
    studies made efforts towards unveiling the power of those LLMs. The proposal of
    the concept of the *Chain-of-Thought* (CoT) [[21](#bib.bib21)] indicates that
    incorporating intermediate reasoning steps can lead to a significant improvement
    in the performance of large language models on reasoning tasks. The proposal of
    the *Self-consistency* [[19](#bib.bib19)] suggest that aggregating multiple reasoning
    paths, rather than relying on greedy decoding, can lead to further improvements
    in the accuracy of models on reasoning tasks. LMSI [[7](#bib.bib7)] provides a
    demonstration of how large language models can achieve self-improvement by utilizing
    only unlabelled datasets.
  prefs: []
  type: TYPE_NORMAL
- en: However, it is unknown how to apply proper pre-training to distributed learning
    scenarios, due to substantial differences between centralized large model deployment
    and distributed query demands. In this study, we adopt the recent popular distributed
    machine learning methodology called *Federated Learning* [[10](#bib.bib10), [26](#bib.bib26),
    [6](#bib.bib6), [5](#bib.bib5)] (FL) to fully explore the potentiality of Large
    Language Models to tackle frequently asked questions while preserving data privacy
    for the users. The FL provides a way of learning models over a collection of distributed
    devices while keeping data locality. However, classical FL studies assumed the
    agents in FL can own copies of local models while receiving updates from centralized
    model. In contrast, we focus on a practical scenario that the clients can only
    query answers from centralized Large Language Models without owning any local
    model, due to the practical situations that Large Language Models are simply too
    large and computational extensive to be deployed locally.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Scenarios and Approaches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we describe the federated scenarios that distributed users
    query the LLMs with similar (but not exact the same) questions. We identify two
    types of questions and discuss them in details.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Basic Concepts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Chain-of-Thought (CoT) [[21](#bib.bib21)] is a series of generated intermediate
    reasoning texts that can be added to the original prompts. CoT is proposed for
    enhancing the capability of language models to perform various reasoning tasks
    by allowing LLMs to decompose complex problems into intermediate steps that could
    be solved well step-by-step. Chain-of-thought prompting, i.e. prompting LLMs with
    CoT, is a simple and practical method for improving the reasoning tasks readily
    with no additional efforts of tuning the original LLMs. CoT prompting has shown
    improved reasoning results on arithmetic, commonsense, and symbolic reasoning
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Self-Consistency (SC) [[19](#bib.bib19)] is a decoding strategy that enhances
    language model reasoning with voting ensemble. SC first samples a diverse set
    of answers as reasoning paths of a question, rather than only the greedy path.
    By exploring multiple paths, SC is capable of identifying the most consist answer
    as the final answer by majority voting, i.e., the most voted answer of the LLM
    is taken as the final answer. Compared with a single-path reasoning, SC ensembles
    answers to improve accuracy and filters out noises or outliers. SC has also been
    widely explored in reasoning and QA tasks [[19](#bib.bib19)].
  prefs: []
  type: TYPE_NORMAL
- en: Majority voting(MV)[[11](#bib.bib11)] is a commonly used method in statistical
    decision theory that involves aggregating the opinions or decisions of multiple
    individuals or models, typically by selecting the option with the highest frequency
    of agreement among the voters.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Synonymous Questions with Same Parameters (SP-questions)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We consider a cloud-based LLM system which accepts queries from distributed
    users. The first practical user scenario that we consider is as follows. Given
    a user query, we can retrieve from the cloud database several synonymous questions
    with same parameters (SP-questions).
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Examples of synonymous SP-questions and answers.'
  prefs: []
  type: TYPE_NORMAL
- en: '| *Example1:* |'
  prefs: []
  type: TYPE_TB
- en: '| *Q1: “If a farmer has a certain number of chickens and rabbits in a barn,
    and* |'
  prefs: []
  type: TYPE_TB
- en: '| *there are a total of $32$ heads and 100 feet, how many chickens and how
    many* |'
  prefs: []
  type: TYPE_TB
- en: '| *rabbits does the farmer have?”* |'
  prefs: []
  type: TYPE_TB
- en: '| *A1: “The farmer has $24$ chickens and 8 rabbits.”* (wrong) |'
  prefs: []
  type: TYPE_TB
- en: '| *Q2: “In a barn, there are a certain number of chickens and rabbits that
    have* |'
  prefs: []
  type: TYPE_TB
- en: '| *a total of $32$ feet. how many of each animal are in the barn?”* |'
  prefs: []
  type: TYPE_TB
- en: '| *A2: “Let $x=$ the number of rabbits. We can* |'
  prefs: []
  type: TYPE_TB
- en: '| *set up the following system of equations:$x+y=32(heads)$* |'
  prefs: []
  type: TYPE_TB
- en: '| *(feet), Solving this system of equations, we get $x=20$.Therefore,* |'
  prefs: []
  type: TYPE_TB
- en: '| *there are $20$ rabbits in the barn.”* (wrong) |'
  prefs: []
  type: TYPE_TB
- en: '| *Example2:* |'
  prefs: []
  type: TYPE_TB
- en: '| *Q1: “James writes a $3$ different friends twice a week.How many* |'
  prefs: []
  type: TYPE_TB
- en: '| *pages does he write a year?”* |'
  prefs: []
  type: TYPE_TB
- en: '| *A1: “James writes $3$ pages* |'
  prefs: []
  type: TYPE_TB
- en: '| *a month and $288$ pages a year.”* (wrong) |'
  prefs: []
  type: TYPE_TB
- en: '| *Q2: “If James writes a $3$-page letter to two different friends twice per
    week,what* |'
  prefs: []
  type: TYPE_TB
- en: '| *is the total number of pages he produces every year?”* |'
  prefs: []
  type: TYPE_TB
- en: '| *A2: “James writes two $3$ weeks in a* |'
  prefs: []
  type: TYPE_TB
- en: '| *year. Therefore, James produces a total of $312$).”* |'
  prefs: []
  type: TYPE_TB
- en: '| (wrong) |'
  prefs: []
  type: TYPE_TB
- en: 'For complex reasoning tasks, the LLMs may provide unreliable answers to the
    questions. We provide two failure cases in Table. [1](#S3.T1 "Table 1 ‣ 3.2 Synonymous
    Questions with Same Parameters (SP-questions) ‣ 3 Scenarios and Approaches ‣ Federated
    Prompting and Chain-of-Thought Reasoning for Improving LLMs Answering"). We found
    that in both examples, Q1 and Q2 are synonymous while each of them gets a wrong
    answer from LLM. We summarize the difficulties of tackling the SP-questions as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Most LLMs have unsatisfying accuracy in solving reasoning problems in zero-shot
    way, i.e., prompting the LLMs with questions directly without giving other information.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: LLMs tend to under-perform when understanding complex problems involving multiple
    reasoning steps, such as the arithmetic problems given above.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Thus, our task is to *fully explore the SP-questions as a federation which can
    enhance the answer quality together*, instead of dealing them separately. To this
    end, we propose a technique named Fed-SP-SC (Federated SP-questions with Self-Consistency)
    for answering the questions with the self-consistency technique mentioned above.
    Fed-SP-SC can improve the zero-shot accuracy of the answers by eliciting answers
    from multiple synonymous questions and make a majority vote to disclose the most
    likely answer.
  prefs: []
  type: TYPE_NORMAL
- en: Concretely, we query the database using the user’s prompt to match SP-questions
    in the database. Note that here we assume we can retrieve the SP-questions which
    are just rephrased with synonymous and same parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we generate the answers with LLMs by zero-shot prompting. For SP-questions,
    these answers are presumably same. Assuming that we have generated a total of
    *n* answers of synonymous questions during the Fed-SP-SC process, we can ensure
    the consistency with SC procedure, i.e., we make a majority vote and select the
    most voted answer $A^{SC}$ as the final answer of all SP-questions, as below.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $A^{SC}\leftarrow\operatorname*{arg\,max}_{A\in\mathcal{A}}\sum\mathbf{1}[A==A_{i}],\
    \ \forall i=1,...,n\ .$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: Intuitively, the majority voting filters out outliers and noisy rephrased questions.
    In addition, the most voted answer is the agreement of multiple reasoning paths
    from multiple rephrased SP-questions, thus is more likely to be better than a
    single prompted answer decoded from a single reasoning path.
  prefs: []
  type: TYPE_NORMAL
- en: In our experiments, we demonstrate that Fed-SP-SC achieves a 17.5% improvement
    in accuracy on the GSM8K dataset and a 14% improvement on the SVAMP dataset in
    Table [3](#S4.T3 "Table 3 ‣ 4.2 Results of Fed-SP-SC ‣ 4 Experiment ‣ Federated
    Prompting and Chain-of-Thought Reasoning for Improving LLMs Answering"). In a
    practical system, we can further store these user prompts and the SC-selected
    answer back into the database.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e9d8d8903d45611f6750eb55fb429ab7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The illustration of performing Fed-SP-SC for answering synonymous SP-questions.
    $(A\rightarrow B)$: The most voted answer is returned to the user as the best
    answer. The database could store the query and answer pair back to the database,
    caching for later retrieval. This procedure can grow the database quickly by gathering
    distributed user queries.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Synonymous questions with Different Parameters (DP-questions)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We now describe the second scenario which is named synonymous questions with
    different parameters (DP-questions), which is broader and more practical. Based
    on the user query question, the cloud-deployed system searches and retrieves from
    the database for questions with same meanings but may have different parameters.
  prefs: []
  type: TYPE_NORMAL
- en: DP-questions are more practical yet harder than SP-questions as the question
    parameters retrieved from the database are different. We show two exemplary questions
    Q1 and Q2 below which have the same meaning yet with different parameters *heads*
    and *feet* in Table [2](#S3.T2 "Table 2 ‣ 3.3 Synonymous questions with Different
    Parameters (DP-questions) ‣ 3 Scenarios and Approaches ‣ Federated Prompting and
    Chain-of-Thought Reasoning for Improving LLMs Answering").
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Two examples of DP-questions. *Q1* and *Q2* are synonymous but with
    different question parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: '| The specific description of *Q1* and *Q2:* |'
  prefs: []
  type: TYPE_TB
- en: '| *Q1: “If a farmer has a certain number of chickens and rabbits in a barn
    and*, |'
  prefs: []
  type: TYPE_TB
- en: '| *there are a total of 32 heads and 100 feet, how many chickens and how many*
    |'
  prefs: []
  type: TYPE_TB
- en: '| *rabbits does the farmer have?”* |'
  prefs: []
  type: TYPE_TB
- en: '| *Q2: “A farmer has a total of 20 chickens and rabbits in his barn. If the
    total* |'
  prefs: []
  type: TYPE_TB
- en: '| *number of legs in the barn is 56, how many chickens and how many rabbits*
    |'
  prefs: []
  type: TYPE_TB
- en: '| *are in the barn?”* |'
  prefs: []
  type: TYPE_TB
- en: 'Note that tackling DP-questions would face all the difficulties of SP-questions,
    and would have additional obstacles as summarized below:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There is no uniform ground-truth for DP-questions in the database, since each
    one has different parameters.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Similarly, we cannot apply self-consistency (SC) directly to improve accuracy
    due to different parameters.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If we apply Chain-of-Thought (CoT) together to the original questions as enhanced
    prompts, we cannot guarantee the correctness of the CoT. Incorrect CoT may even
    harm the answering accuracy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To tackle the above challenges, we propose the Federated questions of Different
    Parameters with Chain-of-Thought (Fed-DP-CoT) technique to leverage existing answers
    of DP-questions in CoT forms to improve new query answering.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cc7b5c7cd186a7fbd826092438b6cea2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The illustration of performing Fed-DP-CoT. $(A\rightarrow B)$: The
    system concatenates questions and pseudo-labels, adds a pseudo-label disclaimer
    such as “The examples may have errors.” after, and finally appends the original
    user query and Zero-Shot-CoT to form the complete CoT prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: When a user starts querying the cloud-based LLM service, the cloud system performs
    query-question retrieval first. The system matches several questions with the
    highest similarity in the database. Generally, these the retrieved questions are
    of different parameters (DP-questions). We design the system to perform Fed-DP-CoT
    for understanding DP-questions.
  prefs: []
  type: TYPE_NORMAL
- en: We consider a practical case that these DP-questions have *pseudo-labels* generated
    by self-consistency majority voting in the Fed-SP-SC processes. We call these
    labels “pseudo-labels” as they are not actual ground-truth labels.
  prefs: []
  type: TYPE_NORMAL
- en: Then we utilize these DP-questions with pseudo-labels together as CoT for the
    original query-question. To be specific, we concatenate DP-questions with their
    answers as a single prompt, followed by the error disclaimer “The examples given
    above may contain errors , please think more carefully. ” at the end of this prompt
    as the complete prompt. The error disclaimer reminds the LLMs that the answers
    in CoT are pseudo-labels and could be incorrect. We found this simple practice
    can boost performance by approximately 2%. Finally, we use the entire disclaimed
    CoT as a prefix to the user’s query prompt for the LLMs to provide the final answer.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We evaluate our proposed Fed-SP-SC and Fed-DP-CoT methods on benchmark datasets
    with simulated user scenarios such that SP- and DP-questions are retrieved to
    improve over standalone question answering.
  prefs: []
  type: TYPE_NORMAL
- en: We compare our methods with *Zero-Shot-CoT* [[1](#bib.bib1)], which refers to
    adding *“Let’s think step by step.”* to prompt as a composite prompt, such as
    “[Question] A:Let’s think step by step.”
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Grade School Math (GSM8K) is a math dataset with 7,473 training and 1,319 testing
    examples of grade-school-level word problems[[3](#bib.bib3)]. These math problems
    typically require two to eight calculation steps to arrive at a final answer,
    as shown in Fig.[2](#S3.F2 "Figure 2 ‣ 3.2 Synonymous Questions with Same Parameters
    (SP-questions) ‣ 3 Scenarios and Approaches ‣ Federated Prompting and Chain-of-Thought
    Reasoning for Improving LLMs Answering"). GSM8K is widely used as a benchmark
    dataset for testing the arithmetic and commonsense reasoning capacities of LLMs [[7](#bib.bib7),
    [8](#bib.bib8)].
  prefs: []
  type: TYPE_NORMAL
- en: Simple Variations on Arithmetic Math word Problems (SVAMP) is a dataset of simple
    arithmetic math word problems [[14](#bib.bib14)] with around 6,000 samples. Each
    data instance has a short story and a question about unknown quantities. SVAMP
    provides a benchmark test set for comparing the textual understanding and reasoning
    abilities of LLMs, which is widely compared in recent studies [[14](#bib.bib14),
    [21](#bib.bib21), [19](#bib.bib19)].
  prefs: []
  type: TYPE_NORMAL
- en: In practice, we utilized the OpenAI’s API ¹¹1https://platform.openai.com/docs/models
    text-davinci-002 and text-davinci-003\. We selected text-davinci-003 for the GSM8K
    dataset as text-davinci-002 performed very poorly. Similarly, we used text-davinci-002
    for the SVAMP dataset as text-davinci-003 had an overly high accuracy rate on
    this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Results of Fed-SP-SC
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As a kind reminder, in the following discussions, SP-questions stand for *a
    set of rephrased synonymous questions with same parameters.* Differently, DP-questions
    stand for *a set of rephrased synonymous questions with different parameters.*
    We now describe the experiment procedures shown in Fig. [4](#S4.F4 "Figure 4 ‣
    4.2 Results of Fed-SP-SC ‣ 4 Experiment ‣ Federated Prompting and Chain-of-Thought
    Reasoning for Improving LLMs Answering").
  prefs: []
  type: TYPE_NORMAL
- en: '[SP-questions generation]. We first generate four SP-questions for each of
    the original question with both OpenAI GPT-3 [[1](#bib.bib1)] and GPT-3.5 [[13](#bib.bib13)],
    respectively. Concretely, we add each original question a same prompt prefix “Rephrase
    in 4 ways: [ORIGINAL QUESTION]”, then we collect the generated answers.'
  prefs: []
  type: TYPE_NORMAL
- en: '[SP-questions answering]. We query the cloud-deployed LLM for answering rephrased
    questions generated as above. Specifically, we obtain the improved Zero-Shot-CoT
    answering with the magic sentence “Let’s think step by step” as the prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: We first examine the performance of our proposed Fed-SP-SC which deals with
    SP-questions with the Self-consistency technique. We conducted experiments on
    GSM8K and SVAMP and report the results below.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c0d7a176c58306cd3a2da95bcf44e2a5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The experiment of Fed-SP-SC contains five steps: (1) Load the GSM8K
    and SVAMP datasets as our benchmark and extract the questions and answers in the
    dataset; (2) Add each original question a same prompt prefix “Rephrase in 4 ways:
    [QUESTION]” to generate SP-questions; (3) Prompt both the original and rewritten
    questions to the LLMs to obtain their respective answers; (4) Use the majority
    vote for self-consistency; (5) Get the answer generated by Fed-SP-SC and compare
    it with the answer in the dataset to determine the accuracy rate.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Fed-SP-SC results'
  prefs: []
  type: TYPE_NORMAL
- en: '| Data\Method | Zero-Shot-CoT |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Fed-SP-SC &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (GPT-3 Gen.) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fed-SP-SC &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (GPT-3.5 Gen.) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| GSM8K | 52.5% | 62.7% | 70.6% |'
  prefs: []
  type: TYPE_TB
- en: '| SVAMP | 77.2% | 86.3% | 91.1% |'
  prefs: []
  type: TYPE_TB
- en: We show accuracy of self-consistency after obtaining results from different
    phrasings of the synonymous question on GSM8K and SVAMP in Table [3](#S4.T3 "Table
    3 ‣ 4.2 Results of Fed-SP-SC ‣ 4 Experiment ‣ Federated Prompting and Chain-of-Thought
    Reasoning for Improving LLMs Answering"). We have the following observations.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Fed-SP-SC can improve answering accuracy* of LLMs by federating multiple SP-questions
    through self-consistency.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Fed-SP-SC(GPT-3.5 Gen.) performs best on the GSM8K and SVAMP datasets,* improved
    the performance by $17.5\%$ on the GSM8K and SVAMP datasets, respectively.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The quality of the synonymous questions can affect the accuracy significantly*,
    as seen in the larger improvement from the synonymous questions generated by GPT-3.5
    compared to GPT-3.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.3 Results of Fed-DP-CoT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/59236f8ee011c3a74d46d8c9fc49d557.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The experiment of Fed-DP-CoT contains five steps:(1) Form a new test
    set by rephrasing the questions using different parameters in the benchmark manually
    and providing answers; (2) Extract consistent questions and answers in Fed-SP-SC
    experiment; (3) Add a disclaimer to form the CoT prompt; (4) Add the rephrased
    questions after the CoT prompt; (5) Prompt LLMs with entire CoT prompt and compared
    the answers with the rephrased answers for evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Fed-DP-CoT results.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Setting\Method | Zero-Shot-CoT |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Fed-DP-CoT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (GPT-3 Gen.) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fed-DP-CoT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (GPT-3.5 Gen.) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| GSM8K | 48.3% | 59.2% | 62.5% |'
  prefs: []
  type: TYPE_TB
- en: '| SVAMP | 76.5% | 82.4% | 85.7% |'
  prefs: []
  type: TYPE_TB
- en: We report results of Fed-DP-CoT on GSM8K and SVAMP in Table [4](#S4.T4 "Table
    4 ‣ 4.3 Results of Fed-DP-CoT ‣ 4 Experiment ‣ Federated Prompting and Chain-of-Thought
    Reasoning for Improving LLMs Answering"), and compare with the baseline Zero-Shot-CoT.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Fed-DP-CoT can improve the performance.* Compared to Zero-Shot-CoT, CoT Prompt(GPT-3
    Gen.) and CoT Prompt(GPT-3.5 Gen.) improve by approximately 10.9%-14.2% and 6.6%-10%
    respectively on the datasets GSM8K and SVAMP.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Fed-SP-SC performs better than Fed-DP-CoT.* The results of Fed-SP-SC (GPT-3
    Gen.) and Fed-SP-SC (GPT-3.5 Gen.) on the GSM8K and SVAMP datasets are both higher
    than Fed-DP-CoT (GPT-3 Gen.) and CoT Prompt (GPT-3.5 Gen.), with an approximate
    improvement of 5%.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Less significant performance difference between GPT-3 Gen. and GPT-3.5 Gen.
    compared to Fed-SP-SC experiment.* The reason for this is the disparity in parameters
    employed, coupled with the lack of emphasis on synonym usage in the CoT prompt.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.4 Ablation studies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/433ce48505dbe7f25b41f3d15dca258a.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) GSM8K
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5472ab346020a52db7685a6507791ec8.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) SVAMP
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Ablation study of choice of sampled reasoning paths.'
  prefs: []
  type: TYPE_NORMAL
- en: The number of reasoning paths for self-consistency. We study the effect of using
    different number of sampled reasoning paths for Fed-SP-SC (Sec. [4.2](#S4.SS2
    "4.2 Results of Fed-SP-SC ‣ 4 Experiment ‣ Federated Prompting and Chain-of-Thought
    Reasoning for Improving LLMs Answering")) to apply self-consistency. We conduct
    hyper-parameter search with a subset of the data for this ablation study due to
    the limits of accesses of the OpenAI API.
  prefs: []
  type: TYPE_NORMAL
- en: We vary the number of sampled reasoning paths of synonymous questions from one
    to nine. Figure [6](#S4.F6 "Figure 6 ‣ 4.4 Ablation studies ‣ 4 Experiment ‣ Federated
    Prompting and Chain-of-Thought Reasoning for Improving LLMs Answering") shows
    that increasing the number of sampled reasoning paths of the synonymous questions
    does not always improve the accuracy of the model.
  prefs: []
  type: TYPE_NORMAL
- en: In the line chart, as the number of sampled reasoning paths increases from one
    to five, the accuracy rate gradually increases. However, when the number of synonymous questions
    exceeds five, the accuracy of the model starts to decrease.
  prefs: []
  type: TYPE_NORMAL
- en: We speculate that this is because introducing synonymous questions also introduces
    noisy phrases, causing a deviation in the semantic meaning of the original questions.
    This deviation is particularly evident in synonymous questions generated by GPT-3
    (blue lines), while the generation results of GPT-3.5 (orange lines) exhibit stronger
    robustness.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: GSM8K disclaimer ablation.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method\Setting |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Zero-shot &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -CoT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fed-DP-CoT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (GPT-3 Gen.) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fed-DP-CoT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (GPT-3.5 Gen.) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| w/o disclaimer | 48.3% | 57.7% | 60% |'
  prefs: []
  type: TYPE_TB
- en: '| w/ disclaimer | NA | 59.2% | 62.5% |'
  prefs: []
  type: TYPE_TB
- en: Disclaimer We investigate whether the disclaimer is effective of correcting
    noisy CoTs in this ablation experiment. As Zero-Shot-CoT does not employ pseudo-labels,
    we do not conduct disclaimer ablation on it. Table [5](#S4.T5 "Table 5 ‣ 4.4 Ablation
    studies ‣ 4 Experiment ‣ Federated Prompting and Chain-of-Thought Reasoning for
    Improving LLMs Answering") compares the DP-questions answering accuracy with disclaimer
    or without disclaimer. We observe that the addition of a disclaimer in the questions
    and answers generated by GPT-3 resulted in an increase in accuracy from 57.7%
    to 59.2% for the Fed-DP-CoT task. Similarly, in the case of questions and answers
    generated by GPT-3.5, the accuracy increase from 60% to 62.5%. These results indicate
    that the use of a simple disclaimer can potentially improve the accuracy of LLMs
    by approximately 2% for the Fed-DP-CoT task. We postulate that the improvement
    in accuracy may be attributed to the fact that the disclaimer prompts LLMs to
    be careful of the pseudo-labels and self-examine the reasoning steps.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We investigate the potential benefits of employing synonymous queries from distributed
    users to enhance question-answering beyond what is achievable by a single user.
    Specifically, we explore the use of such queries in a federated manner by extracting
    two common user scenarios whereby the cloud database retrieves either SP- or DP-questions.
    To address these scenarios, we propose the application of self-consistency to
    identify the most consistent answers for SP-questions and utilize them as CoT
    to improve the answers provided for DP-questions. Our experimental results demonstrate
    that this approach yields a significant boost in performance compared to standalone
    zero-shot QA.
  prefs: []
  type: TYPE_NORMAL
- en: Moving forward, future research may investigate the implementation of more realistic
    systems that can efficiently retrieve federated questions while also improving
    CoT correctness to further advance reasoning capabilities. In this study, we assumed
    the DP-questions have already been stored with their answers generated by LLMs,
    and the consistent answers have been generated. Future work can further extend
    to scenarios that part of the DP-questions have no answers or pseudo-answers.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P.,
    Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are
    few-shot learners. Advances in neural information processing systems 33, 1877–1901
    (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts,
    A., Barham, P., Chung, H.W., Sutton, C., Gehrmann, S., et al.: Palm: Scaling language
    modeling with pathways. arXiv preprint arXiv:2204.02311 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Cobbe, K., Kosaraju, V., et al.: Training verifiers to solve math word
    problems. arXiv preprint arXiv:2110.14168 (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of
    deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805
    (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Fan, C., Hu, J., Huang, J.: Private semi-supervised federated learning.
    In: IJCAI. pp. 2009–2015 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Fan, C., Huang, J.: Federated few-shot learning with adversarial learning.
    In: 2021 19th International Symposium on Modeling and Optimization in Mobile,
    Ad hoc, and Wireless Networks (WiOpt). pp. 1–8\. IEEE (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Huang, J., Gu, S.S., Hou, L., Wu, Y., Wang, X., Yu, H., Han, J.: Large
    language models can self-improve. arXiv preprint arXiv:2210.11610 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Kojima, T., Gu, S.S., Reid, M., Matsuo, Y., Iwasawa, Y.: Large language
    models are zero-shot reasoners. arXiv preprint arXiv:2205.11916 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Levine, Y., Dalmedigos, I., Ram, O., Zeldes, Y., Jannai, D., Muhlgay, D.,
    Osin, Y., Lieber, O., Lenz, B., Shalev-Shwartz, S., et al.: Standing on the shoulders
    of giant frozen language models. arXiv preprint arXiv:2204.10019 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] McMahan, H.B., Moore, E., Ramage, D., Hampson, S., y Arcas, B.A.: Communication-efficient
    learning of deep networks from decentralized data. In: AISTATS (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Ongaro, D., Ousterhout, J.: In search of an understandable consensus algorithm.
    In: Proceedings of the 2014 USENIX Conference on USENIX Annual Technical Conference.
    p. 305–320 (2014)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] OpenAI: Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.L., Mishkin,
    P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al.: Training language models
    to follow instructions with human feedback. arXiv preprint arXiv:2203.02155 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Patel, A., Bhattamishra, S., Goyal, N.: Are NLP models really able to
    solve simple math word problems? In: Proceedings of the 2021 Conference of the
    North American Chapter of the Association for Computational Linguistics: Human
    Language Technologies. pp. 2080–2094 (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K.,
    Zettlemoyer, L.: Deep contextualized word representations. In: ACL. pp. 2227–2237
    (2018). https://doi.org/10.18653/v1/N18-1202'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., Catanzaro,
    B.: Megatron-lm: Training multi-billion parameter language models using model
    parallelism. arXiv preprint arXiv:1909.08053 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A.,
    Cheng, H.T., Jin, A., Bos, T., Baker, L., Du, Y., et al.: Lamda: Language models
    for dialog applications. arXiv preprint arXiv:2201.08239 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix,
    T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient
    foundation language models. arXiv preprint arXiv:2302.13971 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery,
    A., Zhou, D.: Self-consistency improves chain of thought reasoning in language
    models (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Wankhade, M., Rao, A.C.S., Kulkarni, C.: A survey on sentiment analysis
    methods, applications, and challenges. Artificial Intelligence Review 55(7), 5731–5780
    (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., Zhou, D.:
    Chain of thought prompting elicits reasoning in large language models. arXiv preprint
    arXiv:2201.11903 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Xu, Y., Zhu, C., et al.: Human parity on commonsenseQA: Augmenting self-attention
    with external attention. arXiv preprint arXiv:2112.03254 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Yadav, D., Desai, J., Yadav, A.K.: Automatic text summarization methods:
    A comprehensive review. arXiv preprint arXiv:2204.01849 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Zaib, M., Zhang, W.E., Sheng, Q.Z., Mahmood, A., Zhang, Y.: Conversational
    question answering: A survey. Knowledge and Information Systems 64(12), 3151–3195
    (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Zhao, W.X., Zhou, K., et al.: A survey of large language models. arXiv
    preprint arXiv:2303.18223 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Zhao, Y., Li, M., Lai, L., Suda, N., Civin, D., Chandra, V.: Federated
    learning with non-iid data. arXiv preprint arXiv:1806.00582 (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
