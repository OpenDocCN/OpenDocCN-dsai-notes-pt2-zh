- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:45:45'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Few shot chain-of-thought driven reasoning to prompt LLMs for open ended medical
    question answering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.04890](https://ar5iv.labs.arxiv.org/html/2403.04890)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Ojas Gramopadhye {ojas.gramopadhye@, yatin.nandwani@, diraghu1@in, jsachind@in}.ibm.com
    Saeel Sandeep Nachane {20d180028@, prateekch@cse ,ganesh@cse, kshitij.jadhav@}.iitb.ac.in
    Prateek Chanda {20d180028@, prateekch@cse ,ganesh@cse, kshitij.jadhav@}.iitb.ac.in
    Ganesh Ramakrishnan {20d180028@, prateekch@cse ,ganesh@cse, kshitij.jadhav@}.iitb.ac.in
    Kshitij Sharad Jadhav {20d180028@, prateekch@cse ,ganesh@cse, kshitij.jadhav@}.iitb.ac.in
    Yatin Nandwani {ojas.gramopadhye@, yatin.nandwani@, diraghu1@in, jsachind@in}.ibm.com
    Dinesh Raghu {ojas.gramopadhye@, yatin.nandwani@, diraghu1@in, jsachind@in}.ibm.com
    Sachindra Joshi {ojas.gramopadhye@, yatin.nandwani@, diraghu1@in, jsachind@in}.ibm.com
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Large Language models (LLMs) have demonstrated significant potential in transforming
    healthcare by automating tasks such as clinical documentation, information retrieval,
    and decision support. In this aspect, carefully engineered prompts have emerged
    as a powerful tool to use LLMs for medical scenarios, e.g., patient clinical scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we propose a modified version of the MedQA-USMLE dataset, which
    is subjective, to mimic real-life clinical scenarios. We explore the Chain of
    Thought (CoT) reasoning based on subjective response generation for the modified
    MedQA-USMLE dataset with appropriate LM driven forward-reasoning for correct responses
    to the medical questions. Keeping in mind the importance of response verification
    in the medical setting, we utilize a reward training mechanism whereby the language
    model also provides an appropriate verified response for a particular response
    to a clinical question. In this regard, we also include human-in-the-loop for
    different evaluation aspects. We develop better in-contrast learning strategies
    by modifying the 5-shot-codex-CoT-prompt from (Liévin et al., [2022](#bib.bib12))
    for the subjective MedQA dataset and developing our incremental-reasoning prompt.
    Our evaluations show that the incremental reasoning prompt performs better than
    the modified codex prompt in certain scenarios. We also show that greedy decoding
    with the incremental reasoning method performs better than other strategies, such
    as prompt chaining and eliminative reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: Few shot chain-of-thought driven reasoning to prompt LLMs for open ended medical
    question answering
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large Language models (LLMs) are increasingly utilized in the healthcare sector,
    particularly for patient query-related tasks. These LLM-driven tools could potentially
    interpret and respond to patient inquiries, provide information on symptoms, diseases,
    treatments, and healthcare guidelines Thirunavukarasu et al. ([2023](#bib.bib17)).
    By analyzing vast amounts of medical literature and data, LLMs could also offer
    precise, up-to-date responses, improving patient education and engagement Singhal
    et al. ([2022](#bib.bib16)). The ability of LLMs to understand and process natural
    language queries makes them accessible and user-friendly, thus enhancing the patient
    experience and satisfaction Clusmann et al. ([2023](#bib.bib5)). As technology
    evolves, LLMs are expected to play a pivotal role in delivering personalized healthcare
    information, contributing to informed decision-making and better health outcomes
    Clusmann et al. ([2023](#bib.bib5)). However, with the recent advances in prompt
    engineering techniques in the Large Language Models space, there is an underlying
    requirement for accuracy and verifiability. In recent years, several methods in
    the space of verifiable LLMs have been proposed, such as Fact verification LLMs
    and COT-based LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a1db88cec0425911aa9c21b54e1187d0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Forward and Eliminative prompting strategies overview'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fact verification LLMs: Fact verification in large language models (LLMs) is
    an evolving field that spans multiple disciplines, including computational linguistics,
    artificial intelligence, and digital media. (Guo et al., [2022](#bib.bib7)) provides
    a comprehensive overview of automated fact-checking, defining it through stages
    such as claim detection, evidence retrieval, and claim verification. This framework
    emphasizes the multifaceted nature of fact-checking, which involves not only the
    assessment of a claim’s veracity but also the identification of relevant evidence
    and the generation of explanations for the verdicts given.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Chain-of-Thought Prompting: Initially, scaling language models up appeared
    to benefit more knowledge-intensive tasks than reasoning-heavy ones (Rae et al.,
    2021). Nevertheless, Wei et al. ([2022](#bib.bib21)) demonstrated that LLMs could
    be applied to System 2 problems by prompting the model to generate step-by-step
    solutions, coined “Chain-of-Thought” (CoT). CoT prompting led to substantial improvements
    in many reasoning-intensive tasks Wei et al. ([2022](#bib.bib21)), Zhou et al.
    ([2022](#bib.bib23)),'
  prefs: []
  type: TYPE_NORMAL
- en: 'Drozdov et al. ([2022](#bib.bib6)); Nye et al. ([2021](#bib.bib14))), allowing
    to bridge the gap with human-level performances for most of the difficult BIG-bench
    tasks Chung et al. ([2022](#bib.bib4)). As an alternative to writing reference
    step-by-step solutions, zero-shot CoT (Kojima et al. ([2022](#bib.bib11))) allows
    for generating CoTs using single and domain-agnostic cues: “Let’s think step by
    step” (see example in Figure 1). The CoTs that result from that prompt not only
    appear to expose valid reasoning but also translate into superior zero-shot performances'
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 Key Contributions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We primarily focus on the USMLE-MedQA dataset (Jin et al., [2021](#bib.bib9)),
    an objective medical exam dataset. The dataset consists of questions sourced from
    professional medical board exams in the USA and is available in three languages:
    English, simplified Chinese, and traditional Chinese. The English subset utilized
    for this study consists of 12,723 questions, forming a comprehensive resource
    for investigating medical question answering in a standardized examination context.
    The current version of the dataset is not practically useful to be deployed in
    a patient query-based healthcare system since the responses to each such query
    are objective responses (either correct response or wrong response).'
  prefs: []
  type: TYPE_NORMAL
- en: Our contributions here are manifold
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose a healthcare-specific language model response generation task that
    utilizes the best prompting methods.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We demonstrate that prospective incremental reasoning-driven prompting mimicking
    real-life clinical scenarios performs significantly better at answering open-ended
    medical questions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We present efforts towards building a novel medical corpus that includes human-evaluated/verified
    subjective responses generated from a language model on the MedQA dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also contribute a modified MedQA dataset conducive to testing medical question-answering
    ability without options.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Background & Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recent advancements in the application of Large Language Models (LLMs) for medical
    question answering have highlighted the potential of these models to demonstrate
    applicability in medical diagnostics, education, and research. Venigalla et. al.,
    introduced BioMedLM, demonstrating its performance on several medical datasets,
    including MedQA, underscoring the capabilities of LLMs in handling complex medical
    queries (Venigalla et al., [2022](#bib.bib19); Jin et al., [2021](#bib.bib9)).
    Furthermore, Singhal et al.’s extensive experiments across a broad spectrum of
    medical question tasks within the MultiMedQA suite—encompassing datasets such
    as MedQA, MedMCQA, PubMedQA, and MMLU—have demonstrated the versatility and depth
    of knowledge encoded in these models (Singhal et al., [2022](#bib.bib16); Pal
    et al., [2022](#bib.bib15); Jin et al., [2019](#bib.bib10); Hendrycks et al.,
    [2020](#bib.bib8)).
  prefs: []
  type: TYPE_NORMAL
- en: The exploration of LLMs in generating not only accurate but also reasoning-based
    responses to medical questions marks a significant step forward. Models like PubMedGPT
    Bolton et al. ([2022](#bib.bib2)) and Codex (Liévin et al., [2022](#bib.bib12))
    have established benchmarks on datasets like MedQA through innovative approaches,
    including Classification head, Chain-of-Thought, and Knowledge Grounding, highlighting
    the importance of not just what is answered, but how the answer is derived. Advanced
    LLMs, such as Med-Palm2 Singhal et al. ([2022](#bib.bib16)), and Flan-Palm Chung
    et al. ([2022](#bib.bib4)), have further raised the bar for performance, although
    their limited availability poses challenges for widespread research and application
    in the medical field.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to these contributions, the work by authors such as Hendrycks et.
    al., on the Measuring Massive Multitask Language Understanding (MMLU) dataset
    presents a comprehensive evaluation of LLMs across a range of subjects, including
    medicine, pointing to the broad applicability and potential of LLMs beyond single-domain
    tasks (Hendrycks et al., [2020](#bib.bib8)). Another notable direction is the
    investigation into the interpretability and explainability of model predictions,
    as highlighted by Mesinovic et. al.,, which is critical for trust and reliability
    in medical applications (Mesinovic et al., [2023](#bib.bib13)).
  prefs: []
  type: TYPE_NORMAL
- en: '| MedQA-Original (MCQ Type) | MedQA-No-Opt (Descriptive Type) |'
  prefs: []
  type: TYPE_TB
- en: '| Question: Four weeks after starting hydrochlorothiazide, a 49-year-old man
    with hypertension comes to the physician because of muscle cramps and weakness.
    His home medications also include amlodipine. His blood pressure today is 176/87
    mm Hg. Physical examination shows no abnormalities. The precordial leads of a
    12-lead ECG are shown. The addition of which of the following is most likely to
    have prevented this patient’s condition? (A) Torsemide (B) Nifedipine (C) Eplerenone
    (D) Hydralazine | Question: Four weeks after starting hydrochlorothiazide, a 49-year-old
    man with hypertension comes to the physician because of muscle cramps and weakness.
    His home medications also include amlodipine. His blood pressure today is 176/87
    mm Hg. Physical examination shows no abnormalities. The precordial leads of a
    12-lead ECG are shown. The addition of what is most likely to have prevented this
    patient’s condition? |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Sample Question from the USMLE-MedQA dataset along with its modified
    form. For more details, check Appendix A'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/13aae22efdc2faabec77da69cfd8dd0b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Codex COT looks at generating options in a more eliminative approach,
    often not catering to the context of clinical investigation, unlike differential
    diagnostics as per MedCodex'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/169ef6a36aa47e4947fe2df7c7ecb275.png)'
  prefs: []
  type: TYPE_IMG
- en: ((a)) MedQA-Original Codex vs MedCodex
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/34ed3fee6a7421731719057d0b53ebc5.png)'
  prefs: []
  type: TYPE_IMG
- en: ((b)) MedQA-No-Opt Codex vs MedCodex
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: Illustrative Example: Showcasing different Prompting Strategies across
    two dataset variants. context of the answer and corresponding reasoning and Future
    actions are highlighted.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Problem Setting and Preliminary Notations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let us consider a language model at inference time $\mathcal{L}\mathcal{M}_{\boldsymbol{\theta}}:\mathcal{X}\rightarrow\mathcal{Y}$.
  prefs: []
  type: TYPE_NORMAL
- en: Let the prompt $\boldsymbol{\mathcal{P}}=\langle q_{i},\boldsymbol{\mathcal{O}_{i}},\mathcal{R}^{i}\rangle$
    includes the corresponding reasoning quality as well as the correctness of the
    response based on ground truth.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments, Evaluation Methods and Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Datasets & Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Datasets The MedQA (Zhang et al., [2018](#bib.bib22)) dataset consists of multiple-choice
    questions based on United States Medical License Exams (USMLE). We modify the
    questions to seek descriptive responses and not remain objective multiple choice
    questions. We consider two variations of the MedQA dataset for our further experimentation.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MedQA-Original : We utilize the original MCQ type options format of the MedQA
    dataset, where we have the following $\langle\textit{Question, Option List}\rangle$.
    A sample is shown in Table [1](#S2.T1 "Table 1 ‣ 2 Background & Related Work ‣
    Few shot chain-of-thought driven reasoning to prompt LLMs for open ended medical
    question answering").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MedQA-No-Opt : Here, we consider the following format where the option list
    for a particular question $q_{i}$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.1.1 Conversion of MCQ type questions to descriptive type
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The conversion of MCQ-type questions (MedQA-Original) to descriptive questions
    (MedQA-No-Opt) is aimed to emulate real-world medical scenarios where open-ended
    inquiries are prevalent. This modification required our model to respond without
    predefined choices, fostering holistic reasoning and integration of diverse knowledge
    sources. By eliminating answer options, we assessed the model’s depth and quality
    of reasoning skills, ensuring a more realistic evaluation of its performance in
    complex medical scenarios. To cater to a descriptive scenario, we slightly modify
    the question $q_{i}$ is done under clinical supervision with the help of medical
    experts.
  prefs: []
  type: TYPE_NORMAL
- en: Models We use Llama2-7B chat and Llama2-70B chat model (Touvron et al., [2023](#bib.bib18))
    for our entire experimental evaluations.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Prompting strategies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is evident to see how we can utilize different prompting strategies for $\boldsymbol{\mathcal{P}}$
    i.e., how we would like the model to reason about reaching to a particular correct
    response.
  prefs: []
  type: TYPE_NORMAL
- en: 'Codex FewShot Prompts: Such prompting strategies first introduced in (Chen
    et al., [2021](#bib.bib3)) and subsequently in medical literature (Liévin et al.,
    [2022](#bib.bib12)) have shown to perform well on MCQ type questionnaire. For
    an MCQ-type question, the reasoning structure seems to follow the pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{R}_{i}=\{o_{j}^{i}\in\boldsymbol{\mathcal{O}_{i}},r_{ij}\}_{j=1}^{m}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: However, such kind of prompting strategies tends to be eliminative in nature,
    as shown in Fig. [2](#S2.F2 "Figure 2 ‣ 2 Background & Related Work ‣ Few shot
    chain-of-thought driven reasoning to prompt LLMs for open ended medical question
    answering") and often contradicts real-life clinical scenarios where prospective
    additive reasoning is needed for reaching a differential diagnosis. Further, the
    Codex prompt builds the entire context in one shot to reach the final answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'MedCodex FewShot Prompts: Here, we utilize the intermittent reasoning structure
    given a unique medical context which builds upon every additive context in answering
    clinical questions that mimics the usual way of reaching a final diagnosis. In
    real clinical scenarios, there are no four options to choose from; the clinician
    takes a medical history, forms the mental structure for differential diagnosis,
    performs examinations, adds or deletes the potential diagnosis based on contextual
    information, and then ultimately takes into consideration the laboratory investigations
    to finally reach the diagnosis. The MedCodex FewShot Prompts mimics this strategy
    to reach a final answer.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Few shot chain-of-thought driven
    reasoning to prompt LLMs for open ended medical question answering") refers to
    different classifications of prompting strategies based on their reasoning structure
    as well as their basis on the corresponding dataset (MCQ/ Descriptive Questions).
    In both prompt strategies, we use few shots $K=5$.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Codex FewShot Prompts vs MedCodex FewShot Prompts on Original MedQA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Experimental Setup: We first utilize MedCodex FewShot Prompts due to its underlying
    prospective incremental reasoning structure, which typically follows the line
    of reasoning employed by medical professionals. Along with this, we compare responses
    collected using Codex FewShot Prompts where the reasoning is more eliminative
    in nature based on seeing the options and less attention is provided towards the
    clinical flow of argument (which also involves forming the entire clinical context
    in one go). The Llama2 70B Base and 70B Chat model were prompted using both strategies.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation Strategy: The results of this experiment on the MedQA-Original’s
    1273 questions from the Test set are described in Table [2](#S4.T2 "Table 2 ‣
    4.3 Codex FewShot Prompts vs MedCodex FewShot Prompts on Original MedQA ‣ 4 Experiments,
    Evaluation Methods and Results ‣ Few shot chain-of-thought driven reasoning to
    prompt LLMs for open ended medical question answering")'
  prefs: []
  type: TYPE_NORMAL
- en: 'Observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | Accuracy Scores |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-70B-Base (Codex) | 54% |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-70B-Base (MedCodex) | 23% |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-70B-Chat (Codex) | 52% |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-70B-Chat (MedCodex) | 50% |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: CODEX FEWSHOT PROMPTS vs MEDCODEX FEWSHOT PROMPTS on Llama2-7B and
    Llama2-70B base and chat models'
  prefs: []
  type: TYPE_NORMAL
- en: 'Interpretation: On the MedQA-Original, dataset, the MedCodex FewShot Prompts
    performed poorly as compared to Codex FewShot Prompts both on Llama2-70B base
    and 70B Chat model. Due to its inherent eliminative approach, the Codex FewShot
    Prompts has a smaller search space and results in higher accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Codex FewShot Prompts vs. MedCodex FewShot Prompts on Descriptive MedQA
    questions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Experimental Setup: We perform the experiment of comparing a modified version
    of the Codex FewShot Prompts and MedCodex FewShot Prompts on MedQA-No-Opt dataset.
    The Codex prompt is modified to remove options and their labels, to make it suited
    for a no options setting (Section: [4.2](#S4.SS2 "4.2 Prompting strategies ‣ 4
    Experiments, Evaluation Methods and Results ‣ Few shot chain-of-thought driven
    reasoning to prompt LLMs for open ended medical question answering")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation Strategy: We select 100 questions from MedQA-No-Opt and evaluate
    via medical experts the final reasoning quality and final answer on a 3 point
    Likert scale (Batterton and Hale, [2017](#bib.bib1)) Agree, Neutral, Disagree.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Observations: As indicated in Figures [6](#S4.F6 "Figure 6 ‣ 4.6.1 Differential
    diagnosis generation with Trained Verifier on MedQA-on-opt Dataset ‣ 4.6 Experiments
    with Verifier ‣ 4 Experiments, Evaluation Methods and Results ‣ Few shot chain-of-thought
    driven reasoning to prompt LLMs for open ended medical question answering") and
    [7](#S4.F7 "Figure 7 ‣ 4.6.1 Differential diagnosis generation with Trained Verifier
    on MedQA-on-opt Dataset ‣ 4.6 Experiments with Verifier ‣ 4 Experiments, Evaluation
    Methods and Results ‣ Few shot chain-of-thought driven reasoning to prompt LLMs
    for open ended medical question answering"), for 82% and 77.85% of the questions,
    the expert medical personnel agreed with the reasoning and the evaluation provided
    by the MedCodex FewShot Prompts after prompting Llama2-7B-chat and Llama2-70B-chat
    models respectively. However, this was 72% and 82.5% subsequent to prompting Llama2-7B
    and Llama2-70B models, respectively, in the case of Codex FewShot Prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Interpretations: Given the open-ended nature of MedQA-No-Opt dataset, the incremental
    generative approach of reasoning to reach the final answer by the MedCodex FewShot
    Prompts is qualitatively superior compared to Codex FewShot Prompts for Llama2-7B-chat
    model. This could be because the Codex FewShot Prompts is designed to work best
    on a restricted universe of options for MCQ-based questions. This also indicates
    that mimicking the human process of reaching the final answer in diverse medical
    scenarios (as performed by MedCodex FewShot Prompts) is better in scenarios of
    open-ended questions while prompting smaller parameter models. However, Codex
    FewShot Prompts is better able to leverage a larger model (Llama2-70B-chat).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Differential diagnosis generation with Codex for Selection on MedQA-no-opt
    Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Experimental Setup: Each question $\boldsymbol{q_{i}}$ with the Codex FewShot
    Prompts to select the most appropriate option. To create a set of 4 options through
    sampling, 10 unique options are sampled by filtering on a word-level match basis
    (to avoid selecting options that are repetitive). Out of these, the top 4 are
    selected through the perplexity of model outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation Methods: We evaluate via medical expert the final reasoning quality
    and final answer on a 3-point Likert scale (Batterton and Hale, [2017](#bib.bib1))
    Agree, Neutral, Disagree.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Observations: As indicated in Figures [6](#S4.F6 "Figure 6 ‣ 4.6.1 Differential
    diagnosis generation with Trained Verifier on MedQA-on-opt Dataset ‣ 4.6 Experiments
    with Verifier ‣ 4 Experiments, Evaluation Methods and Results ‣ Few shot chain-of-thought
    driven reasoning to prompt LLMs for open ended medical question answering") and
    [7](#S4.F7 "Figure 7 ‣ 4.6.1 Differential diagnosis generation with Trained Verifier
    on MedQA-on-opt Dataset ‣ 4.6 Experiments with Verifier ‣ 4 Experiments, Evaluation
    Methods and Results ‣ Few shot chain-of-thought driven reasoning to prompt LLMs
    for open ended medical question answering"), for 80% and 89.5% of the questions,
    the expert medical personnel agreed with the reasoning and the evaluation provided
    by the Differential diagnosis generation (using MedCodex FewShot Prompts) followed
    by Codex FewShot Prompts for Selection on MedQA-No-Opt Dataset after prompting
    Llama2-7B-chat and Llama2-70B-chat parameter models respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Interpretation: Since the MedQA-No-Opt dataset questions are open-ended, we
    tried to restrict the pool from which the correct answers could be selected by
    generating four top answers. This was inspired by the Self-consistency approach
    of Codex Few Shot paper Wei et al. ([2022](#bib.bib21)). As expected, the larger
    Llama2-70B model prompting worked better than the Llama2-7B model. Importantly,
    this process was significantly better than all prompting strategies indicating
    the utility of leveraging the option creation process using MedCodex FewShot Prompts
    and subsequent selection by Codex FewShot Prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.6 Experiments with Verifier
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Motivation for Verifier and its training So far, we have approached the problem
    with an in-context learning perspective without changing the model parameters.
    We believe building on top of powerful models like the Llama2 chat series could
    give us a significant advantage in this scenario. We endeavored to improve performance
    by substituting the Codex FewShot Prompts prompt with a verifier Reward learning
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Verifier Dataset Contribution The training dataset for the verifier $\boldsymbol{\mathcal{D}_{ver}}$
    was constructed by gathering medical expert evaluations towards the correctness
    of model-generated responses. A representative sample of 100 questions was selected
    from the MedQA-No-Opt and divided into two subsets for inter-annotator agreement
    evaluation. Multiple medical experts independently assessed the correctness of
    responses generated by the Llama2-70B chat model, utilizing a newly designed verification
    prompt on the 3-point scale. The response sets encompassed both positive (correct
    answer) and negative (incorrect answer) pairs to evaluate the verifier’s ability
    to distinguish between them. Anonymity was maintained by not disclosing the actual
    correctness of the responses to the medical experts. The medical experts exhibited
    a commendable ability to discern the reasoning behind incorrect answers, emphasizing
    the value of their annotations for training the verifier. This annotated data
    was employed to train our reward model, with positive pairs acting as chosen pairs
    and negative pairs as rejected pairs.
  prefs: []
  type: TYPE_NORMAL
- en: We use pairwise “chosen-rejected” pairs of strings consisting of $\langle\boldsymbol{q}_{i};\boldsymbol{\mathcal{R}}_{i};\boldsymbol{\mathcal{A}}_{i}\rangle$.
    We use the MedQA-Original dataset with 4 options to create these pairs. Here,
    we take the known correct option for the ‘chosen’ part and switch the incorrect
    options for the ‘rejected’ part to create three pairs with one question sample.
    We borrow the Question and Answer parts of the triplet from the MedQA dataset
    and generate the Reasoning artificially by prompting the Llama2-7b-chat model.
    These results have been verified by medical experts. The prompt and input format
    for this are given below in Fig [4](#S4.F4 "Figure 4 ‣ 4.6 Experiments with Verifier
    ‣ 4 Experiments, Evaluation Methods and Results ‣ Few shot chain-of-thought driven
    reasoning to prompt LLMs for open ended medical question answering").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fd06c07e3a6a2cd8e659480d16dbc418.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Verifier Reasoning Generation'
  prefs: []
  type: TYPE_NORMAL
- en: Training We have fine-tuned the Llama2 7B-chat model for the reward model training
    followed by a linear head. We are applying LoRA for fine-tuning using the trl
    library from Hugging Face for this purpose. The model is trained on a reward modeling
    loss from Wang et al. ([2024](#bib.bib20)).
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Where $\boldsymbol{q_{i}}$ denotes the reward margin. In our case, we utilize
    the reward margin = 0.
  prefs: []
  type: TYPE_NORMAL
- en: 4.6.1 Differential diagnosis generation with Trained Verifier on MedQA-on-opt
    Dataset
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Experimental Setup: We sample forward options as in the previous experiment
    [4.5](#S4.SS5 "4.5 Differential diagnosis generation with Codex for Selection
    on MedQA-no-opt Dataset ‣ 4 Experiments, Evaluation Methods and Results ‣ Few
    shot chain-of-thought driven reasoning to prompt LLMs for open ended medical question
    answering"), then pass the generated options with their reasoning, as shown in
    Figure [5](#S4.F5 "Figure 5 ‣ 4.6.1 Differential diagnosis generation with Trained
    Verifier on MedQA-on-opt Dataset ‣ 4.6 Experiments with Verifier ‣ 4 Experiments,
    Evaluation Methods and Results ‣ Few shot chain-of-thought driven reasoning to
    prompt LLMs for open ended medical question answering") figure, to the verifier.
    We choose the option that produces the highest scalar reward. This experiment
    was performed only for the Llama2 7B parameter model due to resource constraints.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation Methods: We select 100 questions from MedQA-No-Opt and evaluate
    via medical experts the final reasoning quality and final answer on a 3 point
    Likert scale (Batterton and Hale, [2017](#bib.bib1)) Agree, Neutral, Disagree.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Observations: As indicated in Figure [6](#S4.F6 "Figure 6 ‣ 4.6.1 Differential
    diagnosis generation with Trained Verifier on MedQA-on-opt Dataset ‣ 4.6 Experiments
    with Verifier ‣ 4 Experiments, Evaluation Methods and Results ‣ Few shot chain-of-thought
    driven reasoning to prompt LLMs for open ended medical question answering"), for
    86% of the questions, the expert medical personnel agreed with the reasoning and
    the evaluation provided by the Differential diagnosis generation (using MedCodex
    FewShot Prompts) followed by Verifier for Selection on MedQA-No-Opt Dataset after
    prompting Llama2 7B-chat model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Interpretation: The development of the Verifier substantially improves the
    ability of Llama2 7B parameter model to select the response with high quality
    of explainability as well as high accuracy, indicating the viability of such an
    approach to beat larger parameter models by alternative prompting strategies.
    We hypothesize that the Verifier being trained to identify good quality reasoning
    can result in better abilities to select the most probable answer subsequent to
    the 4 option generation driven by MedCodex FewShot Prompts. We suspect that testing
    this strategy on the Llama70B model would provide significantly better results.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/94b314398a42f52d3055709ff442238c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Verifier Input'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/36396fb00ded260bc0e2bc8fcad0edb9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Results for experiments on MedQA-no-opt dataset with Llama-2-7B-chat'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b267c728041ac76740e8831f70d57ea3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Results for experiments on MedQA-no-opt dataset with Llama-2-70B-chat'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion and Future work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The incremental reasoning chain of thought prompting is a novel prompting methodology
    developed by us that follows the usual clinical approach of reaching a decision
    in real-life clinical settings. We demonstrate that this strategy gives significantly
    better results than the CODEX prompting strategy, which is designed for MCQ-type
    questions. Further, we demonstrate that the verifier developed using reasoning
    performs much better at selecting agreeable responses from the Llama-2 models.
    Further research will focus on testing the Verifier-driven answer selection using
    the Llama2-70B-chat model, and we would explore the generalizability of this approach
    by testing on other open-source LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '5 Limitations:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper does not train an LLM from scratch and only leverages pre-trained
    models. The quality of the model response depends on the quality of the forward-looking
    prompt. The current process has been only demonstrated on Llama-2 models and needs
    to be tested on other models to demonstrate generalizability.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Batterton and Hale (2017) Katherine A Batterton and Kimberly N Hale. 2017. The
    likert scale what it is and how to use it. *Phalanx*, 50(2):32–39.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bolton et al. (2022) E Bolton et al. 2022. Pubmedgpt 2.7 b. Technical report,
    Technical report. Stanford University Center for Research on Foundation ….
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, et al. 2021. Evaluating large language models trained on code.
    *arXiv preprint arXiv:2107.03374*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay,
    William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
    2022. Scaling instruction-finetuned language models. *arXiv preprint arXiv:2210.11416*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clusmann et al. (2023) Jan Clusmann, Fiona R Kolbinger, Hannah Sophie Muti,
    Zunamys I Carrero, Jan-Niklas Eckardt, Narmin Ghaffari Laleh, Chiara Maria Lavinia
    Löffler, Sophie-Caroline Schwarzkopf, Michaela Unger, Gregory P Veldhuizen, et al.
    2023. The future landscape of large language models in medicine. *Communications
    Medicine*, 3(1):141.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drozdov et al. (2022) Andrew Drozdov, Nathanael Schärli, Ekin Akyürek, Nathan
    Scales, Xinying Song, Xinyun Chen, Olivier Bousquet, and Denny Zhou. 2022. Compositional
    semantic parsing with large language models. *arXiv preprint arXiv:2209.15003*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2022) Zhijiang Guo, Michael Schlichtkrull, and Andreas Vlachos.
    2022. A survey on automated fact-checking. *Transactions of the Association for
    Computational Linguistics*, 10:178–206.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask
    language understanding. *arXiv preprint arXiv:2009.03300*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jin et al. (2021) Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi
    Fang, and Peter Szolovits. 2021. What disease does this patient have? a large-scale
    open domain question answering dataset from medical exams. *Applied Sciences*,
    11(14):6421.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jin et al. (2019) Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W Cohen,
    and Xinghua Lu. 2019. Pubmedqa: A dataset for biomedical research question answering.
    *arXiv preprint arXiv:1909.06146*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners.
    *Advances in neural information processing systems*, 35:22199–22213.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liévin et al. (2022) Valentin Liévin, Christoffer Egeberg Hother, and Ole Winther.
    2022. Can large language models reason about medical questions? *arXiv preprint
    arXiv:2207.08143*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mesinovic et al. (2023) Munib Mesinovic, Peter Watkinson, and Tingting Zhu.
    2023. Explainable ai for clinical risk prediction: a survey of concepts, methods,
    and modalities. *arXiv preprint arXiv:2308.08407*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nye et al. (2021) Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk
    Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten
    Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation
    with language models. *arXiv preprint arXiv:2112.00114*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pal et al. (2022) Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu.
    2022. Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain
    question answering. In *Conference on Health, Inference, and Learning*, pages
    248–260\. PMLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singhal et al. (2022) Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi,
    Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen
    Pfohl, et al. 2022. Large language models encode clinical knowledge. *arXiv preprint
    arXiv:2212.13138*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thirunavukarasu et al. (2023) Arun James Thirunavukarasu, Darren Shu Jeng Ting,
    Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. 2023.
    Large language models in medicine. *Nature medicine*, 29(8):1930–1940.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Venigalla et al. (2022) A Venigalla, J Frankle, and M Carbin. 2022. Biomedlm:
    a domain-specific large language model for biomedical text. *MosaicML. Accessed:
    Dec*, 23(3):2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2024) Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang
    Huang, Wei Shen, Senjie Jin, Enyu Zhou, Chenyu Shi, Songyang Gao, Nuo Xu, Yuhao
    Zhou, Xiaoran Fan, Zhiheng Xi, Jun Zhao, Xiao Wang, Tao Ji, Hang Yan, Lixing Shen,
    Zhan Chen, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang, Zuxuan Wu, and Yu-Gang
    Jiang. 2024. [Secrets of rlhf in large language models part ii: Reward modeling](http://arxiv.org/abs/2401.06080).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems*, 35:24824–24837.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2018) Xiao Zhang, Ji Wu, Zhiyang He, Xien Liu, and Ying Su. 2018.
    Medical exam question answering with large-scale reading comprehension. In *Proceedings
    of the AAAI conference on artificial intelligence*, volume 32.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2022) Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu.
    2022. Learning to prompt for vision-language models. *International Journal of
    Computer Vision*, 130(9):2337–2348.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Appendix:'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Results of Human Evaluations of Llama-2-7B chat responses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '|  | Agree | Neutral | Disagree |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Medical Expert 1 | 88 | 10 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| Medical Expert 2 | 86 | 12 | 2 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Human evaluated results of reasoning of MedCodex-Greedy based approach
    for Set 1 (50 questions)'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Agree | Neutral | Disagree |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Medical Expert 1 | 82 | 2 | 16 |'
  prefs: []
  type: TYPE_TB
- en: '| Medical Expert 2 | 72 | 14 | 14 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Human evaluated results of reasoning of MedCodex-Greedy based approach
    for Set 2 (50 questions)'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Agree | Neutral | Disagree |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Medical Expert 1 | 76 | 18 | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| Medical Expert 2 | 28 | 24 | 48 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Human evaluated results of reasoning of MedCodex-Codex based approach
    for Set 1 (50 questions)'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Agree | Neutral | Disagree |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Medical Expert 1 | 72 | 14 | 14 |'
  prefs: []
  type: TYPE_TB
- en: '| Medical Expert 2 | 68 | 20 | 12 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Human evaluated results of reasoning of MedCodex-Codex based approach
    for Set 2 (50 questions)'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Agree | Neutral | Disagree |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Medical Expert 1 | 84 | 0 | 16 |'
  prefs: []
  type: TYPE_TB
- en: '| Medical Expert 2 | 86 | 4 | 8 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Human evaluated results of reasoning of MedCodex-Reward Model-based
    approach for Set 1 (50 questions)'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Agree | Neutral | Disagree |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Medical Expert 1 | 84 | 10 | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| Medical Expert 2 | 88 | 6 | 6 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: Human evaluated results of reasoning of MedCodex-Reward Model-based
    approach for Set 2 (50 questions)'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Results of Human Evaluations of Llama-2-70B chat responses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '|  | Agree | Neutral | Disagree |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Medical Expert 1 | 89 | 0 | 11 |'
  prefs: []
  type: TYPE_TB
- en: '| Medical Expert 2 | 92.3 | 7.7 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Medical Expert 3 | 87 | 2 | 11 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: Human evaluated results of reasoning of MedCodex-Greedy approach'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Agree | Neutral | Disagree |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Medical Expert 1 | 92 | 0 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| Medical Expert 2 | 72 | 18 | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| Medical Expert 3 | 76 | 14 | 10 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10: Human evaluated results of reasoning of Codex approach'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Agree | Neutral | Disagree |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Medical Expert 1 | 87 | 6 | 7 |'
  prefs: []
  type: TYPE_TB
- en: '| Medical Expert 2 | 92 | 0 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| Medical Expert 3 | 89 | 1 | 7 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 11: Human evaluated results of reasoning of MedCodex-Codex approach'
  prefs: []
  type: TYPE_NORMAL
