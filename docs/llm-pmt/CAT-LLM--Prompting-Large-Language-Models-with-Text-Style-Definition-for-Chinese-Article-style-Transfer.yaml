- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:47:09'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'CAT-LLM: Prompting Large Language Models with Text Style Definition for Chinese
    Article-style Transfer'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2401.05707](https://ar5iv.labs.arxiv.org/html/2401.05707)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Zhen Tao¹    Dinghao Xi¹    Zhiyu Li²    Liumin Tang¹&Wei Xu¹ ¹School of Information,
    Renmin University of China
  prefs: []
  type: TYPE_NORMAL
- en: ²Institute for Advanced Algorithms Research, Shanghai {taozhen, xidinghao}@ruc.edu.cn,
    lizy@iaar.ac.cn, {tangliumin, weixu}@ruc.edu.cn Corresponding author
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Text style transfer is increasingly prominent in online entertainment and social
    media. However, existing research mainly concentrates on style transfer within
    individual English sentences, while ignoring the complexity of long Chinese texts,
    which limits the wider applicability of style transfer in digital media realm.
    To bridge this gap, we propose a Chinese Article-style Transfer framework (CAT-LLM),
    leveraging the capabilities of Large Language Models (LLMs). CAT-LLM incorporates
    a bespoke, pluggable Text Style Definition (TSD) module aimed at comprehensively
    analyzing text features in articles, prompting LLMs to efficiently transfer Chinese
    article-style. The TSD module integrates a series of machine learning algorithms
    to analyze article-style from both words and sentences levels, thereby aiding
    LLMs thoroughly grasp the target style without compromising the integrity of the
    original text. In addition, this module supports dynamic expansion of internal
    style trees, showcasing robust compatibility and allowing flexible optimization
    in subsequent research. Moreover, we select five Chinese articles with distinct
    styles and create five parallel datasets using ChatGPT, enhancing the models’
    performance evaluation accuracy and establishing a novel paradigm for evaluating
    subsequent research on article-style transfer. Extensive experimental results
    affirm that CAT-LLM outperforms current research in terms of transfer accuracy
    and content preservation, and has remarkable applicability to various types of
    LLMs. Source code is available at GitHub¹¹1https://github.com/TaoZhen1110/CAT-LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Text style transfer plays a crucial role in the field of Natural Language Processing
    (NLP). Its core objective is to adjust the style of the text while retaining the
    information from the original content Toshevska and Gievska ([2021](#bib.bib22));
    Yi et al. ([2021](#bib.bib25)). Recent advancements have been particularly notable
    in areas like news detection Przybyla ([2020](#bib.bib15)) and social media Wang
    et al. ([2023](#bib.bib24)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ad55d5094ca085cb7d670dfe52efc9c6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: An example that transfer stylishless text to “The Scream” style text.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before the advent of LLMs, the frameworks for text style transfer predominantly
    rely on the small deep learning models. In accordance with the process of text
    transfer, these frameworks generally fall into two categories: end-to-end models
    and two-stage models. However, these models exhibit lower transfer accuracy and
    are mainly effective for simple style transfer tasks, such as emotion Yi et al.
    ([2021](#bib.bib25)), politeness Danescu-Niculescu-Mizil et al. ([2013](#bib.bib4)),
    and etiquette Sheikha and Inkpen ([2010](#bib.bib19)). Due to the lack of parallel
    corpora, previous researches could only use non-parallel datasets to train classifiers
    for evaluating the transfer accuracy of unsupervised models. It is worth noting
    that an author’s style is often highly associated with specific writing themes.
    For example, the term (Camel Xiangzi) essentially only appears in the works of
    Lao She. Therefore, classifiers trained on non-parallel datasets may result in
    significant evaluation errors. Furthermore, existing studies primarily focus on
    English sentences level style transfer, with limited research dedicated to Chinese
    long texts. Considering the distinct linguistic structures, expressions, and cultural
    nuances of Chinese, it is necessary to design models specifically tailored for
    style transfer in Chinese long texts. Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ CAT-LLM: Prompting Large Language Models with Text Style Definition for Chinese
    Article-style Transfer") shows an example of Chinese article-style transfer from
    ordinary vernacular to a fragment of the article “The Scream”.'
  prefs: []
  type: TYPE_NORMAL
- en: Recently, LLMs have demonstrated the ability to handle more complex NLP tasks
    such as summarization generation and role-playing through zero-shot transfer without
    the need for dedicated task-specific training Brown et al. ([2020](#bib.bib1));
    Song et al. ([2023](#bib.bib21)), attracting significant attention from both academia
    and industry. Given the strong capabilities of LLMs in semantic understanding
    and text generation, they can effectively address challenges faced by small models
    of style transfer, such as the lack of parallel data and low transfer accuracy.
    Recent studies have made significant headway in text style transfer leveraging
    LLMs. Lai et al. Lai et al. ([2023](#bib.bib10)) employ ChatGPT to evaluate the
    transfer accuracy and content preservation of various text style transfer models.
    Shanahan Shanahan et al. ([2023](#bib.bib17)) and Wang Wang et al. ([2023](#bib.bib24))
    explore role-play text generation in conversational agents using LLMs. However,
    designating unfamiliar authors may trigger hallucinatory issues in LLMs. In addition,
    considering the diversity and complexity of Chinese, including idioms, rhetorical
    devices, directly reading articles with LLMs not only consumes more tokens, affecting
    inference speed, but also results in insufficient understanding of the article’s
    style when only reading partial fragments. Furthermore, these approaches also
    lack guidance and interpretability.
  prefs: []
  type: TYPE_NORMAL
- en: 'To overcome the above challenges, we propose a Chinese Article-style Transfer
    framework (CAT-LLM) based on LLMs. Firstly, CAT-LLM combines various machine learning
    algorithms to comprehensively learn article styles, creating a detailed style
    definition. Our analysis focuses on words and sentences levels styles, which are
    not only academically reasonable but also easy to understand. Then, we design
    style-enhanced prompts for style transfer based on style definition. Coupled with
    the LLMs’ powerful capabilities in semantic understanding and long-text generation,
    we obtain the final transfer text. In addition, we select five classic Chinese
    literary works and use ChatGPT to generate stylishless text, creating large parallel
    datasets to support more accurate performance evaluation of style transfer models.
    Extensive experiments demonstrate that our framework surpasses state-of-the-art
    researches in both style accuracy and content preservation. The primary contributions
    of this paper are summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose CAT-LLM, a pioneering Chinese Article-style Transfer framework utilizing
    LLMs for the first time in chinese article-style transfer. Extensive experiments
    demonstrate that CAT-LLM surpasses state-of-the-art models in style transfer accuracy
    and content preservation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We engineer a Text Style Definition (TSD) module that integrates various small
    models to proficiently summarize text styles at the words and sentences levels.
    This module not only enhances the LLMs’ style transfer precision, but also supports
    dynamic expansion of internal style trees, allowing for flexible optimization
    in subsequent research.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We create five parallel datasets of Chinese article-style transfer to assess
    the models’ performance more accurately, offering a novel evaluation paradigm
    for subsequent research.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Text Style Transfer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Text style transfer has been a focus in NLP research, attracting widespread
    attention in computer science and literature. Hu et al. Hu et al. ([2017](#bib.bib6))
    pioneer the introduction of the concept of style transfer from image to text,
    laying a cornerstone for subsequent explorations in text style transfer. With
    the development of NLP, an increasing number of text style transfer methods have
    emerged. Based on the transfer processes, we can categorize previous works into
    two main families. The first family treats the content and style of a sentence
    as separate entities. It involves stripping the original style from the content
    and supplanting it with the desired style. Tian et al. Lee et al. ([2021](#bib.bib11))
    propose using reverse attention to implicitly remove style information from each
    token to preserve the original content, and introducing conditional layer normalization
    to construct content-dependent style representations. StoryTrans Zhu et al. ([2023](#bib.bib27))
    utilize discourse representations to capture source content information and transfer
    it into the target style through learnable style embeddings. However, in sentences
    and paragraphs, certain tokens mainly reflect distinct personal characteristics.
    Consequently, this family face challenges in guaranteeing the integrity of the
    original textual content. The second family directly designs an end-to-end model
    for style transfer. The Style Transformer Dai et al. ([2019](#bib.bib3)) leverages
    the Transformer as the backbone network, directly supplying the target style embedding
    to the decoder without explicit disentanglement. StyIns Yi et al. ([2021](#bib.bib25))
    utilizes generative flow for a unique style space and provides strong style signals
    to the attention-based decoder through multiple style instances. However, such
    family mainly focuses on the transfer of sentiment and tense in individual English
    sentences, with fewer studies conduct on Chinese long texts.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f4b6d2b1a1ebcf68737ed4f73ae74e56.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The framework of the proposed CAT-LLM. The English translation of
    the Chinese examples appearing in the framework can refer to subsequent Appendix
    Figure A1.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Large Language Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The advent of LLMs has revolutionized NLP research, shifting the focus from
    traditional tasks like translation and classification to more complex, agent-level
    tasks such as role-playing and dialogue systems. This marks a significant advancement
    in the scope of NLP applications and the achievement of higher-level language
    comprehension and generation tasks. In terms of the openness of the LLMs source
    code, it can be categorized into two types: open-source models and closed-source
    models. Open-source LLMs mainly include LLaMA and Yi models, while closed-source
    models are represented by the highly anticipated GPT series. The latest ChatGPT
    can perform tasks that have never been encountered before when providing prompt
    instructions, demonstrating its flexibility and adaptability in facing complex
    and novel challenges. Therefore, designing appropriate prompts becomes particularly
    crucial Brown et al. ([2020](#bib.bib1)). Shao et al. Shao et al. ([2023](#bib.bib18))
    propose Prophet to prompt LLMs with answer heuristics for knowledge-based Visual
    Question Answering. Jang et al. Jang et al. ([2023](#bib.bib7)) investigate the
    effectiveness of negated prompts in directing LLMs, revealing limitations in their
    ability to follow such prompts accurately. Singh et al. Singh et al. ([2023](#bib.bib20))
    introduce a procedural LLMs prompt structure that enable LLMs to directly generate
    sequences of robot operations during task planning. Additionally, Li et al. Li
    et al. ([2023](#bib.bib13)) propose a directed stimulus prompt to guide LLMs in
    achieving specific desired outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Task Definition and The Whole Framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Assuming there is a set of datasets $\{D_{i}\}_{i=1}^{K}$ to comprehensively
    evaluate the performance of the style transfer framework. Our research primarily
    centers on TSD module design and prompt construction, which will be elaborated
    upon in the subsequent text.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Text Style Definition Module
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As shown in Figure [2](#S2.F2 "Figure 2 ‣ 2.1 Text Style Transfer ‣ 2 Related
    Work ‣ CAT-LLM: Prompting Large Language Models with Text Style Definition for
    Chinese Article-style Transfer"), we propose the text style definition (TSD) module,
    which computes the style of the style definition part $D_{i2}$ from both words
    and sentences levels and provides precise style definitions. The TSD module is
    designed to enhance the accurate identification of text styles, providing a clear
    and comprehensive prompt of styles for LLMs. In addition, this module can be seamlessly
    integrated into various LLMs to further optimize the model’s performance in style
    transfer. Simultaneously, it provides interpretability for researchers, enabling
    them to gain deeper insights into the underlying mechanisms of the models in terms
    of text style recognition and transfer.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 Words Level
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Words form the cornerstone of constructing text and play a crucial role in shaping
    language features Serrano et al. ([2009](#bib.bib16)); Castillo and Tolchinsky
    ([2018](#bib.bib2)). Quantitative analysis of the use of vocabulary in texts can
    reveal language style differences between different texts. In this section, we
    conduct an in-depth analysis of the word style of the article from multiple perspectives
    such as part of speech, word length and syllabic words, modal particles and idioms.
  prefs: []
  type: TYPE_NORMAL
- en: Part of Speech.We apply the $Cut$ corresponds to the sequence of part-of-speech
    tags. The part-of-speech of words is mainly divided into content words and function
    words. Content words include nouns, verbs, adjectives, etc., while function words
    include adverbs, prepositions, conjunctions, etc. By classifying and statistically
    analyzing the vocabulary in an article, the distribution of part-of-speech can
    be inferred.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f_{ps}=\underset{i\in\{1,2,...,n_{w}\}}{\rm argmax}\{I_{ps}(w_{i},t_{i})\}$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $I_{ps}(\cdot)$.
  prefs: []
  type: TYPE_NORMAL
- en: Words Length and Syllabic Words. When writers create articles, they have personal
    choices and unique preferences for using words of different lengths Lewis and
    Frank ([2016](#bib.bib12)). By conducting length statistics on the entire article’s
    words, we deduce the word length characteristics of the article.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f_{l}=\underset{i\in\{1,2,...,n_{w}\}}{\rm argTop}K_{l}\{I_{l}(len(w_{i}))\}$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where $len(\cdot)$ most frequently occurring words in monosyllabic and polysyllabic
    words, respectively, as representatives of syllabic words.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f_{mo},f_{po}={\rm argTop}K_{sy}\{freq(W_{mo},W_{po})\}$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $W_{mo}$ function is used to count the frequency of each word.
  prefs: []
  type: TYPE_NORMAL
- en: Modal Particles and idioms. For the statistics of modal particles and idioms,
    we use the existing modal library³³3https://baike.baidu.com/ and idiom library⁴⁴4https://github.com/crazywhalecc/idiom-database
    for regular matching, thereby extracting the top several modal particles and idioms
    with the highest frequency respectively as representatives.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f_{modal}=\underset{i\in\{1,2,...,n_{w}\}}{\rm argTop}K_{modal}\{freq(Re(w_{i},D_{modal}))\}$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $f_{idiom}=\underset{i\in\{1,2,...,n_{w}\}}{\rm argTop}K_{idiom}\{freq(Re(w_{i},D_{idiom}))\}$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: where $D_{modal}$ is the regularization operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'After obtaining various sub-features of words, we integrate these features
    to construct a comprehensive definition of words features:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f_{W}={\rm concat}(f_{ps},f_{l},f_{mo},f_{po},f_{modal},f_{idiom})$ |  |
    (6) |'
  prefs: []
  type: TYPE_TB
- en: 3.2.2 Sentences Level
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Compared to the fine-grained analysis at the words level, a comprehensive examination
    of sentences style allows for a more holistic understanding of the author’s unique
    personality and characteristics in language expression Kakoma and Namagero ([2020](#bib.bib8));
    Gajda ([2022](#bib.bib5)). This contributes to a clearer perception of the overall
    style of the text, providing a more global perspective for a profound understanding
    of the article’s stylistic nuances. We mainly analyze the sentences style from
    the perspectives of sentences length, emotion, sentences structure and rhetoric.
  prefs: []
  type: TYPE_NORMAL
- en: Sentences Length. To evaluate the average sentence length, we consider periods,
    question marks, and exclamation marks as sentences terminators, and use regular
    matching to segment the text into a set of sentences $S=(s_{1},s_{2},...,s_{n_{s}})$.
    By calculating the length of the sentences set and characters set, the average
    sentences length of the article can be obtained.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f_{len}=len(C)/len(S)$ |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: 'In addition, when analyzing the tone of sentences length throughout the article,
    we evaluate the length of each sentence and classify it into short or long sentences
    based on a threshold of 20 words Karya and Mahardika ([2019](#bib.bib9)); Wallwork
    and Wallwork ([2016](#bib.bib23)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f_{ls}=\underset{i\in\{1,2,...,n_{s}\}}{\rm argmax}\{I_{ls}(s_{i})\}$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: where $I_{ls}(\cdot)$ is a function for calculating the counts of long sentences
    and short sentences. If the article predominantly contains long sentences, it
    is classified as having more long sentences; conversely, it is categorized as
    having more short sentences. If the numbers are roughly equal, the article is
    considered to have a combination of both long and short sentences.
  prefs: []
  type: TYPE_NORMAL
- en: Emotion. For the emotional tone expressed by the sentences, we employ the $Emotion$
    library⁵⁵5https://github.com/hidadeng/cnsenti to evaluate. This function outputs
    the score of each sentence on seven emotions (good, joy, sadness, anger, fear,
    disgust, surprise), and add up the scores of each sentence to determine the dominant
    emotion of the article.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f_{em}=\underset{i\in\{1,2,...,n_{s}\}}{\rm argmax}\{I_{em}(Emotion(s_{i}))\}$
    |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: where $I_{em}(\cdot)$ is used to calculate the total score of each emotion category.
  prefs: []
  type: TYPE_NORMAL
- en: Sentences Structure and Rhetoric. When evaluating the integrated and scattered
    sentences, we introduce the deep learning model to make classification decisions.
    Given the powerful text generation capabilities of ChatGPT, we design prompts
    to guide it to generate a classification dataset containing integrated and scattered
    sentences, and use this dataset to fine tune the MacBERT model⁶⁶6https://github.com/ymcui/MacBERT,
    developing a specialized Chinese sentences structure binary classification model
    $BERT_{st}$. Classify each sentence in the article to assess the distribution
    of integrated and scattered sentences. If the number of integrated sentences exceeds
    that of scattered sentences, it can be inferred that the entire article is predominantly
    composed of integrated sentences, otherwise more scattered sentences.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f_{st}=\underset{i\in\{1,2,...,n_{s}\}}{\rm argmax}\{I_{st}(BERT_{st}(s_{i}))\}$
    |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: where $I_{st}(\cdot)$.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f_{rhe}=\underset{i\in\{1,2,...,n_{s}\}}{\rm argTop}K_{rhe}\{I_{rhe}(BERT_{rhe}(s_{i}))\}$
    |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: 'where $I_{rhe}(\cdot)$ rhetorical devices with the highest number as the main
    rhetorical features of the article. Subsequently, we merge all sentences features
    to form a complete set of sentences features:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f_{S}={\rm concat}(f_{len},f_{ls},f_{em},f_{st},f_{rhe})$ |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: 'Finally, we develop a comprehensive definition of article style by combining
    the features of both words and sentences levels:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $F={\rm concat}(f_{W},f_{S})$ |  | (13) |'
  prefs: []
  type: TYPE_TB
- en: 3.3 Style-enhanced Prompt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this stage, leveraging the style definition $F$ generated by the TSD module,
    we design a style-enhanced prompt to enhance the zero-shot learning capability
    of LLMs Chinese article-style transfer. The style-enhanced prompt consists of
    a original text, task description, style definition and output indication. The
    complete format of the prompt is shown in Figure [3](#S3.F3 "Figure 3 ‣ 3.3 Style-enhanced
    Prompt ‣ 3 Methodology ‣ CAT-LLM: Prompting Large Language Models with Text Style
    Definition for Chinese Article-style Transfer"). Given that our text feature calculations
    mainly involves statistical analysis of vocabulary and syntactic structures, mechanically
    enumerating these structures may lead to misunderstandings in LLMs, resulting
    in unnecessary hallucinations. Therefore, we appropriately add professional descriptions
    of feature structures for LLMs to better understand stylistic features. For example,
    this style has many long sentences, which make the article rich in connotation,
    specific in narrative, detailed in reasoning, and full of emotion. Furthermore,
    considering the challenges of understanding negative prompts for LLMs, we carefully
    design the task description to ensure that the generated text strikes an optimal
    balance between high transfer accuracy and content preservation, while avoiding
    the introduction of additional textual information.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a03b5fedc39ee17e8e88df83a4427f8a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Style-enhanced article-style transfer prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We mainly select five literary works with diverse styles covering modern and
    contemporary China to form our dataset. Fortress Besieged is authored by Qian
    Zhongshu and is classified as a modern novel characterized by humor and satire.
    The Scream is a novel written by Lu Xun and is considered one of his works in
    the genre of realism. The Fifteen Year of the Wanli Era is a historical work written
    by Huang Renyu. The Family Instructions of Zeng Guofan is an ancient family instruction
    authored by Zeng Guofan. The Three-Body Problem is a science fiction novel written
    by Liu Cixin.
  prefs: []
  type: TYPE_NORMAL
- en: 'We divide each book into two main parts: the style transfer part and the style
    definition part. We creat five parallel Chinese article-style transfer datasets
    by transfering the style definition part into stylishless text using ChatGPT.
    The number of test samples for each book is approximately 400, with each sample
    consisting of 120 to 140 words. Table [1](#S4.T1 "Table 1 ‣ 4.1 Datasets ‣ 4 Experiment
    ‣ CAT-LLM: Prompting Large Language Models with Text Style Definition for Chinese
    Article-style Transfer") presents detailed datasets statistics.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Statistics of five datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Article | Numbers of style | Numbers of style \bigstrut[t] |'
  prefs: []
  type: TYPE_TB
- en: '| transfer examples | definition examples \bigstrut[b] |'
  prefs: []
  type: TYPE_TB
- en: '| Fortress Besieged | 623 | 946 \bigstrut[t] |'
  prefs: []
  type: TYPE_TB
- en: '| The Scream | 217 | 284 |'
  prefs: []
  type: TYPE_TB
- en: '| The Fifteen Year of | 413 | 617 |'
  prefs: []
  type: TYPE_TB
- en: '| the Wanli Era |'
  prefs: []
  type: TYPE_TB
- en: '| The family instructions | 302 | 1026 |'
  prefs: []
  type: TYPE_TB
- en: '| of Zeng Guofan |'
  prefs: []
  type: TYPE_TB
- en: '| The Three-Body | 513 | 1762 |'
  prefs: []
  type: TYPE_TB
- en: '| Problem | \bigstrut[b] |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Automatic evaluation results on “Fortress Besieged” and “The Scream”.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Article | Models | Transfer Accuracy(%) |  | Content Preservation(%)    \bigstrut
    |'
  prefs: []
  type: TYPE_TB
- en: '| BLEU-1 | BLEU-2 | Precision | Recall | F1 |  | BLEU-1 | BLEU-2 | Precision
    | Recall | F1 \bigstrut |'
  prefs: []
  type: TYPE_TB
- en: '| Fortress Besieged | Style Transformer | 17.80 | 11.03 | 55.45 | 58.79 | 57.08
    |  | 95.34 | 90.93 | 96.87 | 96.50 | 96.69 \bigstrut[t] |'
  prefs: []
  type: TYPE_TB
- en: '| RALoCN | 22.86 | 17.01 | 58.52 | 58.71 | 58.70 |  | 92.27 | 88.07 | 95.34
    | 95.84 | 95.59 \bigstrut[b] |'
  prefs: []
  type: TYPE_TB
- en: '| Role-play+ChatGLM3-6B-chat | 33.37 | 18.36 | 71.18 | 72.26 | 71.68 |  | 56.85
    | 50.51 | 81.58 | 82.70 | 82.10 \bigstrut[t] |'
  prefs: []
  type: TYPE_TB
- en: '| Read+ChatGLM3-6B-chat | 21.68 | 7.36 | 60.96 | 62.66 | 61.77 |  | 28.01 |
    14.39 | 63.72 | 64.69 | 64.15 |'
  prefs: []
  type: TYPE_TB
- en: '| CAT+ChatGLM3-6B-chat | 42.32 | 24.16 | 75.67 | 76.73 | 76.16 |  | 81.44 |
    79.44 | 90.30 | 90.99 | 90.59 |'
  prefs: []
  type: TYPE_TB
- en: '| Role-play+Baichuan-13B-chat | 35.13 | 16.54 | 72.18 | 73.88 | 73.01 |  |
    85.86 | 82.24 | 87.45 | 87.15 | 87.29 |'
  prefs: []
  type: TYPE_TB
- en: '| Read+Baichuan-13B-chat | 17.71 | 6.91 | 60.23 | 63.91 | 62.00 |  | 29.91
    | 21.92 | 64.27 | 67.34 | 65.74 |'
  prefs: []
  type: TYPE_TB
- en: '| CAT+Baichuan-13B-chat | 43.76 | 26.00 | 76.78 | 79.36 | 77.98 |  | 86.15
    | 84.57 | 92.72 | 93.83 | 93.14 |'
  prefs: []
  type: TYPE_TB
- en: '| Role-play+GPT-3.5-turbo | 38.57 | 20.67 | 74.81 | 76.70 | 75.73 |  | 64.40
    | 53.79 | 88.32 | 88.63 | 88.46 |'
  prefs: []
  type: TYPE_TB
- en: '| Read+GPT-3.5-turbo | 16.59 | 6.76 | 59.77 | 63.87 | 61.74 |  | 27.03 | 19.05
    | 64.21 | 68.09 | 66.07 |'
  prefs: []
  type: TYPE_TB
- en: '| CAT+GPT-3.5-turbo | 45.42 | 26.87 | 77.37 | 79.52 | 78.42 |  | 88.96 | 85.01
    | 96.07 | 96.49 | 96.27 \bigstrut[b] |'
  prefs: []
  type: TYPE_TB
- en: '| The Scream | Style Transformer | 16.89 | 10.22 | 59.58 | 60.71 | 60.13 |  |
    92.71 | 90.38 | 98.23 | 98.13 | 98.18 \bigstrut[t] |'
  prefs: []
  type: TYPE_TB
- en: '| RALoCN | 24.72 | 17.14 | 66.09 | 65.48 | 65.78 |  | 89.02 | 87.16 | 95.12
    | 95.85 | 95.49 \bigstrut[b] |'
  prefs: []
  type: TYPE_TB
- en: '| Role-play+ChatGLM3-6B-chat | 38.67 | 19.87 | 76.89 | 77.12 | 76.99 |  | 75.62
    | 68.16 | 88.64 | 89.56 | 89.06 \bigstrut[t] |'
  prefs: []
  type: TYPE_TB
- en: '| Read+ChatGLM3-6B-chat | 13.72 | 4.17 | 57.88 | 62.96 | 60.30 |  | 13.89 |
    4.82 | 57.99 | 62.84 | 60.30 |'
  prefs: []
  type: TYPE_TB
- en: '| CAT+ChatGLM3-6B-chat | 43.70 | 24.96 | 77.40 | 77.73 | 77.53 |  | 77.75 |
    75.22 | 91.75 | 92.52 | 92.09 |'
  prefs: []
  type: TYPE_TB
- en: '| Role-play+Baichuan-13B-chat | 36.07 | 16.93 | 75.24 | 76.22 | 75.72 |  |
    80.90 | 78.51 | 88.37 | 88.17 | 88.27 |'
  prefs: []
  type: TYPE_TB
- en: '| Read+Baichuan-13B-chat | 17.14 | 7.05 | 59.32 | 64.18 | 61.64 |  | 21.54
    | 12.78 | 61.24 | 65.85 | 63.44 |'
  prefs: []
  type: TYPE_TB
- en: '| CAT+Baichuan-13B-chat | 41.68 | 23.85 | 77.08 | 78.47 | 77.71 |  | 84.15
    | 82.23 | 90.41 | 91.13 | 90.63 |'
  prefs: []
  type: TYPE_TB
- en: '| Role-play+GPT-3.5-turbo | 41.13 | 22.91 | 77.60 | 78.45 | 78.01 |  | 73.82
    | 66.03 | 92.42 | 92.27 | 92.34 |'
  prefs: []
  type: TYPE_TB
- en: '| Read+GPT-3.5-turbo | 19.76 | 8.94 | 60.91 | 64.75 | 62.74 |  | 29.70 | 22.23
    | 64.94 | 68.59 | 66.68 |'
  prefs: []
  type: TYPE_TB
- en: '| CAT+GPT-3.5-turbo | 45.42 | 26.39 | 78.54 | 79.79 | 79.15 |  | 88.89 | 84.69
    | 96.38 | 96.64 | 96.51 \bigstrut[b] |'
  prefs: []
  type: TYPE_TB
- en: 4.2 Implementation Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the algorithm parameters at the words level, we set $K_{l}$. For the configuration
    of LLMs, the temperature parameter is set to 0, and the seed parameter is uniformly
    set to 42.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Evaluation Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our evaluation primarily focus on style transfer accuracy and content preservation.
    A high-quality Chinese article-style transfer model should balance these two aspects.
    We compare the generated text with the original text of the article to calculate
    the transfer accuracy. Simultaneously, comparisons between the generated text
    and the stylishless text are conducted to assess the degree of content preservation.
    The evaluation metrics adopted for both perspectives are exactly the same. Specifically,
    we measure the lexical and semantic similarity between two texts using BLEU-n
    (n=1,2) Papineni et al. ([2002](#bib.bib14)) and BERTScore Zhang et al. ([2020](#bib.bib26)),
    where BERTScore mainly includes Precision, Recall, and F1 score. In addition,
    considering the randomness of LLMs generating text, we transfer each type of text
    ten times to calculate the final average metrics results.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/86d8b79a0343ca32fb8576cdf99dc06d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Radar charts of experimental results from other datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Baselines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We choose three currently popular LLMs as our baseline models: GPT-3.5-turbo,
    ChatGLM3-6B-chat, and Baichuan-13B-chat. The GPT-3.5-turbo model is the novel
    generation of LLMs from OpenAI, ChatGLM3-6B-chat is the latest open source model
    of Zhipu AI, and Baichuan-13B-chat represents the latest research on Chinese LLMs.
    These three models represent the current forefront of LLMs, capable of comprehensively
    assessing the robustness and applicability of our framework. Simultaneously, we
    conduct a comparative study on the transfer results of two classic models, Style
    Transformer Dai et al. ([2019](#bib.bib3)) and RACoLN Lee et al. ([2021](#bib.bib11)).
    These two models cover the two paradigms described in section 2.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Ablation study results on “The Scream”.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Style Arrangement | Transfer Accuracy(%) |  | Content Preservation(%)   
    \bigstrut |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| BLEU-1 | BLEU-2 | Precision | Recall | F1 |  | BLEU-1 | BLEU-2 | Precision
    | Recall | F1 \bigstrut |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Sentences | 42.92 | 23.81 | 76.48 | 78.28 | 77.35 |  | 79.22 | 71.60 | 92.09
    | 93.33 | 92.69 \bigstrut[t] |'
  prefs: []
  type: TYPE_TB
- en: '| Words | 44.25 | 25.22 | 77.74 | 78.92 | 78.32 |  | 86.40 | 83.44 | 96.03
    | 96.19 | 96.11 |'
  prefs: []
  type: TYPE_TB
- en: '| Sentences+Words | 44.86 | 25.67 | 78.04 | 79.41 | 78.71 |  | 85.94 | 80.55
    | 95.29 | 95.73 | 95.50 |'
  prefs: []
  type: TYPE_TB
- en: '| Words+Sentences | 45.42 | 26.39 | 78.54 | 79.79 | 79.15 |  | 88.89 | 84.69
    | 96.38 | 96.64 | 96.51 \bigstrut[b] |'
  prefs: []
  type: TYPE_TB
- en: 4.5 Experimental Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our experimental article transfer models include the proposed CAT-LLM, direct
    reading of articles based on LLMs, role-playing under different LLMs, Style Transformer
    and RACoLN. It is worthy that, for experiments involving LLMs directly reading
    the articles, we ensure the tokens used for reading the articles are roughly consistent
    with the style definition tokens of CAT-LLM to ensure the experiment fairness.
    As shown in Table [2](#S4.T2 "Table 2 ‣ 4.1 Datasets ‣ 4 Experiment ‣ CAT-LLM:
    Prompting Large Language Models with Text Style Definition for Chinese Article-style
    Transfer"), we present the experimental results of the models on the first two
    datasets. The experimental results of the LLMs on other datasets are presented
    in radar charts shown in Figure [4](#S4.F4 "Figure 4 ‣ 4.3 Evaluation Metrics
    ‣ 4 Experiment ‣ CAT-LLM: Prompting Large Language Models with Text Style Definition
    for Chinese Article-style Transfer"). The experimental outcomes of all models
    are delineated in the subsequent Appendix(Table A1,A2,A3). CAT-LLM based on various
    LLMs achieves a better balance between style transfer and content preservation
    in each LLM transfer, providing more precise control over style transfer in generated
    text.'
  prefs: []
  type: TYPE_NORMAL
- en: In terms of style transfer accuracy, the combined application of CAT+GPT-3.5-turbo
    achieve the best performance on all datasets. In terms of lexical similarity evaluation
    index BLEU-1, CAT+GPT-3.5-turbo has all reached above 45%, while maintaining a
    level of around 80% in semantic similarity. This success is not only attributed
    to the outstanding capabilities of GPT-3.5-turbo but also highlights the effectiveness
    of the TSD module’s style prompts, enabling CAT+GPT-3.5-turbo to excel in article-style
    transfer. However, the experimental results of LLMs directly reading article fragments
    for style transfer are not satisfactory. This is mainly that the LLMs only summarize
    the style of certain fragments of the article and lack understanding of the overall
    style, and illustrate the challenges LLMs face in handling more abstract and deeper
    semantic relationships, reasoning, and text comprehension. For role-playing, LLMs
    excel at imitating their familiar character writing style, while on the contrary,
    the transfer results are significantly weakened. For example, in the style transfer
    of Zeng Guofan’s family teachings, the effect of role-playing is not satisfactory.
    Furthermore, the transfer accuracy of both small models is at a low level. We
    examine the text generated by the small model and find that both the Style Transformer
    and RACoLN exhibit a tendency to replicate the input and selectively replace certain
    words, falling short in comprehending the semantic style features of the text.
    This is also why both exhibit lower transfer accuracy but higher content preservation
    results.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding content preservation, CAT-LLM has also achieved competitive results
    in the article-style transfer of various LLMs. In the transfer experiment of directly
    reading articles with LLMs, we find the problem of introducing a large number
    of unrelated words, indicating that LLMs may be prone to hallucinations when dealing
    with complex problems. It is worth noting that the combination of our CAT-LLM
    also outperforms the Role-play+GPT-3.5-turbo, indicating that the proposed TSD
    module can develop the enormous potential of LLMs in article-style transfer.
  prefs: []
  type: TYPE_NORMAL
- en: 4.6 Ablation Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To further investigate the role and interaction of words level and sentences
    level style definitions in the TSD module, we conduct ablation experiments on
    the “The Scream” dataset based on GPT-3.5-turbo. As shown in Table [3](#S4.T3
    "Table 3 ‣ 4.4 Baselines ‣ 4 Experiment ‣ CAT-LLM: Prompting Large Language Models
    with Text Style Definition for Chinese Article-style Transfer"), words level prompts
    have a more significant positive impact on LLMs’ understanding of text style than
    sentences level prompts. This may be attributed to the fact that words level prompts
    provide more granular and localized style information, enabling LLMs to more precisely
    capture subtle semantic nuances and stylistic variations in the text. In contrast,
    sentences level prompts may be more macroscopic and struggle to convey the finer
    stylistic differences within the text. Furthermore, placing words level prompts
    before sentences level prompts yields better results in style transfer experiments
    compared to the reverse order. This may be because words level prompts can guide
    the LLMs to initially focus on fine-grained semantics and local stylistic information
    in the text. This arrangement can more effectively learn and retain the subtle
    semantic differences in the text, to more accurately convey the required style
    and enhance the sensitivity of the LLMs to the overall style of the text.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion and Future Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we propose the Chinese Article-style Transfer framework (CAT-LLM),
    a pioneering application of LLMs in the realm of Chinese long-text style transfer.
    The CAT-LLM framework, with its Text Style Definition (TSD) module, meticulously
    analyzes text at both words and sentences levels. By integrating various machine
    learning small models, our TSD module enhances LLMs’ directed generation, enriching
    literary analysis and computational linguistics. This approach effectively guides
    LLMs to grasp the stylistic nuances of texts more profoundly. Furthermore, leveraging
    ChatGPT, we create five parallel datasets of Chinese long texts, providing a new
    paradigm of the evaluation of transfer models for future researchers. Extensive
    experiments across these datasets confirm that CAT-LLM outperforms existing methods
    in balancing transfer accuracy and content preservation.
  prefs: []
  type: TYPE_NORMAL
- en: Given the tremendous success of our framework, we plan to not directly use prompt
    in future work, but instead use the datasets and style data summarized in this
    study to construct supervised fine-tuning data and train LLMs to improve their
    applicability and efficiency in style transfer tasks. We highlight this task and
    leave it for future work.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. Advances in neural information
    processing systems, 33:1877–1901, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Castillo and Tolchinsky [2018] Cristina Castillo and L. Tolchinsky. The contribution
    of vocabulary knowledge and semantic orthographic fluency to text quality through
    elementary school in catalan. Reading and Writing, 31:293–323, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai et al. [2019] Ning Dai, Jianze Liang, Xipeng Qiu, and Xuanjing Huang. Style
    transformer: Unpaired text style transfer without disentangled latent representation.
    In Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics, pages 5997–6007, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Danescu-Niculescu-Mizil et al. [2013] Cristian Danescu-Niculescu-Mizil, Moritz
    Sudhof, Dan Jurafsky, Jure Leskovec, and Christopher Potts. A computational approach
    to politeness with application to social factors. In Proceedings of the 51st Annual
    Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
    pages 250–259, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gajda [2022] S. Gajda. The faces of style and stylistics. Journal of Linguistics/Jazykovedný
    casopis, 73:7–26, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. [2017] Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov,
    and Eric P. Xing. Toward controlled generation of text. In Proceedings of the
    34th International Conference on Machine Learning - Volume 70, page 1587–1596,
    2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jang et al. [2023] Joel Jang, Seonghyeon Ye, and Minjoon Seo. Can large language
    models truly understand prompts? a case study with negated prompts. In Transfer
    Learning for Natural Language Processing Workshop, pages 52–62, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kakoma and Namagero [2020] G. Kakoma and Dr. Shira Tendo Namagero. A stylistic
    analysis of ’o uganda, land of beauty’ by prof.george kakoma. International Journal
    on Studies in English Language and Literature, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Karya and Mahardika [2019] I Wayan Sidha Karya and Ida Bagus Adhika Mahardika.
    A study on how long and short sentences show the story’s pacing in anthony horowitz’s
    raven’s gate. SPHOTA: Jurnal Linguistik dan Sastra, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lai et al. [2023] Huiyuan Lai, Antonio Toral, and Malvina Nissim. Multidimensional
    evaluation for text style transfer using chatgpt. arXiv preprint arXiv:2304.13462,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. [2021] Dongkyu Lee, Zhiliang Tian, Lanqing Xue, and Nevin L. Zhang.
    Enhancing content preservation in text style transfer using reverse attention
    and conditional layer normalization. In Proceedings of the 59th Annual Meeting
    of the Association for Computational Linguistics and the 11th International Joint
    Conference on Natural Language Processing (Volume 1: Long Papers), pages 93–102,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lewis and Frank [2016] M. Lewis and Michael C. Frank. The length of words reflects
    their conceptual complexity. Cognition, 153:182–195, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2023] Zekun Li, Baolin Peng, Pengcheng He, Michel Galley, Jianfeng
    Gao, and Xifeng Yan. Guiding large language models via directional stimulus prompting.
    arXiv preprint arXiv:2302.11520, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Papineni et al. [2002] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings
    of the 40th annual meeting of the Association for Computational Linguistics, pages
    311–318, 2002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Przybyla [2020] Piotr Przybyla. Capturing the style of fake news. In Proceedings
    of the AAAI conference on artificial intelligence, volume 34, pages 490–497, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serrano et al. [2009] M. Serrano, A. Flammini, and F. Menczer. Modeling statistical
    properties of written text. PLoS ONE, 4, 2009.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shanahan et al. [2023] Murray Shanahan, Kyle McDonell, and Laria Reynolds. Role
    play with large language models. Nature, pages 1–6, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shao et al. [2023] Zhenwei Shao, Zhou Yu, Meng Wang, and Jun Yu. Prompting large
    language models with answer heuristics for knowledge-based visual question answering.
    In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 14974–14983, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sheikha and Inkpen [2010] Fadi Abu Sheikha and Diana Inkpen. Automatic classification
    of documents by formality. In Proceedings of the 6th international conference
    on natural language processing and knowledge engineering (nlpke-2010), pages 1–5.
    IEEE, 2010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Singh et al. [2023] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal,
    Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. Progprompt:
    Generating situated robot task plans using large language models. In 2023 IEEE
    International Conference on Robotics and Automation (ICRA), pages 11523–11530\.
    IEEE, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Song et al. [2023] Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M Sadler,
    Wei-Lun Chao, and Yu Su. Llm-planner: Few-shot grounded planning for embodied
    agents with large language models. In Proceedings of the IEEE/CVF International
    Conference on Computer Vision, pages 2998–3009, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Toshevska and Gievska [2021] Martina Toshevska and Sonja Gievska. A review of
    text style transfer using deep learning. IEEE Transactions on Artificial Intelligence,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wallwork and Wallwork [2016] Adrian Wallwork and Adrian Wallwork. Teaching
    students to recognize the pros and cons of short and long sentences. English for
    Academic Research: A Guide for Teachers, pages 69–77, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2023] Zekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu,
    Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang, et al.
    Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large
    language models. arXiv preprint arXiv:2310.00746, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yi et al. [2021] Xiaoyuan Yi, Zhenghao Liu, Wenhao Li, and Maosong Sun. Text
    style transfer via learning style instance supported latent space. In Proceedings
    of the Twenty-Ninth International Conference on International Joint Conferences
    on Artificial Intelligence, pages 3801–3807, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2020] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger,
    and Yoav Artzi. Bertscore: Evaluating text generation with bert. In International
    Conference on Learning Representations, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. [2023] Xuekai Zhu, Jian Guan, Minlie Huang, and Juan Liu. StoryTrans:
    Non-parallel story author-style transfer with discourse representations and content
    enhancing. In Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers), pages 14803–14819, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
