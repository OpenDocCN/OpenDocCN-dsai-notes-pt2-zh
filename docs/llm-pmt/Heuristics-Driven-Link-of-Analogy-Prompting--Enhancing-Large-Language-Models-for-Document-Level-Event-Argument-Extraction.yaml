- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:48:15'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 18:48:15'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Heuristics-Driven Link-of-Analogy Prompting: Enhancing Large Language Models
    for Document-Level Event Argument Extraction'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启发式驱动的类比提示：提升大型语言模型在文档级事件论据提取中的表现
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2311.06555](https://ar5iv.labs.arxiv.org/html/2311.06555)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2311.06555](https://ar5iv.labs.arxiv.org/html/2311.06555)
- en: 'Hanzhang Zhou^(1,2), Junlang Qian¹, Zijian Feng^(1,2), Hui Lu¹, Zixiao Zhu^(1,2),
    Kezhi Mao^(1,2)¹¹footnotemark: 1'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 'Hanzhang Zhou^(1,2), Junlang Qian¹, Zijian Feng^(1,2), Hui Lu¹, Zixiao Zhu^(1,2),
    Kezhi Mao^(1,2)¹¹footnotemark: 1'
- en: ¹Nanyang Technological University, Singapore
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹南洋理工大学，新加坡
- en: ²Future Resilient Systems Programme, Singapore-ETH Centre, Singapore
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ²未来韧性系统计划，新加坡-ETH 中心，新加坡
- en: '{hanzhang001, junlang001, feng0119, hui007, zixiao001}@e.ntu.edu.sg'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '{hanzhang001, junlang001, feng0119, hui007, zixiao001}@e.ntu.edu.sg'
- en: ekzmao@ntu.edu.sg
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ekzmao@ntu.edu.sg
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: In this study, we investigate in-context learning (ICL) in document-level event
    argument extraction (EAE). The paper identifies key challenges in this problem,
    including example selection, context length limitation, abundance of event types,
    and the limitation of Chain-of-Thought (CoT) prompting in non-reasoning tasks.
    To address these challenges, we introduce the Heuristic-Driven Link-of-Analogy
    (HD-LoA) prompting method. Specifically, we hypothesize and validate that LLMs
    learn task-specific heuristics from demonstrations via ICL. Building upon this
    hypothesis, we introduce an explicit heuristic-driven demonstration construction
    approach, which transforms the haphazard example selection process into a methodical
    method that emphasizes task heuristics. Additionally, inspired by the analogical
    reasoning of human, we propose the link-of-analogy prompting, which enables LLMs
    to process new situations by drawing analogies to known situations, enhancing
    their adaptability. Extensive experiments show that our method outperforms the
    existing prompting methods and few-shot supervised learning methods, exhibiting
    F1 score improvements of $4.53\%$, indicating its effectiveness across different
    tasks.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们探讨了文档级事件论据提取（EAE）中的上下文学习（ICL）。论文识别了这个问题中的关键挑战，包括示例选择、上下文长度限制、事件类型丰富性以及
    Chain-of-Thought（CoT）提示在非推理任务中的局限性。为了解决这些挑战，我们引入了启发式驱动的类比（HD-LoA）提示方法。具体而言，我们假设并验证了
    LLMs 通过 ICL 从示例中学习任务特定的启发式规则。基于这一假设，我们提出了一种明确的启发式驱动的示例构建方法，将随意的示例选择过程转变为强调任务启发式规则的系统方法。此外，受到类比推理的启发，我们提出了类比提示，这使得
    LLMs 通过类比已知情况来处理新情况，从而增强其适应性。大量实验表明，我们的方法在不同任务中表现优于现有提示方法和少量监督学习方法，F1 分数提高了 $4.53\%$，显示了其在不同任务中的有效性。
- en: 'Heuristics-Driven Link-of-Analogy Prompting: Enhancing Large Language Models
    for Document-Level Event Argument Extraction'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 启发式驱动的类比提示：提升大型语言模型在文档级事件论据提取中的表现
- en: 'Hanzhang Zhou^(1,2), Junlang Qian¹, Zijian Feng^(1,2), Hui Lu¹, Zixiao Zhu^(1,2),
    Kezhi Mao^(1,2)¹¹footnotemark: 1 ¹Nanyang Technological University, Singapore
    ²Future Resilient Systems Programme, Singapore-ETH Centre, Singapore {hanzhang001,
    junlang001, feng0119, hui007, zixiao001}@e.ntu.edu.sg ekzmao@ntu.edu.sg'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 'Hanzhang Zhou^(1,2), Junlang Qian¹, Zijian Feng^(1,2), Hui Lu¹, Zixiao Zhu^(1,2),
    Kezhi Mao^(1,2)¹¹footnotemark: 1 ¹南洋理工大学，新加坡 ²未来韧性系统计划，新加坡-ETH 中心，新加坡 {hanzhang001,
    junlang001, feng0119, hui007, zixiao001}@e.ntu.edu.sg ekzmao@ntu.edu.sg'
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Document-level Event Argument Extraction (EAE) aims to transform unstructured
    event information from documents into structured formats encapsulating event arguments,
    facilitating their interpretation and application in various domains (Grishman,
    [2019](#bib.bib16)). The prevalent approach for this task relies on the collection
    of labeled data and the subsequent model training via supervised learning (Liu
    et al., [2023a](#bib.bib20); Pouran Ben Veyseh et al., [2022](#bib.bib28); Zhou
    and Mao, [2022](#bib.bib45); Du and Cardie, [2020a](#bib.bib7)). While effective,
    this approach comes with the significant drawback: it necessitates a substantial
    amount of training data, which is particularly burdensome and costly given the
    complexity inherent to document-level EAE.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 文档级事件论证提取（EAE）旨在将文档中的非结构化事件信息转化为封装事件论证的结构化格式，从而促进其在各个领域的解释和应用 (Grishman, [2019](#bib.bib16))。这一任务的主流方法依赖于标注数据的收集以及通过监督学习进行的模型训练
    (Liu et al., [2023a](#bib.bib20); Pouran Ben Veyseh et al., [2022](#bib.bib28);
    Zhou and Mao, [2022](#bib.bib45); Du and Cardie, [2020a](#bib.bib7))。虽然有效，但这一方法有一个显著的缺点：它需要大量的训练数据，而考虑到文档级EAE的复杂性，这既繁琐又昂贵。
- en: In this context, in-context learning (ICL) (Brown et al., [2020](#bib.bib4);
    Liu et al., [2022](#bib.bib19); Zhou et al., [2022](#bib.bib44)), an emergent
    ability of large language models (LLMs), offers a promising alternative to supervised
    learning. ICL alleviates the need for large-scale data as it only uses a few examples
    as input-output pairs of the prompt to guide LLMs in performing the task on an
    unseen example.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，内部学习（ICL） (Brown et al., [2020](#bib.bib4); Liu et al., [2022](#bib.bib19);
    Zhou et al., [2022](#bib.bib44))，一种大型语言模型（LLM）的新兴能力，为监督学习提供了一个有前景的替代方案。ICL减少了对大规模数据的需求，因为它只使用少量的示例作为输入-输出对，指导LLM在未见示例上执行任务。
- en: '![Refer to caption](img/01d00cc7c409fef85d91b3a113a0dcd1.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/01d00cc7c409fef85d91b3a113a0dcd1.png)'
- en: 'Figure 1: CoT’s step-by-step reasoning degrades to a mere single step for non-reasoning
    tasks. Reasoning steps of two reasoning tasks (in orange) and two non-reasoning
    tasks (in blue) are compared. Different colors indicate distinct reasoning steps.
    Prompts are adopted from (Shum et al., [2023](#bib.bib31))'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：CoT的逐步推理在非推理任务中退化为单一步骤。比较了两个推理任务（橙色）和两个非推理任务（蓝色）的推理步骤。不同的颜色表示不同的推理步骤。提示词采自
    (Shum et al., [2023](#bib.bib31))
- en: .
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: .
- en: 'However, applying ICL to document-level EAE presents numerous challenges. The
    design of in-context demonstrations is crucial to the effectiveness of ICL, with
    the performance of ICL being significantly influenced by factors such as the selection
    of examples and the formatting of reasoning steps (Zhang et al., [2023](#bib.bib43);
    Liu et al., [2022](#bib.bib19); Zhang et al., [2022](#bib.bib42); Fu et al., [2022](#bib.bib10)).
    Consequently, several crucial challenges emerge concerning the prompting strategy:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，将ICL应用于文档级EAE面临众多挑战。上下文示例的设计对ICL的有效性至关重要，ICL的性能受示例选择和推理步骤格式等因素的显著影响 (Zhang
    et al., [2023](#bib.bib43); Liu et al., [2022](#bib.bib19); Zhang et al., [2022](#bib.bib42);
    Fu et al., [2022](#bib.bib10))。因此，关于提示策略出现了几个关键挑战：
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Example selection. Gaining clarity on how to select demonstration examples requires
    an in-depth understanding of what LLMs learn from these demonstrations in ICL.
    At present, however, the understanding of this aspect remains limited (Dong et al.,
    [2022](#bib.bib6)). Therefore, what are the guiding principles for example selection,
    and what characterizes a ’good’ example for document-level EAE?
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 示例选择。对如何选择演示示例的清晰理解需要深入了解LLM在ICL中的学习方式。然而，目前对这一方面的理解仍然有限 (Dong et al., [2022](#bib.bib6))。因此，选择示例的指导原则是什么，什么样的示例被认为是文档级EAE的“好”示例？
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Context length limit. Following the experimental set-up of chain-of-thought
    (CoT) prompting (Wei et al., [2022](#bib.bib38)), demonstrations typically incorporate
    six or eight examples. However, in the context of document-level EAE, selecting
    eight documents as examples might significantly extend the context length, potentially
    surpassing the token limit of LLMs.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上下文长度限制。根据链式推理（CoT）提示的实验设置 (Wei et al., [2022](#bib.bib38))，示例通常包含六或八个。然而，在文档级EAE的背景下，选择八个文档作为示例可能会显著延长上下文长度，可能超出LLM的令牌限制。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Abundance of Event Types. The EAE task can feature more than a hundred distinct
    event types and argument roles. Yet, in-context examples can only capture a narrow
    subset of these, leaving the majority of argument roles unseen. Creating individual
    prompts for each unique event type and argument role is both labor-intensive and
    impractical. This leads to the question: How to design a universal prompting strategy
    that effectively addresses a wide range of unseen event types and argument roles?'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 事件类型的丰富性。EAE任务可能包含超过一百种不同的事件类型和论证角色。然而，上下文示例只能捕捉到这些中的一小部分，导致大多数论证角色未被看到。为每种独特的事件类型和论证角色创建单独的提示既劳动密集又不切实际。这就引出了一个问题：如何设计一种通用的提示策略，以有效应对广泛的未见事件类型和论证角色？
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Prompting strategy for non-reasoning task. While the CoT prompting strategy
    is extensively used across a variety of tasks, both reasoning and non-reasoning,
    it was originally designed only for reasoning tasks such as arithmetic reasoning.
    As shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Heuristics-Driven Link-of-Analogy
    Prompting: Enhancing Large Language Models for Document-Level Event Argument Extraction"),
    applying CoT to non-reasoning tasks will degrade its step-by-step reasoning into
    a single-step process, compromising its effectiveness. Consequently, there is
    a need for a reasoning strategy tailored for non-reasoning tasks.'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 非推理任务的提示策略。虽然链式思维（CoT）提示策略在各种任务中得到了广泛使用，包括推理任务和非推理任务，但它最初仅设计用于推理任务，如算术推理。如图[1](#S1.F1
    "图 1 ‣ 1 引言 ‣ 启发式驱动的类比提示：增强大型语言模型在文档级事件论证提取中的能力")所示，将CoT应用于非推理任务将其逐步推理降级为单步过程，从而影响其效果。因此，需要为非推理任务量身定制的推理策略。
- en: '![Refer to caption](img/c9b0d55ffed6814941a8ee80315fb18a.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c9b0d55ffed6814941a8ee80315fb18a.png)'
- en: 'Figure 2: The implicit heuristics behind in-context examples.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：上下文示例背后的隐含启发式。
- en: 'In this work, we put forward a novel hypothesis that LLMs learn task-specific
    heuristics from examples and validate the hypothesis through experiments. Building
    upon this hypothesis, we propose heuristic-driven link-of-analogy prompting to
    address the aforementioned questions. To elaborate:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了一个新颖的假设，即大型语言模型（LLMs）从示例中学习任务特定的启发式，并通过实验验证了这一假设。在此假设的基础上，我们提出了启发式驱动的类比提示方法，以解决上述问题。具体说明如下：
- en: 'We propose and empirically validate the hypothesis that LLMs learn task specific
    heuristics from examples in ICL. The hypothesis is derived by drawing an analogy
    from the training of deep learning models and the fine-tuning of pretrained language
    models (PLMs). Both these models achieve enhanced performance by learning task-specific
    patterns, either through training or fine-tuning on task data (Zhou and Srikumar,
    [2022](#bib.bib46); Shachaf et al., [2021](#bib.bib30); Kim et al., [2018](#bib.bib17);
    Najafabadi et al., [2015](#bib.bib24)). Drawing a parallel, we hypothesize that,
    although LLMs’ parameters are frozen in ICL, providing them with in-context examples
    allows LLMs to learn task-specific patterns during inference. These patterns encompasses
    both lower-level patterns like label space and input text distribution, and higher-level
    heuristics from demonstrations. We notice the learning of lower-level patterns
    has been validated Min et al. ([2022](#bib.bib23)). Therefore, we focus on substantiating
    the learning of task-specific heuristics in ICL. An illustration of the implicit
    heuristics behind in-context examples can be found in Figure [2](#S1.F2 "Figure
    2 ‣ 1 Introduction ‣ Heuristics-Driven Link-of-Analogy Prompting: Enhancing Large
    Language Models for Document-Level Event Argument Extraction"), while quantitative
    validations of our hypothesis are detailed in Section [2](#S2 "2 What LLMs learn
    from the demonstration? ‣ Heuristics-Driven Link-of-Analogy Prompting: Enhancing
    Large Language Models for Document-Level Event Argument Extraction").'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '我们提出并实证验证了LLMs从ICL示例中学习任务特定启发式的假设。该假设通过类比深度学习模型的训练和预训练语言模型（PLMs）的微调得出。这些模型通过在任务数据上训练或微调学习任务特定模式，从而提高性能（Zhou
    和 Srikumar，[2022](#bib.bib46)；Shachaf 等，[2021](#bib.bib30)；Kim 等，[2018](#bib.bib17)；Najafabadi
    等，[2015](#bib.bib24)）。类比推理，我们假设尽管LLMs的参数在ICL中是冻结的，但通过提供上下文示例，LLMs可以在推理过程中学习任务特定模式。这些模式包括标签空间和输入文本分布等较低级的模式，以及来自示例的较高级启发式。我们注意到较低级模式的学习已在Min等人（[2022](#bib.bib23)）的研究中得到验证。因此，我们重点验证了ICL中任务特定启发式的学习。上下文示例中隐含启发式的说明见图[2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ Heuristics-Driven Link-of-Analogy Prompting: Enhancing
    Large Language Models for Document-Level Event Argument Extraction")，而我们假设的定量验证详见第[2](#S2
    "2 What LLMs learn from the demonstration? ‣ Heuristics-Driven Link-of-Analogy
    Prompting: Enhancing Large Language Models for Document-Level Event Argument Extraction")节。'
- en: We propose a heuristic-driven demonstration construction method. Based on our
    hypothesis, task heuristics are crucial for the ICL performance of LLMs. Yet,
    these heuristics are implicitly conveyed through examples, leading to complexity
    for ultilizing these heuristics and uncertainties about whether LLMs have recognized
    these heuristics. To address this, we propose to explicitly provide task heuristics
    in demonstrations. This approach simplify the LLM’s two-step process of implicit
    heuristic recognition and subsequent inference into a more straightforward, singular
    step of heuristic-based inference.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种基于启发式驱动的示例构建方法。基于我们的假设，任务启发式对LLMs的ICL性能至关重要。然而，这些启发式通常通过示例隐式传达，导致利用这些启发式的复杂性，以及不确定LLMs是否识别了这些启发式。为了解决这一问题，我们建议在示例中明确提供任务启发式。这种方法将LLMs隐式启发式识别和后续推理的两步过程简化为更直接的单步启发式推理。
- en: We propose the link-of-analogy prompting method for document-level EAE that
    suitable for non-reasoning tasks. To address the aforementioned challenges of
    abundance of event types in EAE and the limitations of CoT prompting on non-reasoning
    tasks, we present the link-of-analogy prompting. Inspired by the analogical reasoning––a
    core mechanism of human cognition–this approach enables LLMs to extrapolate knowledge
    from known event types to unseen ones. By enabling LLMs to process new situations
    through drawing an analogy to known situations, the link-of-analogy prompting
    significantly enhances the performance of ICL.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了适用于文档级EAE的类比链接提示方法，适合非推理任务。为了应对EAE中事件类型丰富性和CoT提示在非推理任务上的局限性，我们提出了类比链接提示。受到类比推理——人类认知的核心机制——的启发，这种方法使LLMs能够将已知事件类型的知识外推到未见事件。通过将新情况与已知情况类比处理，这种方法显著提升了ICL的性能。
- en: We implement our heuristic-driven link-of-analogy prompting method across three
    LLM models on two document-level EAE datasets and two datasets on sentiment analysis
    and natural language inference tasks. Extensive experiments demonstrate that our
    method outperforms the state-of-the-art prompting methods and few-shot supervised
    learning methods, as evidenced by absolute F1 score increases of $4.53\%$, respectively.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在三个LLM模型上实施了基于启发式驱动的类比提示方法，应用于两个文档级EAE数据集和两个情感分析与自然语言推理任务的数据集。广泛的实验表明，我们的方法优于最先进的提示方法和少量监督学习方法，F1分数绝对提高了$4.53\%$。
- en: 2 What LLMs learn from the demonstration?
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 LLM从演示中学到了什么？
- en: '![Refer to caption](img/31af91b41129c98ec37954fe8b83359f.png)![Refer to caption](img/f6cffaf318ad31e980cb7858913b2c64.png)![Refer
    to caption](img/c734e8830f06ffae2c529dd6ed71aae5.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/31af91b41129c98ec37954fe8b83359f.png)![参考说明](img/f6cffaf318ad31e980cb7858913b2c64.png)![参考说明](img/c734e8830f06ffae2c529dd6ed71aae5.png)'
- en: 'Figure 3: An illustration of the correlation between example quantity and heuristic
    diversity in well-designed prompts. # Examples: the number of examples used in
    each prompt of the corresponding paper. # Heuristics: the number of heuristics
    identified in each prompt of the corresponding paper. # Heuristics in Rand.: the
    average number of heuristics in the prompt composed of random selected examples.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '图3：展示了设计良好的提示中，示例数量与启发式多样性之间的相关性。 # 示例：每个提示中使用的示例数量。 # 启发式：每个提示中识别的启发式方法数量。
    # 随机启发式：由随机选择的示例组成的提示中启发式方法的平均数量。'
- en: There is minimal understanding of what LLMs learn from the demonstration of
    ICL currently available. In this work, we hypothesize that LLMs learn task-specific
    heuristics from examples of ICL based on an analogy from the training of deep
    learning models. We confirm this hypothesis with carefully designed experiments
    in two aspects.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 目前对LLM从ICL演示中学习到的内容理解甚微。在这项工作中，我们假设LLM基于深度学习模型训练中的类比，从ICL示例中学习任务特定的启发式方法。我们通过精心设计的实验从两个方面确认了这一假设。
- en: 2.1 Correlation between Example Quantity and Heuristic Diversity in Well-Designed
    Prompts
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 例子数量与设计良好的提示中的启发式多样性之间的相关性
- en: If our hypothesis holds, it implies that well-constructed prompts should incorporate
    a diverse range of heuristics. To validate this implication, we assess both the
    quantity of examples and the quantity of different heuristics present in prompts
    from published studies.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的假设成立，这意味着精心构建的提示应包含多样化的启发式方法。为了验证这一含义，我们评估了已发布研究中的示例数量和提示中不同启发式方法的数量。
- en: 'However, manually estimating the number of heuristics can introduce subjectivity,
    leading to potential biases. Thus, the challenge is to objectively identify the
    implicit heuristics inherent in demonstration examples. In this work, we employ
    GPT-4 to recognize the implicit heuristics for each example and to determine if
    there are shared heuristics across multiple examples. An detailed example of the
    prompt we used, along with the heuristics identified by GPT-4, can be found in
    Appendix [B](#A2 "Appendix B Recognize Implicit Heuristics of In-Context Examples
    by GPT-4 ‣ Heuristics-Driven Link-of-Analogy Prompting: Enhancing Large Language
    Models for Document-Level Event Argument Extraction").'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，手动估计启发式方法的数量可能引入主观性，导致潜在的偏差。因此，挑战在于客观识别示例中的隐含启发式方法。在这项工作中，我们使用GPT-4来识别每个示例中的隐含启发式方法，并确定是否存在跨多个示例的共享启发式方法。我们使用的提示及GPT-4识别的启发式方法的详细示例见附录[B](#A2
    "附录 B 通过GPT-4识别上下文示例的隐含启发式方法 ‣ 启发式驱动的类比提示：提升大型语言模型在文档级事件论证提取中的表现")。
- en: 'We investigate the relationship between the number of examples used in the
    prompt and the number of heuristics inherent in the corresponding prompt, across
    six cutting-edge prompting papers on three datasets. Specifically, SOTA prompting
    methods including CoT (Wei et al., [2022](#bib.bib38)), Automate-CoT (Shum et al.,
    [2023](#bib.bib31)), Auto-CoT (Zhang et al., [2023](#bib.bib43)), Iter-CoT (Sun
    et al., [2023](#bib.bib34)), Boosted (Pitis et al., [2023](#bib.bib27)), Active-CoT
    (Diao et al., [2023](#bib.bib5)) are investigated and datasets of commonsense
    reasoning and arithmetic reasoning are evaluated. The experimental results are
    demonstrated in Figure [3](#S2.F3 "Figure 3 ‣ 2 What LLMs learn from the demonstration?
    ‣ Heuristics-Driven Link-of-Analogy Prompting: Enhancing Large Language Models
    for Document-Level Event Argument Extraction"). It is observable that in meticulously
    designed prompts, the number of heuristics closely matches the number of examples.
    Furthermore, the number of heuristics in these carefully constructed prompts significantly
    exceeds that in prompts composed of randomly selected examples. This observation
    substantiates our hypothesis that well-constructed prompts indeed encapsulate
    diverse heuristics in examples.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调查了提示中使用的示例数量与对应提示中固有启发式数量之间的关系，涵盖了三种数据集上的六篇前沿提示研究论文。具体来说，我们研究了包括CoT（Wei等人，[2022](#bib.bib38)）、Automate-CoT（Shum等人，[2023](#bib.bib31)）、Auto-CoT（Zhang等人，[2023](#bib.bib43)）、Iter-CoT（Sun等人，[2023](#bib.bib34)）、Boosted（Pitis等人，[2023](#bib.bib27)）、Active-CoT（Diao等人，[2023](#bib.bib5)）在内的SOTA（State-of-the-Art）提示方法，并评估了常识推理和算术推理的数据集。实验结果展示在图[3](#S2.F3
    "图 3 ‣ 2 大型语言模型从演示中学到了什么？ ‣ 启发式驱动的类比提示：增强大型语言模型的文档级事件论证提取")中。可以观察到，在精心设计的提示中，启发式的数量与示例数量密切匹配。此外，这些精心构建的提示中的启发式数量显著高于由随机选择的示例组成的提示。这一观察结果证实了我们的假设，即精心构建的提示确实包含了多样化的启发式方法。
- en: '![Refer to caption](img/cb5008944067c92086af0d94b1520a95.png)![Refer to caption](img/bc6dc26f467b60c07264ce7cfa4ef01a.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/cb5008944067c92086af0d94b1520a95.png)![参考说明](img/bc6dc26f467b60c07264ce7cfa4ef01a.png)'
- en: 'Figure 4: Comparison of ICL performance using single-heuristic strategy versus
    diverse-heuristics strategy across different number of example on the StrategyQA
    and SST-2 Dataset.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：使用单一启发式策略与多样化启发式策略在不同示例数量下的ICL性能比较，基于StrategyQA和SST-2数据集。
- en: '![Refer to caption](img/d35914d1238e191fa862ed7102a55051.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d35914d1238e191fa862ed7102a55051.png)'
- en: 'Figure 5: An illustration of HD-LoA prompting.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：HD-LoA提示的示意图。
- en: 2.2 Comparing Diverse-Heuristics and Single-Heuristic Strategies
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 比较多样化启发式与单一启发式策略
- en: If our hypothesis holds true, then incorporating a broader range of heuristics
    in demonstrations should enable LLMs to identify and leverage more heuristics,
    leading to enhanced performance. With this in mind, we examine the impact of heuristic
    diversity in in-context examples on the performance of ICL. Specifically, we juxtapose
    two in-context example selection strategies on the Strategy QA dataset. The single-heuristic
    strategy formulates prompts where all rationales of in-context examples follow
    a same heuristic. Conversely, the diverse-heuristic strategy constructs prompts
    where all rationales of in-context examples exhibit different heuristics. We construct
    prompts that follow the two strategies based on the original prompts from Diao
    et al. ([2023](#bib.bib5)); Shum et al. ([2023](#bib.bib31)).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的假设成立，那么在演示中融入更多的启发式方法应能使大型语言模型（LLMs）识别和利用更多的启发式方法，从而提升性能。考虑到这一点，我们研究了上下文示例中的启发式多样性对ICL（In-Context
    Learning）性能的影响。具体来说，我们对比了在Strategy QA数据集上两种上下文示例选择策略。单一启发式策略制定的提示中，上下文示例的所有理由都遵循相同的启发式方法。相对而言，多样化启发式策略构建的提示中，上下文示例的所有理由展示了不同的启发式方法。我们根据Diao等人的原始提示（[2023](#bib.bib5)）和Shum等人的原始提示（[2023](#bib.bib31)）构建了遵循这两种策略的提示。
- en: 'The performance comparison of prompts constructed by the diverse-heuristic
    strategy and the single-heuristic strategy on the StrategyQA (Geva et al., [2021](#bib.bib14))
    and SST-2 (Socher et al., [2013](#bib.bib32)) datasets is illustrated in Figure
    [4](#S2.F4 "Figure 4 ‣ 2.1 Correlation between Example Quantity and Heuristic
    Diversity in Well-Designed Prompts ‣ 2 What LLMs learn from the demonstration?
    ‣ Heuristics-Driven Link-of-Analogy Prompting: Enhancing Large Language Models
    for Document-Level Event Argument Extraction"). From the results, it’s evident
    that with the same number of examples, the diverse-heuristics strategy significantly
    outperforms the single-heuristic approach. This finding also supports our hypothesis
    that LLMs indeed learn heuristics from in-context examples.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图[4](#S2.F4 "图 4 ‣ 2.1 示例数量与设计良好的提示中的启发式多样性之间的相关性 ‣ 2 LLM 从演示中学到什么？ ‣ 启发式驱动的类比链接提示：增强大语言模型以进行文档级事件论证提取")展示了多启发式策略与单一启发式策略在StrategyQA
    (Geva et al., [2021](#bib.bib14))和SST-2 (Socher et al., [2013](#bib.bib32))数据集上的提示性能比较。从结果来看，同样数量的示例中，多启发式策略显著优于单一启发式方法。这一发现也支持了我们的假设，即LLM确实从上下文示例中学习启发式。
- en: 3 Heuristic-Driven Demonstration Construction
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 启发式驱动的示例构建
- en: 'Building on our understanding of heuristic learning during ICL, we aim to address
    the challenge of example selection for ICL. From our validation experiments in
    the previous section, we observed that: (1) LLMs can recognize the heuristics
    underlying in-context examples and leverage them to enhance performance. (2) These
    heuristics are implicitly embedded within the explanations of in-context examples.
    As a result, LLMs’ utilization of heuristics involves a two-step process: first
    recognizing the implicit heuristics and then making inferences based on these
    heuristics.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们在ICL期间对启发式学习的理解，我们旨在解决ICL示例选择的挑战。通过前一部分的验证实验，我们观察到： (1) LLM可以识别上下文示例中隐含的启发式，并利用这些启发式提升性能。
    (2) 这些启发式隐含在上下文示例的解释中。因此，LLM对启发式的利用涉及两个步骤：首先识别隐含的启发式，然后基于这些启发式进行推断。
- en: 'The above observation leads to our idea: Can we simplify the two-step process
    by explicitly providing LLMs with task-specific heuristics? Specifically, rather
    than providing examples of input-output pairs with heuristics implicitly embedded
    in the output, our approach replaces most examples in the prompt with distinct
    task-specific heuristics, as demonstrated by the heuristics in Figure [5](#S2.F5
    "Figure 5 ‣ 2.1 Correlation between Example Quantity and Heuristic Diversity in
    Well-Designed Prompts ‣ 2 What LLMs learn from the demonstration? ‣ Heuristics-Driven
    Link-of-Analogy Prompting: Enhancing Large Language Models for Document-Level
    Event Argument Extraction").'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 上述观察引发了我们的想法：我们是否可以通过明确提供特定任务的启发式来简化这两个步骤？具体而言，与其提供带有启发式的输入-输出对示例，我们的方法用不同的任务特定启发式替换了提示中的大部分示例，如图[5](#S2.F5
    "图 5 ‣ 2.1 示例数量与设计良好的提示中的启发式多样性之间的相关性 ‣ 2 LLM 从演示中学到什么？ ‣ 启发式驱动的类比链接提示：增强大语言模型以进行文档级事件论证提取")所示。
- en: 'It is worth noting that our method still retains a minimum number of examples
    for two reasons: First, the formatting of reasoning steps, such as the link-of-analogy
    prompting proposed in the following section, requires clear illustration through
    the output of an example. Second, a minimum number of examples are necessary to
    mitigate bias in the answers. For instance, in sentiment analysis tasks, it is
    essential to provide at least two examples, one positive and one negative, to
    prevent bias towards a particular sentiment. In the context of EAE task, we only
    maintain one example to demonstrate the formatting of reasoning steps.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，我们的方法仍然保留了最少数量的示例，原因有两个：首先，如下部分提出的类比链接提示等推理步骤的格式化，需要通过示例的输出进行清晰的说明。其次，最少数量的示例是为了减轻答案中的偏见。例如，在情感分析任务中，必须提供至少两个示例，一个正面和一个负面，以防对特定情感产生偏见。在EAE任务的背景下，我们只保留一个示例来演示推理步骤的格式化。
- en: 'There are three advantages of our approach. Firstly, it provides a guidance
    on the example selection process. The example selection process of ICL is usually
    an indiscriminate, manual process (Liu et al., [2023b](#bib.bib21); Wei et al.,
    [2022](#bib.bib38); Zhou et al., [2022](#bib.bib44)). However, our method addresses
    the prevailing uncertainty in ICL example selection, converting the directionless
    and indiscriminate process into a methodical approach that emphasizes task-specific
    heuristics. In addition, the two-step recognize and inference process of utilizing
    heuristics is simplified to only one step, thus simplifying the reasoning complexity
    for LLMs. Finally, it condenses lengthy examples that consists of input-output
    pairs into compact heuristics, reducing the context length of prompts. Specifically,
    we propose the heuristic-driven demonstration construction method that contains
    two parts: low-level pattern demonstration and heuristics generation.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法有三个优势。首先，它提供了关于示例选择过程的指导。ICL 的示例选择过程通常是一个随意的人工过程（Liu et al., [2023b](#bib.bib21);
    Wei et al., [2022](#bib.bib38); Zhou et al., [2022](#bib.bib44)）。然而，我们的方法解决了 ICL
    示例选择中普遍存在的不确定性，将无方向和随意的过程转变为一种有条理的方法，强调任务特定的启发式规则。此外，利用启发式规则的两步识别和推理过程被简化为仅一步，从而简化了
    LLMs 的推理复杂性。最后，它将包含输入输出对的冗长示例压缩为紧凑的启发式规则，从而减少了提示的上下文长度。具体来说，我们提出了一种启发式驱动的演示构建方法，包括两个部分：低级模式演示和启发式生成。
- en: To provide a clearer and more precise understanding of our heuristic-driven
    demonstration construction method, we also describe it using mathematical expressions
    as follows.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更清晰、准确地理解我们的启发式驱动演示构建方法，我们还使用数学表达式进行了描述。
- en: 'In standard demonstration construction, given the question $\mathbf{Q}$ of
    the LLM’s output is generated as:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在标准演示构建中，给定问题 $\mathbf{Q}$ 的 LLM 输出生成如下：
- en: '|  | $\displaystyle o_{j}=f_{LLM}(\mathbf{C}_{8},\mathbf{Q},o_{<j})$ |  | (1)
    |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle o_{j}=f_{LLM}(\mathbf{C}_{8},\mathbf{Q},o_{<j})$ |  | (1)
    |'
- en: where $o_{<j}$
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $o_{<j}$
- en: 'In this work, our heuristic-driven demonstration construction method converts
    most examples into heuristics, retaining only the minimum number of examples necessary
    for demonstrating the formatting of reasoning steps. Therefore, the $jth$ of the
    LLM’s output, when employing our proposed method, is generated as:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们的启发式驱动演示构建方法将大多数示例转换为启发式规则，仅保留展示推理步骤格式所需的最少数量的示例。因此，当采用我们提出的方法时，LLM
    的第 $j$ 个输出生成如下：
- en: '|  | $\displaystyle o_{j}=f_{LLM}(\mathbf{H},\mathbf{C}_{m},\mathbf{Q},o_{<j})$
    |  | (2) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle o_{j}=f_{LLM}(\mathbf{H},\mathbf{C}_{m},\mathbf{Q},o_{<j})$
    |  | (2) |'
- en: where $\mathbf{H}=\{\mathbf{h}_{1},\mathbf{h}_{2},\cdots,\mathbf{h}_{l}\}$ as
    number of examples.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{H}=\{\mathbf{h}_{1},\mathbf{h}_{2},\cdots,\mathbf{h}_{l}\}$ 为示例数量。
- en: 4 Link-of-Analogy Prompting
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 类比提示链接
- en: 'We propose the link-of-analogy prompting to address the challenges below: First,
    the EAE task is characterized by its extensive variety of argument roles and event
    types, often exceeding a hundred, yet ICL examples can only cover a very limited
    subset. This discrepancy raises a critical challenge: designing a prompting strategy
    that effectively addresses unseen event types. Additionally, to streamline heuristic
    generation process, we provide heuristics for a specific argument, giver, within
    the prompt. This leads to the question of how to extend the effectiveness of giver
    heuristics to other argument roles. Finally, as highlighted in the Introduction,
    applying CoT prompting to non-reasoning tasks tends to degrade the step-by-step
    analysis into a one-step rationale (Shum et al., [2023](#bib.bib31); Diao et al.,
    [2023](#bib.bib5)), necessitating more proper prompting strategies for such tasks.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了类比提示链接以解决以下挑战：首先，EAE 任务具有广泛的论证角色和事件类型，通常超过一百种，而 ICL 示例只能覆盖非常有限的子集。这种差异带来了一个关键挑战：设计一种有效处理未见事件类型的提示策略。此外，为了简化启发式生成过程，我们在提示中提供了针对特定论证角色（giver）的启发式规则。这就产生了如何将
    giver 启发式规则的有效性扩展到其他论证角色的问题。最后，正如引言中所强调的，将 CoT 提示应用于非推理任务往往会将逐步分析降级为一步推理（Shum
    et al., [2023](#bib.bib31); Diao et al., [2023](#bib.bib5)），需要针对这些任务的更合适的提示策略。
- en: 'Inspired by the analogical reasoning (Gentner and Smith, [2013](#bib.bib13)),
    a core mechanism of human cognition, we seek to resolve the challenges presented.
    Humans often understand a new situation by drawing an analogy to a familiar situation.
    For example, students often solve new problems by mapping solutions from known
    problems (Ross, [1987](#bib.bib29)). Similarly, we anticipate that LLMs will be
    able to extract information of unseen events or generate heuristics for unseen
    argument roles by drawing an analogy to events and heuristics provided in in-context
    examples. Empirically, we find that LLMs are indeed capable of doing analogical
    reasoning when prompted appropriately. For example, when provided with the heuristic
    for giver in the prompt: "[giver] is the person, group, or organization in the
    document that gives the grant or gift", LLMs can make an analogy and generate
    the heuristic for the argument vehicle in target questions: "[vehicle] is the
    means of transport used to move the person or object".'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 受到类比推理（Gentner 和 Smith，[2013](#bib.bib13)）这一人类认知核心机制的启发，我们寻求解决所面临的挑战。人类通常通过将新情况与熟悉情况进行类比来理解新情况。例如，学生们通常通过将已知问题的解决方案映射到新问题来解决新问题（Ross，[1987](#bib.bib29)）。同样，我们预计
    LLMs 能够通过将未见事件或未见论据角色的信息类比为上下文示例中提供的事件和启发式来提取信息或生成启发式。经验上，我们发现，当给予适当提示时，LLMs 确实能够进行类比推理。例如，当在提示中提供“[giver]
    是文档中给予资助或礼物的人、团体或组织”的启发式时，LLMs 可以类比并生成目标问题中的论据车辆的启发式：“[vehicle] 是用来移动人或物体的运输工具”。
- en: 'To further enhance the analogical reasoning capabilities of LLMs, we introduce
    our link-of-analogy (LoA) prompting strategy, which emulates the analogical reasoning
    process of human. Cognitive science studies reveals that humans perform analogical
    reasoning through a sequence of retrieval, mapping, and evaluation (Gentner and
    Forbus, [2011](#bib.bib11); Gentner and Markman, [1997](#bib.bib12)). In alignment
    with this process, our method involves retrieving the most appropriate heuristic
    from the provided set, mapping this heuristic onto the target argument role to
    get new heuristic and extracting the argument based on this heuristic, and finally
    evaluating the extracted argument. This methodology is depicted through the answer
    of the in-context example in Figure [5](#S2.F5 "Figure 5 ‣ 2.1 Correlation between
    Example Quantity and Heuristic Diversity in Well-Designed Prompts ‣ 2 What LLMs
    learn from the demonstration? ‣ Heuristics-Driven Link-of-Analogy Prompting: Enhancing
    Large Language Models for Document-Level Event Argument Extraction").'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步增强大语言模型（LLMs）的类比推理能力，我们引入了类比链（LoA）提示策略，该策略模拟了人类的类比推理过程。认知科学研究表明，人类通过检索、映射和评估的过程进行类比推理（Gentner
    和 Forbus，[2011](#bib.bib11)；Gentner 和 Markman，[1997](#bib.bib12)）。与这一过程一致，我们的方法包括从提供的集合中检索最合适的启发式，将这一启发式映射到目标论据角色以获得新的启发式，并基于该启发式提取论据，最后评估提取的论据。此方法通过图
    [5](#S2.F5 "图 5 ‣ 2.1 示例数量与启发式多样性在良好设计提示中的相关性 ‣ 2 LLMs 从示例中学到了什么？ ‣ 启发式驱动的类比链提示：增强大型语言模型的文档级事件论据提取")
    中的上下文示例答案进行了描述。
- en: 5 Experiments
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 个实验
- en: '| Method | RAMS | DocEE-Normal | DocEE-Cross |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | RAMS | DocEE-Normal | DocEE-Cross |'
- en: '| Arg-I | Arg-C | Arg-C | Arg-C |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| Arg-I | Arg-C | Arg-C | Arg-C |'
- en: '| Supervised learning (few-shot) | EEQA (Du and Cardie, [2020b](#bib.bib8))
    |  | 19.54 |  |  |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 监督学习（少量样本） | EEQA (Du 和 Cardie，[2020b](#bib.bib8)) |  | 19.54 |  |  |'
- en: '|  | PAIE (Ma et al., [2022](#bib.bib22)) |  | 29.86 |  |  |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | PAIE (Ma 等，[2022](#bib.bib22)) |  | 29.86 |  |  |'
- en: '|  | TSAR (Xu et al., [2022](#bib.bib40)) | - | 26.67 | - | - |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  | TSAR (Xu 等，[2022](#bib.bib40)) | - | 26.67 | - | - |'
- en: '|  | CRP (Liu et al., [2023a](#bib.bib20)) |  | 30.09 |  |  |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | CRP (Liu 等，[2023a](#bib.bib20)) |  | 30.09 |  |  |'
- en: '|  | FewDocAE (Yang et al., [2023](#bib.bib41)) |  | - | 12.07 | 10.51 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | FewDocAE (Yang 等，[2023](#bib.bib41)) |  | - | 12.07 | 10.51 |'
- en: '| text-davinci-003 | Standard (Agrawal et al., [2022](#bib.bib1)) | 39.96 |
    31.6 | 25.55 | 25.41 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-003 | 标准 (Agrawal 等，[2022](#bib.bib1)) | 39.96 | 31.6 | 25.55
    | 25.41 |'
- en: '|  | CoT (Wei et al., [2022](#bib.bib38)) | 43.03 | 34.94 | 27.68 | 28.64 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | CoT (Wei 等，[2022](#bib.bib38)) | 43.03 | 34.94 | 27.68 | 28.64 |'
- en: '|  | HD-LoA (ours) | 46.08 | 39.47 | 30.37 | 31.21 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | HD-LoA (我们的) | 46.08 | 39.47 | 30.37 | 31.21 |'
- en: '| gpt-3.5-turbo | Standard (Agrawal et al., [2022](#bib.bib1)) | 42.63 | 32.11
    | 26.56 | 25.64 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| gpt-3.5-turbo | 标准 (Agrawal 等，[2022](#bib.bib1)) | 42.63 | 32.11 | 26.56
    | 25.64 |'
- en: '|  | CoT (Wei et al., [2022](#bib.bib38)) | 33.49 | 27.86 | 25.66 | 26.17 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | CoT (Wei 等，[2022](#bib.bib38)) | 33.49 | 27.86 | 25.66 | 26.17 |'
- en: '|  | HD-LoA (ours) | 38.33 | 31.5 | 27.85 | 28.46 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | HD-LoA (我们的) | 38.33 | 31.5 | 27.85 | 28.46 |'
- en: '| gpt-4 | Standard (Agrawal et al., [2022](#bib.bib1)) | 44.73 | 37.08 |  |  |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| gpt-4 | 标准 (Agrawal et al., [2022](#bib.bib1)) | 44.73 | 37.08 |  |  |'
- en: '|  | CoT (Wei et al., [2022](#bib.bib38)) | 44.93 | 38.09 | - | - |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  | CoT (Wei et al., [2022](#bib.bib38)) | 44.93 | 38.09 | - | - |'
- en: '|  | HD-LoA (ours) | 50.4 | 42.75 |  |  |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | HD-LoA (我们的) | 50.4 | 42.75 |  |  |'
- en: 'Table 1: Overall performance. In few-shot setting, supervised learning methods
    utilize 70 samples on the RAMS dataset and 2 samples on the DocEE dataset for
    training. In contrast, our HD-LoA method only uses a single sample.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：总体表现。在少样本设置下，监督学习方法在 RAMS 数据集上使用 70 个样本进行训练，在 DocEE 数据集上使用 2 个样本。相比之下，我们的
    HD-LoA 方法仅使用一个样本。
- en: In this section, we aim to explore the following research questions (RQs) regarding
    our Heuristic-Driven Link-of-Analogy (HD-LoA) prompting. RQ1 Does HD-LoA prompting
    improve in-context learning performance in document-level EAE task? RQ2 How does
    the HD-LoA prompting compare to supervised learning methods under few-shot setting?
    RQ3 Is the HD-LoA prompting method effective when applied to tasks beyond document-level
    EAE? RQ4 Does different components of the HD-LoA prompting effectively contributing
    to its performance?
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们旨在探讨以下有关我们的启发式驱动的类比连接 (HD-LoA) 提示的研究问题 (RQs)。RQ1 HD-LoA 提示是否能提升文档级 EAE
    任务中的上下文学习表现？RQ2 在少样本设置下，HD-LoA 提示如何与监督学习方法进行比较？RQ3 当应用于文档级 EAE 之外的任务时，HD-LoA 提示方法是否有效？RQ4
    HD-LoA 提示的不同组件是否有效地贡献于其表现？
- en: 5.1 Experimental Setup
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 实验设置
- en: 'Dataset: For the evaluation of the document-level EAE task, we adopt RAMS (Ebner
    et al., [2020](#bib.bib9)) and DocEE (Tong et al., [2022](#bib.bib35)) datasets.
    The WIKIEVENTS dataset (Li et al., [2021](#bib.bib18)) is excluded from our study
    because it relies on preprocessed entity candidates for annotating event arguments
    the annotation, which diverges from the direct argument identification of LLMs.
    For evaluation, we follow the metrics in (Ma et al., [2022](#bib.bib22)), namely
    the argument identification F1 score (Arg-I), and the argument classification
    F1 score (Arg-C).'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集：为了评估文档级 EAE 任务，我们采用了 RAMS (Ebner et al., [2020](#bib.bib9)) 和 DocEE (Tong
    et al., [2022](#bib.bib35)) 数据集。WIKIEVENTS 数据集 (Li et al., [2021](#bib.bib18))
    被排除在我们的研究之外，因为它依赖于预处理的实体候选项来标注事件参数，这与 LLM 的直接参数识别不同。为了评估，我们遵循 (Ma et al., [2022](#bib.bib22))
    中的指标，即参数识别 F1 分数 (Arg-I) 和参数分类 F1 分数 (Arg-C)。
- en: 'Additionally, we utilize the SST-2 (Socher et al., [2013](#bib.bib32)) and
    SNLI (Bowman et al., [2015](#bib.bib3)) datasets to assess the effectiveness of
    our HD-LoA prompting strategy on other non-reasoning tasks: sentiment analysis
    and natural language inference. The detailed statistics of the datasets and the
    number of tested samples are listed in Appendix [A](#A1 "Appendix A Experimental
    Details ‣ Heuristics-Driven Link-of-Analogy Prompting: Enhancing Large Language
    Models for Document-Level Event Argument Extraction")'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还利用 SST-2 (Socher et al., [2013](#bib.bib32)) 和 SNLI (Bowman et al., [2015](#bib.bib3))
    数据集来评估我们 HD-LoA 提示策略在其他非推理任务上的有效性：情感分析和自然语言推理。数据集的详细统计信息和测试样本的数量列在附录 [A](#A1 "附录
    A 实验细节 ‣ 启发式驱动的类比连接提示：提升大语言模型在文档级事件参数提取中的表现")。
- en: Baselines Our HD-LoA approach is compared against several state-of-the-art prompting
    methods, including the standard prompting method used in document-level EAE (Agrawal
    et al., [2022](#bib.bib1)), and the Chain-of-Thought (CoT) prompting (Wei et al.,
    [2022](#bib.bib38)). Additionally, we compare our method with various supervised
    learning methods in EAE, such as FewDocAE (Yang et al., [2023](#bib.bib41)), CRP
    (Liu et al., [2023a](#bib.bib20)), PAIE (Ma et al., [2022](#bib.bib22)), TSAR
    (Xu et al., [2022](#bib.bib40)), EEQA (Du and Cardie, [2020b](#bib.bib8)), etc.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 基准：我们的 HD-LoA 方法与几种最先进的提示方法进行了比较，包括文档级 EAE 使用的标准提示方法 (Agrawal et al., [2022](#bib.bib1))
    和 Chain-of-Thought (CoT) 提示 (Wei et al., [2022](#bib.bib38))。此外，我们还将我们的方法与 EAE
    中的各种监督学习方法进行了比较，如 FewDocAE (Yang et al., [2023](#bib.bib41))、CRP (Liu et al.,
    [2023a](#bib.bib20))、PAIE (Ma et al., [2022](#bib.bib22))、TSAR (Xu et al., [2022](#bib.bib40))、EEQA
    (Du and Cardie, [2020b](#bib.bib8)) 等。
- en: 'LLMs: The experiments are carried out using three prominent large language
    models: the publicly available GPT-3 (Brown et al., [2020](#bib.bib4)) in its
    text-davinci-003 and gpt-3.5-turbo versions (Ouyang et al., [2022](#bib.bib26)),
    as well as GPT-4 (OpenAI, [2023](#bib.bib25)). Due to the high cost associated
    with using GPT-4, its evaluation is limited solely to the RAMS dataset. These
    models are accessed via the public APIs from OpenAI’s services¹¹1https://openai.com/api/.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs：实验使用了三种知名的大型语言模型：公开的 GPT-3 (Brown et al., [2020](#bib.bib4)) 的 text-davinci-003
    和 gpt-3.5-turbo 版本 (Ouyang et al., [2022](#bib.bib26))，以及 GPT-4 (OpenAI, [2023](#bib.bib25))。由于使用
    GPT-4 的成本较高，其评估仅限于 RAMS 数据集。这些模型通过 OpenAI 服务的公开 API 访问¹¹1https://openai.com/api/。
- en: 'Experimental Details: Please find more information about experimental details
    in Appendix [A](#A1 "Appendix A Experimental Details ‣ Heuristics-Driven Link-of-Analogy
    Prompting: Enhancing Large Language Models for Document-Level Event Argument Extraction")
    and the prompts we used in Appendix [C](#A3 "Appendix C Full Prompts ‣ Appendix
    B Recognize Implicit Heuristics of In-Context Examples by GPT-4 ‣ Heuristics-Driven
    Link-of-Analogy Prompting: Enhancing Large Language Models for Document-Level
    Event Argument Extraction").'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 实验细节：有关实验细节的更多信息请参见附录 [A](#A1 "附录 A 实验细节 ‣ 启发式驱动的类比链接提示：提升大型语言模型的文档级事件论证提取")
    和我们使用的提示请参见附录 [C](#A3 "附录 C 完整提示 ‣ 附录 B 通过 GPT-4 识别上下文示例的隐性启发式 ‣ 启发式驱动的类比链接提示：提升大型语言模型的文档级事件论证提取")。
- en: 5.2 Overall Experimental Results
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 总体实验结果
- en: 'Addressing RQ1, the experimental results presented in Table [1](#S5.T1 "Table
    1 ‣ 5 Experiments ‣ Heuristics-Driven Link-of-Analogy Prompting: Enhancing Large
    Language Models for Document-Level Event Argument Extraction") indicate that our
    HD-LoA prompting significantly enhances in-context learning for document-level
    EAE task. The HD-LoA method consistently surpasses CoT prompting (Wei et al.,
    [2022](#bib.bib38)) across all three LLMs and both datasets, achieving the largest
    F1 score improvements of $4.53\%$ on the text-davinci-003 model. However, both
    CoT and HD-LoA prompting show suboptimal performance on the gpt-3.5-turbo model
    within the RAMS dataset, possibly due to the model’s tradeoff, which favor enhanced
    dialogue ability at the expense of reasoning ability, potentially impeding the
    effectiveness of reasoning-based prompting methods.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 针对 RQ1，表 [1](#S5.T1 "表 1 ‣ 5 实验 ‣ 启发式驱动的类比链接提示：提升大型语言模型的文档级事件论证提取") 中展示的实验结果表明，我们的
    HD-LoA 提示显著增强了文档级 EAE 任务中的上下文学习。HD-LoA 方法在所有三种 LLM 和两个数据集上始终超越了 CoT 提示 (Wei et
    al., [2022](#bib.bib38))，在 text-davinci-003 模型上取得了 $4.53\%$ 的最大 F1 分数提升。然而，CoT
    和 HD-LoA 提示在 RAMS 数据集中的 gpt-3.5-turbo 模型上表现均不佳，可能是因为该模型在对话能力和推理能力之间的权衡，可能影响了基于推理的提示方法的效果。
- en: In response to RQ2, our HD-LoA method not only significantly enhances performance
    in few-shot settings compared to supervised learning approaches but also achieves
    these impressive results with considerably fewer labeled samples. On the RAMS
    dataset, HD-LoA achieves a $9.38\%$ improvement. Experimental findings indicate
    that our method can successfully mitigate the document-level EAE task’s reliance
    on extensive labeled data.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 针对 RQ2，我们的 HD-LoA 方法不仅在少样本设置中显著提升了性能，相比于监督学习方法，而且以显著更少的标注样本取得了这些令人印象深刻的结果。在
    RAMS 数据集上，HD-LoA 实现了 $9.38\%$ 的提升。实验结果表明，我们的方法可以成功减轻文档级 EAE 任务对大量标注数据的依赖。
- en: '|  | SST-2 | SNLI |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | SST-2 | SNLI |'
- en: '| CoT | 91.39 | 77.97 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| CoT | 91.39 | 77.97 |'
- en: '| HD-LoA (ours) | 94.26 | 80.60 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| HD-LoA (我们的) | 94.26 | 80.60 |'
- en: 'Table 2: Evaluation of the HD-LoA prompting on sentiment analysis and natural
    language inference tasks.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：HD-LoA 提示在情感分析和自然语言推理任务中的评估。
- en: 5.3 Adaptability of HD-LoA Prompting for Other Tasks
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 HD-LoA 提示在其他任务中的适应性
- en: 'In addressing RQ3, we have extended our HD-LoA prompting method to sentiment
    analysis (SA) and natural language inference (NLI) tasks, utilizing the SST-2
    (Socher et al., [2013](#bib.bib32)) and SNLI (Bowman et al., [2015](#bib.bib3))
    datasets for evaluation. We adopt the CoT style prompts on these two datasets
    from Shum et al. ([2023](#bib.bib31)). Experimental results are presented in Table
    [2](#S5.T2 "Table 2 ‣ 5.2 Overall Experimental Results ‣ 5 Experiments ‣ Heuristics-Driven
    Link-of-Analogy Prompting: Enhancing Large Language Models for Document-Level
    Event Argument Extraction"). Compared to CoT prompting, our method gets accuracy
    enhancements of $2.87\%$ on SST-2 and SNLI datasets, respectively. These findings
    indicate that our HD-LoA prompting can be effectively adapted to a diverse array
    of non-reasoning NLP tasks. The prompts for SA and NLI tasks are provided in Appendix
    [C](#A3 "Appendix C Full Prompts ‣ Appendix B Recognize Implicit Heuristics of
    In-Context Examples by GPT-4 ‣ Heuristics-Driven Link-of-Analogy Prompting: Enhancing
    Large Language Models for Document-Level Event Argument Extraction").'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '针对RQ3，我们将HD-LoA提示方法扩展到情感分析（SA）和自然语言推理（NLI）任务，利用SST-2 (Socher et al., [2013](#bib.bib32))
    和SNLI (Bowman et al., [2015](#bib.bib3))数据集进行评估。我们采用了Shum et al. ([2023](#bib.bib31))的CoT风格提示。在表[2](#S5.T2
    "Table 2 ‣ 5.2 Overall Experimental Results ‣ 5 Experiments ‣ Heuristics-Driven
    Link-of-Analogy Prompting: Enhancing Large Language Models for Document-Level
    Event Argument Extraction")中展示了实验结果。与CoT提示法相比，我们的方法在SST-2和SNLI数据集上分别获得了$2.87\%$的准确性提升。这些发现表明我们的HD-LoA提示可以有效适应各种非推理的NLP任务。SA和NLI任务的提示见附录[C](#A3
    "Appendix C Full Prompts ‣ Appendix B Recognize Implicit Heuristics of In-Context
    Examples by GPT-4 ‣ Heuristics-Driven Link-of-Analogy Prompting: Enhancing Large
    Language Models for Document-Level Event Argument Extraction")。'
- en: '| Method | RAMS | DocEE-Normal | DocEE-Cross |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | RAMS | DocEE-Normal | DocEE-Cross |'
- en: '| --- | --- | --- | --- |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Arg-I | Arg-C | Arg-C | Arg-C |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| Arg-I | Arg-C | Arg-C | Arg-C |'
- en: '| --- | --- | --- | --- |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Supervised learning | EEQA (Du and Cardie, [2020b](#bib.bib8)) | 48.70 |
    46.70 | 33.50 | 24.00 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 监督学习 | EEQA (Du and Cardie, [2020b](#bib.bib8)) | 48.70 | 46.70 | 33.50 |
    24.00 |'
- en: '|  | MG-Reader (Du and Cardie, [2020a](#bib.bib7)) | - | - | 32.90 | 21.40
    |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  | MG-Reader (Du and Cardie, [2020a](#bib.bib7)) | - | - | 32.90 | 21.40
    |'
- en: '|  | BART-Gen (Li et al., [2021](#bib.bib18)) | 51.20 | 47.10 | - | - |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  | BART-Gen (Li et al., [2021](#bib.bib18)) | 51.20 | 47.10 | - | - |'
- en: '|  | OntologyQA (Tong et al., [2022](#bib.bib35)) | - | - | 41.00 | 29.80 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  | OntologyQA (Tong et al., [2022](#bib.bib35)) | - | - | 41.00 | 29.80 |'
- en: '|  | PAIE (Ma et al., [2022](#bib.bib22)) | 56.80 | 52.20 | - | - |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  | PAIE (Ma et al., [2022](#bib.bib22)) | 56.80 | 52.20 | - | - |'
- en: '| text-davinci-003 | HD-LoA (ours) | 46.08 | 39.47 | 30.37 | 31.21 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-003 | HD-LoA (我们的) | 46.08 | 39.47 | 30.37 | 31.21 |'
- en: 'Table 3: Comparison with Fully Trained Supervised Models.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：与完全训练的监督模型的比较。
- en: 5.4 Comparison with Fully Trained Supervised Models
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 与完全训练的监督模型的比较
- en: 'Additionally, we compare our HD-LoA method with supervised learning method
    that trained on the entire dataset for document-level EAE task. As illustrated
    in Table [3](#S5.T3 "Table 3 ‣ 5.3 Adaptability of HD-LoA Prompting for Other
    Tasks ‣ 5 Experiments ‣ Heuristics-Driven Link-of-Analogy Prompting: Enhancing
    Large Language Models for Document-Level Event Argument Extraction"), it is anticipated
    that these models trained on thousands of samples would exhibit higher accuracy
    compared to our HD-LoA method, which employs only a single labeled sample. Nevertheless,
    HD-LoA prompting demonstrates competitive performance against supervised methods
    and even outperform these extensively trained models on the DocEE dataset in the
    cross-domain setting. This finding also illustrates the effectiveness of our HD-LoA
    prompting strategy, particularly in scenarios where it is impractical and costly
    to build large annotated datasets.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，我们将我们的HD-LoA方法与在整个数据集上训练的监督学习方法进行了比较，用于文档级EAE任务。如表[3](#S5.T3 "Table 3 ‣
    5.3 Adaptability of HD-LoA Prompting for Other Tasks ‣ 5 Experiments ‣ Heuristics-Driven
    Link-of-Analogy Prompting: Enhancing Large Language Models for Document-Level
    Event Argument Extraction")所示，预计这些在数千个样本上训练的模型将比仅使用一个标记样本的HD-LoA方法具有更高的准确性。尽管如此，HD-LoA提示法在与监督方法的竞争表现中仍然表现出竞争力，甚至在DocEE数据集的跨域设置中超越了这些广泛训练的模型。这一发现还说明了我们的HD-LoA提示策略的有效性，特别是在构建大型标注数据集不切实际且成本高昂的情况下。'
- en: '|  | RAMS |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  | RAMS |'
- en: '| --- | --- |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Without heuristic | 36.42 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 无启发式 | 36.42 |'
- en: '| Without LoA | 36.03 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 无 LoA | 36.03 |'
- en: '| HD-LoA | 39.47 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| HD-LoA | 39.47 |'
- en: 'Table 4: Ablations on the HD-LoA prompting method.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：对HD-LoA提示方法的消融实验。
- en: 5.5 Ablations
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 消融实验
- en: 'To address RQ4, we carry out ablation experiments to discern how each component
    contributes to the overall performance of our HD-LoA prompting method. As presented
    in Table [4](#S5.T4 "Table 4 ‣ 5.4 Comparison with Fully Trained Supervised Models
    ‣ 5 Experiments ‣ Heuristics-Driven Link-of-Analogy Prompting: Enhancing Large
    Language Models for Document-Level Event Argument Extraction"), experimental results
    demonstrate that removing either the task-specific heuristics or link-of-analogy
    prompting will significantly degrade the eperformance of the HD-LoA prompting,
    suggesting the importance of each part of our prompting strategy.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '为了解决RQ4，我们进行消融实验，以辨别每个组件对我们HD-LoA提示方法整体性能的贡献。如表[4](#S5.T4 "Table 4 ‣ 5.4 Comparison
    with Fully Trained Supervised Models ‣ 5 Experiments ‣ Heuristics-Driven Link-of-Analogy
    Prompting: Enhancing Large Language Models for Document-Level Event Argument Extraction")所示，实验结果表明，去除任务特定启发式方法或类比链接提示将显著降低HD-LoA提示的表现，表明我们提示策略中每个部分的重要性。'
- en: 6 Related Work
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 相关工作
- en: Document-level EAE Existing document-level EAE studies are mostly based on supervised
    learning methods, which relies on the extensive collection of labeled data (Ma
    et al., [2022](#bib.bib22); Pouran Ben Veyseh et al., [2022](#bib.bib28); Zhou
    and Mao, [2022](#bib.bib45); Xu et al., [2021](#bib.bib39); Ebner et al., [2020](#bib.bib9);
    Du and Cardie, [2020a](#bib.bib7)), only Agrawal et al. ([2022](#bib.bib1)) exploit
    adopting LLMs on clinical EAE though standard prompts that are not enhanced by
    reasoning steps. Considering the revolutionize impact of LLMs on NLP community
    and the potential of ICL to reduce the dependency on large-scale labeled datasets,
    it is lack of study on the exploration of adopting SOTA prompting strategies on
    EAE.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 文档级EAE 现有的文档级EAE研究大多基于监督学习方法，这些方法依赖于大量标记数据的收集（Ma et al., [2022](#bib.bib22);
    Pouran Ben Veyseh et al., [2022](#bib.bib28); Zhou and Mao, [2022](#bib.bib45);
    Xu et al., [2021](#bib.bib39); Ebner et al., [2020](#bib.bib9); Du and Cardie,
    [2020a](#bib.bib7)），只有Agrawal et al. ([2022](#bib.bib1))探索了在临床EAE中采用LLMs，尽管标准提示并未通过推理步骤得到增强。考虑到LLMs对NLP社区的革命性影响及ICL在减少对大规模标记数据集依赖的潜力，目前尚缺乏关于在EAE中采用SOTA提示策略的研究。
- en: In-context learning ICL enables LLMs to perform a target task by feeding a few
    prompted examples as part of the input (Brown et al., [2020](#bib.bib4)). Without
    gradient update, ICL allows a single LLM to perform various tasks universally.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文学习 ICL使得LLMs可以通过将几个提示示例作为输入的一部分来执行目标任务（Brown et al., [2020](#bib.bib4)）。在没有梯度更新的情况下，ICL允许单个LLM普遍执行各种任务。
- en: 'Despite being promising, the work mechanism for ICL remains unclear (Dong et al.,
    [2022](#bib.bib6)). Few studies have conducted preliminary explorations: Min et al.
    ([2022](#bib.bib23)) showed that the label space, input text distribution and
    overall format contribute substantially to the ICL performance. Liu et al. ([2022](#bib.bib19))
    concluded that examples that are semantically-similar to the test sample are more
    effective in unleashing LLM’s extensive knowledge. Akyürek et al. ([2022](#bib.bib2))
    found that transformer-based ICL can implement standard finetuning algorithms
    implicitly. In this work, we further hypothesize and validate that LLMs learn
    task-task specific heuristics from examples via ICL.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前景广阔，ICL的工作机制仍不明确（Dong et al., [2022](#bib.bib6)）。已有少量研究进行了初步探索：Min et al.
    ([2022](#bib.bib23))表明标签空间、输入文本分布和整体格式对ICL性能有重要影响。Liu et al. ([2022](#bib.bib19))得出结论，语义上类似于测试样本的示例更能有效释放LLM的广泛知识。Akyürek
    et al. ([2022](#bib.bib2))发现基于transformer的ICL可以隐式地实现标准微调算法。在本工作中，我们进一步假设并验证LLMs通过ICL从示例中学习任务特定启发式方法。
- en: In addition, the performance of ICL is very sensitive to example selection (Gonen
    et al., [2022](#bib.bib15)). Studies suggested different ways of selecting examples
    based on complexity (Fu et al., [2022](#bib.bib10)), mutual information (Sorensen
    et al., [2022](#bib.bib33)), diversity (Zhang et al., [2023](#bib.bib43)), labeled
    dataset (Shum et al., [2023](#bib.bib31)), and so on. In this work, we convert
    the indiscriminate example selection process into a methodical approach that emphasizes
    task heuristics, making the example selection process more transparent.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，ICL的表现对示例选择非常敏感（Gonen et al., [2022](#bib.bib15)）。研究建议了基于复杂性（Fu et al., [2022](#bib.bib10)）、互信息（Sorensen
    et al., [2022](#bib.bib33)）、多样性（Zhang et al., [2023](#bib.bib43)）、标记数据集（Shum et
    al., [2023](#bib.bib31)）等的不同示例选择方法。在本工作中，我们将随意示例选择过程转换为强调任务启发式的系统方法，使示例选择过程更加透明。
- en: Chain-of-thought prompting CoT is proposed by (Wei et al., [2022](#bib.bib38))
    and adopted in its following works (Wang et al., [2022a](#bib.bib36); Fu et al.,
    [2022](#bib.bib10); Zhang et al., [2023](#bib.bib43)) as a paradigm for prompting
    LLMs. It elicits LLMs to decompose the complex problem into intermediate reasoning
    steps and improves the performance on several reasoning tasks like arithmetic
    reasoning, commonsense reasoning, and symbolic reasoning. However, when applying
    CoT on non-reasoning tasks, due to the different nature of task, the step-by-step
    reasoning process is degraded to a one-step reasoning process (Shum et al., [2023](#bib.bib31);
    Wang et al., [2022b](#bib.bib37)), thus limited its performance on non-reasoning
    tasks.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Chain-of-thought提示CoT由（Wei等，[2022](#bib.bib38)）提出，并在其后续工作中采用（Wang等，[2022a](#bib.bib36)；Fu等，[2022](#bib.bib10)；Zhang等，[2023](#bib.bib43)）作为提示LLMs的范式。它促使LLMs将复杂问题分解为中间推理步骤，并提高了在算术推理、常识推理和符号推理等几个推理任务中的表现。然而，在非推理任务中应用CoT时，由于任务性质的不同，逐步推理过程被退化为一步推理过程（Shum等，[2023](#bib.bib31)；Wang等，[2022b](#bib.bib37)），从而限制了其在非推理任务中的表现。
- en: 7 Conclusion
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: In this work, we hypothesize and validate that LLMs learn task-specific heuristics
    from demonstrations via in-context learning, which can provide a guidance on the
    example selection process. Building upon this hypothesis, we introduce an explicit
    heuristic-driven demonstration construction strategy, and propose a link-of-analogy
    prompting method. Extensive experimentation reveals that the HD-LoA prompting
    not only outperforms cutting-edge prompting methods and few-shot supervised methods
    in document-level EAE task, but also exhibits effectiveness across various non-reasoning
    NLP tasks.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们假设并验证了LLMs通过上下文学习从示例中学习任务特定启发式，这可以为示例选择过程提供指导。在此假设基础上，我们引入了显式启发式驱动的示例构建策略，并提出了一种类比链接提示方法。广泛的实验揭示了HD-LoA提示不仅在文档级EAE任务中优于最前沿的提示方法和少量监督方法，而且在各种非推理自然语言处理任务中也表现出有效性。
- en: Limitations
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: In this work, we propose the HD-LoA prompting strategy for non-reasoning NLP
    tasks. Far fewer examples are adopted in the prompt and the analogical reasoning
    ability is activated in the HD-LoA prompting. Thus our prompting method relies
    on the advanced reasoning ability of LLMs. As a result, it is not suitable for
    LLMs that lack enough reasoning ability, such as gpt-turbo.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了HD-LoA提示策略用于非推理自然语言处理任务。在提示中采用的示例较少，并且在HD-LoA提示中激活了类比推理能力。因此，我们的提示方法依赖于LLMs的高级推理能力。因此，这种方法不适用于缺乏足够推理能力的LLMs，如gpt-turbo。
- en: Acknowledgements
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: The authors would like to thank Edmond Lo, Lihui Chen, Xiyu Zhang for their
    constructive comments and suggestions. The research was conducted at the Future
    Resilient Systems at the Singapore-ETH Centre, which was established collaboratively
    between ETH Zurich and the National Research Foundation Singapore. This research
    is supported by the National Research Foundation Singapore (NRF) under its Campus
    for Research Excellence and Technological Enterprise (CREATE) programme.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 作者感谢Edmond Lo, Lihui Chen, Xiyu Zhang对他们建设性的评论和建议。研究在新加坡-ETH中心的未来韧性系统进行，该中心由苏黎世联邦理工学院与新加坡国家研究基金会合作建立。该研究得到新加坡国家研究基金会（NRF）在其卓越研究和技术企业（CREATE）计划下的支持。
- en: References
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Agrawal et al. (2022) Monica Agrawal, Stefan Hegselmann, Hunter Lang, Yoon Kim,
    and David Sontag. 2022. [Large language models are few-shot clinical information
    extractors](https://aclanthology.org/2022.emnlp-main.130). In *Proceedings of
    the 2022 Conference on Empirical Methods in Natural Language Processing*, pages
    1998–2022, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agrawal等（2022）Monica Agrawal, Stefan Hegselmann, Hunter Lang, Yoon Kim, 和David
    Sontag. 2022. [大型语言模型是少量临床信息提取器](https://aclanthology.org/2022.emnlp-main.130)。在*2022年自然语言处理实证方法会议论文集*中，第1998–2022页，阿布扎比，阿拉伯联合酋长国。计算语言学协会。
- en: Akyürek et al. (2022) Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma,
    and Denny Zhou. 2022. What learning algorithm is in-context learning? investigations
    with linear models. *arXiv preprint arXiv:2211.15661*.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Akyürek等（2022）Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, 和Denny
    Zhou. 2022. 什么学习算法是上下文学习？用线性模型进行的调查。*arXiv预印本 arXiv:2211.15661*。
- en: Bowman et al. (2015) Samuel R. Bowman, Gabor Angeli, Christopher Potts, and
    Christopher D. Manning. 2015. [A large annotated corpus for learning natural language
    inference](https://doi.org/10.18653/v1/D15-1075). In *Proceedings of the 2015
    Conference on Empirical Methods in Natural Language Processing*, pages 632–642,
    Lisbon, Portugal. Association for Computational Linguistics.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bowman 等 (2015) Samuel R. Bowman, Gabor Angeli, Christopher Potts 和 Christopher
    D. Manning. 2015. [用于学习自然语言推理的大型标注语料库](https://doi.org/10.18653/v1/D15-1075)。在
    *2015年自然语言处理实证方法会议论文集*中，第632–642页，葡萄牙里斯本。计算语言学协会。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等 (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell 等. 2020. 语言模型是少样本学习者。*神经信息处理系统进展*, 33:1877–1901。
- en: Diao et al. (2023) Shizhe Diao, Pengcheng Wang, Yong Lin, and Tong Zhang. 2023.
    Active prompting with chain-of-thought for large language models. *arXiv preprint
    arXiv:2302.12246*.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Diao 等 (2023) Shizhe Diao, Pengcheng Wang, Yong Lin 和 Tong Zhang. 2023. 利用链式思维的主动提示用于大型语言模型。*arXiv
    预印本 arXiv:2302.12246*。
- en: Dong et al. (2022) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao
    Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022. A survey for in-context learning.
    *arXiv preprint arXiv:2301.00234*.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等 (2022) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao
    Chang, Xu Sun, Jingjing Xu 和 Zhifang Sui. 2022. 上下文学习调查。*arXiv 预印本 arXiv:2301.00234*。
- en: Du and Cardie (2020a) Xinya Du and Claire Cardie. 2020a. [Document-level event
    role filler extraction using multi-granularity contextualized encoding](https://doi.org/10.18653/v1/2020.acl-main.714).
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pages 8010–8020, Online. Association for Computational Linguistics.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du 和 Cardie (2020a) Xinya Du 和 Claire Cardie. 2020a. [使用多粒度上下文化编码的文档级事件角色填充提取](https://doi.org/10.18653/v1/2020.acl-main.714)。在
    *第58届计算语言学协会年会论文集*中，第8010–8020页，在线。计算语言学协会。
- en: Du and Cardie (2020b) Xinya Du and Claire Cardie. 2020b. [Event extraction by
    answering (almost) natural questions](https://doi.org/10.18653/v1/2020.emnlp-main.49).
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*, pages 671–683, Online. Association for Computational Linguistics.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du 和 Cardie (2020b) Xinya Du 和 Claire Cardie. 2020b. [通过回答（几乎是）自然问题进行事件提取](https://doi.org/10.18653/v1/2020.emnlp-main.49)。在
    *2020年自然语言处理实证方法会议（EMNLP）论文集*中，第671–683页，在线。计算语言学协会。
- en: Ebner et al. (2020) Seth Ebner, Patrick Xia, Ryan Culkin, Kyle Rawlins, and
    Benjamin Van Durme. 2020. [Multi-sentence argument linking](https://doi.org/10.18653/v1/2020.acl-main.718).
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pages 8057–8077, Online. Association for Computational Linguistics.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ebner 等 (2020) Seth Ebner, Patrick Xia, Ryan Culkin, Kyle Rawlins 和 Benjamin
    Van Durme. 2020. [多句子论证链接](https://doi.org/10.18653/v1/2020.acl-main.718)。在 *第58届计算语言学协会年会论文集*中，第8057–8077页，在线。计算语言学协会。
- en: Fu et al. (2022) Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar
    Khot. 2022. Complexity-based prompting for multi-step reasoning. *arXiv preprint
    arXiv:2210.00720*.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等 (2022) Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark 和 Tushar Khot. 2022.
    基于复杂性的多步推理提示。*arXiv 预印本 arXiv:2210.00720*。
- en: 'Gentner and Forbus (2011) Dedre Gentner and Kenneth D Forbus. 2011. Computational
    models of analogy. *Wiley interdisciplinary reviews: cognitive science*, 2(3):266–276.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gentner 和 Forbus (2011) Dedre Gentner 和 Kenneth D Forbus. 2011. 类比的计算模型。*Wiley
    跨学科评论：认知科学*, 2(3):266–276。
- en: Gentner and Markman (1997) Dedre Gentner and Arthur B Markman. 1997. Structure
    mapping in analogy and similarity. *American psychologist*, 52(1):45.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gentner 和 Markman (1997) Dedre Gentner 和 Arthur B Markman. 1997. 类比和相似中的结构映射。*美国心理学家*,
    52(1):45。
- en: Gentner and Smith (2013) Dedre Gentner and Linsey A Smith. 2013. Analogical
    learning and reasoning. In *The Oxford Handbook of Cognitive Psychology*. Oxford
    University Press.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gentner 和 Smith (2013) Dedre Gentner 和 Linsey A Smith. 2013. 类比学习与推理。在 *牛津认知心理学手册*。牛津大学出版社。
- en: Geva et al. (2021) Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth,
    and Jonathan Berant. 2021. Did aristotle use a laptop? a question answering benchmark
    with implicit reasoning strategies. *Transactions of the Association for Computational
    Linguistics*, 9:346–361.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geva et al. (2021) Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth,
    和 Jonathan Berant. 2021. 亚里士多德是否使用了笔记本电脑？一个带有隐式推理策略的问答基准。*计算语言学协会会刊*，9：346–361。
- en: Gonen et al. (2022) Hila Gonen, Srini Iyer, Terra Blevins, Noah A Smith, and
    Luke Zettlemoyer. 2022. Demystifying prompts in language models via perplexity
    estimation. *arXiv preprint arXiv:2212.04037*.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gonen et al. (2022) Hila Gonen, Srini Iyer, Terra Blevins, Noah A Smith, 和 Luke
    Zettlemoyer. 2022. 通过困惑度估计解密语言模型中的提示。*arXiv预印本 arXiv:2212.04037*。
- en: Grishman (2019) Ralph Grishman. 2019. Twenty-five years of information extraction.
    *Natural Language Engineering*, 25(6):677–692.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grishman (2019) Ralph Grishman. 2019. 二十五年的信息提取。*自然语言工程*，25(6)：677–692。
- en: 'Kim et al. (2018) Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James
    Wexler, Fernanda Viegas, et al. 2018. Interpretability beyond feature attribution:
    Quantitative testing with concept activation vectors (tcav). In *International
    conference on machine learning*, pages 2668–2677\. PMLR.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. (2018) Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James
    Wexler, Fernanda Viegas 等. 2018. 超越特征归因的可解释性：使用概念激活向量（tcav）的定量测试。在 *国际机器学习会议*
    中，页码2668–2677。PMLR。
- en: 'Li et al. (2021) Sha Li, Heng Ji, and Jiawei Han. 2021. [Document-level event
    argument extraction by conditional generation](https://doi.org/10.18653/v1/2021.naacl-main.69).
    In *Proceedings of the 2021 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 894–908, Online.
    Association for Computational Linguistics.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2021) Sha Li, Heng Ji, 和 Jiawei Han. 2021. [基于条件生成的文档级事件论证提取](https://doi.org/10.18653/v1/2021.naacl-main.69)。在
    *2021年北美计算语言学协会年会论文集：人类语言技术* 中，页码894–908，在线。计算语言学协会。
- en: 'Liu et al. (2022) Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence
    Carin, and Weizhu Chen. 2022. [What makes good in-context examples for GPT-3?](https://doi.org/10.18653/v1/2022.deelio-1.10)
    In *Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on
    Knowledge Extraction and Integration for Deep Learning Architectures*, pages 100–114,
    Dublin, Ireland and Online. Association for Computational Linguistics.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2022) Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence
    Carin, 和 Weizhu Chen. 2022. [什么样的上下文示例对GPT-3有帮助？](https://doi.org/10.18653/v1/2022.deelio-1.10)
    在 *深入学习内部（DeeLIO 2022）：第三届深度学习架构知识提取与整合研讨会* 中，页码100–114，爱尔兰都柏林及在线。计算语言学协会。
- en: 'Liu et al. (2023a) Jian Liu, Chen Liang, Jinan Xu, Haoyan Liu, and Zhe Zhao.
    2023a. [Document-level event argument extraction with a chain reasoning paradigm](https://doi.org/10.18653/v1/2023.acl-long.532).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 9570–9583, Toronto, Canada. Association
    for Computational Linguistics.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2023a) Jian Liu, Chen Liang, Jinan Xu, Haoyan Liu, 和 Zhe Zhao. 2023a.
    [文档级事件论证提取与链式推理范式](https://doi.org/10.18653/v1/2023.acl-long.532)。在 *第61届计算语言学协会年会论文集（第1卷：长篇论文）*
    中，页码9570–9583，加拿大多伦多。计算语言学协会。
- en: 'Liu et al. (2023b) Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki
    Hayashi, and Graham Neubig. 2023b. Pre-train, prompt, and predict: A systematic
    survey of prompting methods in natural language processing. *ACM Computing Surveys*,
    55(9):1–35.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2023b) Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki
    Hayashi, 和 Graham Neubig. 2023b. 预训练、提示和预测：自然语言处理中的提示方法系统综述。*ACM计算调查*，55(9)：1–35。
- en: 'Ma et al. (2022) Yubo Ma, Zehao Wang, Yixin Cao, Mukai Li, Meiqi Chen, Kun
    Wang, and Jing Shao. 2022. [Prompt for extraction? PAIE: Prompting argument interaction
    for event argument extraction](https://doi.org/10.18653/v1/2022.acl-long.466).
    In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 6759–6774, Dublin, Ireland. Association
    for Computational Linguistics.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma et al. (2022) Yubo Ma, Zehao Wang, Yixin Cao, Mukai Li, Meiqi Chen, Kun Wang,
    和 Jing Shao. 2022. [用于提取的提示？PAIE：用于事件论证提取的提示交互](https://doi.org/10.18653/v1/2022.acl-long.466)。在
    *第60届计算语言学协会年会论文集（第1卷：长篇论文）* 中，页码6759–6774，爱尔兰都柏林。计算语言学协会。
- en: 'Min et al. (2022) Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis,
    Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. [Rethinking the role of demonstrations:
    What makes in-context learning work?](https://aclanthology.org/2022.emnlp-main.759)
    In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language
    Processing*, pages 11048–11064, Abu Dhabi, United Arab Emirates. Association for
    Computational Linguistics.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Min 等人（2022）Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh
    Hajishirzi 和 Luke Zettlemoyer. 2022. [重新思考示范的作用：是什么使得上下文学习有效？](https://aclanthology.org/2022.emnlp-main.759)
    载于 *2022 年自然语言处理实证方法会议论文集*，第 11048–11064 页，阿布扎比，阿联酋。计算语言学协会。
- en: Najafabadi et al. (2015) Maryam M Najafabadi, Flavio Villanustre, Taghi M Khoshgoftaar,
    Naeem Seliya, Randall Wald, and Edin Muharemagic. 2015. Deep learning applications
    and challenges in big data analytics. *Journal of big data*, 2(1):1–21.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Najafabadi 等人（2015）Maryam M Najafabadi, Flavio Villanustre, Taghi M Khoshgoftaar,
    Naeem Seliya, Randall Wald 和 Edin Muharemagic. 2015. 大数据分析中的深度学习应用与挑战。*大数据杂志*，2(1):1–21。
- en: OpenAI (2023) OpenAI. 2023. Gpt-4 technical report. *ArXiv*, abs/2303.08774.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2023）OpenAI. 2023. GPT-4 技术报告。*ArXiv*，abs/2303.08774。
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. 2022. Training language models to follow instructions with human feedback.
    *Advances in Neural Information Processing Systems*, 35:27730–27744.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等人（2022）Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray 等人。2022.
    通过人类反馈训练语言模型以遵循指令。*神经信息处理系统进展*，35:27730–27744。
- en: Pitis et al. (2023) Silviu Pitis, Michael R Zhang, Andrew Wang, and Jimmy Ba.
    2023. Boosted prompt ensembles for large language models. *arXiv preprint arXiv:2304.05970*.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pitis 等人（2023）Silviu Pitis, Michael R Zhang, Andrew Wang 和 Jimmy Ba. 2023. 增强提示集成用于大规模语言模型。*arXiv
    预印本 arXiv:2304.05970*。
- en: 'Pouran Ben Veyseh et al. (2022) Amir Pouran Ben Veyseh, Minh Van Nguyen, Franck
    Dernoncourt, Bonan Min, and Thien Nguyen. 2022. [Document-level event argument
    extraction via optimal transport](https://doi.org/10.18653/v1/2022.findings-acl.130).
    In *Findings of the Association for Computational Linguistics: ACL 2022*, pages
    1648–1658, Dublin, Ireland. Association for Computational Linguistics.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pouran Ben Veyseh 等人（2022）Amir Pouran Ben Veyseh, Minh Van Nguyen, Franck Dernoncourt,
    Bonan Min 和 Thien Nguyen. 2022. [文档级事件论元提取通过最优传输](https://doi.org/10.18653/v1/2022.findings-acl.130)。载于
    *计算语言学协会：ACL 2022 发现*，第 1648–1658 页，都柏林，爱尔兰。计算语言学协会。
- en: 'Ross (1987) Brian H Ross. 1987. This is like that: The use of earlier problems
    and the separation of similarity effects. *Journal of Experimental Psychology:
    Learning, Memory, and Cognition*, 13(4):629.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ross（1987）Brian H Ross. 1987. 这像那个：早期问题的使用和相似性效应的分离。*实验心理学杂志：学习、记忆与认知*，13(4):629。
- en: Shachaf et al. (2021) Gal Shachaf, Alon Brutzkus, and Amir Globerson. 2021.
    A theoretical analysis of fine-tuning with linear teachers. *Advances in Neural
    Information Processing Systems*, 34:15382–15394.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shachaf 等人（2021）Gal Shachaf, Alon Brutzkus 和 Amir Globerson. 2021. 使用线性教师进行微调的理论分析。*神经信息处理系统进展*，34:15382–15394。
- en: Shum et al. (2023) KaShun Shum, Shizhe Diao, and Tong Zhang. 2023. Automatic
    prompt augmentation and selection with chain-of-thought from labeled data. *arXiv
    preprint arXiv:2302.12822*.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shum 等人（2023）KaShun Shum, Shizhe Diao 和 Tong Zhang. 2023. 基于标记数据的自动提示增强和选择。*arXiv
    预印本 arXiv:2302.12822*。
- en: Socher et al. (2013) Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,
    Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. [Recursive deep
    models for semantic compositionality over a sentiment treebank](https://aclanthology.org/D13-1170).
    In *Proceedings of the 2013 Conference on Empirical Methods in Natural Language
    Processing*, pages 1631–1642, Seattle, Washington, USA. Association for Computational
    Linguistics.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Socher 等人（2013）Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher
    D. Manning, Andrew Ng 和 Christopher Potts. 2013. [递归深度模型在情感树库上的语义组合性](https://aclanthology.org/D13-1170)。载于
    *2013 年自然语言处理实证方法会议论文集*，第 1631–1642 页，西雅图，华盛顿，美国。计算语言学协会。
- en: 'Sorensen et al. (2022) Taylor Sorensen, Joshua Robinson, Christopher Rytting,
    Alexander Shaw, Kyle Rogers, Alexia Delorey, Mahmoud Khalil, Nancy Fulda, and
    David Wingate. 2022. [An information-theoretic approach to prompt engineering
    without ground truth labels](https://doi.org/10.18653/v1/2022.acl-long.60). In
    *Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics
    (Volume 1: Long Papers)*, pages 819–862, Dublin, Ireland. Association for Computational
    Linguistics.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sorensen等（2022）Taylor Sorensen, Joshua Robinson, Christopher Rytting, Alexander
    Shaw, Kyle Rogers, Alexia Delorey, Mahmoud Khalil, Nancy Fulda, 和 David Wingate.
    2022. [一种无基础真值标签的提示工程信息理论方法](https://doi.org/10.18653/v1/2022.acl-long.60)。在*第60届计算语言学协会年会（第1卷：长篇论文）*中，页码819–862，都柏林，爱尔兰。计算语言学协会。
- en: Sun et al. (2023) Jiashuo Sun, Yi Luo, Yeyun Gong, Chen Lin, Yelong Shen, Jian
    Guo, and Nan Duan. 2023. Enhancing chain-of-thoughts prompting with iterative
    bootstrapping in large language models. *arXiv preprint arXiv:2304.11657*.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun等（2023）Jiashuo Sun, Yi Luo, Yeyun Gong, Chen Lin, Yelong Shen, Jian Guo,
    和 Nan Duan. 2023. 通过迭代自举在大型语言模型中增强思维链提示。*arXiv预印本 arXiv:2304.11657*。
- en: 'Tong et al. (2022) MeiHan Tong, Bin Xu, Shuai Wang, Meihuan Han, Yixin Cao,
    Jiangqi Zhu, Siyu Chen, Lei Hou, and Juanzi Li. 2022. [DocEE: A large-scale and
    fine-grained benchmark for document-level event extraction](https://doi.org/10.18653/v1/2022.naacl-main.291).
    In *Proceedings of the 2022 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 3970–3982,
    Seattle, United States. Association for Computational Linguistics.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tong等（2022）MeiHan Tong, Bin Xu, Shuai Wang, Meihuan Han, Yixin Cao, Jiangqi
    Zhu, Siyu Chen, Lei Hou, 和 Juanzi Li. 2022. [DocEE: 一项大规模且细粒度的文档级事件抽取基准](https://doi.org/10.18653/v1/2022.naacl-main.291)。在*2022年北美计算语言学协会会议：人类语言技术会议论文集*中，页码3970–3982，西雅图，美国。计算语言学协会。'
- en: Wang et al. (2022a) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022a. Self-consistency improves
    chain of thought reasoning in language models. *arXiv preprint arXiv:2203.11171*.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等（2022a）Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan
    Narang, Aakanksha Chowdhery, 和 Denny Zhou. 2022a. 自一致性改善了语言模型中的思维链推理。*arXiv预印本
    arXiv:2203.11171*。
- en: Wang et al. (2022b) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    and Denny Zhou. 2022b. Rationale-augmented ensembles in language models. *arXiv
    preprint arXiv:2207.00747*.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等（2022b）Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, 和 Denny
    Zhou. 2022b. 理由增强的语言模型集成。*arXiv预印本 arXiv:2207.00747*。
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi,
    Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in
    large language models. *arXiv preprint arXiv:2201.11903*.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei等（2022）Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc
    Le, 和 Denny Zhou. 2022. 思维链提示在大型语言模型中引发推理。*arXiv预印本 arXiv:2201.11903*。
- en: 'Xu et al. (2021) Runxin Xu, Tianyu Liu, Lei Li, and Baobao Chang. 2021. [Document-level
    event extraction via heterogeneous graph-based interaction model with a tracker](https://doi.org/10.18653/v1/2021.acl-long.274).
    In *Proceedings of the 59th Annual Meeting of the Association for Computational
    Linguistics and the 11th International Joint Conference on Natural Language Processing
    (Volume 1: Long Papers)*, pages 3533–3546, Online. Association for Computational
    Linguistics.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu等（2021）Runxin Xu, Tianyu Liu, Lei Li, 和 Baobao Chang. 2021. [通过异质图基交互模型及跟踪器的文档级事件抽取](https://doi.org/10.18653/v1/2021.acl-long.274)。在*第59届计算语言学协会年会及第11届国际联合自然语言处理会议（第1卷：长篇论文）*中，页码3533–3546，在线。计算语言学协会。
- en: 'Xu et al. (2022) Runxin Xu, Peiyi Wang, Tianyu Liu, Shuang Zeng, Baobao Chang,
    and Zhifang Sui. 2022. [A two-stream AMR-enhanced model for document-level event
    argument extraction](https://doi.org/10.18653/v1/2022.naacl-main.370). In *Proceedings
    of the 2022 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies*, pages 5025–5036, Seattle, United States.
    Association for Computational Linguistics.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu等（2022）Runxin Xu, Peiyi Wang, Tianyu Liu, Shuang Zeng, Baobao Chang, 和 Zhifang
    Sui. 2022. [一种用于文档级事件论元抽取的双流AMR增强模型](https://doi.org/10.18653/v1/2022.naacl-main.370)。在*2022年北美计算语言学协会会议：人类语言技术会议论文集*中，页码5025–5036，西雅图，美国。计算语言学协会。
- en: 'Yang et al. (2023) Xianjun Yang, Yujie Lu, and Linda Petzold. 2023. [Few-shot
    document-level event argument extraction](https://doi.org/10.18653/v1/2023.acl-long.446).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 8029–8046, Toronto, Canada. Association
    for Computational Linguistics.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2023) Xianjun Yang, Yujie Lu, 和 Linda Petzold. 2023. [少量样本文档级事件论证提取](https://doi.org/10.18653/v1/2023.acl-long.446)。在
    *第61届计算语言学协会年会会议记录（第1卷：长篇论文）*，页面8029–8046，多伦多，加拿大。计算语言学协会。
- en: Zhang et al. (2022) Yiming Zhang, Shi Feng, and Chenhao Tan. 2022. [Active example
    selection for in-context learning](https://aclanthology.org/2022.emnlp-main.622).
    In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language
    Processing*, pages 9134–9148, Abu Dhabi, United Arab Emirates. Association for
    Computational Linguistics.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2022) Yiming Zhang, Shi Feng, 和 Chenhao Tan. 2022. [上下文学习的主动示例选择](https://aclanthology.org/2022.emnlp-main.622)。在
    *2022年自然语言处理实证方法会议记录*，页面9134–9148，阿布扎比，阿联酋。计算语言学协会。
- en: Zhang et al. (2023) Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2023.
    Automatic chain of thought prompting in large language models. In *The Eleventh
    International Conference on Learning Representations (ICLR 2023)*.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2023) Zhuosheng Zhang, Aston Zhang, Mu Li, 和 Alex Smola. 2023.
    在大语言模型中自动链式思维提示。 在 *第十一届国际学习表征会议（ICLR 2023）*。
- en: Zhou et al. (2022) Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan
    Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. 2022.
    Least-to-most prompting enables complex reasoning in large language models. *arXiv
    preprint arXiv:2205.10625*.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2022) Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan
    Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, 和 Ed Chi. 2022.
    Least-to-most prompting enables complex reasoning in large language models. *arXiv预印本
    arXiv:2205.10625*。
- en: 'Zhou and Mao (2022) Hanzhang Zhou and Kezhi Mao. 2022. [Document-level event
    argument extraction by leveraging redundant information and closed boundary loss](https://doi.org/10.18653/v1/2022.naacl-main.222).
    In *Proceedings of the 2022 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 3041–3052,
    Seattle, United States. Association for Computational Linguistics.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 和 Mao (2022) Hanzhang Zhou 和 Kezhi Mao. 2022. [通过利用冗余信息和闭合边界损失的文档级事件论证提取](https://doi.org/10.18653/v1/2022.naacl-main.222)。在
    *2022年北美计算语言学协会年会：人类语言技术会议记录*，页面3041–3052，西雅图，美国。计算语言学协会。
- en: 'Zhou and Srikumar (2022) Yichu Zhou and Vivek Srikumar. 2022. [A closer look
    at how fine-tuning changes BERT](https://doi.org/10.18653/v1/2022.acl-long.75).
    In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 1046–1061, Dublin, Ireland. Association
    for Computational Linguistics.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 和 Srikumar (2022) Yichu Zhou 和 Vivek Srikumar. 2022. [深入了解微调如何改变BERT](https://doi.org/10.18653/v1/2022.acl-long.75)。在
    *第60届计算语言学协会年会会议记录（第1卷：长篇论文）*，页面1046–1061，都柏林，爱尔兰。计算语言学协会。
- en: Appendix A Experimental Details
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 实验细节
- en: '| Dataset | Task Type | # Example | # Eval. | Eval. Split |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 任务类型 | 样本数量 | 评估数量 | 评估拆分 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| RAMS (Ebner et al., [2020](#bib.bib9)) | Doc-Level EAE | 1 | 871 | Test |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| RAMS (Ebner et al., [2020](#bib.bib9)) | 文档级EAE | 1 | 871 | 测试 |'
- en: '| DocEE (Tong et al., [2022](#bib.bib35)) | Doc-Level EAE | 1 | 800 | Test
    |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| DocEE (Tong et al., [2022](#bib.bib35)) | 文档级EAE | 1 | 800 | 测试 |'
- en: '| SST-2 (Socher et al., [2013](#bib.bib32)) | Sentiment Analysis | 2 | 872
    | Validation |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| SST-2 (Socher et al., [2013](#bib.bib32)) | 情感分析 | 2 | 872 | 验证 |'
- en: '| SNLI (Bowman et al., [2015](#bib.bib3)) | Natural Language Inference | 3
    | 500 | Test |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| SNLI (Bowman et al., [2015](#bib.bib3)) | 自然语言推理 | 3 | 500 | 测试 |'
- en: 'Table 5: The overall statistics of the dataset. # Example: The number of examples
    used in the HD-LoA prompting. # EVAL.: the number of samples used for evaluation
    of different prompting methods. EVAL. Split: evaluation split.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '表5：数据集的总体统计数据。 # 示例：HD-LoA提示中使用的示例数量。 # EVAL.: 用于评估不同提示方法的样本数量。EVAL.拆分：评估拆分。'
- en: 'The statistics of the dataset are provided in Table [5](#A1.T5 "Table 5 ‣ Appendix
    A Experimental Details ‣ Heuristics-Driven Link-of-Analogy Prompting: Enhancing
    Large Language Models for Document-Level Event Argument Extraction"). We use the
    test split of RAMS dataset and the validation split of SST-2 for evaluation, following
    the setting in Wang et al. ([2022b](#bib.bib37)). Considering the extensive size
    of the DocEE and SNLI datasets, which makes a full-scale evaluation using LLMs
    impractical, we follow Shum et al. ([2023](#bib.bib31)); Wang et al. ([2022b](#bib.bib37))
    and evaluate a subset of these datasets. Owing to the substantial costs associated
    with deploying GPT-4, we restrict its evaluation on the RAMS dataset to 100 samples.
    In addition, regarding the DocEE dataset, it presents two distinct settings. In
    the conventional configuration, the training and testing data share an identical
    distribution. Conversely, the cross-domain setup features training and testing
    data composed of non-overlapping event types. Furthermore, our heuristic-driven
    demonstration construction method necessitates far fewer examples than traditional
    prompting methods, only keeping the minimum number of examples to avoid bias in
    example answers. Specifically, for the EAE task, we use only one example, and
    for sentiment analysis and natural language inference tasks, two and three examples
    are employed respectively.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '数据集的统计信息见表格[5](#A1.T5 "Table 5 ‣ Appendix A Experimental Details ‣ Heuristics-Driven
    Link-of-Analogy Prompting: Enhancing Large Language Models for Document-Level
    Event Argument Extraction")。我们使用RAMS数据集的测试集和SST-2的验证集进行评估，遵循Wang等人（[2022b](#bib.bib37)）的设置。考虑到DocEE和SNLI数据集的庞大规模，使得使用LLM进行全面评估不切实际，我们遵循Shum等人（[2023](#bib.bib31)）；Wang等人（[2022b](#bib.bib37)），并评估了这些数据集的一个子集。由于部署GPT-4的高昂成本，我们将其在RAMS数据集上的评估限制为100个样本。此外，关于DocEE数据集，它呈现出两种不同的设置。在传统配置中，训练和测试数据共享相同的分布。相反，跨领域设置则包括由不重叠事件类型组成的训练和测试数据。此外，我们的启发式驱动演示构建方法需要的示例远少于传统的提示方法，只保留最少数量的示例以避免示例答案的偏差。具体而言，对于EAE任务，我们只使用一个示例，对于情感分析和自然语言推理任务，分别使用两个和三个示例。'
- en: We evaluate our prompting method on text-davinci-003, gpt-3.5-turbo-0301 and
    GPT-4 (OpenAI, [2023](#bib.bib25)). We employ these LLM models from the OpenAI
    API. During the all experiments, the temperature is fixed as $0$. In the evaluation
    of document-level EAE datasets, we omit articles from both the ground truth and
    the prediction during the assessment to align the more closely with the real content
    of event arguments.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在text-davinci-003、gpt-3.5-turbo-0301和GPT-4（OpenAI，[2023](#bib.bib25)）上评估了我们的提示方法。我们使用来自OpenAI
    API的这些LLM模型。在所有实验中，温度固定为$0$。在对文档级EAE数据集的评估中，我们在评估过程中省略了文章内容，以便更紧密地对齐事件论据的真实内容。
- en: Appendix B Recognize Implicit Heuristics of In-Context Examples by GPT-4
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 通过GPT-4识别上下文示例中的隐性启发式方法
- en: The prompt we use to recognize implicit heuristics inherent in examples and
    the output of GPT-4 is given below.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用于识别示例中固有隐性启发式方法的提示，以及GPT-4的输出，如下所示。
- en: '| Prompt Input |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 提示输入 |'
- en: '| What  is  the  most  critical  and  profound  heuristic  at  play  in  each  of  the  following  examples?  If  any  two  examples  share  the  same  heuristic,  please  indicate  this  connection.  Use  no  more  than  two  sentences  to  illustrate  the  heuristics  of  each  example.
    example  1 Q:  Did  the  death  of  Helen  Palmer  have  a  significant  effect  on  Dr.  Seuss?
    A:  Helen  Palmer  was  Dr.  Seuss’s  wife.  The  death  of  a  spouse  can  have  a  significant  effect  on  a  person.  Thus,  the  death  of  Helen  Palmer  could  have  a  significant  effect  on  Dr.  Seuss.  So  the  answer  is  yes.
    example  2 Q:  Would  a  moose  hypothetically  be  too  much  for  a  minotaur  to  devour  whole?
    A:  A  moose  is  about  1,000  pounds.  A  minotaur  is  about  1,000  pounds.  Thus,  a  moose  would  be  too  much  for  a  minotaur  to  devour  whole.  So  the  answer  is  yes.
    example  3 Q:  Yes  or  no:  Is  H’s  most  common  two  letter  pair  partner  a  freebie  in  Wheel  of  Fortune  bonus  round?
    A:  The  most  common  two  letter  pair  partner  for  H  is  E.  E  is  a  freebie  in  the  Wheel  of  Fortune  bonus  round.  Thus,  H’s  most  common  two  letter  pair  partner  is  a  freebie  in  the  Wheel  of  Fortune  bonus  round.  So  the  answer  is  yes.
    example  4 Q:  Yes  or  no:  Would  bald  eagle  deliver  an  urgent  message  before  B-52?
    A:  The  top  speed  of  a  bald  eagle  is  about  80  mph.  The  top  speed  of  a  B-52  is  about  650  mph.  Thus,  a  B-52  would  be  faster  than  a  bald  eagle.  So  the  answer  is  no.
    example  5 Q:  Yes  or  no:  Does  an  individual  oceanographer  study  many  sciences?
    A:  Oceanography  is  the  study  of  the  ocean.  The  ocean  is  a  complex  system.  Thus,  oceanography  is  a  multidisciplinary  science.  So  the  answer  is  yes.
    example  6 Q:  Yes  or  no:  Could  Steven  Spielberg  send  emails  as  a  child?
    A:  Steven  Spielberg  was  born  in  1946.  Email  was  invented  in  1971.  Thus,  Steven  Spielberg  could  not  send  emails  as  a  child.  So  the  answer  is  no.
    |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 在以下每个例子中，最关键且深刻的启发式方法是什么？如果有两个例子共享相同的启发式方法，请指出这种联系。用不超过两句话来阐述每个例子的启发式方法。例子
    1 问：海伦·帕尔默的去世对苏斯博士有重大影响吗？答：海伦·帕尔默是苏斯博士的妻子。配偶的去世可能对一个人产生重大影响。因此，海伦·帕尔默的去世可能对苏斯博士产生重大影响。所以答案是肯定的。例子
    2 问：假设一头驼鹿是否会对迷宫人来说太过庞大，以至于无法吞下？答：一头驼鹿约有 1,000 磅重，而迷宫人也约有 1,000 磅重。因此，一头驼鹿对迷宫人来说太过庞大，无法吞下。所以答案是肯定的。例子
    3 问：是或否：H 最常见的两个字母搭配是否在《财富之轮》奖金轮中是免费的？答：H 最常见的两个字母搭配是 E。E 在《财富之轮》奖金轮中是免费的。因此，H
    最常见的两个字母搭配在《财富之轮》奖金轮中是免费的。所以答案是肯定的。例子 4 问：是或否：白头鹰是否会比 B-52 先送达紧急信息？答：白头鹰的最高速度约为
    80 mph，而 B-52 的最高速度约为 650 mph。因此，B-52 会比白头鹰更快。所以答案是否定的。例子 5 问：是或否：个人海洋学家是否研究多种科学？答：海洋学是研究海洋的学科。海洋是一个复杂的系统。因此，海洋学是一门多学科的科学。所以答案是肯定的。例子
    6 问：是或否：史蒂文·斯皮尔伯格是否在儿童时期能发送电子邮件？答：史蒂文·斯皮尔伯格出生于 1946 年。电子邮件是在 1971 年发明的。因此，史蒂文·斯皮尔伯格在儿童时期不能发送电子邮件。所以答案是否定的。'
- en: '| LLM Output |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| LLM 输出 |'
- en: '| Example  1:  This  employs  the  "empathetic  reasoning"  heuristic,  assuming  that  because  most  people  would  be  significantly  affected  by  the  death  of  a  spouse,  so  too  would  Dr.  Seuss.
    Example  2:  The  "comparison  heuristic"  is  used  here,  comparing  the  size  and  weight  of  a  moose  to  that  of  a  minotaur  to  make  a  conclusion  about  the  feasibility  of  consumption.
    Example  3:  This  uses  the  "knowledge-based"  heuristic,  using  known  facts  about  a  game  show’s  rules  to  answer  the  question.
    Example  4:  This  is  another  instance  of  the  "comparison  heuristic,"  this  time  comparing  the  top  speeds  of  a  bald  eagle  and  a  B-52  to  determine  which  would  deliver  a  message  faster.
    Example  5:  The  "definition-based"  heuristic  is  applied  here,  relying  on  the  understanding  of  what  the  field  of  oceanography  entails  to  conclude  that  it  involves  the  study  of  multiple  sciences.
    Example  6:  The  "chronological  heuristic"  is  used  here,  comparing  the  timelines  of  Spielberg’s  childhood  and  the  invention  of  email  to  answer  the  question.
    |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 示例 1:  这里使用了“同理推理”启发式，假设因为大多数人会因配偶的去世而受到重大影响，所以**苏斯博士**也会。 示例 2:  这里使用了“比较启发式”，将驼鹿的大小和重量与米诺陶洛斯进行比较，以得出关于其食用可能性的结论。
    示例 3:  这使用了“基于知识”的启发式，利用已知的游戏节目规则来回答问题。 示例 4:  这是另一个“比较启发式”的实例，这次比较了秃鹰和 B-52 的最高速度，以确定哪个更快地传递信息。
    示例 5:  这里应用了“基于定义”的启发式，依赖于对海洋学领域的理解，以得出它涉及多种科学的结论。 示例 6:  这里使用了“时间顺序启发式”，比较了斯皮尔伯格的童年和电子邮件的发明时间线来回答问题。
    |'
- en: Appendix C Full Prompts
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 完整提示
- en: C.1 The Demonstration for SST-2 Dataset
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 SST-2 数据集的演示
- en: 'Instruction:  You  are  to  perform  a  2-class  sentiment  analysis  on  the  given  sentences.  For  each  sentence,  determine  if  the  sentiment  is  either  "positive"  or  "negative".  Specifically,  you  will  leverage  the  pattern  in  the  pattern  list  below  to  recognize  the  sentiment  of  the  target  sentence.  For  each  sentence,  you  will  select  the  most  appropriate  one  or  two  patterns  to  identify  its  sentiment.  The  identification  of  a  sentence  based  on  different  patterns  should  not  be  interconnected.  Finally,  re-evaluate  the  identified  sentiment  to  get  the  final  answer.Pattern  list:[Overall  Context  Pattern:  Examine  the  general  tone  of  the  entire  sentence.  Determine  if  it  overall  express  a  positive  or  negative  sentiment,  regardless  of  individual  words.Descriptive  Phrases  pattern:  Identify  key  adjectives  or  descriptive  phrases  that  convey  strong  feelings  or  opinions  about  the  subject.Negation  Handling  Pattern:  Look  for  negative  words  like  "not,"  "didn’t,"  or  "never"  that  might  reverse  the  sentiment  of  the  words  following  them.  Ensure  the  reversed  sentiment  is  understood  correctly.Comparatives  Pattern:  Search  for  words  or  phrases  that  compare  the  subject  to  something  else,  suggesting  superiority,  inferiority,  or  equality.Adverbs  and  Intensifiers  Pattern:  Spot  adverbs  or  intensifiers  that  amplify  the  sentiment  of  the  associated  adjective  or  verb.  They  often  provide  a  clue  about  the  strength  of  the  sentiment.]Question:  What  is  the  sentiment  of  the  following  sentence?Sentence:  "more  than  another  "  best  man  "  clone  by  weaving  a  theme  throughout  this  funny  film"Answer:Step  1:  Selected  Patterns:  Overall  Context  Pattern  and  Descriptive  Phrases  Pattern  (Phrases:  "weaving  a  theme"  and  "funny  film").Step  2:  Apply  selected  patterns  to  identify  the  sentiment  of  the  sentence  independently.Step  2.1:  Based  on  Overall  Context  pattern:  The  entirety  of  the  sentence  portrays  a  film  that  is  superior  and  offers  added  value,  emphasizing  its  distinctiveness  and  humorous  quality.  The  overall  sentiment  is  positive.Step  2.2:  Based  on  Descriptive  Phrases  pattern:  The  descriptive  phrases  highlight  a  positive  sentiment  in  the  film  being  more  than  just  a  clone  and  having  a  funny  theme.  The  sentiment  is  positive.Step  3:  Re-evaluate  sentiment:sentiment:  positiveQuestion:  What  is  the  sentiment  of  the  following  sentence?Sentence:  "contains  no  wit  ,  only  labored  gags"Answer:Step  1:  Selected  Patterns:  Overall  Context  Pattern  and  Negation  Handling  Pattern  (Phrase:  "contains  no").Step  2:  Apply  selected  patterns  to  identify  the  sentiment  of  the  sentence  independently.Step  2.1:  Based  on  Overall  Context  pattern:  The  entire  sentence  conveys  a  lack  of  genuine  humor  and  wit,  and  instead  presents  the  humor  as  contrived  or  forced.  The  overall  sentiment  is  negative.Step  2.2:  Based  on  Negation  Handling  Pattern:  The  negation  "contains  no"  highlights  a  lack  of  wit.  It  is  further  emphasized  by  "labored  gags",  suggesting  forced  or  contrived  humor.  Thus,  the  sentiment  is  negative  regarding  the  quality  or  genuineness  of  the  humor.Step  3:  Re-evaluate  sentiment:sentiment:  negative'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 指令：你需要对给定的句子进行二分类情感分析。对于每个句子，判断情感是“积极”还是“消极”。具体而言，你将利用下面的模式列表中的模式来识别目标句子的情感。对于每个句子，你将选择最合适的一到两个模式来识别其情感。基于不同模式对句子的识别不应相互关联。最后，重新评估识别的情感，以得到最终答案。模式列表：[整体上下文模式：检查整个句子的总体语气。判断它总体上表达的是积极还是消极情感，而不考虑单个词语。描述性短语模式：识别传达强烈情感或观点的关键形容词或描述性短语。否定处理模式：寻找如“not”，“didn’t”或“never”等负面词汇，这些词可能会反转后面词语的情感。确保正确理解反转后的情感。比较模式：寻找比较主题与其他事物的词语或短语，暗示优越性、劣势或平等性。副词和加强词模式：发现副词或加强词，这些词放大了相关形容词或动词的情感。它们通常提供关于情感强度的线索。]问题：以下句子的情感是什么？句子：“比另一个‘最佳男’克隆更具主题性的幽默影片”答案：步骤1：选择的模式：整体上下文模式和描述性短语模式（短语：“编织主题”和“幽默影片”）。步骤2：独立应用所选模式来识别句子的情感。步骤2.1：根据整体上下文模式：整个句子描绘了一部优越的影片，提供了额外的价值，强调了其独特性和幽默质量。总体情感是积极的。步骤2.2：根据描述性短语模式：描述性短语突出影片的积极情感，认为它不仅仅是一个克隆，还拥有幽默的主题。情感是积极的。步骤3：重新评估情感：情感：积极问题：以下句子的情感是什么？句子：“没有机智，只有做作的笑话”答案：步骤1：选择的模式：整体上下文模式和否定处理模式（短语：“没有”）。步骤2：独立应用所选模式来识别句子的情感。步骤2.1：根据整体上下文模式：整个句子传达了缺乏真正幽默和机智，而是呈现了做作的幽默。总体情感是消极的。步骤2.2：根据否定处理模式：否定“没有”突出缺乏机智。再加上“做作的笑话”，暗示了强迫或做作的幽默。因此，情感是关于幽默质量或真实性的消极的。步骤3：重新评估情感：情感：消极
- en: C.2 The Demonstration for SNLI Dataset
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2 SNLI 数据集的演示
- en: 'Please  solve  the  natural  language  inference  task.  Specifically,  given  a  premise  and  a  hypothesis,  determine  the  validity  of  the  hypothesis  based  on  the  premise:Yes:  The  hypothesis  is  logically  derived  or  directly  follows  from  the  premise.No:  The  premise  provides  evidence  that  refutes  the  hypothesis.It  is  impossible  to  tell:  The  premise  does  not  provide  sufficient  information  to  confirm  or  refute  the  hypothesis.You  will  select  the  most  appropriate  pattern  in  the  pattern  list  below  to  classify  the  natural  language  inference  task.  For  each  sentence,  you  will  use  the  selected  patterns  to  identify  the  relationship  between  the  premise  and  the  hypothesis.Pattern  list:[Explicit  Evidence  Pattern:  When  the  hypothesis  directly  restates  or  paraphrases  information  present  in  the  premise,  i.e.,  premise  provides  direct  evidence  that  supports  the  hypothesis,  the  answer  is  "yes".Explicit  Contradiction  Pattern:  The  hypothesis  contains  information  that  directly  negates  or  opposes  a  clear  statement  in  the  premise.  If  this  condition  is  met,  the  answer  is  "no".Confident  Neutral  Pattern:  If  it  is  very  certain  that  the  hypothesis  neither  contradicts  nor  supports  the  premise  in  any  evident  or  implicit  manner,  and  the  relationship  between  them  is  clearly  independent,  the  answer  is  "it  is  not  possible  to  tell".Implicit  Contradiction  or  Neutral  Pattern:  In  this  case,  no  direct  contradiction  is  found.  If  the  hypothesis,  when  extended  logically,  negates  or  conflicts  with  any  part  of  the  premise,  even  if  not  directly.  If  it  does,  it  leans  towards  contradiction  (’no).  If  no  such  implicit  contradiction  is  found  and  the  relationship  between  hypothesis  and  premise  is  remains  ambiguous,  it  could  be  neutral  (it  is  not  possible  to  tell).Implicit  Evidence  or  Neutral  Pattern:  In  cases  where  no  direct  evidence  in  the  premise  supports  the  hypothesis,  the  following  steps  should  be  applied:  Check  each  element  of  the  hypothesis  against  the  premise.  If  each  element  of  the  hypothesis,  when  drawing  from  the  premise  using  world  knowledge  or  logical  reasoning,  one  can  infer  or  reasonably  support  the  entire  hypothesis,  it  leans  towards  implicit  entailment  (’yes’).  If  any  part  of  the  hypothesis  lacks  inferable  evidence  from  the  premise  or  if  the  connection  between  the  entire  hypothesis  and  premise  remains  ambiguous,  it  leans  towards  neutral  (’it  is  not  possible  to  tell’).Implicit  Contradiction  or  Neutral  Pattern:  In  cases  where  no  direct  evidence  in  the  premise  negates  the  hypothesis,  the  following  steps  should  be  applied:  Check  each  element  of  the  hypothesis  against  the  premise.  If  any  element  of  the  hypothesis,  when  juxtaposed  with  the  premise  and  utilizing  world  knowledge  or  logical  reasoning,  can  subtly  negate  or  contradict  any  part  of  the  premise,  it  leans  towards  implicit  contradiction  (’no’).  If  each  element  of  the  hypothesis  does  not  provide  such  subtle  contradictions  or  if  the  connection  between  the  entire  hypothesis  and  premise  remains  ambiguous,  it  leans  towards  neutral  (’it  is  not  possible  to  tell’).]Premise:  "Children  smiling  and  waving  at  camera."Based  on  this  premise,  can  we  conclude  the  hypothesis  "There  are  children  present."  is  true?OPTIONS:-  yes-  no-  it  is  not  possible  to  tellAnswer:Step  1:  Select  the  most  appropriate  pattern:  Explicit  Evidence  PatternStep  2:  Using  the  Explicit  Evidence  Pattern,  the  premise  directly  states  that  there  are  children  who  are  smiling  and  waving  at  the  camera.  This  information  supports  the  hypothesis,  which  claims  that  there  are  children  present.  So,  this  heuristic  suggest  a  "yes"  answer.Therefore,  the  answer  is:  yesPremise:  "A  couple  play  in  the  tide  with  their  young  son."Based  on  this  premise,can  we  conclude  the  hypothesis  "The  family  is  sitting  down  for  dinner."  is  true?OPTIONS:-  yes-  no-  it  is  not  possible  to  tellAnswer:Step  1:  Select  the  most  appropriate  pattern:  Implicit  Contradiction  or  Neutral  PatternStep  2:  Using  Implicit  Contradiction  or  Neutral  Pattern,  although  there  is  no  contradiction  in  the  premise  describes  a  scene  where  a  couple  and  their  son  are  playing  in  the  tide,  which  suggests  they  are  outdoors  and  likely  at  a  beach  or  coastal  setting.  The  hypothesis,  on  the  other  hand,  presents  a  scenario  where  the  family  is  sitting  down  for  dinner,  typically  an  indoor  activity.  These  two  activities  -  playing  in  the  tide  and  sitting  down  for  dinner  are  mutually  exclusive  in  the  context  of  a  single  time  frame.  Because  some  element  of  the  hypothesis  is  identified  implicitly  contradicts  the  premise.  This  heuristic  leans  towards  the  answer  "no".Therefore,  the  answer  is:  noPremise:  "A  young  family  enjoys  feeling  ocean  waves  lap  at  their  feet."Based  on  this  premise,  can  we  conclude  the  hypothesis  "A  young  man  and  woman  take  their  child  to  the  beach  for  the  first  time."  is  true?OPTIONS:-  yes-  no-  it  is  not  possible  to  tellAnswer:Step  1:  Select  the  most  appropriate  pattern:  Implicit  Evidence  or  Neutral  PatternStep  2:  Using  Implicit  Evidence  or  Neutral  Pattern,  "a  young  family"  typically  implies  the  presence  of  a  younger  couple  and  potentially  their  child,  so  this  part  can  be  inferred.  However,  the  statement  about  it  being  the  child’s  "first  time"  at  the  beach  cannot  be  inferred  or  supported  from  the  premise.  Because  not  all  elements  of  the  hypothesis  can  be  inferred  or  supported  from  the  premise,  this  heuristic  leans  towards  "it  is  not  possible  to  tell".Therefore,  the  answer  is:  it  is  not  possible  to  tell.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 请解决自然语言推理任务。具体而言，给定一个前提和一个假设，根据前提确定假设的有效性：是的：假设是从前提中逻辑推导出的或直接从前提中得出的。否：前提提供了驳斥假设的证据。无法确定：前提没有提供足够的信息来确认或驳斥假设。您将选择下面模式列表中最合适的模式来对自然语言推理任务进行分类。对于每个句子，您将使用选定的模式来确定前提和假设之间的关系。模式列表：[明确证据模式：当假设直接重述或释义前提中的信息时，即前提提供了直接支持假设的证据，答案是“是”。明确矛盾模式：假设包含直接否定或反对前提中明确声明的信息。如果满足此条件，则答案是“否”。自信中立模式：如果非常确定假设在任何明显或隐含的方式上既不矛盾也不支持前提，并且它们之间的关系明显独立，则答案是“无法确定”。隐含矛盾或中立模式：在这种情况下，没有直接的矛盾。如果假设在逻辑上扩展时否定或冲突前提的任何部分，即使不是直接的。如果存在这样的隐含矛盾，则倾向于矛盾（“否”）。如果没有发现这样的隐含矛盾，并且假设和前提之间的关系仍然模糊，则可能是中立（“无法确定”）。隐含证据或中立模式：在没有直接证据支持假设的前提情况下，应采取以下步骤：检查假设的每个元素与前提的关系。如果假设的每个元素在运用世界知识或逻辑推理时，可以推断或合理支持整个假设，则倾向于隐含蕴涵（“是”）。如果假设的任何部分没有从前提中推断出的证据，或者整个假设和前提之间的联系仍然模糊，则倾向于中立（“无法确定”）。隐含矛盾或中立模式：在前提中没有直接证据否定假设的情况下，应采取以下步骤：检查假设的每个元素与前提的关系。如果假设的任何元素在与前提对比时，利用世界知识或逻辑推理，可以微妙地否定或矛盾前提的任何部分，则倾向于隐含矛盾（“否”）。如果假设的每个元素不提供这种微妙的矛盾，或者整个假设和前提之间的联系仍然模糊，则倾向于中立（“无法确定”）。]前提：“孩子们在对着相机微笑和挥手。”根据这个前提，我们能否得出假设“存在孩子”是真的？选项：-
    是 - 否 - 无法确定答案：步骤1：选择最合适的模式：明确证据模式步骤2：使用明确证据模式，前提直接陈述了有孩子们在对着相机微笑和挥手。这一信息支持了假设，即存在孩子。因此，这一启发式建议答案是“是”。因此，答案是：是前提：“一对夫妇和他们的小儿子在潮水中玩耍。”根据这个前提，我们能否得出假设“这个家庭正在坐下吃晚餐”是真的？选项：-
    是 - 否 - 无法确定答案：步骤1：选择最合适的模式：隐含矛盾或中立模式步骤2：使用隐含矛盾或中立模式，尽管前提没有直接矛盾，但前提描述了一个夫妇和他们的儿子在潮水中玩耍的场景，这表明他们在户外，可能是在海滩或海岸环境中。而假设则呈现了家庭坐下来吃晚餐的场景，通常是室内活动。这两种活动——在潮水中玩耍和坐下来吃晚餐，在同一时间框架内是相互排斥的。因为假设的某些元素隐含地矛盾于前提。这一启发式倾向于“否”作为答案。因此，答案是：否前提：“一个年轻家庭享受着海浪拍打他们的脚。”根据这个前提，我们能否得出假设“一对年轻男女带着他们的孩子第一次去海滩”是真的？选项：-
    是 - 否 - 无法确定答案：步骤1：选择最合适的模式：隐含证据或中立模式步骤2：使用隐含证据或中立模式，“一个年轻家庭”通常暗示年轻夫妇及其孩子的存在，因此这部分可以推断。然而，关于孩子“第一次”去海滩的陈述无法从前提中推断或支持。因为假设的所有元素不能从前提中推断或支持，这一启发式倾向于“无法确定”。因此，答案是：无法确定。
- en: C.3 The Demonstration for RAMS Dataset
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.3 RAMS 数据集的演示
- en: 'Instruction:  Your  task  is  Event  Argument  Extraction.  In  this  task,  you  will  be  provided  with  a  document  that  describes  an  event.  Your  job  is  to  identify  and  extract  the  key  components  (event  arguments)  that  correspond  to  each  argument  role  associated  with  the  event.  The  trigger  word,  which  is  located  between  special  tokens  "<t>"  and  "<\t>"  in  the  document,  serves  to  indicate  the  presence  of  the  event.  Only  the  event  explicitly  linked  to  the  trigger  word  should  be  considered.Specifically,  you  will  use  the  pattern  in  the  pattern  list  below  to  identify  event  arguments,  and  re-evaluate  the  identified  argument  candidates  to  get  the  final  answer.pattern  list:[Semantic  Pattern:  [giver]  is  the  person,  group,  or  organization  in  the  document  that  gives  the  grant  or  gift.  For  example  "[John]  is  the  donor".Syntactic.SubjectVerbObject  Pattern:  [giver]  may  be  the  subject  of  a  verb  that  indicates  the  act  of  giving,  or  granting.  For  example,  "[John]  gave  Mary  a  book"Syntactic.PassiveVoiceSyntax  Pattern:  In  sentences  using  passive  voice,  the  [giver]  might  be  introduced  by  prepositions  such  as  ’by’  to  describe  the  act  of  giving  or  granting.  For  example,  "The  book  was  given  to  Mary  by  [John]".Syntactic.PrepositionalPhrases  Pattern:  [giver]  may  be  indicated  by  prepositions  such  as  ’from’,  and  ’of’,  which  typically  precede  the  giver  in  the  context  of  a  giving  or  granting  event.  For  example,  "She  received  a  gift  from  [John]".Syntactic.PossessiveNoun  Pattern:  [giver]  may  be  the  possessor  in  noun  phrases  that  describe  the  object  being  given  or  granted.  For  example,  "[John]’s  donation".Multi-Sentence  Pattern:  the  giver  is  not  explicitly  understood  from  only  one  sentence,  but  it  can  be  understood  by  combining  the  information  of  multiple  sentences.  For  example,  consider  "The  charity  received  a  generous  donation.  This  was  from  an  [anonymous  man]."  The  ’anonymous  man’  as  the  giver  is  not  explicit  in  the  second  sentence  alone.  However,  if  we  link  ’this’  in  the  second  sentence  to  the  ’donation’  in  the  first,  we  understand  that  the  ’anonymous  man’  is  the  giver  of  the  donation.]Example  task:Question:  What  are  the  event  arguments  of  giver,  beneficiary,  and  recipient  in  the  "transaction.transaction.giftgrantprovideaid"  event  in  the  provided  news  document?  The  trigger  word  is  "granted"  located  between  "<t>"  and  "</t>".  If  an  event  argument  is  explicitly  stated  in  the  document,  ensure  that  the  identified  argument  is  quoted  exactly  as  the  entity  appears  in  the  given  document.  Note,  if  an  event  argument  is  not  explicitly  indicated  in  the  document,  or  is  not  directly  linked  to  the  event  signaled  by  the  trigger  word,  use  "not  specified"  as  the  answer.’Document:  a  news  documenttrigger  sentence:  "The  access  to  the  research  center  in  the  city  was  <t>granted</t>  by  the  administrator.  The  man,  Ripley  Johnson,  earned  it."Answer:Elaborate  the  meaning  of  event  type  and  its  argument  roles:"transaction.transaction.giftgrantprovideaid":  The  event  involves  a  transfer  of  money  or  resources  in  the  form  of  a  gift,  grant,  or  provision  of  aid,  signaled  by  the  action  of  granting.[giver]:  the  giver  is  the  person,  group,  or  organization  that  provides  or  grants  money,  resources,  or  access  in  the  event.[beneficiary]:  the  beneficiary  is  the  party  who  ultimately  benefits  from  the  transaction.[recipient]:  the  recipient  is  the  entity  that  receives  the  money,  resources,  or  access  granted  in  the  event.Recognizing  [giver]  in  the  given  document:Step  1  Select  one  or  two  patterns  in  the  pattern  list  that  are  best  suited  to  identify  the  [giver]  in  the  news  document:  Semantic  Pattern  and  Syntactic.PassiveVoiceSyntax  Pattern  because  "was  granted"  indicates  Syntactic.PassiveVoiceSyntax.Step  2:  Apply  selected  patterns  to  identify  [giver]  independently.  Each  pattern  should  be  applied  in  isolation,  without  considering  the  other  pattern.Step  2.1  Identify  the  [giver]  based  on  Semantic  Pattern  f1:  "[giver]  is  the  person,  group,  or  organization  that  gives  the  grant  or  gift  in  the  document".  Applying  pattern  f1  to  the  document,  "administrator"  is  the  entity  that  gives  access  of  the  research  center.Step  2.2  Identify  the  [giver]  based  on  Syntactic.PassiveVoiceSyntax  Pattern  f2:  "In  sentences  using  passive  voice,  the  [giver]  might  be  the  object  of  a  verb  describe  the  act  of  giving  or  granting."  Applying  pattern  f2  to  the  document,  "research  center"  is  the  object  of  "granted"  which  describes  the  act  of  giving  or  granting.Step  3:  reevaluate_argument_candidates(["administrator",  "research  center"]):Is  argument  "administrator"  alignment  with  the  argument  role  [giver]?  yes,  because  "administrator"  granted  access  to  the  research  center.Is  argument  "research  center"  alignment  with  the  argument  role  [giver]?  no,  because  "research  center"  is  the  place  that  access  has  been  granted  to,  but  it  doesn’t  give  access.[giver]:  "administrator"Recognizing  [beneficiary]  in  the  given  document:Step  1  Select  one  or  two  patterns  in  the  pattern  list  that  are  best  suited  to  identify  the  [beneficiary]  in  the  news  document:  Semantic  PatternStep  2:  Apply  selected  patterns  to  identify  [beneficiary]  independently.  Each  pattern  should  be  applied  in  isolation,  without  considering  the  other  pattern.Step  2.1  Identify  the  [beneficiary]  based  on  semantic  pattern  f1:  "[beneficiary]  is  the  entity  that  ultimately  benefits  from  the  gift  or  grant".  Applying  pattern  f1  to  the  given  document,  the  entity  that  ultimately  benefits  from  the  grant  is  "not  specified".Step  3:  reevaluate_argument_candidates("not  specified"):Is  argument  "not  specified"  alignment  with  argument  role  [beneficiary]?  Yes,  the  [beneficiary]  is  not  explicitly  mentioned  so  "not  specified"  is  correct.[beneficiary]:  "not  specified"Recognizing  [recipient]  in  the  given  document:Step  1  Select  one  or  two  patterns  in  the  pattern  list  that  are  best  suited  to  identify  the  [recipient]  in  the  news  document:  Semantic  Pattern  and  Multi-Sentence  Pattern  because  recognizing  [recipient]  requires  an  understanding  of  "it"  in  "earned  it"  across  sentencesStep  2:  Apply  selected  patterns  to  identify  [recipient]  independently.  Each  pattern  should  be  applied  in  isolation,  without  considering  the  other  pattern.Step  2.1  Identify  the  [recipient]  based  on  Semantic  Pattern  f1:  "[recipient]  is  the  entity  that  receives  the  gift  or  grant".  Applying  pattern  f1  to  the  given  document,  the  entity  that  receives  the  gift  or  grant  is  "Ripley  Johnson".Step  2.2  Identify  the  [recipient]  based  on  Multi-Sentence  Pattern  f2:  in  the  phrase  "The  man,  Ripley  Johnson,  earned  it",  "it"  refers  to  the  access.  Therefore,  "The  man"  earned  the  granted  access.  [recipient]  is  "The  man".Step  3:  reevaluate_argument_candidates(["Ripley  Johnson",  "The  man"]):Is  argument  "Ripley  Johnson"  alignment  with  the  argument  role  [recipient]?  yes,  because  "Ripley  Johnson"  earned  the  access.Is  argument  "Ripley  Johnson"  alignment  with  the  argument  role  [recipient]?  yes,  because  "The  man"  earned  the  access."The  man"  is  preferred  because  it  is  closer  to  the  trigger  word.[recipient]:  "The  man"Target  task:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 指示：你的任务是事件论元提取。在此任务中，你将获得一个描述事件的文档。你的工作是识别并提取与事件相关的每个论元角色相对应的关键组成部分（事件论元）。触发词位于文档中的特殊标记
    "<t>" 和 "<\t>" 之间，用于指示事件的存在。只有与触发词明确关联的事件应被考虑。具体而言，你将使用以下模式列表中的模式来识别事件论元，并重新评估识别出的论元候选以获得最终答案。模式列表：[语义模式：
    [giver] 是文档中给予赠款或礼物的个人、团体或组织。例如 "[John] 是捐赠者"。句法.主谓宾模式：[giver] 可能是指示给予或授予行为的动词的主语。例如，"[John]
    给了 Mary 一本书"。句法.被动语态模式：在使用被动语态的句子中，[giver] 可能通过诸如 ’by’ 的介词引入，以描述给予或授予的行为。例如，“这本书由
    [John] 赠送给 Mary”。句法.介词短语模式：[giver] 可能通过介词如 ’from’ 和 ’of’ 指出，这些介词通常在给予或授予事件的上下文中位于给予者之前。例如，“她从
    [John] 那里收到了一份礼物”。句法.所有格名词模式：[giver] 可能是描述被给予或授予物体的名词短语中的所有者。例如，"[John] 的捐赠"。多句模式：给予者在单个句子中没有明确提及，但可以通过结合多个句子中的信息来理解。例如，“慈善机构收到了一笔慷慨的捐赠。这来自一位
    [匿名人士]。”在第二个句子中，‘匿名人士’ 作为给予者并不明确。然而，如果我们将第二个句子中的‘这’与第一个句子中的‘捐赠’关联起来，我们可以理解‘匿名人士’
    是捐赠的给予者。] 示例任务：问题：在提供的新闻文档中，“transaction.transaction.giftgrantprovideaid”事件的 giver、beneficiary
    和 recipient 论元是什么？触发词是 "<t>granted</t>"。如果事件论元在文档中明确陈述，请确保所识别的论元与文档中出现的实体完全一致。注意，如果事件论元未在文档中明确指出，或未直接与触发词所示事件关联，请使用
    "未指定" 作为答案。文档：新闻文档 触发句子：“城市的研究中心的访问权限由管理员 <t>授予</t>。这个人，Ripley Johnson，赚到了它。”答案：阐述事件类型及其论元角色的含义：“transaction.transaction.giftgrantprovideaid”：该事件涉及以赠礼、资助或援助的形式转移金钱或资源，由授予的行为信号表示。[giver]：给予者是指在事件中提供或授予金钱、资源或访问权限的个人、团体或组织。[beneficiary]：受益者是最终从交易中获益的一方。[recipient]：接受者是事件中获得授予的金钱、资源或访问权限的实体。识别文档中的
    [giver]：第1步：选择一个或两个最适合识别新闻文档中 [giver] 的模式：语义模式和句法.被动语态模式，因为“was granted” 指示句法.被动语态模式。第2步：独立应用所选模式识别
    [giver]。每个模式应单独应用，不考虑其他模式。第2.1步：基于语义模式 f1 识别 [giver]：“[giver] 是在文档中给予资助或礼物的个人、团体或组织”。应用模式
    f1 到文档中，“管理员” 是给予研究中心访问权限的实体。第2.2步：基于句法.被动语态模式 f2 识别 [giver]：“在使用被动语态的句子中，[giver]
    可能是描述给予或授予行为的动词的宾语。”应用模式 f2 到文档中，“研究中心” 是“授予”的宾语，描述了给予或授予的行为。第3步：重新评估论元候选（["管理员",
    "研究中心"]）：论元 “管理员” 是否与 [giver] 的论元角色一致？是的，因为“管理员” 授予了研究中心的访问权限。论元 “研究中心” 是否与 [giver]
    的论元角色一致？不，因为“研究中心” 是获得访问权限的地方，但它并不授予访问权限。[giver]：“管理员” 识别文档中的 [beneficiary]：第1步：选择一个或两个最适合识别新闻文档中
    [beneficiary] 的模式：语义模式。第2步：独立应用所选模式识别 [beneficiary]。每个模式应单独应用，不考虑其他模式。第2.1步：基于语义模式
    f1 识别 [beneficiary]：“[beneficiary] 是最终从礼物或赠款中获益的实体。”应用模式 f1 到给定文档中，最终从赠款中获益的实体是“未指定”。第3步：重新评估论元候选（“未指定”）：论元
    “未指定” 是否与 [beneficiary] 的论元角色一致？是的，[beneficiary] 没有明确提到，所以“未指定” 是正确的。[beneficiary]：“未指定”
    识别文档中的 [recipient]：第1步：选择一个或两个最适合识别新闻文档中 [recipient] 的模式：语义模式和多句模式，因为识别 [recipient]
    需要理解“earned it” 中的“it” 跨句子。第2步：独立应用所选模式识别 [recipient]。每个模式应单独应用，不考虑其他模式。第2.1步：基于语义模式
    f1 识别 [recipient]：“[recipient] 是接受礼物或赠款的实体。”应用模式 f1 到给定文档中，接受礼物或赠款的实体是“Ripley
    Johnson”。第2.2步：基于多句模式 f2 识别 [recipient]：在短语 “这个人，Ripley Johnson，赚到了它” 中，“它” 指的是访问权限。因此，“这个人”
    赚到了授予的访问权限。[recipient] 是“这个人”。第3步：重新评估论元候选（["Ripley Johnson", "这个人"]）：论元 “Ripley
    Johnson” 是否与 [recipient] 的论元角色一致？是的，因为“Ripley Johnson” 赚到了访问权限。论元 “Ripley Johnson”
    是否与 [recipient] 的论元角色一致？是的，因为“这个人” 赚到了访问权限。“这个人” 更合适，因为它离触发词更近。[recipient]：“这个人”
- en: C.4 The demonstration for DocEE Dataset
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.4 DocEE 数据集的演示
- en: 'Instruction:  Your  task  is  Event  Argument  Extraction.  In  this  task,  you  will  be  provided  with  a  document  that  describes  an  event.  Your  job  is  to  extract  and  quote  the  entities  (event  arguments)  from  the  document  that  correspond  to  each  argument  role  associated  with  the  event.Specifically,  you  will  use  the  patterns  in  the  "Pattern  List"  below  to  identify  event  arguments.  The  answers  should  also  satisfy  the  "Argument  Format  Principle"  and  "Argument  Number  Principle"  below.Pattern  List:["Semantic  Pattern":  [giver]  is  the  person,  group,  or  organization  in  the  document  that  gives  the  grant  or  gift.  For  example,  "[John]  is  the  donor"."Syntactic.SubjectVerbObject  Pattern":  [giver]  may  be  the  subject  of  a  verb  that  indicates  the  act  of  giving,  or  granting.  For  example,  "[John]  gave  Mary  a  book"."Syntactic.PrepositionalPhrases  Pattern":  [giver]  may  be  indicated  by  prepositions  such  as  ’from’,  and  ’of’,  which  typically  precede  the  giver  in  the  context  of  a  giving  or  granting  event.  For  example,  "She  received  a  gift  from  [John]".]Argument  Extracting  Principles:["Argument  Format  Principle":  articles  and  prepositions  are  not  included  in  the  identified  event  argument.  For  example,  select  "damaged  car"  among  "damaged  car",  "damaged  car  belonging  to  the  victim"  and  "the  damaged  car";  select  "the  officer"  rather  than  "the  officer  under  investigation"."Argument  Number  Principle":  Most  event  arguments  only  have  one  unique  mention.  However,  only  in  situations  where  you  are  strongly  sure  that  the  mentions  associated  with  an  argument  role  are  distinctly  different,  you  are  permitted  to  extract  <=  3  mentions.]Example  Task:Question:  What  are  the  event  arguments  of  ’Date’,  ’Magnitude’,  ’Number  of  Destroyed  Building’,  ’Casualties  and  Losses’  in  the  ’Earthquakes’  event  in  the  provided  news  document  below?  Ensure  that  each  identified  argument  is  quoted  exactly  as  the  entity  appears  in  the  given  document.  If  an  event  argument  is  not  explicitly  mentioned,  please  answer  "not  specified".Document:  a  given  newsAnswer:Elaborate  the  meaning  of  event  type  and  its  argument  roles:’Earthquakes’:  The  event  involves  the  shaking  of  the  surface  of  the  Earth  resulting  from  a  sudden  release  of  energy  in  the  Earth’s  lithosphere.[Date]:  when  the  earthquake  occurred.[Magnitude]:  the  measure  of  the  size  of  the  earthquake.[Number  of  Destroyed  Building]:  the  number  of  buildings  or  structures  that  were  damaged  or  destroyed  due  to  the  earthquake.[Casualties  and  Losses]:  the  number  of  people  killed  or  injured,  and  the  amount  of  economic  losses  caused  by  the  earthquake.Recognizing  [Date]  in  the  given  document:Step  1  Select  a  pattern  in  the  pattern  list  that  are  best  suited  to  identify  the  [Date]  in  the  news  document:  Semantic  Pattern.Step  2.1  Identify  the  argument  based  on  Semantic  Pattern  f1:  "[Date]  is  when  the  earthquake  occurred".  The  [Date]  is  "not  specified"  because  the  document  didn’t  mention  when  the  earthquake  occurred.Step  3:  reevaluate_argument_candidates(["not  specified"]):Is  "not  specified"  alignment  with  the  argument  role  [Date]?  Yes,  because  [Date]  is  not  explicitly  mentioned  in  the  document,  so  "not  specified"  is  correct.[Date]:  "not  specified"Recognizing  [Magnitude]  in  the  given  document:Step  1  Select  a  pattern  in  the  pattern  list  that  are  best  suited  to  identify  the  [Magnitude]  in  the  news  document:  Semantic  Pattern.Step  2.1  Identify  the  argument  based  on  Semantic  Pattern  f1:  "[Magnitude]  is  the  measure  of  the  size  of  the  earthquake".  The  [Magnitude]  is  "6.6"  because  the  document  mentions  "The  magnitude  6.6  shock".Step  3:  reevaluate_argument_candidates(["6.6"]):Is  "6.6"  alignment  with  the  argument  role  [Magnitude]?  Yes,  because  it  quantifies  the  size  of  the  earthquake.[Magnitude]:  "6.6"Recognizing  [Number  of  Destroyed  Building]  in  the  given  document:Step  1  Select  a  pattern  in  the  pattern  list  that  are  best  suited  to  identify  the  [Number  of  Destroyed  Building]  in  the  news  document:  Semantic  PatternStep  2.1  Identify  the  argument  based  on  Semantic  Pattern  f1:  "[Number  of  Destroyed  Building]  is  the  people  killed,  injured,  or  properties  affected  by  the  drought.".  The  [Number  of  Destroyed  Building]  is  "more  than  80%  of  the  houses"  because  the  document  mentions  this  specific  proportion  related  to  houses  destroyed.Step  3:  reevaluate_argument_candidates(["more  than  80%  of  the  houses"]):Is  "more  than  80%  of  the  houses"  alignment  with  the  argument  role  [Number  of  Destroyed  Building]?  No,  because  it  describes  the  percentage  not  the  specific  number  of  buildings  destroyed.[Number  of  Destroyed  Building]:  "not  specified"Recognizing  [Casualties  and  Losses]  in  the  given  document:Step  1  Select  a  pattern  in  the  pattern  list  that  are  best  suited  to  identify  the  [Casualties  and  Losses]  in  the  news  document:  Semantic  PatternStep  2  Identify  the  argument  based  on  Semantic  Pattern  f1:  "[Casualties  and  Losses]  is  the  number  of  people  killed  or  injured,  and  the  amount  of  economic  losses  caused  by  the  earthquake".  The  [Casualties  and  Losses]  is  "claimed  142  deaths"  and  "800  houses  were  damaged"  because  the  document  provides  this  explicit  numbers  about  people  killed  due  to  the  earthquake  and  mentions  this  number  of  houses  destroyed,  representing  economic  losses.Step  3:  reevaluate_argument_candidates(["claimed  142  deaths",  "800  houses  were  damaged"]):Is  "claimed  142  deaths"  alignment  with  the  argument  role  [Casualties  and  Losses]?  Yes,  because  it  provides  details  about  the  number  of  people  killed  or  injured  due  to  the  earthquake.Is  "800  houses  were  damaged"  alignment  with  the  argument  role  [Casualties  and  Losses]?  Yes,  because  it  describes  the  extent  of  property  damage  caused  by  the  earthquake.[Casualties  and  Losses]:  "claimed  142  deaths",  "800  houses  were  damaged"End  of  answerTarget  task:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 指令：你的任务是事件参数提取。在这个任务中，你将获得一个描述事件的文档。你的工作是从文档中提取并引用与事件相关的每个参数角色相对应的实体（事件参数）。具体来说，你将使用下面的“模式列表”中的模式来识别事件参数。答案还应满足下面的“参数格式原则”和“参数数量原则”。模式列表：[语义模式]：[giver]
    是文档中给予资助或礼物的个人、团体或组织。例如，“[John] 是捐赠者”。[句法.主语动词宾语模式]：[giver] 可能是表示给予或授予行为的动词的主语。例如，“[John]
    给 Mary 一本书”。[句法.介词短语模式]：[giver] 可能通过如“from”和“of”等介词来表示，这些介词通常在给予或授予事件的上下文中位于 giver
    前面。例如，“她从 [John] 那里收到了一份礼物”。]参数提取原则：[参数格式原则]：在识别的事件参数中不包括冠词和介词。例如，从“损坏的车”，“属于受害者的损坏的车”和“损坏的车”中选择“损坏的车”；选择“警官”而不是“正在调查中的警官”。[参数数量原则]：大多数事件参数只有一个唯一的提及。然而，只有在你非常确定与参数角色相关的提及明显不同的情况下，你才允许提取
    <= 3 个提及。]示例任务：问题：在提供的新闻文档中，“地震”事件的“日期”，“震级”，“受损建筑数量”，“伤亡和损失”事件参数是什么？确保每个识别出的参数都与文档中的实体完全一致。如果事件参数没有明确提及，请回答“未指定”。文档：某新闻。答案：阐述事件类型及其参数角色的含义：“地震”：该事件涉及由于地球地壳突然释放能量而引起的地球表面的震动。[日期]：地震发生的时间。[震级]：地震大小的衡量标准。[受损建筑数量]：因地震造成损坏或毁坏的建筑物或结构的数量。[伤亡和损失]：因地震造成的人员死亡或受伤的数量，以及经济损失的金额。识别给定文档中的
    [日期]：步骤1 选择最适合识别新闻文档中 [日期] 的模式列表中的模式：语义模式。步骤2.1 根据语义模式 f1 识别参数：“[日期] 是地震发生的时间”。[日期]
    是“未指定”，因为文档中没有提及地震发生的时间。步骤3：重新评估参数候选项（[“未指定”]）：“未指定”是否符合 [日期] 的参数角色？是，因为文档中没有明确提及
    [日期]，所以“未指定”是正确的。[日期]：“未指定”识别给定文档中的 [震级]：步骤1 选择最适合识别新闻文档中 [震级] 的模式列表中的模式：语义模式。步骤2.1
    根据语义模式 f1 识别参数：“[震级] 是地震大小的衡量标准”。[震级] 是“6.6”，因为文档提到“震中震级为 6.6”。步骤3：重新评估参数候选项（[“6.6”]）：“6.6”是否符合
    [震级] 的参数角色？是，因为它量化了地震的大小。[震级]：“6.6”识别给定文档中的 [受损建筑数量]：步骤1 选择最适合识别新闻文档中 [受损建筑数量]
    的模式列表中的模式：语义模式。步骤2.1 根据语义模式 f1 识别参数：“[受损建筑数量] 是因干旱造成的人员死亡、受伤或财产受损的数量。”[受损建筑数量]
    是“超过 80% 的房屋”，因为文档提到与受损房屋相关的具体比例。步骤3：重新评估参数候选项（[“超过 80% 的房屋”]）：“超过 80% 的房屋”是否符合
    [受损建筑数量] 的参数角色？不，因为它描述了百分比而不是具体的受损建筑数量。[受损建筑数量]：“未指定”识别给定文档中的 [伤亡和损失]：步骤1 选择最适合识别新闻文档中
    [伤亡和损失] 的模式列表中的模式：语义模式。步骤2 根据语义模式 f1 识别参数：“[伤亡和损失] 是因地震造成的人员死亡或受伤的数量，以及经济损失的金额”。[伤亡和损失]
    是“声称 142 人死亡”和“800 所房屋受损”，因为文档提供了关于地震造成的人员死亡的明确数字，并提到了这一数量的房屋受损，代表了经济损失。步骤3：重新评估参数候选项（[“声称
    142 人死亡”，“800 所房屋受损”]）：“声称 142 人死亡”是否符合 [伤亡和损失] 的参数角色？是，因为它提供了有关因地震造成的人员死亡或受伤的详细信息。“800
    所房屋受损”是否符合 [伤亡和损失] 的参数角色？是，因为它描述了地震造成的财产损害的程度。[伤亡和损失]：“声称 142 人死亡”，“800 所房屋受损”
