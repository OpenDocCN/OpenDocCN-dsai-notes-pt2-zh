- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:48:15'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Heuristics-Driven Link-of-Analogy Prompting: Enhancing Large Language Models
    for Document-Level Event Argument Extraction'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2311.06555](https://ar5iv.labs.arxiv.org/html/2311.06555)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Hanzhang Zhou^(1,2), Junlang Qian¹, Zijian Feng^(1,2), Hui Lu¹, Zixiao Zhu^(1,2),
    Kezhi Mao^(1,2)¹¹footnotemark: 1'
  prefs: []
  type: TYPE_NORMAL
- en: ¹Nanyang Technological University, Singapore
  prefs: []
  type: TYPE_NORMAL
- en: ²Future Resilient Systems Programme, Singapore-ETH Centre, Singapore
  prefs: []
  type: TYPE_NORMAL
- en: '{hanzhang001, junlang001, feng0119, hui007, zixiao001}@e.ntu.edu.sg'
  prefs: []
  type: TYPE_NORMAL
- en: ekzmao@ntu.edu.sg
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this study, we investigate in-context learning (ICL) in document-level event
    argument extraction (EAE). The paper identifies key challenges in this problem,
    including example selection, context length limitation, abundance of event types,
    and the limitation of Chain-of-Thought (CoT) prompting in non-reasoning tasks.
    To address these challenges, we introduce the Heuristic-Driven Link-of-Analogy
    (HD-LoA) prompting method. Specifically, we hypothesize and validate that LLMs
    learn task-specific heuristics from demonstrations via ICL. Building upon this
    hypothesis, we introduce an explicit heuristic-driven demonstration construction
    approach, which transforms the haphazard example selection process into a methodical
    method that emphasizes task heuristics. Additionally, inspired by the analogical
    reasoning of human, we propose the link-of-analogy prompting, which enables LLMs
    to process new situations by drawing analogies to known situations, enhancing
    their adaptability. Extensive experiments show that our method outperforms the
    existing prompting methods and few-shot supervised learning methods, exhibiting
    F1 score improvements of $4.53\%$, indicating its effectiveness across different
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Heuristics-Driven Link-of-Analogy Prompting: Enhancing Large Language Models
    for Document-Level Event Argument Extraction'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hanzhang Zhou^(1,2), Junlang Qian¹, Zijian Feng^(1,2), Hui Lu¹, Zixiao Zhu^(1,2),
    Kezhi Mao^(1,2)¹¹footnotemark: 1 ¹Nanyang Technological University, Singapore
    ²Future Resilient Systems Programme, Singapore-ETH Centre, Singapore {hanzhang001,
    junlang001, feng0119, hui007, zixiao001}@e.ntu.edu.sg ekzmao@ntu.edu.sg'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Document-level Event Argument Extraction (EAE) aims to transform unstructured
    event information from documents into structured formats encapsulating event arguments,
    facilitating their interpretation and application in various domains (Grishman,
    [2019](#bib.bib16)). The prevalent approach for this task relies on the collection
    of labeled data and the subsequent model training via supervised learning (Liu
    et al., [2023a](#bib.bib20); Pouran Ben Veyseh et al., [2022](#bib.bib28); Zhou
    and Mao, [2022](#bib.bib45); Du and Cardie, [2020a](#bib.bib7)). While effective,
    this approach comes with the significant drawback: it necessitates a substantial
    amount of training data, which is particularly burdensome and costly given the
    complexity inherent to document-level EAE.'
  prefs: []
  type: TYPE_NORMAL
- en: In this context, in-context learning (ICL) (Brown et al., [2020](#bib.bib4);
    Liu et al., [2022](#bib.bib19); Zhou et al., [2022](#bib.bib44)), an emergent
    ability of large language models (LLMs), offers a promising alternative to supervised
    learning. ICL alleviates the need for large-scale data as it only uses a few examples
    as input-output pairs of the prompt to guide LLMs in performing the task on an
    unseen example.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/01d00cc7c409fef85d91b3a113a0dcd1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: CoT’s step-by-step reasoning degrades to a mere single step for non-reasoning
    tasks. Reasoning steps of two reasoning tasks (in orange) and two non-reasoning
    tasks (in blue) are compared. Different colors indicate distinct reasoning steps.
    Prompts are adopted from (Shum et al., [2023](#bib.bib31))'
  prefs: []
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: 'However, applying ICL to document-level EAE presents numerous challenges. The
    design of in-context demonstrations is crucial to the effectiveness of ICL, with
    the performance of ICL being significantly influenced by factors such as the selection
    of examples and the formatting of reasoning steps (Zhang et al., [2023](#bib.bib43);
    Liu et al., [2022](#bib.bib19); Zhang et al., [2022](#bib.bib42); Fu et al., [2022](#bib.bib10)).
    Consequently, several crucial challenges emerge concerning the prompting strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example selection. Gaining clarity on how to select demonstration examples requires
    an in-depth understanding of what LLMs learn from these demonstrations in ICL.
    At present, however, the understanding of this aspect remains limited (Dong et al.,
    [2022](#bib.bib6)). Therefore, what are the guiding principles for example selection,
    and what characterizes a ’good’ example for document-level EAE?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Context length limit. Following the experimental set-up of chain-of-thought
    (CoT) prompting (Wei et al., [2022](#bib.bib38)), demonstrations typically incorporate
    six or eight examples. However, in the context of document-level EAE, selecting
    eight documents as examples might significantly extend the context length, potentially
    surpassing the token limit of LLMs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Abundance of Event Types. The EAE task can feature more than a hundred distinct
    event types and argument roles. Yet, in-context examples can only capture a narrow
    subset of these, leaving the majority of argument roles unseen. Creating individual
    prompts for each unique event type and argument role is both labor-intensive and
    impractical. This leads to the question: How to design a universal prompting strategy
    that effectively addresses a wide range of unseen event types and argument roles?'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prompting strategy for non-reasoning task. While the CoT prompting strategy
    is extensively used across a variety of tasks, both reasoning and non-reasoning,
    it was originally designed only for reasoning tasks such as arithmetic reasoning.
    As shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Heuristics-Driven Link-of-Analogy
    Prompting: Enhancing Large Language Models for Document-Level Event Argument Extraction"),
    applying CoT to non-reasoning tasks will degrade its step-by-step reasoning into
    a single-step process, compromising its effectiveness. Consequently, there is
    a need for a reasoning strategy tailored for non-reasoning tasks.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c9b0d55ffed6814941a8ee80315fb18a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The implicit heuristics behind in-context examples.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this work, we put forward a novel hypothesis that LLMs learn task-specific
    heuristics from examples and validate the hypothesis through experiments. Building
    upon this hypothesis, we propose heuristic-driven link-of-analogy prompting to
    address the aforementioned questions. To elaborate:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We propose and empirically validate the hypothesis that LLMs learn task specific
    heuristics from examples in ICL. The hypothesis is derived by drawing an analogy
    from the training of deep learning models and the fine-tuning of pretrained language
    models (PLMs). Both these models achieve enhanced performance by learning task-specific
    patterns, either through training or fine-tuning on task data (Zhou and Srikumar,
    [2022](#bib.bib46); Shachaf et al., [2021](#bib.bib30); Kim et al., [2018](#bib.bib17);
    Najafabadi et al., [2015](#bib.bib24)). Drawing a parallel, we hypothesize that,
    although LLMs’ parameters are frozen in ICL, providing them with in-context examples
    allows LLMs to learn task-specific patterns during inference. These patterns encompasses
    both lower-level patterns like label space and input text distribution, and higher-level
    heuristics from demonstrations. We notice the learning of lower-level patterns
    has been validated Min et al. ([2022](#bib.bib23)). Therefore, we focus on substantiating
    the learning of task-specific heuristics in ICL. An illustration of the implicit
    heuristics behind in-context examples can be found in Figure [2](#S1.F2 "Figure
    2 ‣ 1 Introduction ‣ Heuristics-Driven Link-of-Analogy Prompting: Enhancing Large
    Language Models for Document-Level Event Argument Extraction"), while quantitative
    validations of our hypothesis are detailed in Section [2](#S2 "2 What LLMs learn
    from the demonstration? ‣ Heuristics-Driven Link-of-Analogy Prompting: Enhancing
    Large Language Models for Document-Level Event Argument Extraction").'
  prefs: []
  type: TYPE_NORMAL
- en: We propose a heuristic-driven demonstration construction method. Based on our
    hypothesis, task heuristics are crucial for the ICL performance of LLMs. Yet,
    these heuristics are implicitly conveyed through examples, leading to complexity
    for ultilizing these heuristics and uncertainties about whether LLMs have recognized
    these heuristics. To address this, we propose to explicitly provide task heuristics
    in demonstrations. This approach simplify the LLM’s two-step process of implicit
    heuristic recognition and subsequent inference into a more straightforward, singular
    step of heuristic-based inference.
  prefs: []
  type: TYPE_NORMAL
- en: We propose the link-of-analogy prompting method for document-level EAE that
    suitable for non-reasoning tasks. To address the aforementioned challenges of
    abundance of event types in EAE and the limitations of CoT prompting on non-reasoning
    tasks, we present the link-of-analogy prompting. Inspired by the analogical reasoning––a
    core mechanism of human cognition–this approach enables LLMs to extrapolate knowledge
    from known event types to unseen ones. By enabling LLMs to process new situations
    through drawing an analogy to known situations, the link-of-analogy prompting
    significantly enhances the performance of ICL.
  prefs: []
  type: TYPE_NORMAL
- en: We implement our heuristic-driven link-of-analogy prompting method across three
    LLM models on two document-level EAE datasets and two datasets on sentiment analysis
    and natural language inference tasks. Extensive experiments demonstrate that our
    method outperforms the state-of-the-art prompting methods and few-shot supervised
    learning methods, as evidenced by absolute F1 score increases of $4.53\%$, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 2 What LLMs learn from the demonstration?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/31af91b41129c98ec37954fe8b83359f.png)![Refer to caption](img/f6cffaf318ad31e980cb7858913b2c64.png)![Refer
    to caption](img/c734e8830f06ffae2c529dd6ed71aae5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: An illustration of the correlation between example quantity and heuristic
    diversity in well-designed prompts. # Examples: the number of examples used in
    each prompt of the corresponding paper. # Heuristics: the number of heuristics
    identified in each prompt of the corresponding paper. # Heuristics in Rand.: the
    average number of heuristics in the prompt composed of random selected examples.'
  prefs: []
  type: TYPE_NORMAL
- en: There is minimal understanding of what LLMs learn from the demonstration of
    ICL currently available. In this work, we hypothesize that LLMs learn task-specific
    heuristics from examples of ICL based on an analogy from the training of deep
    learning models. We confirm this hypothesis with carefully designed experiments
    in two aspects.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Correlation between Example Quantity and Heuristic Diversity in Well-Designed
    Prompts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If our hypothesis holds, it implies that well-constructed prompts should incorporate
    a diverse range of heuristics. To validate this implication, we assess both the
    quantity of examples and the quantity of different heuristics present in prompts
    from published studies.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, manually estimating the number of heuristics can introduce subjectivity,
    leading to potential biases. Thus, the challenge is to objectively identify the
    implicit heuristics inherent in demonstration examples. In this work, we employ
    GPT-4 to recognize the implicit heuristics for each example and to determine if
    there are shared heuristics across multiple examples. An detailed example of the
    prompt we used, along with the heuristics identified by GPT-4, can be found in
    Appendix [B](#A2 "Appendix B Recognize Implicit Heuristics of In-Context Examples
    by GPT-4 ‣ Heuristics-Driven Link-of-Analogy Prompting: Enhancing Large Language
    Models for Document-Level Event Argument Extraction").'
  prefs: []
  type: TYPE_NORMAL
- en: 'We investigate the relationship between the number of examples used in the
    prompt and the number of heuristics inherent in the corresponding prompt, across
    six cutting-edge prompting papers on three datasets. Specifically, SOTA prompting
    methods including CoT (Wei et al., [2022](#bib.bib38)), Automate-CoT (Shum et al.,
    [2023](#bib.bib31)), Auto-CoT (Zhang et al., [2023](#bib.bib43)), Iter-CoT (Sun
    et al., [2023](#bib.bib34)), Boosted (Pitis et al., [2023](#bib.bib27)), Active-CoT
    (Diao et al., [2023](#bib.bib5)) are investigated and datasets of commonsense
    reasoning and arithmetic reasoning are evaluated. The experimental results are
    demonstrated in Figure [3](#S2.F3 "Figure 3 ‣ 2 What LLMs learn from the demonstration?
    ‣ Heuristics-Driven Link-of-Analogy Prompting: Enhancing Large Language Models
    for Document-Level Event Argument Extraction"). It is observable that in meticulously
    designed prompts, the number of heuristics closely matches the number of examples.
    Furthermore, the number of heuristics in these carefully constructed prompts significantly
    exceeds that in prompts composed of randomly selected examples. This observation
    substantiates our hypothesis that well-constructed prompts indeed encapsulate
    diverse heuristics in examples.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cb5008944067c92086af0d94b1520a95.png)![Refer to caption](img/bc6dc26f467b60c07264ce7cfa4ef01a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Comparison of ICL performance using single-heuristic strategy versus
    diverse-heuristics strategy across different number of example on the StrategyQA
    and SST-2 Dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d35914d1238e191fa862ed7102a55051.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: An illustration of HD-LoA prompting.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Comparing Diverse-Heuristics and Single-Heuristic Strategies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If our hypothesis holds true, then incorporating a broader range of heuristics
    in demonstrations should enable LLMs to identify and leverage more heuristics,
    leading to enhanced performance. With this in mind, we examine the impact of heuristic
    diversity in in-context examples on the performance of ICL. Specifically, we juxtapose
    two in-context example selection strategies on the Strategy QA dataset. The single-heuristic
    strategy formulates prompts where all rationales of in-context examples follow
    a same heuristic. Conversely, the diverse-heuristic strategy constructs prompts
    where all rationales of in-context examples exhibit different heuristics. We construct
    prompts that follow the two strategies based on the original prompts from Diao
    et al. ([2023](#bib.bib5)); Shum et al. ([2023](#bib.bib31)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The performance comparison of prompts constructed by the diverse-heuristic
    strategy and the single-heuristic strategy on the StrategyQA (Geva et al., [2021](#bib.bib14))
    and SST-2 (Socher et al., [2013](#bib.bib32)) datasets is illustrated in Figure
    [4](#S2.F4 "Figure 4 ‣ 2.1 Correlation between Example Quantity and Heuristic
    Diversity in Well-Designed Prompts ‣ 2 What LLMs learn from the demonstration?
    ‣ Heuristics-Driven Link-of-Analogy Prompting: Enhancing Large Language Models
    for Document-Level Event Argument Extraction"). From the results, it’s evident
    that with the same number of examples, the diverse-heuristics strategy significantly
    outperforms the single-heuristic approach. This finding also supports our hypothesis
    that LLMs indeed learn heuristics from in-context examples.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Heuristic-Driven Demonstration Construction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Building on our understanding of heuristic learning during ICL, we aim to address
    the challenge of example selection for ICL. From our validation experiments in
    the previous section, we observed that: (1) LLMs can recognize the heuristics
    underlying in-context examples and leverage them to enhance performance. (2) These
    heuristics are implicitly embedded within the explanations of in-context examples.
    As a result, LLMs’ utilization of heuristics involves a two-step process: first
    recognizing the implicit heuristics and then making inferences based on these
    heuristics.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The above observation leads to our idea: Can we simplify the two-step process
    by explicitly providing LLMs with task-specific heuristics? Specifically, rather
    than providing examples of input-output pairs with heuristics implicitly embedded
    in the output, our approach replaces most examples in the prompt with distinct
    task-specific heuristics, as demonstrated by the heuristics in Figure [5](#S2.F5
    "Figure 5 ‣ 2.1 Correlation between Example Quantity and Heuristic Diversity in
    Well-Designed Prompts ‣ 2 What LLMs learn from the demonstration? ‣ Heuristics-Driven
    Link-of-Analogy Prompting: Enhancing Large Language Models for Document-Level
    Event Argument Extraction").'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is worth noting that our method still retains a minimum number of examples
    for two reasons: First, the formatting of reasoning steps, such as the link-of-analogy
    prompting proposed in the following section, requires clear illustration through
    the output of an example. Second, a minimum number of examples are necessary to
    mitigate bias in the answers. For instance, in sentiment analysis tasks, it is
    essential to provide at least two examples, one positive and one negative, to
    prevent bias towards a particular sentiment. In the context of EAE task, we only
    maintain one example to demonstrate the formatting of reasoning steps.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three advantages of our approach. Firstly, it provides a guidance
    on the example selection process. The example selection process of ICL is usually
    an indiscriminate, manual process (Liu et al., [2023b](#bib.bib21); Wei et al.,
    [2022](#bib.bib38); Zhou et al., [2022](#bib.bib44)). However, our method addresses
    the prevailing uncertainty in ICL example selection, converting the directionless
    and indiscriminate process into a methodical approach that emphasizes task-specific
    heuristics. In addition, the two-step recognize and inference process of utilizing
    heuristics is simplified to only one step, thus simplifying the reasoning complexity
    for LLMs. Finally, it condenses lengthy examples that consists of input-output
    pairs into compact heuristics, reducing the context length of prompts. Specifically,
    we propose the heuristic-driven demonstration construction method that contains
    two parts: low-level pattern demonstration and heuristics generation.'
  prefs: []
  type: TYPE_NORMAL
- en: To provide a clearer and more precise understanding of our heuristic-driven
    demonstration construction method, we also describe it using mathematical expressions
    as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'In standard demonstration construction, given the question $\mathbf{Q}$ of
    the LLM’s output is generated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle o_{j}=f_{LLM}(\mathbf{C}_{8},\mathbf{Q},o_{<j})$ |  | (1)
    |'
  prefs: []
  type: TYPE_TB
- en: where $o_{<j}$
  prefs: []
  type: TYPE_NORMAL
- en: 'In this work, our heuristic-driven demonstration construction method converts
    most examples into heuristics, retaining only the minimum number of examples necessary
    for demonstrating the formatting of reasoning steps. Therefore, the $jth$ of the
    LLM’s output, when employing our proposed method, is generated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle o_{j}=f_{LLM}(\mathbf{H},\mathbf{C}_{m},\mathbf{Q},o_{<j})$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{H}=\{\mathbf{h}_{1},\mathbf{h}_{2},\cdots,\mathbf{h}_{l}\}$ as
    number of examples.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Link-of-Analogy Prompting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We propose the link-of-analogy prompting to address the challenges below: First,
    the EAE task is characterized by its extensive variety of argument roles and event
    types, often exceeding a hundred, yet ICL examples can only cover a very limited
    subset. This discrepancy raises a critical challenge: designing a prompting strategy
    that effectively addresses unseen event types. Additionally, to streamline heuristic
    generation process, we provide heuristics for a specific argument, giver, within
    the prompt. This leads to the question of how to extend the effectiveness of giver
    heuristics to other argument roles. Finally, as highlighted in the Introduction,
    applying CoT prompting to non-reasoning tasks tends to degrade the step-by-step
    analysis into a one-step rationale (Shum et al., [2023](#bib.bib31); Diao et al.,
    [2023](#bib.bib5)), necessitating more proper prompting strategies for such tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Inspired by the analogical reasoning (Gentner and Smith, [2013](#bib.bib13)),
    a core mechanism of human cognition, we seek to resolve the challenges presented.
    Humans often understand a new situation by drawing an analogy to a familiar situation.
    For example, students often solve new problems by mapping solutions from known
    problems (Ross, [1987](#bib.bib29)). Similarly, we anticipate that LLMs will be
    able to extract information of unseen events or generate heuristics for unseen
    argument roles by drawing an analogy to events and heuristics provided in in-context
    examples. Empirically, we find that LLMs are indeed capable of doing analogical
    reasoning when prompted appropriately. For example, when provided with the heuristic
    for giver in the prompt: "[giver] is the person, group, or organization in the
    document that gives the grant or gift", LLMs can make an analogy and generate
    the heuristic for the argument vehicle in target questions: "[vehicle] is the
    means of transport used to move the person or object".'
  prefs: []
  type: TYPE_NORMAL
- en: 'To further enhance the analogical reasoning capabilities of LLMs, we introduce
    our link-of-analogy (LoA) prompting strategy, which emulates the analogical reasoning
    process of human. Cognitive science studies reveals that humans perform analogical
    reasoning through a sequence of retrieval, mapping, and evaluation (Gentner and
    Forbus, [2011](#bib.bib11); Gentner and Markman, [1997](#bib.bib12)). In alignment
    with this process, our method involves retrieving the most appropriate heuristic
    from the provided set, mapping this heuristic onto the target argument role to
    get new heuristic and extracting the argument based on this heuristic, and finally
    evaluating the extracted argument. This methodology is depicted through the answer
    of the in-context example in Figure [5](#S2.F5 "Figure 5 ‣ 2.1 Correlation between
    Example Quantity and Heuristic Diversity in Well-Designed Prompts ‣ 2 What LLMs
    learn from the demonstration? ‣ Heuristics-Driven Link-of-Analogy Prompting: Enhancing
    Large Language Models for Document-Level Event Argument Extraction").'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Method | RAMS | DocEE-Normal | DocEE-Cross |'
  prefs: []
  type: TYPE_TB
- en: '| Arg-I | Arg-C | Arg-C | Arg-C |'
  prefs: []
  type: TYPE_TB
- en: '| Supervised learning (few-shot) | EEQA (Du and Cardie, [2020b](#bib.bib8))
    |  | 19.54 |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | PAIE (Ma et al., [2022](#bib.bib22)) |  | 29.86 |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | TSAR (Xu et al., [2022](#bib.bib40)) | - | 26.67 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '|  | CRP (Liu et al., [2023a](#bib.bib20)) |  | 30.09 |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | FewDocAE (Yang et al., [2023](#bib.bib41)) |  | - | 12.07 | 10.51 |'
  prefs: []
  type: TYPE_TB
- en: '| text-davinci-003 | Standard (Agrawal et al., [2022](#bib.bib1)) | 39.96 |
    31.6 | 25.55 | 25.41 |'
  prefs: []
  type: TYPE_TB
- en: '|  | CoT (Wei et al., [2022](#bib.bib38)) | 43.03 | 34.94 | 27.68 | 28.64 |'
  prefs: []
  type: TYPE_TB
- en: '|  | HD-LoA (ours) | 46.08 | 39.47 | 30.37 | 31.21 |'
  prefs: []
  type: TYPE_TB
- en: '| gpt-3.5-turbo | Standard (Agrawal et al., [2022](#bib.bib1)) | 42.63 | 32.11
    | 26.56 | 25.64 |'
  prefs: []
  type: TYPE_TB
- en: '|  | CoT (Wei et al., [2022](#bib.bib38)) | 33.49 | 27.86 | 25.66 | 26.17 |'
  prefs: []
  type: TYPE_TB
- en: '|  | HD-LoA (ours) | 38.33 | 31.5 | 27.85 | 28.46 |'
  prefs: []
  type: TYPE_TB
- en: '| gpt-4 | Standard (Agrawal et al., [2022](#bib.bib1)) | 44.73 | 37.08 |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | CoT (Wei et al., [2022](#bib.bib38)) | 44.93 | 38.09 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '|  | HD-LoA (ours) | 50.4 | 42.75 |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Overall performance. In few-shot setting, supervised learning methods
    utilize 70 samples on the RAMS dataset and 2 samples on the DocEE dataset for
    training. In contrast, our HD-LoA method only uses a single sample.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we aim to explore the following research questions (RQs) regarding
    our Heuristic-Driven Link-of-Analogy (HD-LoA) prompting. RQ1 Does HD-LoA prompting
    improve in-context learning performance in document-level EAE task? RQ2 How does
    the HD-LoA prompting compare to supervised learning methods under few-shot setting?
    RQ3 Is the HD-LoA prompting method effective when applied to tasks beyond document-level
    EAE? RQ4 Does different components of the HD-LoA prompting effectively contributing
    to its performance?
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Experimental Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Dataset: For the evaluation of the document-level EAE task, we adopt RAMS (Ebner
    et al., [2020](#bib.bib9)) and DocEE (Tong et al., [2022](#bib.bib35)) datasets.
    The WIKIEVENTS dataset (Li et al., [2021](#bib.bib18)) is excluded from our study
    because it relies on preprocessed entity candidates for annotating event arguments
    the annotation, which diverges from the direct argument identification of LLMs.
    For evaluation, we follow the metrics in (Ma et al., [2022](#bib.bib22)), namely
    the argument identification F1 score (Arg-I), and the argument classification
    F1 score (Arg-C).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, we utilize the SST-2 (Socher et al., [2013](#bib.bib32)) and
    SNLI (Bowman et al., [2015](#bib.bib3)) datasets to assess the effectiveness of
    our HD-LoA prompting strategy on other non-reasoning tasks: sentiment analysis
    and natural language inference. The detailed statistics of the datasets and the
    number of tested samples are listed in Appendix [A](#A1 "Appendix A Experimental
    Details ‣ Heuristics-Driven Link-of-Analogy Prompting: Enhancing Large Language
    Models for Document-Level Event Argument Extraction")'
  prefs: []
  type: TYPE_NORMAL
- en: Baselines Our HD-LoA approach is compared against several state-of-the-art prompting
    methods, including the standard prompting method used in document-level EAE (Agrawal
    et al., [2022](#bib.bib1)), and the Chain-of-Thought (CoT) prompting (Wei et al.,
    [2022](#bib.bib38)). Additionally, we compare our method with various supervised
    learning methods in EAE, such as FewDocAE (Yang et al., [2023](#bib.bib41)), CRP
    (Liu et al., [2023a](#bib.bib20)), PAIE (Ma et al., [2022](#bib.bib22)), TSAR
    (Xu et al., [2022](#bib.bib40)), EEQA (Du and Cardie, [2020b](#bib.bib8)), etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'LLMs: The experiments are carried out using three prominent large language
    models: the publicly available GPT-3 (Brown et al., [2020](#bib.bib4)) in its
    text-davinci-003 and gpt-3.5-turbo versions (Ouyang et al., [2022](#bib.bib26)),
    as well as GPT-4 (OpenAI, [2023](#bib.bib25)). Due to the high cost associated
    with using GPT-4, its evaluation is limited solely to the RAMS dataset. These
    models are accessed via the public APIs from OpenAI’s services¹¹1https://openai.com/api/.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Experimental Details: Please find more information about experimental details
    in Appendix [A](#A1 "Appendix A Experimental Details ‣ Heuristics-Driven Link-of-Analogy
    Prompting: Enhancing Large Language Models for Document-Level Event Argument Extraction")
    and the prompts we used in Appendix [C](#A3 "Appendix C Full Prompts ‣ Appendix
    B Recognize Implicit Heuristics of In-Context Examples by GPT-4 ‣ Heuristics-Driven
    Link-of-Analogy Prompting: Enhancing Large Language Models for Document-Level
    Event Argument Extraction").'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Overall Experimental Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Addressing RQ1, the experimental results presented in Table [1](#S5.T1 "Table
    1 ‣ 5 Experiments ‣ Heuristics-Driven Link-of-Analogy Prompting: Enhancing Large
    Language Models for Document-Level Event Argument Extraction") indicate that our
    HD-LoA prompting significantly enhances in-context learning for document-level
    EAE task. The HD-LoA method consistently surpasses CoT prompting (Wei et al.,
    [2022](#bib.bib38)) across all three LLMs and both datasets, achieving the largest
    F1 score improvements of $4.53\%$ on the text-davinci-003 model. However, both
    CoT and HD-LoA prompting show suboptimal performance on the gpt-3.5-turbo model
    within the RAMS dataset, possibly due to the model’s tradeoff, which favor enhanced
    dialogue ability at the expense of reasoning ability, potentially impeding the
    effectiveness of reasoning-based prompting methods.'
  prefs: []
  type: TYPE_NORMAL
- en: In response to RQ2, our HD-LoA method not only significantly enhances performance
    in few-shot settings compared to supervised learning approaches but also achieves
    these impressive results with considerably fewer labeled samples. On the RAMS
    dataset, HD-LoA achieves a $9.38\%$ improvement. Experimental findings indicate
    that our method can successfully mitigate the document-level EAE task’s reliance
    on extensive labeled data.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | SST-2 | SNLI |'
  prefs: []
  type: TYPE_TB
- en: '| CoT | 91.39 | 77.97 |'
  prefs: []
  type: TYPE_TB
- en: '| HD-LoA (ours) | 94.26 | 80.60 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Evaluation of the HD-LoA prompting on sentiment analysis and natural
    language inference tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Adaptability of HD-LoA Prompting for Other Tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In addressing RQ3, we have extended our HD-LoA prompting method to sentiment
    analysis (SA) and natural language inference (NLI) tasks, utilizing the SST-2
    (Socher et al., [2013](#bib.bib32)) and SNLI (Bowman et al., [2015](#bib.bib3))
    datasets for evaluation. We adopt the CoT style prompts on these two datasets
    from Shum et al. ([2023](#bib.bib31)). Experimental results are presented in Table
    [2](#S5.T2 "Table 2 ‣ 5.2 Overall Experimental Results ‣ 5 Experiments ‣ Heuristics-Driven
    Link-of-Analogy Prompting: Enhancing Large Language Models for Document-Level
    Event Argument Extraction"). Compared to CoT prompting, our method gets accuracy
    enhancements of $2.87\%$ on SST-2 and SNLI datasets, respectively. These findings
    indicate that our HD-LoA prompting can be effectively adapted to a diverse array
    of non-reasoning NLP tasks. The prompts for SA and NLI tasks are provided in Appendix
    [C](#A3 "Appendix C Full Prompts ‣ Appendix B Recognize Implicit Heuristics of
    In-Context Examples by GPT-4 ‣ Heuristics-Driven Link-of-Analogy Prompting: Enhancing
    Large Language Models for Document-Level Event Argument Extraction").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | RAMS | DocEE-Normal | DocEE-Cross |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Arg-I | Arg-C | Arg-C | Arg-C |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Supervised learning | EEQA (Du and Cardie, [2020b](#bib.bib8)) | 48.70 |
    46.70 | 33.50 | 24.00 |'
  prefs: []
  type: TYPE_TB
- en: '|  | MG-Reader (Du and Cardie, [2020a](#bib.bib7)) | - | - | 32.90 | 21.40
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | BART-Gen (Li et al., [2021](#bib.bib18)) | 51.20 | 47.10 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '|  | OntologyQA (Tong et al., [2022](#bib.bib35)) | - | - | 41.00 | 29.80 |'
  prefs: []
  type: TYPE_TB
- en: '|  | PAIE (Ma et al., [2022](#bib.bib22)) | 56.80 | 52.20 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| text-davinci-003 | HD-LoA (ours) | 46.08 | 39.47 | 30.37 | 31.21 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Comparison with Fully Trained Supervised Models.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Comparison with Fully Trained Supervised Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Additionally, we compare our HD-LoA method with supervised learning method
    that trained on the entire dataset for document-level EAE task. As illustrated
    in Table [3](#S5.T3 "Table 3 ‣ 5.3 Adaptability of HD-LoA Prompting for Other
    Tasks ‣ 5 Experiments ‣ Heuristics-Driven Link-of-Analogy Prompting: Enhancing
    Large Language Models for Document-Level Event Argument Extraction"), it is anticipated
    that these models trained on thousands of samples would exhibit higher accuracy
    compared to our HD-LoA method, which employs only a single labeled sample. Nevertheless,
    HD-LoA prompting demonstrates competitive performance against supervised methods
    and even outperform these extensively trained models on the DocEE dataset in the
    cross-domain setting. This finding also illustrates the effectiveness of our HD-LoA
    prompting strategy, particularly in scenarios where it is impractical and costly
    to build large annotated datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | RAMS |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Without heuristic | 36.42 |'
  prefs: []
  type: TYPE_TB
- en: '| Without LoA | 36.03 |'
  prefs: []
  type: TYPE_TB
- en: '| HD-LoA | 39.47 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Ablations on the HD-LoA prompting method.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Ablations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To address RQ4, we carry out ablation experiments to discern how each component
    contributes to the overall performance of our HD-LoA prompting method. As presented
    in Table [4](#S5.T4 "Table 4 ‣ 5.4 Comparison with Fully Trained Supervised Models
    ‣ 5 Experiments ‣ Heuristics-Driven Link-of-Analogy Prompting: Enhancing Large
    Language Models for Document-Level Event Argument Extraction"), experimental results
    demonstrate that removing either the task-specific heuristics or link-of-analogy
    prompting will significantly degrade the eperformance of the HD-LoA prompting,
    suggesting the importance of each part of our prompting strategy.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Document-level EAE Existing document-level EAE studies are mostly based on supervised
    learning methods, which relies on the extensive collection of labeled data (Ma
    et al., [2022](#bib.bib22); Pouran Ben Veyseh et al., [2022](#bib.bib28); Zhou
    and Mao, [2022](#bib.bib45); Xu et al., [2021](#bib.bib39); Ebner et al., [2020](#bib.bib9);
    Du and Cardie, [2020a](#bib.bib7)), only Agrawal et al. ([2022](#bib.bib1)) exploit
    adopting LLMs on clinical EAE though standard prompts that are not enhanced by
    reasoning steps. Considering the revolutionize impact of LLMs on NLP community
    and the potential of ICL to reduce the dependency on large-scale labeled datasets,
    it is lack of study on the exploration of adopting SOTA prompting strategies on
    EAE.
  prefs: []
  type: TYPE_NORMAL
- en: In-context learning ICL enables LLMs to perform a target task by feeding a few
    prompted examples as part of the input (Brown et al., [2020](#bib.bib4)). Without
    gradient update, ICL allows a single LLM to perform various tasks universally.
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite being promising, the work mechanism for ICL remains unclear (Dong et al.,
    [2022](#bib.bib6)). Few studies have conducted preliminary explorations: Min et al.
    ([2022](#bib.bib23)) showed that the label space, input text distribution and
    overall format contribute substantially to the ICL performance. Liu et al. ([2022](#bib.bib19))
    concluded that examples that are semantically-similar to the test sample are more
    effective in unleashing LLM’s extensive knowledge. Akyürek et al. ([2022](#bib.bib2))
    found that transformer-based ICL can implement standard finetuning algorithms
    implicitly. In this work, we further hypothesize and validate that LLMs learn
    task-task specific heuristics from examples via ICL.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the performance of ICL is very sensitive to example selection (Gonen
    et al., [2022](#bib.bib15)). Studies suggested different ways of selecting examples
    based on complexity (Fu et al., [2022](#bib.bib10)), mutual information (Sorensen
    et al., [2022](#bib.bib33)), diversity (Zhang et al., [2023](#bib.bib43)), labeled
    dataset (Shum et al., [2023](#bib.bib31)), and so on. In this work, we convert
    the indiscriminate example selection process into a methodical approach that emphasizes
    task heuristics, making the example selection process more transparent.
  prefs: []
  type: TYPE_NORMAL
- en: Chain-of-thought prompting CoT is proposed by (Wei et al., [2022](#bib.bib38))
    and adopted in its following works (Wang et al., [2022a](#bib.bib36); Fu et al.,
    [2022](#bib.bib10); Zhang et al., [2023](#bib.bib43)) as a paradigm for prompting
    LLMs. It elicits LLMs to decompose the complex problem into intermediate reasoning
    steps and improves the performance on several reasoning tasks like arithmetic
    reasoning, commonsense reasoning, and symbolic reasoning. However, when applying
    CoT on non-reasoning tasks, due to the different nature of task, the step-by-step
    reasoning process is degraded to a one-step reasoning process (Shum et al., [2023](#bib.bib31);
    Wang et al., [2022b](#bib.bib37)), thus limited its performance on non-reasoning
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we hypothesize and validate that LLMs learn task-specific heuristics
    from demonstrations via in-context learning, which can provide a guidance on the
    example selection process. Building upon this hypothesis, we introduce an explicit
    heuristic-driven demonstration construction strategy, and propose a link-of-analogy
    prompting method. Extensive experimentation reveals that the HD-LoA prompting
    not only outperforms cutting-edge prompting methods and few-shot supervised methods
    in document-level EAE task, but also exhibits effectiveness across various non-reasoning
    NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we propose the HD-LoA prompting strategy for non-reasoning NLP
    tasks. Far fewer examples are adopted in the prompt and the analogical reasoning
    ability is activated in the HD-LoA prompting. Thus our prompting method relies
    on the advanced reasoning ability of LLMs. As a result, it is not suitable for
    LLMs that lack enough reasoning ability, such as gpt-turbo.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The authors would like to thank Edmond Lo, Lihui Chen, Xiyu Zhang for their
    constructive comments and suggestions. The research was conducted at the Future
    Resilient Systems at the Singapore-ETH Centre, which was established collaboratively
    between ETH Zurich and the National Research Foundation Singapore. This research
    is supported by the National Research Foundation Singapore (NRF) under its Campus
    for Research Excellence and Technological Enterprise (CREATE) programme.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Agrawal et al. (2022) Monica Agrawal, Stefan Hegselmann, Hunter Lang, Yoon Kim,
    and David Sontag. 2022. [Large language models are few-shot clinical information
    extractors](https://aclanthology.org/2022.emnlp-main.130). In *Proceedings of
    the 2022 Conference on Empirical Methods in Natural Language Processing*, pages
    1998–2022, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Akyürek et al. (2022) Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma,
    and Denny Zhou. 2022. What learning algorithm is in-context learning? investigations
    with linear models. *arXiv preprint arXiv:2211.15661*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bowman et al. (2015) Samuel R. Bowman, Gabor Angeli, Christopher Potts, and
    Christopher D. Manning. 2015. [A large annotated corpus for learning natural language
    inference](https://doi.org/10.18653/v1/D15-1075). In *Proceedings of the 2015
    Conference on Empirical Methods in Natural Language Processing*, pages 632–642,
    Lisbon, Portugal. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diao et al. (2023) Shizhe Diao, Pengcheng Wang, Yong Lin, and Tong Zhang. 2023.
    Active prompting with chain-of-thought for large language models. *arXiv preprint
    arXiv:2302.12246*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. (2022) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao
    Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022. A survey for in-context learning.
    *arXiv preprint arXiv:2301.00234*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Du and Cardie (2020a) Xinya Du and Claire Cardie. 2020a. [Document-level event
    role filler extraction using multi-granularity contextualized encoding](https://doi.org/10.18653/v1/2020.acl-main.714).
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pages 8010–8020, Online. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Du and Cardie (2020b) Xinya Du and Claire Cardie. 2020b. [Event extraction by
    answering (almost) natural questions](https://doi.org/10.18653/v1/2020.emnlp-main.49).
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*, pages 671–683, Online. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ebner et al. (2020) Seth Ebner, Patrick Xia, Ryan Culkin, Kyle Rawlins, and
    Benjamin Van Durme. 2020. [Multi-sentence argument linking](https://doi.org/10.18653/v1/2020.acl-main.718).
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pages 8057–8077, Online. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fu et al. (2022) Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar
    Khot. 2022. Complexity-based prompting for multi-step reasoning. *arXiv preprint
    arXiv:2210.00720*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gentner and Forbus (2011) Dedre Gentner and Kenneth D Forbus. 2011. Computational
    models of analogy. *Wiley interdisciplinary reviews: cognitive science*, 2(3):266–276.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gentner and Markman (1997) Dedre Gentner and Arthur B Markman. 1997. Structure
    mapping in analogy and similarity. *American psychologist*, 52(1):45.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gentner and Smith (2013) Dedre Gentner and Linsey A Smith. 2013. Analogical
    learning and reasoning. In *The Oxford Handbook of Cognitive Psychology*. Oxford
    University Press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geva et al. (2021) Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth,
    and Jonathan Berant. 2021. Did aristotle use a laptop? a question answering benchmark
    with implicit reasoning strategies. *Transactions of the Association for Computational
    Linguistics*, 9:346–361.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gonen et al. (2022) Hila Gonen, Srini Iyer, Terra Blevins, Noah A Smith, and
    Luke Zettlemoyer. 2022. Demystifying prompts in language models via perplexity
    estimation. *arXiv preprint arXiv:2212.04037*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grishman (2019) Ralph Grishman. 2019. Twenty-five years of information extraction.
    *Natural Language Engineering*, 25(6):677–692.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2018) Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James
    Wexler, Fernanda Viegas, et al. 2018. Interpretability beyond feature attribution:
    Quantitative testing with concept activation vectors (tcav). In *International
    conference on machine learning*, pages 2668–2677\. PMLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2021) Sha Li, Heng Ji, and Jiawei Han. 2021. [Document-level event
    argument extraction by conditional generation](https://doi.org/10.18653/v1/2021.naacl-main.69).
    In *Proceedings of the 2021 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 894–908, Online.
    Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2022) Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence
    Carin, and Weizhu Chen. 2022. [What makes good in-context examples for GPT-3?](https://doi.org/10.18653/v1/2022.deelio-1.10)
    In *Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on
    Knowledge Extraction and Integration for Deep Learning Architectures*, pages 100–114,
    Dublin, Ireland and Online. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023a) Jian Liu, Chen Liang, Jinan Xu, Haoyan Liu, and Zhe Zhao.
    2023a. [Document-level event argument extraction with a chain reasoning paradigm](https://doi.org/10.18653/v1/2023.acl-long.532).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 9570–9583, Toronto, Canada. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023b) Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki
    Hayashi, and Graham Neubig. 2023b. Pre-train, prompt, and predict: A systematic
    survey of prompting methods in natural language processing. *ACM Computing Surveys*,
    55(9):1–35.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. (2022) Yubo Ma, Zehao Wang, Yixin Cao, Mukai Li, Meiqi Chen, Kun
    Wang, and Jing Shao. 2022. [Prompt for extraction? PAIE: Prompting argument interaction
    for event argument extraction](https://doi.org/10.18653/v1/2022.acl-long.466).
    In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 6759–6774, Dublin, Ireland. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Min et al. (2022) Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis,
    Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. [Rethinking the role of demonstrations:
    What makes in-context learning work?](https://aclanthology.org/2022.emnlp-main.759)
    In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language
    Processing*, pages 11048–11064, Abu Dhabi, United Arab Emirates. Association for
    Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Najafabadi et al. (2015) Maryam M Najafabadi, Flavio Villanustre, Taghi M Khoshgoftaar,
    Naeem Seliya, Randall Wald, and Edin Muharemagic. 2015. Deep learning applications
    and challenges in big data analytics. *Journal of big data*, 2(1):1–21.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. 2023. Gpt-4 technical report. *ArXiv*, abs/2303.08774.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. 2022. Training language models to follow instructions with human feedback.
    *Advances in Neural Information Processing Systems*, 35:27730–27744.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pitis et al. (2023) Silviu Pitis, Michael R Zhang, Andrew Wang, and Jimmy Ba.
    2023. Boosted prompt ensembles for large language models. *arXiv preprint arXiv:2304.05970*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pouran Ben Veyseh et al. (2022) Amir Pouran Ben Veyseh, Minh Van Nguyen, Franck
    Dernoncourt, Bonan Min, and Thien Nguyen. 2022. [Document-level event argument
    extraction via optimal transport](https://doi.org/10.18653/v1/2022.findings-acl.130).
    In *Findings of the Association for Computational Linguistics: ACL 2022*, pages
    1648–1658, Dublin, Ireland. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ross (1987) Brian H Ross. 1987. This is like that: The use of earlier problems
    and the separation of similarity effects. *Journal of Experimental Psychology:
    Learning, Memory, and Cognition*, 13(4):629.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shachaf et al. (2021) Gal Shachaf, Alon Brutzkus, and Amir Globerson. 2021.
    A theoretical analysis of fine-tuning with linear teachers. *Advances in Neural
    Information Processing Systems*, 34:15382–15394.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shum et al. (2023) KaShun Shum, Shizhe Diao, and Tong Zhang. 2023. Automatic
    prompt augmentation and selection with chain-of-thought from labeled data. *arXiv
    preprint arXiv:2302.12822*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Socher et al. (2013) Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,
    Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. [Recursive deep
    models for semantic compositionality over a sentiment treebank](https://aclanthology.org/D13-1170).
    In *Proceedings of the 2013 Conference on Empirical Methods in Natural Language
    Processing*, pages 1631–1642, Seattle, Washington, USA. Association for Computational
    Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sorensen et al. (2022) Taylor Sorensen, Joshua Robinson, Christopher Rytting,
    Alexander Shaw, Kyle Rogers, Alexia Delorey, Mahmoud Khalil, Nancy Fulda, and
    David Wingate. 2022. [An information-theoretic approach to prompt engineering
    without ground truth labels](https://doi.org/10.18653/v1/2022.acl-long.60). In
    *Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics
    (Volume 1: Long Papers)*, pages 819–862, Dublin, Ireland. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2023) Jiashuo Sun, Yi Luo, Yeyun Gong, Chen Lin, Yelong Shen, Jian
    Guo, and Nan Duan. 2023. Enhancing chain-of-thoughts prompting with iterative
    bootstrapping in large language models. *arXiv preprint arXiv:2304.11657*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tong et al. (2022) MeiHan Tong, Bin Xu, Shuai Wang, Meihuan Han, Yixin Cao,
    Jiangqi Zhu, Siyu Chen, Lei Hou, and Juanzi Li. 2022. [DocEE: A large-scale and
    fine-grained benchmark for document-level event extraction](https://doi.org/10.18653/v1/2022.naacl-main.291).
    In *Proceedings of the 2022 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 3970–3982,
    Seattle, United States. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022a) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022a. Self-consistency improves
    chain of thought reasoning in language models. *arXiv preprint arXiv:2203.11171*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022b) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    and Denny Zhou. 2022b. Rationale-augmented ensembles in language models. *arXiv
    preprint arXiv:2207.00747*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi,
    Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in
    large language models. *arXiv preprint arXiv:2201.11903*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2021) Runxin Xu, Tianyu Liu, Lei Li, and Baobao Chang. 2021. [Document-level
    event extraction via heterogeneous graph-based interaction model with a tracker](https://doi.org/10.18653/v1/2021.acl-long.274).
    In *Proceedings of the 59th Annual Meeting of the Association for Computational
    Linguistics and the 11th International Joint Conference on Natural Language Processing
    (Volume 1: Long Papers)*, pages 3533–3546, Online. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2022) Runxin Xu, Peiyi Wang, Tianyu Liu, Shuang Zeng, Baobao Chang,
    and Zhifang Sui. 2022. [A two-stream AMR-enhanced model for document-level event
    argument extraction](https://doi.org/10.18653/v1/2022.naacl-main.370). In *Proceedings
    of the 2022 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies*, pages 5025–5036, Seattle, United States.
    Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2023) Xianjun Yang, Yujie Lu, and Linda Petzold. 2023. [Few-shot
    document-level event argument extraction](https://doi.org/10.18653/v1/2023.acl-long.446).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 8029–8046, Toronto, Canada. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2022) Yiming Zhang, Shi Feng, and Chenhao Tan. 2022. [Active example
    selection for in-context learning](https://aclanthology.org/2022.emnlp-main.622).
    In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language
    Processing*, pages 9134–9148, Abu Dhabi, United Arab Emirates. Association for
    Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2023) Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2023.
    Automatic chain of thought prompting in large language models. In *The Eleventh
    International Conference on Learning Representations (ICLR 2023)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2022) Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan
    Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. 2022.
    Least-to-most prompting enables complex reasoning in large language models. *arXiv
    preprint arXiv:2205.10625*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou and Mao (2022) Hanzhang Zhou and Kezhi Mao. 2022. [Document-level event
    argument extraction by leveraging redundant information and closed boundary loss](https://doi.org/10.18653/v1/2022.naacl-main.222).
    In *Proceedings of the 2022 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 3041–3052,
    Seattle, United States. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou and Srikumar (2022) Yichu Zhou and Vivek Srikumar. 2022. [A closer look
    at how fine-tuning changes BERT](https://doi.org/10.18653/v1/2022.acl-long.75).
    In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 1046–1061, Dublin, Ireland. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Experimental Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Dataset | Task Type | # Example | # Eval. | Eval. Split |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| RAMS (Ebner et al., [2020](#bib.bib9)) | Doc-Level EAE | 1 | 871 | Test |'
  prefs: []
  type: TYPE_TB
- en: '| DocEE (Tong et al., [2022](#bib.bib35)) | Doc-Level EAE | 1 | 800 | Test
    |'
  prefs: []
  type: TYPE_TB
- en: '| SST-2 (Socher et al., [2013](#bib.bib32)) | Sentiment Analysis | 2 | 872
    | Validation |'
  prefs: []
  type: TYPE_TB
- en: '| SNLI (Bowman et al., [2015](#bib.bib3)) | Natural Language Inference | 3
    | 500 | Test |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: The overall statistics of the dataset. # Example: The number of examples
    used in the HD-LoA prompting. # EVAL.: the number of samples used for evaluation
    of different prompting methods. EVAL. Split: evaluation split.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The statistics of the dataset are provided in Table [5](#A1.T5 "Table 5 ‣ Appendix
    A Experimental Details ‣ Heuristics-Driven Link-of-Analogy Prompting: Enhancing
    Large Language Models for Document-Level Event Argument Extraction"). We use the
    test split of RAMS dataset and the validation split of SST-2 for evaluation, following
    the setting in Wang et al. ([2022b](#bib.bib37)). Considering the extensive size
    of the DocEE and SNLI datasets, which makes a full-scale evaluation using LLMs
    impractical, we follow Shum et al. ([2023](#bib.bib31)); Wang et al. ([2022b](#bib.bib37))
    and evaluate a subset of these datasets. Owing to the substantial costs associated
    with deploying GPT-4, we restrict its evaluation on the RAMS dataset to 100 samples.
    In addition, regarding the DocEE dataset, it presents two distinct settings. In
    the conventional configuration, the training and testing data share an identical
    distribution. Conversely, the cross-domain setup features training and testing
    data composed of non-overlapping event types. Furthermore, our heuristic-driven
    demonstration construction method necessitates far fewer examples than traditional
    prompting methods, only keeping the minimum number of examples to avoid bias in
    example answers. Specifically, for the EAE task, we use only one example, and
    for sentiment analysis and natural language inference tasks, two and three examples
    are employed respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: We evaluate our prompting method on text-davinci-003, gpt-3.5-turbo-0301 and
    GPT-4 (OpenAI, [2023](#bib.bib25)). We employ these LLM models from the OpenAI
    API. During the all experiments, the temperature is fixed as $0$. In the evaluation
    of document-level EAE datasets, we omit articles from both the ground truth and
    the prediction during the assessment to align the more closely with the real content
    of event arguments.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Recognize Implicit Heuristics of In-Context Examples by GPT-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The prompt we use to recognize implicit heuristics inherent in examples and
    the output of GPT-4 is given below.
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt Input |'
  prefs: []
  type: TYPE_TB
- en: '| [⬇](data:text/plain;base64,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)
    What  is  the  most  critical  and  profound  heuristic  at  play  in  each  of  the  following  examples?  If  any  two  examples  share  the  same  heuristic,  please  indicate  this  connection.  Use  no  more  than  two  sentences  to  illustrate  the  heuristics  of  each  example.
    example  1 Q:  Did  the  death  of  Helen  Palmer  have  a  significant  effect  on  Dr.  Seuss?
    A:  Helen  Palmer  was  Dr.  Seuss’s  wife.  The  death  of  a  spouse  can  have  a  significant  effect  on  a  person.  Thus,  the  death  of  Helen  Palmer  could  have  a  significant  effect  on  Dr.  Seuss.  So  the  answer  is  yes.
    example  2 Q:  Would  a  moose  hypothetically  be  too  much  for  a  minotaur  to  devour  whole?
    A:  A  moose  is  about  1,000  pounds.  A  minotaur  is  about  1,000  pounds.  Thus,  a  moose  would  be  too  much  for  a  minotaur  to  devour  whole.  So  the  answer  is  yes.
    example  3 Q:  Yes  or  no:  Is  H’s  most  common  two  letter  pair  partner  a  freebie  in  Wheel  of  Fortune  bonus  round?
    A:  The  most  common  two  letter  pair  partner  for  H  is  E.  E  is  a  freebie  in  the  Wheel  of  Fortune  bonus  round.  Thus,  H’s  most  common  two  letter  pair  partner  is  a  freebie  in  the  Wheel  of  Fortune  bonus  round.  So  the  answer  is  yes.
    example  4 Q:  Yes  or  no:  Would  bald  eagle  deliver  an  urgent  message  before  B-52?
    A:  The  top  speed  of  a  bald  eagle  is  about  80  mph.  The  top  speed  of  a  B-52  is  about  650  mph.  Thus,  a  B-52  would  be  faster  than  a  bald  eagle.  So  the  answer  is  no.
    example  5 Q:  Yes  or  no:  Does  an  individual  oceanographer  study  many  sciences?
    A:  Oceanography  is  the  study  of  the  ocean.  The  ocean  is  a  complex  system.  Thus,  oceanography  is  a  multidisciplinary  science.  So  the  answer  is  yes.
    example  6 Q:  Yes  or  no:  Could  Steven  Spielberg  send  emails  as  a  child?
    A:  Steven  Spielberg  was  born  in  1946.  Email  was  invented  in  1971.  Thus,  Steven  Spielberg  could  not  send  emails  as  a  child.  So  the  answer  is  no.
    |'
  prefs: []
  type: TYPE_TB
- en: '| LLM Output |'
  prefs: []
  type: TYPE_TB
- en: '| [⬇](data:text/plain;base64,RXhhbXBsZSAxOiBUaGlzIGVtcGxveXMgdGhlICJlbXBhdGhldGljIHJlYXNvbmluZyIgaGV1cmlzdGljLCBhc3N1bWluZyB0aGF0IGJlY2F1c2UgbW9zdCBwZW9wbGUgd291bGQgYmUgc2lnbmlmaWNhbnRseSBhZmZlY3RlZCBieSB0aGUgZGVhdGggb2YgYSBzcG91c2UsIHNvIHRvbyB3b3VsZCBEci4gU2V1c3MuCgpFeGFtcGxlIDI6IFRoZSAiY29tcGFyaXNvbiBoZXVyaXN0aWMiIGlzIHVzZWQgaGVyZSwgY29tcGFyaW5nIHRoZSBzaXplIGFuZCB3ZWlnaHQgb2YgYSBtb29zZSB0byB0aGF0IG9mIGEgbWlub3RhdXIgdG8gbWFrZSBhIGNvbmNsdXNpb24gYWJvdXQgdGhlIGZlYXNpYmlsaXR5IG9mIGNvbnN1bXB0aW9uLgoKRXhhbXBsZSAzOiBUaGlzIHVzZXMgdGhlICJrbm93bGVkZ2UtYmFzZWQiIGhldXJpc3RpYywgdXNpbmcga25vd24gZmFjdHMgYWJvdXQgYSBnYW1lIHNob3cncyBydWxlcyB0byBhbnN3ZXIgdGhlIHF1ZXN0aW9uLgoKRXhhbXBsZSA0OiBUaGlzIGlzIGFub3RoZXIgaW5zdGFuY2Ugb2YgdGhlICJjb21wYXJpc29uIGhldXJpc3RpYywiIHRoaXMgdGltZSBjb21wYXJpbmcgdGhlIHRvcCBzcGVlZHMgb2YgYSBiYWxkIGVhZ2xlIGFuZCBhIEItNTIgdG8gZGV0ZXJtaW5lIHdoaWNoIHdvdWxkIGRlbGl2ZXIgYSBtZXNzYWdlIGZhc3Rlci4KCkV4YW1wbGUgNTogVGhlICJkZWZpbml0aW9uLWJhc2VkIiBoZXVyaXN0aWMgaXMgYXBwbGllZCBoZXJlLCByZWx5aW5nIG9uIHRoZSB1bmRlcnN0YW5kaW5nIG9mIHdoYXQgdGhlIGZpZWxkIG9mIG9jZWFub2dyYXBoeSBlbnRhaWxzIHRvIGNvbmNsdWRlIHRoYXQgaXQgaW52b2x2ZXMgdGhlIHN0dWR5IG9mIG11bHRpcGxlIHNjaWVuY2VzLgoKRXhhbXBsZSA2OiBUaGUgImNocm9ub2xvZ2ljYWwgaGV1cmlzdGljIiBpcyB1c2VkIGhlcmUsIGNvbXBhcmluZyB0aGUgdGltZWxpbmVzIG9mIFNwaWVsYmVyZydzIGNoaWxkaG9vZCBhbmQgdGhlIGludmVudGlvbiBvZiBlbWFpbCB0byBhbnN3ZXIgdGhlIHF1ZXN0aW9uLgoK)
    Example  1:  This  employs  the  "empathetic  reasoning"  heuristic,  assuming  that  because  most  people  would  be  significantly  affected  by  the  death  of  a  spouse,  so  too  would  Dr.  Seuss.
    Example  2:  The  "comparison  heuristic"  is  used  here,  comparing  the  size  and  weight  of  a  moose  to  that  of  a  minotaur  to  make  a  conclusion  about  the  feasibility  of  consumption.
    Example  3:  This  uses  the  "knowledge-based"  heuristic,  using  known  facts  about  a  game  show’s  rules  to  answer  the  question.
    Example  4:  This  is  another  instance  of  the  "comparison  heuristic,"  this  time  comparing  the  top  speeds  of  a  bald  eagle  and  a  B-52  to  determine  which  would  deliver  a  message  faster.
    Example  5:  The  "definition-based"  heuristic  is  applied  here,  relying  on  the  understanding  of  what  the  field  of  oceanography  entails  to  conclude  that  it  involves  the  study  of  multiple  sciences.
    Example  6:  The  "chronological  heuristic"  is  used  here,  comparing  the  timelines  of  Spielberg’s  childhood  and  the  invention  of  email  to  answer  the  question.
    |'
  prefs: []
  type: TYPE_TB
- en: Appendix C Full Prompts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: C.1 The Demonstration for SST-2 Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[⬇](data:text/plain;base64,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)Instruction:  You  are  to  perform  a  2-class  sentiment  analysis  on  the  given  sentences.  For  each  sentence,  determine  if  the  sentiment  is  either  "positive"  or  "negative".  Specifically,  you  will  leverage  the  pattern  in  the  pattern  list  below  to  recognize  the  sentiment  of  the  target  sentence.  For  each  sentence,  you  will  select  the  most  appropriate  one  or  two  patterns  to  identify  its  sentiment.  The  identification  of  a  sentence  based  on  different  patterns  should  not  be  interconnected.  Finally,  re-evaluate  the  identified  sentiment  to  get  the  final  answer.Pattern  list:[Overall  Context  Pattern:  Examine  the  general  tone  of  the  entire  sentence.  Determine  if  it  overall  express  a  positive  or  negative  sentiment,  regardless  of  individual  words.Descriptive  Phrases  pattern:  Identify  key  adjectives  or  descriptive  phrases  that  convey  strong  feelings  or  opinions  about  the  subject.Negation  Handling  Pattern:  Look  for  negative  words  like  "not,"  "didn’t,"  or  "never"  that  might  reverse  the  sentiment  of  the  words  following  them.  Ensure  the  reversed  sentiment  is  understood  correctly.Comparatives  Pattern:  Search  for  words  or  phrases  that  compare  the  subject  to  something  else,  suggesting  superiority,  inferiority,  or  equality.Adverbs  and  Intensifiers  Pattern:  Spot  adverbs  or  intensifiers  that  amplify  the  sentiment  of  the  associated  adjective  or  verb.  They  often  provide  a  clue  about  the  strength  of  the  sentiment.]Question:  What  is  the  sentiment  of  the  following  sentence?Sentence:  "more  than  another  "  best  man  "  clone  by  weaving  a  theme  throughout  this  funny  film"Answer:Step  1:  Selected  Patterns:  Overall  Context  Pattern  and  Descriptive  Phrases  Pattern  (Phrases:  "weaving  a  theme"  and  "funny  film").Step  2:  Apply  selected  patterns  to  identify  the  sentiment  of  the  sentence  independently.Step  2.1:  Based  on  Overall  Context  pattern:  The  entirety  of  the  sentence  portrays  a  film  that  is  superior  and  offers  added  value,  emphasizing  its  distinctiveness  and  humorous  quality.  The  overall  sentiment  is  positive.Step  2.2:  Based  on  Descriptive  Phrases  pattern:  The  descriptive  phrases  highlight  a  positive  sentiment  in  the  film  being  more  than  just  a  clone  and  having  a  funny  theme.  The  sentiment  is  positive.Step  3:  Re-evaluate  sentiment:sentiment:  positiveQuestion:  What  is  the  sentiment  of  the  following  sentence?Sentence:  "contains  no  wit  ,  only  labored  gags"Answer:Step  1:  Selected  Patterns:  Overall  Context  Pattern  and  Negation  Handling  Pattern  (Phrase:  "contains  no").Step  2:  Apply  selected  patterns  to  identify  the  sentiment  of  the  sentence  independently.Step  2.1:  Based  on  Overall  Context  pattern:  The  entire  sentence  conveys  a  lack  of  genuine  humor  and  wit,  and  instead  presents  the  humor  as  contrived  or  forced.  The  overall  sentiment  is  negative.Step  2.2:  Based  on  Negation  Handling  Pattern:  The  negation  "contains  no"  highlights  a  lack  of  wit.  It  is  further  emphasized  by  "labored  gags",  suggesting  forced  or  contrived  humor.  Thus,  the  sentiment  is  negative  regarding  the  quality  or  genuineness  of  the  humor.Step  3:  Re-evaluate  sentiment:sentiment:  negative'
  prefs: []
  type: TYPE_NORMAL
- en: C.2 The Demonstration for SNLI Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[⬇](data:text/plain;base64,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)Please  solve  the  natural  language  inference  task.  Specifically,  given  a  premise  and  a  hypothesis,  determine  the  validity  of  the  hypothesis  based  on  the  premise:Yes:  The  hypothesis  is  logically  derived  or  directly  follows  from  the  premise.No:  The  premise  provides  evidence  that  refutes  the  hypothesis.It  is  impossible  to  tell:  The  premise  does  not  provide  sufficient  information  to  confirm  or  refute  the  hypothesis.You  will  select  the  most  appropriate  pattern  in  the  pattern  list  below  to  classify  the  natural  language  inference  task.  For  each  sentence,  you  will  use  the  selected  patterns  to  identify  the  relationship  between  the  premise  and  the  hypothesis.Pattern  list:[Explicit  Evidence  Pattern:  When  the  hypothesis  directly  restates  or  paraphrases  information  present  in  the  premise,  i.e.,  premise  provides  direct  evidence  that  supports  the  hypothesis,  the  answer  is  "yes".Explicit  Contradiction  Pattern:  The  hypothesis  contains  information  that  directly  negates  or  opposes  a  clear  statement  in  the  premise.  If  this  condition  is  met,  the  answer  is  "no".Confident  Neutral  Pattern:  If  it  is  very  certain  that  the  hypothesis  neither  contradicts  nor  supports  the  premise  in  any  evident  or  implicit  manner,  and  the  relationship  between  them  is  clearly  independent,  the  answer  is  "it  is  not  possible  to  tell".Implicit  Contradiction  or  Neutral  Pattern:  In  this  case,  no  direct  contradiction  is  found.  If  the  hypothesis,  when  extended  logically,  negates  or  conflicts  with  any  part  of  the  premise,  even  if  not  directly.  If  it  does,  it  leans  towards  contradiction  (’no).  If  no  such  implicit  contradiction  is  found  and  the  relationship  between  hypothesis  and  premise  is  remains  ambiguous,  it  could  be  neutral  (it  is  not  possible  to  tell).Implicit  Evidence  or  Neutral  Pattern:  In  cases  where  no  direct  evidence  in  the  premise  supports  the  hypothesis,  the  following  steps  should  be  applied:  Check  each  element  of  the  hypothesis  against  the  premise.  If  each  element  of  the  hypothesis,  when  drawing  from  the  premise  using  world  knowledge  or  logical  reasoning,  one  can  infer  or  reasonably  support  the  entire  hypothesis,  it  leans  towards  implicit  entailment  (’yes’).  If  any  part  of  the  hypothesis  lacks  inferable  evidence  from  the  premise  or  if  the  connection  between  the  entire  hypothesis  and  premise  remains  ambiguous,  it  leans  towards  neutral  (’it  is  not  possible  to  tell’).Implicit  Contradiction  or  Neutral  Pattern:  In  cases  where  no  direct  evidence  in  the  premise  negates  the  hypothesis,  the  following  steps  should  be  applied:  Check  each  element  of  the  hypothesis  against  the  premise.  If  any  element  of  the  hypothesis,  when  juxtaposed  with  the  premise  and  utilizing  world  knowledge  or  logical  reasoning,  can  subtly  negate  or  contradict  any  part  of  the  premise,  it  leans  towards  implicit  contradiction  (’no’).  If  each  element  of  the  hypothesis  does  not  provide  such  subtle  contradictions  or  if  the  connection  between  the  entire  hypothesis  and  premise  remains  ambiguous,  it  leans  towards  neutral  (’it  is  not  possible  to  tell’).]Premise:  "Children  smiling  and  waving  at  camera."Based  on  this  premise,  can  we  conclude  the  hypothesis  "There  are  children  present."  is  true?OPTIONS:-  yes-  no-  it  is  not  possible  to  tellAnswer:Step  1:  Select  the  most  appropriate  pattern:  Explicit  Evidence  PatternStep  2:  Using  the  Explicit  Evidence  Pattern,  the  premise  directly  states  that  there  are  children  who  are  smiling  and  waving  at  the  camera.  This  information  supports  the  hypothesis,  which  claims  that  there  are  children  present.  So,  this  heuristic  suggest  a  "yes"  answer.Therefore,  the  answer  is:  yesPremise:  "A  couple  play  in  the  tide  with  their  young  son."Based  on  this  premise,can  we  conclude  the  hypothesis  "The  family  is  sitting  down  for  dinner."  is  true?OPTIONS:-  yes-  no-  it  is  not  possible  to  tellAnswer:Step  1:  Select  the  most  appropriate  pattern:  Implicit  Contradiction  or  Neutral  PatternStep  2:  Using  Implicit  Contradiction  or  Neutral  Pattern,  although  there  is  no  contradiction  in  the  premise  describes  a  scene  where  a  couple  and  their  son  are  playing  in  the  tide,  which  suggests  they  are  outdoors  and  likely  at  a  beach  or  coastal  setting.  The  hypothesis,  on  the  other  hand,  presents  a  scenario  where  the  family  is  sitting  down  for  dinner,  typically  an  indoor  activity.  These  two  activities  -  playing  in  the  tide  and  sitting  down  for  dinner  are  mutually  exclusive  in  the  context  of  a  single  time  frame.  Because  some  element  of  the  hypothesis  is  identified  implicitly  contradicts  the  premise.  This  heuristic  leans  towards  the  answer  "no".Therefore,  the  answer  is:  noPremise:  "A  young  family  enjoys  feeling  ocean  waves  lap  at  their  feet."Based  on  this  premise,  can  we  conclude  the  hypothesis  "A  young  man  and  woman  take  their  child  to  the  beach  for  the  first  time."  is  true?OPTIONS:-  yes-  no-  it  is  not  possible  to  tellAnswer:Step  1:  Select  the  most  appropriate  pattern:  Implicit  Evidence  or  Neutral  PatternStep  2:  Using  Implicit  Evidence  or  Neutral  Pattern,  "a  young  family"  typically  implies  the  presence  of  a  younger  couple  and  potentially  their  child,  so  this  part  can  be  inferred.  However,  the  statement  about  it  being  the  child’s  "first  time"  at  the  beach  cannot  be  inferred  or  supported  from  the  premise.  Because  not  all  elements  of  the  hypothesis  can  be  inferred  or  supported  from  the  premise,  this  heuristic  leans  towards  "it  is  not  possible  to  tell".Therefore,  the  answer  is:  it  is  not  possible  to  tell.'
  prefs: []
  type: TYPE_NORMAL
- en: C.3 The Demonstration for RAMS Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[⬇](data:text/plain;base64,Instruction: Your task is Event Argument Extraction. In this task, you will be provided with a document that describes an event. Your job is to identify and extract the key components (event arguments) that correspond to each argument role associated with the event. The trigger word, which is located between special tokens "<t>" and "<\t>" in the document, serves to indicate the presence of the event. Only the event explicitly linked to the trigger word should be considered.
Specifically, you will use the pattern in the pattern list below to identify event arguments, and re-evaluate the identified argument candidates to get the final answer.

pattern list:
[
Semantic Pattern: [giver] is the person, group, or organization in the document that gives the grant or gift. For example "[John] is the donor".
Syntactic.SubjectVerbObject Pattern: [giver] may be the subject of a verb that indicates the act of giving, or granting. For example, "[John] gave Mary a book"
Syntactic.PassiveVoiceSyntax Pattern: In sentences using passive voice, the [giver] might be introduced by prepositions such as 'by' to describe the act of giving or granting. For example, "The book was given to Mary by [John]".
Syntactic.PrepositionalPhrases Pattern: [giver] may be indicated by prepositions such as 'from', and 'of', which typically precede the giver in the context of a giving or granting event. For example, "She received a gift from [John]".
Syntactic.PossessiveNoun Pattern: [giver] may be the possessor in noun phrases that describe the object being given or granted. For example, "[John]'s donation".
Multi-Sentence Pattern: the giver is not explicitly understood from only one sentence, but it can be understood by combining the information of multiple sentences. For example, consider "The charity received a generous donation. This was from an [anonymous man]." The 'anonymous man' as the giver is not explicit in the second sentence alone. However, if we link 'this' in the second sentence to the 'donation' in the first, we understand that the 'anonymous man' is the giver of the donation.
]


Example task:
Question: What are the event arguments of giver, beneficiary, and recipient in the "transaction.transaction.giftgrantprovideaid" event in the provided news document? The trigger word is "granted" located between "<t>" and "</t>". If an event argument is explicitly stated in the document, ensure that the identified argument is quoted exactly as the entity appears in the given document. Note, if an event argument is not explicitly indicated in the document, or is not directly linked to the event signaled by the trigger word, use "not specified" as the answer.'

Document: a news document

trigger sentence: "The access to the research center in the city was <t>granted</t> by the administrator. The man, Ripley Johnson, earned it."

Answer:
Elaborate the meaning of event type and its argument roles:
"transaction.transaction.giftgrantprovideaid": The event involves a transfer of money or resources in the form of a gift, grant, or provision of aid, signaled by the action of granting.
[giver]: the giver is the person, group, or organization that provides or grants money, resources, or access in the event.
[beneficiary]: the beneficiary is the party who ultimately benefits from the transaction.
[recipient]: the recipient is the entity that receives the money, resources, or access granted in the event.

Recognizing [giver] in the given document:
Step 1 Select one or two patterns in the pattern list that are best suited to identify the [giver] in the news document: Semantic Pattern and Syntactic.PassiveVoiceSyntax Pattern because "was granted" indicates Syntactic.PassiveVoiceSyntax.
Step 2: Apply selected patterns to identify [giver] independently. Each pattern should be applied in isolation, without considering the other pattern.
Step 2.1 Identify the [giver] based on Semantic Pattern f1: "[giver] is the person, group, or organization that gives the grant or gift in the document". Applying pattern f1 to the document, "administrator" is the entity that gives access of the research center.
Step 2.2 Identify the [giver] based on Syntactic.PassiveVoiceSyntax Pattern f2: "In sentences using passive voice, the [giver] might be the object of a verb describe the act of giving or granting." Applying pattern f2 to the document, "research center" is the object of "granted" which describes the act of giving or granting.
Step 3: reevaluate_argument_candidates(["administrator", "research center"]):
Is argument "administrator" alignment with the argument role [giver]? yes, because "administrator" granted access to the research center.
Is argument "research center" alignment with the argument role [giver]? no, because "research center" is the place that access has been granted to, but it doesn't give access.
[giver]: "administrator"

Recognizing [beneficiary] in the given document:
Step 1 Select one or two patterns in the pattern list that are best suited to identify the [beneficiary] in the news document: Semantic Pattern
Step 2: Apply selected patterns to identify [beneficiary] independently. Each pattern should be applied in isolation, without considering the other pattern.
Step 2.1 Identify the [beneficiary] based on semantic pattern f1: "[beneficiary] is the entity that ultimately benefits from the gift or grant". Applying pattern f1 to the given document, the entity that ultimately benefits from the grant is "not specified".
Step 3: reevaluate_argument_candidates("not specified"):
Is argument "not specified" alignment with argument role [beneficiary]? Yes, the [beneficiary] is not explicitly mentioned so "not specified" is correct.
[beneficiary]: "not specified"

Recognizing [recipient] in the given document:
Step 1 Select one or two patterns in the pattern list that are best suited to identify the [recipient] in the news document: Semantic Pattern and Multi-Sentence Pattern because recognizing [recipient] requires an understanding of "it" in "earned it" across sentences
Step 2: Apply selected patterns to identify [recipient] independently. Each pattern should be applied in isolation, without considering the other pattern.
Step 2.1 Identify the [recipient] based on Semantic Pattern f1: "[recipient] is the entity that receives the gift or grant". Applying pattern f1 to the given document, the entity that receives the gift or grant is "Ripley Johnson".
Step 2.2 Identify the [recipient] based on Multi-Sentence Pattern f2: in the phrase "The man, Ripley Johnson, earned it", "it" refers to the access. Therefore, "The man" earned the granted access. [recipient] is "The man".
Step 3: reevaluate_argument_candidates(["Ripley Johnson", "The man"]):
Is argument "Ripley Johnson" alignment with the argument role [recipient]? yes, because "Ripley Johnson" earned the access.
Is argument "Ripley Johnson" alignment with the argument role [recipient]? yes, because "The man" earned the access.
"The man" is preferred because it is closer to the trigger word.
[recipient]: "The man"


Target task:)Instruction:  Your  task  is  Event  Argument  Extraction.  In  this  task,  you  will  be  provided  with  a  document  that  describes  an  event.  Your  job  is  to  identify  and  extract  the  key  components  (event  arguments)  that  correspond  to  each  argument  role  associated  with  the  event.  The  trigger  word,  which  is  located  between  special  tokens  "<t>"  and  "<\t>"  in  the  document,  serves  to  indicate  the  presence  of  the  event.  Only  the  event  explicitly  linked  to  the  trigger  word  should  be  considered.Specifically,  you  will  use  the  pattern  in  the  pattern  list  below  to  identify  event  arguments,  and  re-evaluate  the  identified  argument  candidates  to  get  the  final  answer.pattern  list:[Semantic  Pattern:  [giver]  is  the  person,  group,  or  organization  in  the  document  that  gives  the  grant  or  gift.  For  example  "[John]  is  the  donor".Syntactic.SubjectVerbObject  Pattern:  [giver]  may  be  the  subject  of  a  verb  that  indicates  the  act  of  giving,  or  granting.  For  example,  "[John]  gave  Mary  a  book"Syntactic.PassiveVoiceSyntax  Pattern:  In  sentences  using  passive  voice,  the  [giver]  might  be  introduced  by  prepositions  such  as  ’by’  to  describe  the  act  of  giving  or  granting.  For  example,  "The  book  was  given  to  Mary  by  [John]".Syntactic.PrepositionalPhrases  Pattern:  [giver]  may  be  indicated  by  prepositions  such  as  ’from’,  and  ’of’,  which  typically  precede  the  giver  in  the  context  of  a  giving  or  granting  event.  For  example,  "She  received  a  gift  from  [John]".Syntactic.PossessiveNoun  Pattern:  [giver]  may  be  the  possessor  in  noun  phrases  that  describe  the  object  being  given  or  granted.  For  example,  "[John]’s  donation".Multi-Sentence  Pattern:  the  giver  is  not  explicitly  understood  from  only  one  sentence,  but  it  can  be  understood  by  combining  the  information  of  multiple  sentences.  For  example,  consider  "The  charity  received  a  generous  donation.  This  was  from  an  [anonymous  man]."  The  ’anonymous  man’  as  the  giver  is  not  explicit  in  the  second  sentence  alone.  However,  if  we  link  ’this’  in  the  second  sentence  to  the  ’donation’  in  the  first,  we  understand  that  the  ’anonymous  man’  is  the  giver  of  the  donation.]Example  task:Question:  What  are  the  event  arguments  of  giver,  beneficiary,  and  recipient  in  the  "transaction.transaction.giftgrantprovideaid"  event  in  the  provided  news  document?  The  trigger  word  is  "granted"  located  between  "<t>"  and  "</t>".  If  an  event  argument  is  explicitly  stated  in  the  document,  ensure  that  the  identified  argument  is  quoted  exactly  as  the  entity  appears  in  the  given  document.  Note,  if  an  event  argument  is  not  explicitly  indicated  in  the  document,  or  is  not  directly  linked  to  the  event  signaled  by  the  trigger  word,  use  "not  specified"  as  the  answer.’Document:  a  news  documenttrigger  sentence:  "The  access  to  the  research  center  in  the  city  was  <t>granted</t>  by  the  administrator.  The  man,  Ripley  Johnson,  earned  it."Answer:Elaborate  the  meaning  of  event  type  and  its  argument  roles:"transaction.transaction.giftgrantprovideaid":  The  event  involves  a  transfer  of  money  or  resources  in  the  form  of  a  gift,  grant,  or  provision  of  aid,  signaled  by  the  action  of  granting.[giver]:  the  giver  is  the  person,  group,  or  organization  that  provides  or  grants  money,  resources,  or  access  in  the  event.[beneficiary]:  the  beneficiary  is  the  party  who  ultimately  benefits  from  the  transaction.[recipient]:  the  recipient  is  the  entity  that  receives  the  money,  resources,  or  access  granted  in  the  event.Recognizing  [giver]  in  the  given  document:Step  1  Select  one  or  two  patterns  in  the  pattern  list  that  are  best  suited  to  identify  the  [giver]  in  the  news  document:  Semantic  Pattern  and  Syntactic.PassiveVoiceSyntax  Pattern  because  "was  granted"  indicates  Syntactic.PassiveVoiceSyntax.Step  2:  Apply  selected  patterns  to  identify  [giver]  independently.  Each  pattern  should  be  applied  in  isolation,  without  considering  the  other  pattern.Step  2.1  Identify  the  [giver]  based  on  Semantic  Pattern  f1:  "[giver]  is  the  person,  group,  or  organization  that  gives  the  grant  or  gift  in  the  document".  Applying  pattern  f1  to  the  document,  "administrator"  is  the  entity  that  gives  access  of  the  research  center.Step  2.2  Identify  the  [giver]  based  on  Syntactic.PassiveVoiceSyntax  Pattern  f2:  "In  sentences  using  passive  voice,  the  [giver]  might  be  the  object  of  a  verb  describe  the  act  of  giving  or  granting."  Applying  pattern  f2  to  the  document,  "research  center"  is  the  object  of  "granted"  which  describes  the  act  of  giving  or  granting.Step  3:  reevaluate_argument_candidates(["administrator",  "research  center"]):Is  argument  "administrator"  alignment  with  the  argument  role  [giver]?  yes,  because  "administrator"  granted  access  to  the  research  center.Is  argument  "research  center"  alignment  with  the  argument  role  [giver]?  no,  because  "research  center"  is  the  place  that  access  has  been  granted  to,  but  it  doesn’t  give  access.[giver]:  "administrator"Recognizing  [beneficiary]  in  the  given  document:Step  1  Select  one  or  two  patterns  in  the  pattern  list  that  are  best  suited  to  identify  the  [beneficiary]  in  the  news  document:  Semantic  PatternStep  2:  Apply  selected  patterns  to  identify  [beneficiary]  independently.  Each  pattern  should  be  applied  in  isolation,  without  considering  the  other  pattern.Step  2.1  Identify  the  [beneficiary]  based  on  semantic  pattern  f1:  "[beneficiary]  is  the  entity  that  ultimately  benefits  from  the  gift  or  grant".  Applying  pattern  f1  to  the  given  document,  the  entity  that  ultimately  benefits  from  the  grant  is  "not  specified".Step  3:  reevaluate_argument_candidates("not  specified"):Is  argument  "not  specified"  alignment  with  argument  role  [beneficiary]?  Yes,  the  [beneficiary]  is  not  explicitly  mentioned  so  "not  specified"  is  correct.[beneficiary]:  "not  specified"Recognizing  [recipient]  in  the  given  document:Step  1  Select  one  or  two  patterns  in  the  pattern  list  that  are  best  suited  to  identify  the  [recipient]  in  the  news  document:  Semantic  Pattern  and  Multi-Sentence  Pattern  because  recognizing  [recipient]  requires  an  understanding  of  "it"  in  "earned  it"  across  sentencesStep  2:  Apply  selected  patterns  to  identify  [recipient]  independently.  Each  pattern  should  be  applied  in  isolation,  without  considering  the  other  pattern.Step  2.1  Identify  the  [recipient]  based  on  Semantic  Pattern  f1:  "[recipient]  is  the  entity  that  receives  the  gift  or  grant".  Applying  pattern  f1  to  the  given  document,  the  entity  that  receives  the  gift  or  grant  is  "Ripley  Johnson".Step  2.2  Identify  the  [recipient]  based  on  Multi-Sentence  Pattern  f2:  in  the  phrase  "The  man,  Ripley  Johnson,  earned  it",  "it"  refers  to  the  access.  Therefore,  "The  man"  earned  the  granted  access.  [recipient]  is  "The  man".Step  3:  reevaluate_argument_candidates(["Ripley  Johnson",  "The  man"]):Is  argument  "Ripley  Johnson"  alignment  with  the  argument  role  [recipient]?  yes,  because  "Ripley  Johnson"  earned  the  access.Is  argument  "Ripley  Johnson"  alignment  with  the  argument  role  [recipient]?  yes,  because  "The  man"  earned  the  access."The  man"  is  preferred  because  it  is  closer  to  the  trigger  word.[recipient]:  "The  man"Target  task:'
  prefs: []
  type: TYPE_NORMAL
- en: C.4 The demonstration for DocEE Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[⬇](data:text/plain;base64,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)Instruction:  Your  task  is  Event  Argument  Extraction.  In  this  task,  you  will  be  provided  with  a  document  that  describes  an  event.  Your  job  is  to  extract  and  quote  the  entities  (event  arguments)  from  the  document  that  correspond  to  each  argument  role  associated  with  the  event.Specifically,  you  will  use  the  patterns  in  the  "Pattern  List"  below  to  identify  event  arguments.  The  answers  should  also  satisfy  the  "Argument  Format  Principle"  and  "Argument  Number  Principle"  below.Pattern  List:["Semantic  Pattern":  [giver]  is  the  person,  group,  or  organization  in  the  document  that  gives  the  grant  or  gift.  For  example,  "[John]  is  the  donor"."Syntactic.SubjectVerbObject  Pattern":  [giver]  may  be  the  subject  of  a  verb  that  indicates  the  act  of  giving,  or  granting.  For  example,  "[John]  gave  Mary  a  book"."Syntactic.PrepositionalPhrases  Pattern":  [giver]  may  be  indicated  by  prepositions  such  as  ’from’,  and  ’of’,  which  typically  precede  the  giver  in  the  context  of  a  giving  or  granting  event.  For  example,  "She  received  a  gift  from  [John]".]Argument  Extracting  Principles:["Argument  Format  Principle":  articles  and  prepositions  are  not  included  in  the  identified  event  argument.  For  example,  select  "damaged  car"  among  "damaged  car",  "damaged  car  belonging  to  the  victim"  and  "the  damaged  car";  select  "the  officer"  rather  than  "the  officer  under  investigation"."Argument  Number  Principle":  Most  event  arguments  only  have  one  unique  mention.  However,  only  in  situations  where  you  are  strongly  sure  that  the  mentions  associated  with  an  argument  role  are  distinctly  different,  you  are  permitted  to  extract  <=  3  mentions.]Example  Task:Question:  What  are  the  event  arguments  of  ’Date’,  ’Magnitude’,  ’Number  of  Destroyed  Building’,  ’Casualties  and  Losses’  in  the  ’Earthquakes’  event  in  the  provided  news  document  below?  Ensure  that  each  identified  argument  is  quoted  exactly  as  the  entity  appears  in  the  given  document.  If  an  event  argument  is  not  explicitly  mentioned,  please  answer  "not  specified".Document:  a  given  newsAnswer:Elaborate  the  meaning  of  event  type  and  its  argument  roles:’Earthquakes’:  The  event  involves  the  shaking  of  the  surface  of  the  Earth  resulting  from  a  sudden  release  of  energy  in  the  Earth’s  lithosphere.[Date]:  when  the  earthquake  occurred.[Magnitude]:  the  measure  of  the  size  of  the  earthquake.[Number  of  Destroyed  Building]:  the  number  of  buildings  or  structures  that  were  damaged  or  destroyed  due  to  the  earthquake.[Casualties  and  Losses]:  the  number  of  people  killed  or  injured,  and  the  amount  of  economic  losses  caused  by  the  earthquake.Recognizing  [Date]  in  the  given  document:Step  1  Select  a  pattern  in  the  pattern  list  that  are  best  suited  to  identify  the  [Date]  in  the  news  document:  Semantic  Pattern.Step  2.1  Identify  the  argument  based  on  Semantic  Pattern  f1:  "[Date]  is  when  the  earthquake  occurred".  The  [Date]  is  "not  specified"  because  the  document  didn’t  mention  when  the  earthquake  occurred.Step  3:  reevaluate_argument_candidates(["not  specified"]):Is  "not  specified"  alignment  with  the  argument  role  [Date]?  Yes,  because  [Date]  is  not  explicitly  mentioned  in  the  document,  so  "not  specified"  is  correct.[Date]:  "not  specified"Recognizing  [Magnitude]  in  the  given  document:Step  1  Select  a  pattern  in  the  pattern  list  that  are  best  suited  to  identify  the  [Magnitude]  in  the  news  document:  Semantic  Pattern.Step  2.1  Identify  the  argument  based  on  Semantic  Pattern  f1:  "[Magnitude]  is  the  measure  of  the  size  of  the  earthquake".  The  [Magnitude]  is  "6.6"  because  the  document  mentions  "The  magnitude  6.6  shock".Step  3:  reevaluate_argument_candidates(["6.6"]):Is  "6.6"  alignment  with  the  argument  role  [Magnitude]?  Yes,  because  it  quantifies  the  size  of  the  earthquake.[Magnitude]:  "6.6"Recognizing  [Number  of  Destroyed  Building]  in  the  given  document:Step  1  Select  a  pattern  in  the  pattern  list  that  are  best  suited  to  identify  the  [Number  of  Destroyed  Building]  in  the  news  document:  Semantic  PatternStep  2.1  Identify  the  argument  based  on  Semantic  Pattern  f1:  "[Number  of  Destroyed  Building]  is  the  people  killed,  injured,  or  properties  affected  by  the  drought.".  The  [Number  of  Destroyed  Building]  is  "more  than  80%  of  the  houses"  because  the  document  mentions  this  specific  proportion  related  to  houses  destroyed.Step  3:  reevaluate_argument_candidates(["more  than  80%  of  the  houses"]):Is  "more  than  80%  of  the  houses"  alignment  with  the  argument  role  [Number  of  Destroyed  Building]?  No,  because  it  describes  the  percentage  not  the  specific  number  of  buildings  destroyed.[Number  of  Destroyed  Building]:  "not  specified"Recognizing  [Casualties  and  Losses]  in  the  given  document:Step  1  Select  a  pattern  in  the  pattern  list  that  are  best  suited  to  identify  the  [Casualties  and  Losses]  in  the  news  document:  Semantic  PatternStep  2  Identify  the  argument  based  on  Semantic  Pattern  f1:  "[Casualties  and  Losses]  is  the  number  of  people  killed  or  injured,  and  the  amount  of  economic  losses  caused  by  the  earthquake".  The  [Casualties  and  Losses]  is  "claimed  142  deaths"  and  "800  houses  were  damaged"  because  the  document  provides  this  explicit  numbers  about  people  killed  due  to  the  earthquake  and  mentions  this  number  of  houses  destroyed,  representing  economic  losses.Step  3:  reevaluate_argument_candidates(["claimed  142  deaths",  "800  houses  were  damaged"]):Is  "claimed  142  deaths"  alignment  with  the  argument  role  [Casualties  and  Losses]?  Yes,  because  it  provides  details  about  the  number  of  people  killed  or  injured  due  to  the  earthquake.Is  "800  houses  were  damaged"  alignment  with  the  argument  role  [Casualties  and  Losses]?  Yes,  because  it  describes  the  extent  of  property  damage  caused  by  the  earthquake.[Casualties  and  Losses]:  "claimed  142  deaths",  "800  houses  were  damaged"End  of  answerTarget  task:'
  prefs: []
  type: TYPE_NORMAL
