- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:40:51'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'GANPrompt: Enhancing Robustness in LLM-Based Recommendations with GAN-Enhanced
    Diversity Prompts'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.09671](https://ar5iv.labs.arxiv.org/html/2408.09671)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Xinyu Li College of Management and Economics, Tianjin UniversityTianjinChina
    [lxyt7642@tju.edu.cn](mailto:lxyt7642@tju.edu.cn) ,  Chuang Zhao Department of
    Electronic and Computer Engineering, The Hong Kong University of Science and TechnologyHong
    KongChina [czhaobo@connect.ust.hk](mailto:czhaobo@connect.ust.hk) ,  Hongke Zhao
    College of Management and Economics, Tianjin UniversityTianjinChina [hongke@tju.edu.cn](mailto:hongke@tju.edu.cn)
    ,  Likang Wu College of Management and Economics, Tianjin UniversityTianjinChina
    [wulk@mail.ustc.edu.cn](mailto:wulk@mail.ustc.edu.cn)  and  Ming HE AI Lab at
    Lenovo ResearchBeijingChina [heming01@foxmail.com](mailto:heming01@foxmail.com)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In recent years, LLM has demonstrated remarkable proficiency in comprehending
    and generating natural language, with a growing prevalence in the domain of recommender
    systems. However, LLM continues to face a significant challenge in that it is
    highly susceptible to the influence of prompt words. This inconsistency in response
    to minor alterations in prompt input may compromise the accuracy and resilience
    of recommendation models. To address this issue, this paper proposes GANPrompt,
    a multi-dimensional large language model prompt diversity framework based on Generative
    Adversarial Networks (GANs). The framework enhances the model’s adaptability and
    stability to diverse prompts by integrating GAN generation techniques with the
    deep semantic understanding capabilities of LLMs. GANPrompt first trains a generator
    capable of producing diverse prompts by analysing multidimensional user behavioural
    data. These diverse prompts are then used to train the LLM to improve its performance
    in the face of unseen prompts. Furthermore, to ensure a high degree of diversity
    and relevance of the prompts, this study introduces a mathematical theory-based
    diversity constraint mechanism that optimises the generated prompts to ensure
    that they are not only superficially distinct, but also semantically cover a wide
    range of user intentions. Through extensive experiments on multiple datasets,
    we demonstrate the effectiveness of the proposed framework, especially in improving
    the adaptability and robustness of recommender systems in complex and dynamic
    environments. The experimental results demonstrate that GANPrompt yields substantial
    enhancements in accuracy and robustness relative to existing state-of-the-art
    methodologies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recommendation Systems, Large Language Model, Generating Adversarial Networks,
    Prompt Learning^†^†ccs: Information systems Recommender systems'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. INTRODUCTION
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since the advent of the Internet, recommender systems have become an indispensable
    component of the digital age. From their inception in the early days of the Internet,
    recommender systems have undergone a significant evolution, progressing from simple
    recommendations based on collaborative filtering to the complex systems of today,
    which incorporate machine learning, artificial intelligence and big data technologies.
    This development can be attributed to the mounting pressure on information overload
    and the relentless pursuit of a personalised user experience. From the initial
    deployment of recommender systems for product recommendations to the current integration
    of these systems into a multitude of domains, including music, video, social networks,
    and others, the impact of recommender systems on our lives has been profound.
    These systems have facilitated the delivery of more personalised and accurate
    services and experiences to users.
  prefs: []
  type: TYPE_NORMAL
- en: The rapid development of artificial intelligence has led to the emergence of
    large language models, which have the potential to revolutionise the field of
    recommender systems. These models, such as OpenAI’s GPT series and Google’s BERT,
    are trained on vast quantities of text data using deep learning techniques, enabling
    them to understand and generate natural language text with exceptional language
    understanding and generation capabilities. The advent of such models offers a
    novel avenue for recommender systems, enabling recommendations to transcend the
    limitations of simple behavioural patterns and instead leverage richer, more nuanced
    semantic information.
  prefs: []
  type: TYPE_NORMAL
- en: 'The application of language models in recommender systems is focused on two
    paradigms: pre-training and prompt-based fine-tuning. The pre-training paradigm
    relies on training large-scale language models (LLMs) on large-scale textual data
    to enable them to capture linguistic nuances, patterns, and structures. However,
    this approach tends to require huge data samples and expensive computational resources,
    which is an unaffordable requirement for most recommender system application scenarios.
    In contrast, the prompt-based fine-tuning paradigm fine-tunes pre-trained LLMs
    by constructing prompt datasets related to specific recommendation tasks. During
    this process, deep semantic understanding and accurate matching of users and items
    are achieved by adjusting model parameters. This approach addresses the discrepancy
    between the training task and the downstream goal by redefining the downstream
    task to fit the pre-trained framework. In comparison to pre-training language
    models from scratch, the prompt-based fine-tuning approach significantly reduces
    the data requirements and computational resources, thus enabling the rapid and
    efficient integration of advanced LLM techniques into existing recommender system
    architectures. This approach has become a mainstream research direction in both
    academic and industrial contexts.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d7ce4b20a47ee9e1df2f6955c29f5c6c.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1. The same user interaction history combined with different prompt templates
    gives different recommendation results in LLM. This is due to the fact that LLM
    is transiently sensitive to the diversity of prompt words, which can disrupt the
    model’s output results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The prompt-based approach directly adjusts the inputs of the LLM by constructing
    specific prompts (prompts) to align with specific recommendation tasks. The core
    of this approach lies in guiding the model to generate outputs that are closely
    related to the recommendation tasks through well-designed prompt statements. Although
    this approach facilitates the rapid deployment of recommender systems, it also
    suffers from over-sensitivity of the model to prompts, which may disturb the model
    outputs and thus affect the robustness of the model [1](#S1.F1 "Figure 1 ‣ 1\.
    INTRODUCTION ‣ GANPrompt: Enhancing Robustness in LLM-Based Recommendations with
    GAN-Enhanced Diversity Prompts"). The choice of prompt words has a direct impact
    on the recommendations generated by LLM, if the prompts are poorly designed or
    inconsistent with the context in which the model is trained, it may result in
    the model outputting recommendations that are highly biased, inaccurate, or even
    completely irrelevant. Even subtle differences in the prompts can significantly
    affect the model’s performance. Furthermore, an over-sensitivity to specific prompts
    may render the model susceptible to malicious manipulation. This can be achieved
    through the disruption of the recommendation results through adversarial prompts,
    which raises security and fairness issues.'
  prefs: []
  type: TYPE_NORMAL
- en: To address the above problems, we propose GANPrompt, a generative adversarial
    network (GAN)-based framework to generate diverse prompts for multidimensional
    LLMs, enhancing the stability and robustness of recommendation systems in complex
    environments. The framework combines the semantic understanding capability of
    large language models with the generative properties of generative adversarial
    networks to improve the adaptability and robustness of downstream recommendation
    tasks to diverse prompts. The framework initiates by training a diversity prompt
    generator, which collects user behaviour data from multidimensional data sources.
    The generator exploits the ability of GANs to create diverse and highly differentiated
    prompt samples, which helps to simulate complex and varied user queries in the
    real world. These generated diversity prompts are then used to train large language
    models, thereby improving the stability and accuracy of the models in the face
    of different, unseen prompts. Furthermore, to ensure that the generated prompts
    are both highly diverse and relevant, the framework introduces a diversity constraint
    mechanism based on mathematical theory. This mechanism adapts the GAN-generated
    prompts through an optimisation algorithm so that each prompt generated is not
    only superficially distinct, but also semantically encompasses a wide range of
    user intentions. This approach effectively extends and improves the model, ultimately
    leading to a more powerful and effective recommender system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, the contributions of this paper are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This paper realises the deep integration of LLM and GAN: by using the encoder
    of LLM as a generator and constructing the corresponding discriminator to implement
    the generative adversarial network, we innovatively use the semantic parsing ability
    of LLM and the generative characteristics of GAN to construct a multi-dimensional
    prompt generator. This combination not only enhances the diversity and quality
    of generated prompts, but also improves the adaptability and accuracy of the model
    in complex recommendation scenarios.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Proposal of multidimensional diversity constraint: In order to ensure that
    the generated prompts are not only diverse but also semantically relevant, we
    propose a mathematical theory-based diversity constraint mechanism. This mechanism
    can effectively control the prompt generation process to ensure that each generated
    prompt covers a wide range of specific user intentions.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Extensive experimental validation: we validate the effectiveness of the proposed
    framework through experiments conducted on multiple datasets. The experimental
    results show that GANPrompt outperforms existing state-of-the-art techniques in
    several recommendation tasks, especially in demonstrating greater robustness and
    higher accuracy when dealing with dynamic and diverse inputs.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2\. RELATED WORK
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, the related work involved includes the application of large language
    models in recommender systems, prompt learning based recommendation methods and
    generative adversarial networks.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1\. Large Language Modelling as Recommendatiom System
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) refer to Transformer language models containing
    hundreds of billions (or more) of parameters that are self-supervised trained
    on large amounts of textual data (Shanahan, [2024](#bib.bib21)), e.g., GPT-3(Brown
    et al., [2020](#bib.bib2)), PaLM(Chowdhery et al., [2023](#bib.bib3)), Galactica(Taylor
    et al., [2022](#bib.bib23)), and LLaMA(Touvron et al., [2023](#bib.bib24)). Due
    to its absorption of massive textual knowledge, LLM exhibits deep contextual semantic
    understanding and achieves impressive performance in solving complex task scenarios
    through text generation.
  prefs: []
  type: TYPE_NORMAL
- en: With the significant achievements of Large Language Models (LLMs) in the field
    of Natural Language Processing (NLP), their use in recommender systems is becoming
    more widespread (Li et al., [2023c](#bib.bib16); Wu et al., [2023](#bib.bib28);
    Liu et al., [2023](#bib.bib18)). Among them, recommendation data enhancement using
    LLM has been widely studied, while it is also widely used in tasks such as sequence
    advancement, dialogue recommendation, and direct recommendation and so on.
  prefs: []
  type: TYPE_NORMAL
- en: In recent studies, researchers have begun to explore the use of large-scale
    language models (LLMs) as a data enhancement tool to improve the performance of
    recommender systems (Dai et al., [2023](#bib.bib4); Li et al., [2022](#bib.bib12)).
    In particular, Liu et al. (Liu et al., [2024](#bib.bib17)) developed a method
    for generating multimodal language-image instruction tracking datasets using LLM.
    These data generated via LLM were used to optimise the model during training,
    significantly improving the model’s performance on visual and language comprehension
    tasks. Furthermore, some researchers have attempted to enhance the personalised
    inputs of recommender systems through LLM. For instance, Chen et al. (Yang et al.,
    [2023](#bib.bib31)) fed historical user behavioural data (e.g., clicks, purchases
    and ratings) into LLM to generate detailed user profiles. These profiles, in conjunction
    with historical interaction sequences and candidate item data, were employed to
    construct personalised recommendation prompts. These prompts were then utilised
    to predict potential interactions between users and items. In addition, Xi et
    al. (Xi et al., [2023](#bib.bib29)) explored an approach that integrates LLM’s
    ability to reason about user preferences with its understanding of item facts
    to generate informative input texts. These texts facilitate the capture of item
    features and details, thereby further enhancing the accuracy and relevance of
    the recommender system. The research of Lyu et al. (Lyu et al., [2023](#bib.bib19))
    is specifically focused on the utilisation of the knowledge and reasoning capabilities
    of the LLM to generate enhanced input text that more accurately captures the features
    and nuances of the items, thereby improving the performance of the recommendation
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: In sequential recommendation, researchers view the history of user interactions
    as a sequence of word tokens and make recommendation predictions. Morelra et al.
    (de Souza Pereira Moreira et al., [2021](#bib.bib5)) present an attempt to build
    a practical e-commerce and news recommendation framework based on the Transformers
    architecture. Sun et al. (Sun et al., [2019](#bib.bib22)) propose a methodwhereby
    som items in a sequence are masked auring training, thereby encouraging the model
    to predict the next recommended item based on the contextual environment through
    a completitive strategy. urthermore, by generating multiple training samples from
    each sequence, the simplification of the training process is avoided, and the
    model is made more robust. This approach avoids the task and makes the model more
    robust. Zhou et al. (Zhou et al., [2020](#bib.bib37)) exploits the mutual information
    between recommendation data and the correlation between sequences of different
    hierarchical levels to derive self-supervised signals and pre-train the way to
    enhance the data representation.
  prefs: []
  type: TYPE_NORMAL
- en: Conversational recommender systems (CRS) are designed to proactively guide user
    preferences and recommend high-quality items through natural language dialogues.
    They provide recommendations in the form of interactive conversations (Jannach
    et al., [2021](#bib.bib10)). Wang et al. (Wang et al., [2022b](#bib.bib26)) propose
    a unified CRS model for knowledge-enhanced prompt learning, which constructs knowledge
    representations and combines task-specific sequential prompts. In addition to
    conversational context. Fu et al.(Fu et al., [2021](#bib.bib6)) consider the impact
    of popularity bias in CRS and achieve a balance between recommendation performance
    and item popularity by integrating semantic features derived from open-ended users’
    real-time discourse and history, as well as item popularity. Zhang(Zhang, [2023](#bib.bib35))
    combines LLM and graph neural networks to propose a graph-based dialogue Path
    Reasoning framework. This represents dialogue as interactive reasoning on knowledge
    graphs, and uses LLM for user modelling and response generation to improve the
    accuracy and user experience of CRS. Wang et al.(Wang et al., [2021](#bib.bib25))
    integrates the recommendation task into dialogue generation by introducing lexical
    pointers and introduces knowledge-aware biases learned from entity-oriented knowledge
    graphs to improve recommendation performance.
  prefs: []
  type: TYPE_NORMAL
- en: All of the above studies involve prompt learning methods, which can be used
    not only to reconstruct the task form to align the goals of the recommendation
    task with the training goals of LLM; but also to introduce knowledge graphs and
    other external knowledge sources. Prompt-based recommendation methods show great
    potential for improving recommendation quality and enhancing user experience.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. recommendations based on Prompt Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prompt learning, an emerging technique, has been shown to be effective in bridging
    the gap between pre-training and downstream recommendation tasks for large language
    models (LLMs). The core idea of prompt learning is to redefine the downstream
    task into a form that matches the pre-training task by means of well-designed
    prompts, thus exploiting the powerful linguistic capabilities of the pre-trained
    model(Lester et al., [2021](#bib.bib11); Wang et al., [2022a](#bib.bib27)).
  prefs: []
  type: TYPE_NORMAL
- en: Discrete prompts are sequences of text or tokens embedded directly into the
    input of the model that motivate or direct the model to generate a specific output.
    These prompts are characterised by their clarity and interpretability, as they
    are explicitly expressed in natural language, such as specific questions, instructions
    or descriptions. LI et al.(Li et al., [2023a](#bib.bib14)) uses user and item
    IDs as prompts to overcome the semantic spatial differences between IDs and language
    models. In contrast, Zhang et al.(Zhang and Wang, [2023](#bib.bib36)) uses news
    titles instead of users and items in news recommendation scenarios, and designs
    specific discrete prompt templates from multi-dimensions to construct a complete-fill-in-the-blanks
    recommendation process. Geng et al.(Geng et al., [2022](#bib.bib8)) utilise personalised
    discrete prompt templates to convert multiple recommendation tasks into common
    natural language sequences, thereby unifying them into a shared framework for
    a ”Pretrain, Personalised Prompt, and Predict Paradigm” text-to-text recommendation
    paradigm. Wang et al.(Wang et al., [2022b](#bib.bib26))employs a prompt design
    that integrates knowledge representation and dialogue context in a session recommendation
    system, thereby providing more adequate contextual information for the task of
    session recommendation. Zhai et al.(Zhai et al., [2023](#bib.bib34)) propose the
    use of discrete prompts as a means of bridging the semantic divide, integrating
    structured knowledge graphs into language model-based sequential recommendation
    tasks in order to enhance the knowledge-driven capabilities of recommendation
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous Prompts, also known as Soft Prompts, is a relatively novel technique
    in which the prompts are incorporated into the model input in the form of learnable
    parameters. These parameters do not necessarily correspond to actual words in
    natural language; rather, they are optimised through the training process to most
    effectively guide the model through a specific task. The advantage of continuous
    cues is that they can be optimised directly through backpropagation, allowing
    for greater flexibility in adapting to different task requirements. Continuous
    cues are typically integrated with the model’s embedding or input layer, enabling
    the model to utilise these learned signals more effectively during the task. Yi
    et al.(Li et al., [2023b](#bib.bib15)) combined a graph neural network with a
    prompt fine-tuning paradigm to construct continuous prompt templates using personalised
    graph structural representations, thereby achieving feature enhancement in sequence
    recommendation. The generation of prompt vectors is achieved through the controlled
    length of the prompt vectors, thus improving the efficiency of LLM fine-tuning.
    Yi et al. (Yi et al., [2023](#bib.bib32)) combine a graph neural network with
    a prompt fine-tuning paradigm to construct continuous prompt templates using personalised
    graph structural representations, thus enhancing the features in sequence recommendation.
  prefs: []
  type: TYPE_NORMAL
- en: Discrete prompts are typically expressed in natural language and are designed
    to provide models with clear and unambiguous task goals, conferring a significant
    advantage in terms of interpretability. However, they tend to be more rigid, not
    easily adaptable to subtle changes in the task, and require a great deal of expert
    knowledge, manual design and adaptation. Continuous prompting offers greater flexibility
    in the form of parameters, which can be directly adjusted by optimisation algorithms
    such as shave descent, allowing the model to be more finely tuned to the specific
    task. However, their interpretability is poor and the concepts of many parameters
    cannot be aligned with human understanding. Despite the advantages and disadvantages
    of each of these approaches, they all generally face a key problem, namely a high
    sensitivity to prompts. The performance of a model is often highly contingent
    on the design and quality of the prompts, and inappropriate prompts may result
    in a significant decline in model performance or divergence from the desired objective.
    Furthermore, models may be excessively tailored to a specific prompt structure
    and may perform poorly in the absence of precise prompts or when the prompt format
    changes. In this paper, we propose a novel approach to prompt learning that combines
    it with generative adversarial networks. This approach enables the construction
    of a diverse prompt generator, which reduces the model’s dependence on precise
    prompts and improves its adaptability and robustness in changing environments
    by enhancing the diversity of training data.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3\. Generating Adversarial Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Generative adversarial network is a deep learning model based on zero-sum game
    theory (Goodfellow et al., [2014](#bib.bib9)), which combines generators and discriminators
    in an adversarial learning manner, and its optimal learning process is a minimum
    game problem (Li et al., [2017](#bib.bib13)), through the minimum-maximum game
    between generators and discriminators to (Goodfellow et al., [2014](#bib.bib9))
    to find the Nash equilibrium of both parties (Nash, [1953](#bib.bib20)), which
    improves the model performance and allows the generator to estimate the distribution
    of data samples.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. METHOD
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we first formalise the problem and then elaborate on the details
    of the overall framework and sub-modules.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Definition of the problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Suppose we have a set of users denoted by $U$ denotes the text of the user’s
    comment on this item.
  prefs: []
  type: TYPE_NORMAL
- en: For each user $u$ means no interaction record.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt Template Definition. In this paper, we forcus on sequence recommendation
    task. Referring to the previous work (Wang et al., [2022b](#bib.bib26)), different
    original recommendation templates are constructed for different recommendation
    tasks. For different template construction and task definition are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sequence Recommendation. In sequence recommendation, the model will be based
    on the user’s interaction sequence $[i_{1},i_{2},i_{3},...,i_{n-1}]$. Specifically,
    two task forms and corresponding prompt templates are used in this paper:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Template:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Input template: {user_entity} has the following purchase history:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '{purchase_history}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: does the user likely to buy {target_item} next ?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Target template: {answer_choices[label]} (yes/no)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this task, discriminative prompts are employed, whereby the model determines
    whether to interact with the next candidate based on the user interaction history.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Our Method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This subsection presents the modelling framework and module details proposed
    in this paper in detail. Firstly, an overview of the framework proposed in this
    paper is given. The framework proposed in this paper contains two major parts:
    the Diversity encoder construction and the recommendation task access. The Diversity
    encoder construction part contains three small modules: the attribute generation
    module, the encoder diversity module based on GANs and the diversity constraint
    module. In the recommendation task access process, the trained Diversity encoder
    will be employed to generate the representation of text data and subsequently
    access the downstream LLM in order to achieve the recommendation task. A detailed
    diagram of the specific framework is shown in [2](#S3.F2 "Figure 2 ‣ 3.2\. Our
    Method ‣ 3\. METHOD ‣ GANPrompt: Enhancing Robustness in LLM-Based Recommendations
    with GAN-Enhanced Diversity Prompts").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ddb212fcaf6dd4c7c97ceeaaaa54c755.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2. Overall framework diagram of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1\. attribute generation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The utilisation of LLM as a data generator can effectively address the necessity
    for task-specific data(Gao et al., [2022](#bib.bib7)). The utilisation of intricate
    attribute prompts to generate disparate attribute-generated data has been demonstrated
    to be an efficacious and expedient addition to the original dataset, thereby enhancing
    the performance and robustness of downstream tasks(Yu et al., [2024](#bib.bib33)).
    Consequently, the objective of this paper is to design complex attribute prompts
    for different tasks and input them into LLM, with the intention of generating
    the corresponding diversity attributes.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of sequence recommendation, the corresponding item description
    sequence $[d_{1},d_{2},d_{3},.....d_{n}]$ is constructed from the original training
    data and input into the following attribute generation template.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Template:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You are a professional shopper, please generate the exact attributes based
    on the user’s historical browsing sequence below:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '{title_seq}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you recommend {dataset_item} to this user, you should focus on {attribute}.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Where {title_seq} corresponds to the title sequence of the user’s interaction
    history; {dataset_item} represents the product category contained in the corresponding
    recommendation dataset; and {attribute} is the attribute generated by the model
    for the data sample.
  prefs: []
  type: TYPE_NORMAL
- en: It can be formalized as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where $T_{Attr}^{seq}$ denotes the corresponding attribute generated for the
    sample of sequence recommendation data.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2\. encoder diversity based on GANs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Although the method of attribute generation helps to increase the diversity
    of the data, there are still limitations. In this paper, inspired by GANs, we
    use the encoder of LLM as a generator $G_{LLM_{e}}(dian)$ to distinguish the generated
    text from the real text, in order to realise the zero-sum game process of GANs.
  prefs: []
  type: TYPE_NORMAL
- en: Generation process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Through the attribute generation process, we can obtain $k+1$ attribute datasets.
    In the generation process, we take the data samples with added attributes as random
    noise and input them into the LLM encoder to generate the latent features of the
    data, which are in the following form for the sequence recommendation task, for
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: In this context, $a_{n}^{seq}$ represents the potential feature generated for
    the nth attribute data of the user.
  prefs: []
  type: TYPE_NORMAL
- en: Discriminative Process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following the generation of potential feature vectors for the corresponding
    data, a simple discriminator was constructed to classify them for the adversarial
    learning gaming process. The discriminator comprises three layers of multilayer
    perceptrons (MLPs):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: The input vector, $x$ represents the layer number, which ranges from 1 to 3\.
    Subsequently, the output vector of the generator is fed into the discriminator,
    which then outputs the predicted category.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $y_{GAN}=D_{MLP}(emb^{seq}),$ |  |'
  prefs: []
  type: TYPE_TB
- en: The input of the data feature vector to the discriminator, denoted by $emb^{seq}$.
    For real data and data of the k attribute, the discriminator is expected to recognize
    the input into the corresponding seven categories.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.3\. diversity constraint
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Diversity is a fundamental problem in classification and clustering tasks, and
    its concept is strongly related to distance; as the distance between two samples
    increases, the similarity between them decreases, thus increasing the diversity
    between the samples(Xia et al., [2015](#bib.bib30)). In order to make the diversity
    encoder more effective in expanding the differences between different samples,
    this paper introduces the cosine similarity distance and the JS dispersion from
    the perspective of mathematical theory to calculate the angle and information
    differences between different samples, in order to measure the diversity between
    samples. And it is used as the diversity constraint index in the encoder optimization
    process, so that the optimized diversity encoder can distinguish samples more
    effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Cosine similarity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cosine similarity uses the cosine of the angle between two vectors to measure
    similarity. It is expected that there will be a small angle between two similar
    vectors $x_{i}^{\prime}$, and the cosine similarity is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Where $d$ decreases, the angle between the two vectors increases, which simultaneously
    implies an increase in the distance between the vectors and a weakening of the
    similarity, which in this paper will be viewed as an increase in the diversity
    between the vector samples.
  prefs: []
  type: TYPE_NORMAL
- en: However, basic cosine similarity has the serious problem of focusing only on
    the directions between patterns, which leads to the computation of the diversity
    metric between vectors being isolated. For this reason, this paper introduces
    JS scattering to complement it.
  prefs: []
  type: TYPE_NORMAL
- en: Jensen-Shannon Divergence.
  prefs: []
  type: TYPE_NORMAL
- en: 'The JS scatter is a symmetric version of the KL (Kullback-Leibler Divergence)
    scatter, which is used to compute the loss of information between two probability
    distributions and thus measure the difference between them. For two similar vectors
    $x_{i}^{\prime}$. The specific formula is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $D_{KL}(x_{i}\parallel x_{i}^{\prime})=\sum x_{i}\log\frac{x_{i}}{x_{i}^{\prime}},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'The JS scatter is based on the KL scatter and symmetrizes and normalizes the
    KL scatter by introducing an intermediate distribution. The specific formula is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $m=\frac{1}{2}(x_{i}+x_{i}^{\prime}),$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $m$ is the mean distribution of the sample.
  prefs: []
  type: TYPE_NORMAL
- en: 'The distance metric used in this paper combines cosine similarity and JS scatter
    in the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where $\alpha$ are the weights in the distance-metric synthesis process. By
    combining the cosine similarity and JS scatter, this metric can synthesize the
    angular distance differences and information differences between sample vectors.
    This diversity metric is involved in the diversity encoder optimisation process
    for the construction of sample diversity in both the distance and the information
    dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3\. Recommendation Task
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Most of the existing fine-tuned large language model recommendation methods
    based on textual cues follow the paradigm of natural language processing: first,
    the prompt text is transformed into a sequence of tokens and encoder, and the
    feature embeddings of tokens and sentences are computed, and after that fine-tuning
    is performed on the structure of the large language model either directly or by
    employing, e.g., lora, and it is the large language model that is adapted to downstream
    recommendation tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With the previous work, we implement an attribute-based that generates embedding
    representations for items combining attributes with a diversity of distance and
    information dimensions. In order to enhance the cue-based big language model to
    be more robust to downstream recommendation tasks, we plug-in this diversity encoder
    as a plug-in module into an existing big language model-based recommender system
    for recommendation task fine-tuning. Taking the sequence recommendation task as
    an example, the recommendation task cue template, user interaction history sequence,
    and corresponding attributes are inputted into the diversity encoder to compute
    the embedded features in the following specific form:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where $T_{rec}^{seq}$ is the data embedding vector generated using the diversity
    encoder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we will use a pre-trained diversity encoder to replace the original
    encoder structure of the large language model, and for LLMs whose hidden layer
    dimensions do not match the encoder, we access the projection layer for dimension
    mapping. The specific form is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $P_{ui}=LLM_{Rec}(\sigma_{i}(W_{i}E_{ui}+b_{i}))),$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $w_{i}$ is the large language model used for the recommendation, and in
    this paper we use the LLama2 base model.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4\. Two-stage optimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, we consider how to optimize the model parameters. First, to obtain diverse
    item representations, we construct a diversity encoder optimization module based
    on adversarial generative networks. This module generates diverse item feature
    vectors, which are then used to fine-tune large language models (LLMs), enhancing
    their robustness. Second, to achieve the recommendation task, we utilize the Lora
    module to assist the LLM in learning the recommendation task. By freezing the
    parameters of the LLM and focusing on optimizing the Lora module, we accelerate
    the tuning process of the large language model. A simple way is to train them
    simultaneously. However, due to the continuous transformation by the diversity
    encoder module, the large language model’s understanding of items might change,
    potentially negatively affecting the consistency of the large model’s understanding
    of the recommendation data text. To address this issue, we adopt a two-step tuning
    approach: diversity encoder fine-tuning and recommendation robustness enhancement.'
  prefs: []
  type: TYPE_NORMAL
- en: '3.4.1\. Stage 1: Optimization of the diversity generator'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the diversity encoder enhancement phase, we construct the generative adversarial
    network structure by categorizing the data samples with added attributes into
    multiple classes and constructing an MLP-based discriminator to classify the embeddings
    of the data samples. In each training iteration, the generator and discriminator
    are alternately optimized until convergence.
  prefs: []
  type: TYPE_NORMAL
- en: Generator Optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, for the generator, which is also known as the diversity encoder, the
    loss function employed is the cross-entropy loss function. This loss function
    measures the difference between the target ranked sequence and the sequence that
    has been predicted by the model.:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{G_{LLM_{e}}}=-\sum_{t=1}^{T}\log P(y_{t}\mid y_{<t},\mathbf{x}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $y_{t}$ given the input sequence and all previous target words. The model
    uses this probability distribution to determine the most likely next token in
    the sequence. To enhance the model’s performance, we introduce a loss function
    that adds a diversity constraint, aiming to encourage the model to generate more
    diverse and less repetitive sequences. The total loss function that incorporates
    the diversity constraint on this basis is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{G}=\mathcal{L}_{G_{LLM_{e}}}+\gamma D_{total}\\ =-\sum_{t=1}^{T}\log
    P(y_{t}\mid y_{<t},\mathbf{x})+\gamma(\alpha D_{cos}(x_{i}\parallel x_{i}^{\prime})+\beta
    D_{JS}(x_{i}\parallel x_{i}^{\prime}))),$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{L}_{G_{LLM_{e}}}$ is used to control the strength of this diversity
    constraint, essentially balancing between the accuracy of the generated samples
    and their diversity.
  prefs: []
  type: TYPE_NORMAL
- en: 'The objective of the optimization process for the generator is to minimize
    the loss function $\mathcal{L}_{G}$. Formally, the optimization objective of the
    generator can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\theta_{G}\leftarrow\arg\min_{\theta_{G}}\mathcal{L}_{G},$ |  |'
  prefs: []
  type: TYPE_TB
- en: $\theta_{G}$ is the optimized parameter in the generator, representing the parameter
    that has been fine-tuned during the training process to ensure the best possible
    performance of the LLM’s encoder.
  prefs: []
  type: TYPE_NORMAL
- en: Discriminator Optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the discriminator $D_{MLP}$, which refers to the multilayer perceptron
    (MLP) classification model, we employ a loss function of multiclassification.
    This loss function is crucial for measuring the performance of the model in distinguishing
    between multiple classes. The computation of this loss function is done in the
    following manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{D_{MLP}}=-\sum_{i=1}^{C}y_{i}\log(\hat{y}_{i}),$ |  |'
  prefs: []
  type: TYPE_TB
- en: In this context, where $y_{i}$ denotes the predicted category of the data attribute,
    the objective of the discriminator is to optimize its performance by minimizing
    the objective function $$\\
  prefs: []
  type: TYPE_NORMAL
- en: 'mathcal{L}_{D_{MLP}}$$. This optimization process ensures that the discriminator
    learns to better distinguish between different categories, enhancing its ability
    to make accurate predictions. The mathematical representation of this objective
    is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\theta_{D}\leftarrow\arg\min_{\theta_{D}}\mathcal{L}_{D_{MLP}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: $\theta_{D}$ is the parameter to be optimized in the discriminant. It represents
    the set of weights and biases used by the model to make classifications based
    on the input features.
  prefs: []
  type: TYPE_NORMAL
- en: '3.4.2\. Stage 2: Recommendation Robustness Fine-tuning'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the stage of enhancing the robustness of the recommendation system, we replace
    the original encoding layer of the large model with a diversity encoder and the
    corresponding feature mapping layer. Subsequently, we focus on optimizing the
    Lora module specifically fine-tuned for the recommendation task. Specifically,
    BCELoss is used as the loss function for the recommendation task:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'In this context, $y_{i}$-th sample. The predicted outcome for each sample is
    expressed through the log-probabilities of the corresponding token in the large
    model’s vocabulary. At this stage, the model parameters that need to be optimized
    include the mapping layer of the diversity encoder and the parameters of the Lora
    module. Formally, this can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\theta_{Rec}\leftarrow\arg\min_{\theta_{D}}\mathcal{L}_{Rec},$ |  |'
  prefs: []
  type: TYPE_TB
- en: The parameter $\theta_{Rec}$ represents the model parameters that need to be
    optimized in the recommendation task. This includes the mapping layer of the diversity
    encoder and the learning parameters of the Lora module.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. EXPERIMENTS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we first introduce the research questions to be answered in
    the experiment, then present the specific details of the experiment, and finally
    provide a detailed analysis of the following research questions.
  prefs: []
  type: TYPE_NORMAL
- en: 'RQ1:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether GANPrompt can improve the recommended performance in the case of cue
    diversity?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'RQ2:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether data diversity is significantly enhanced or not?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.1\. Experiment Setting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section details the datasets used in the experiments, the baseline methods
    and evaluation metrics for the comparisons, and the deployment details of the
    models in this paper.
  prefs: []
  type: TYPE_NORMAL
- en: 'Datesets.We conducted experiments on the Amazon dataset to evaluate GANPrompt.
    Amazon is a product review dataset that records user review behaviour across multiple
    domains. The recorded information includes user-side IDs, product IDs, ratings,
    and timestamps, as well as product-side descriptions, images, and up-level associations.
    To evaluate GANPrompt on this dataset, we selected three evaluation datasets,
    Amazon-Beauty, Amazon-Toys, and Amazon-Sports. Following common data preprocessing
    methods, we only consider user ratings of $5$ ) are used for model training. Table
    [1](#S4.T1 "Table 1 ‣ 4.1\. Experiment Setting ‣ 4\. EXPERIMENTS ‣ GANPrompt:
    Enhancing Robustness in LLM-Based Recommendations with GAN-Enhanced Diversity
    Prompts") gives the statistics of these datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dtaset | #User | #Item | #Iteraction | #Seq num | #Sparsity |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Beauty | 2267 | 1557 | 19119 | 2414 | 99.46% |'
  prefs: []
  type: TYPE_TB
- en: '| Toys | 1774 | 1390 | 13106 | 1818 | 99.47% |'
  prefs: []
  type: TYPE_TB
- en: '| Sports | 6667 | 4021 | 51550 | 6846 | 99.80% |'
  prefs: []
  type: TYPE_TB
- en: Table 1. Statistics on preprocessed data sets
  prefs: []
  type: TYPE_NORMAL
- en: Baselines. For different recommendation tasks, we evaluated the method proposed
    in this paper using a variety of representative recommendation methods, including
    traditional sequential recommendation algorithms, direct recommendation algorithms,
    and LLM-based recommendation methods.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequence recommendation tasks
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Caser: Applying horizontal and vertical convolution on a user’s historical
    behavioural sequence to capture sequential patterns and make recommendations.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DIN: Using an attention mechanism to dynamically capture user interests and
    match them with target items to improve recommendation results.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HGN: Dynamically selecting and aggregating different levels of historical user
    behaviour information through hierarchical gating mechanisms to enhance recommendation
    effectiveness.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GRU4Rec: Modelling user sequential behaviours and making recommendations using
    GRUs'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BERT4Rec: A Transformer-based sequential recommendation model that bi-directionally
    encodes a user’s sequence of historical behaviours by autoregression to capture
    complex contextual dependencies to enhance recommendation effectiveness.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SASRec: It is a sequence recommendation model based on the self-attention mechanism,
    which achieves efficient personalised recommendation by capturing long-range dependencies
    in user behaviour sequences.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Direct referral tasks
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MF: It is a collaborative filtering model that predicts user preferences for
    unrated items by decomposing the user-item interaction matrix into implicit feature
    vectors of the user and the item.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LightGCN: A graph convolutional network model that significantly improves the
    efficiency and effectiveness of collaborative filtering recommendation through
    a simplified neighbour aggregation and jump connection mechanism.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLM-based recommendation model
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'P5: Transforms multiple data into natural language sequences, thus unifying
    multiple recommendation tasks in a shared framework.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TallRec: By structuring recommendation data into instructions and using a lightweight
    tuning approach to align LLMs for recommendation tasks, we improve the performance
    of LLMs in the recommendation domain and show strong generalisation capabilities
    across domains.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CoLLM: Effective integration of collaborative information in recommendation
    tasks is achieved by combining collaborative information with a large-scale language
    model, utilising collaborative embeddings generated by external conventional collaborative
    models and mapping them to the input embedding space of the language model.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: CTRL:Combining collaborative models and pre-trained language models for click-through
    rate prediction tasks through cross-modal knowledge alignment and supervised fine-tuning
    to improve the performance and inference efficiency of recommender systems.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '| model | auc | hit@1 | hit@5 | hit@10 | ndcg@1 | ndcg@5 | ndcg@10 | mrr |'
  prefs: []
  type: TYPE_TB
- en: '| caser | 0.604967 | 0.059991 | 0.173798 | 0.272607 | 0.059991 | 0.095685 |
    0.121545 | 0.106353 |'
  prefs: []
  type: TYPE_TB
- en: '| din | 0.811243* | 0.162770 | 0.479047* | 0.671372* | 0.162770 | 0.259455
    | 0.307251 | 0.242778 |'
  prefs: []
  type: TYPE_TB
- en: '| gru4rec | 0.793985 | 0.119982 | 0.411116 | 0.587120 | 0.119982 | 0.218118
    | 0.262077 | 0.209023 |'
  prefs: []
  type: TYPE_TB
- en: '| HGN | 0.792072 | 0.139832 | 0.438465 | 0.634318 | 0.139832 | 0.241389 | 0.291402
    | 0.229860 |'
  prefs: []
  type: TYPE_TB
- en: '| SASrec | 0.804019 | 0.123952 | 0.423467 | 0.614027 | 0.123952 | 0.227056
    | 0.276191 | 0.217349 |'
  prefs: []
  type: TYPE_TB
- en: '| S3rec | 0.653376 | 0.048081 | 0.208205 | 0.307896 | 0.048081 | 0.101100 |
    0.127164 | 0.108495 |'
  prefs: []
  type: TYPE_TB
- en: '| P5 | 0.634582 | 0.035846 | 0.050945 | 0.069852 | 0.035846 | 0.039751 | 0.043081
    | 0.057462 |'
  prefs: []
  type: TYPE_TB
- en: '| TALLRec | 0.702796 | 0.265642* | 0.425841 | 0.471253 | 0.265642* | 0.438516*
    | 0.481524* | 0.256842* |'
  prefs: []
  type: TYPE_TB
- en: '| GANPrompt | 0.705952 | 0.282794 | 0.578534 | 0.696252 | 0.282794 | 0.552869
    | 0.598784 | 0.291348 |'
  prefs: []
  type: TYPE_TB
- en: '| Improvement (%) | - | +6.46% | +20.77% | +3.71% | +6.46% | +26.08% | +24.35%
    | +13.43% |'
  prefs: []
  type: TYPE_TB
- en: Table 2. Performance metrics on the Beauty dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '| model | auc | hit@1 | hit@5 | hit@10 | ndcg@1 | ndcg@5 | ndcg@10 | mrr |'
  prefs: []
  type: TYPE_TB
- en: '| caser | 0.575176 | 0.046348 | 0.128844 | 0.201890 | 0.046348 | 0.078403 |
    0.099363 | 0.093628 |'
  prefs: []
  type: TYPE_TB
- en: '| din | 0.717959* | 0.087896* | 0.260687* | 0.386081* | 0.087896* | 0.156796*
    | 0.191938* | 0.163307* |'
  prefs: []
  type: TYPE_TB
- en: '| gru4rec | 0.711225 | 0.086396 | 0.247338 | 0.368532 | 0.086396 | 0.148791
    | 0.182685 | 0.156953 |'
  prefs: []
  type: TYPE_TB
- en: '| HGN | 0.696624 | 0.083396 | 0.243588 | 0.357432 | 0.083396 | 0.146091 | 0.178133
    | 0.153098 |'
  prefs: []
  type: TYPE_TB
- en: '| SASrec | 0.705065 | 0.074696 | 0.233838 | 0.353532 | 0.074696 | 0.137811
    | 0.171229 | 0.146856 |'
  prefs: []
  type: TYPE_TB
- en: '| S3rec | 0.541157 | 0.016799 | 0.077096 | 0.137993 | 0.016799 | 0.042931 |
    0.060194 | 0.061554 |'
  prefs: []
  type: TYPE_TB
- en: '| P5 | 0.622521 | 0.028462 | 0.039574 | 0.046895 | 0.028462 | 0.032165 | 0.035846
    | 0.045813 |'
  prefs: []
  type: TYPE_TB
- en: '| TALLRec | 0.606144 | 0.037584 | 0.143585 | 0.261345 | 0.037584 | 0.083193
    | 0.102213 | 0.09754 |'
  prefs: []
  type: TYPE_TB
- en: '| GANPrompt | 0.618993 | 0.088610 | 0.198172 | 0.338204 | 0.088610 | 0.194582
    | 0.245934 | 0.193426 |'
  prefs: []
  type: TYPE_TB
- en: '| Improvement (%) | - | +0.81% | - | - | +0.81% | +24.10% | +28.13% | +18.44%
    |'
  prefs: []
  type: TYPE_TB
- en: Table 3. Performance metrics on the Sports dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '| model | auc | hit@1 | hit@5 | hit@10 | ndcg@1 | ndcg@5 | ndcg@10 | mrr |'
  prefs: []
  type: TYPE_TB
- en: '| caser | 0.563149 | 0.034386 | 0.098083 | 0.161218 | 0.034386 | 0.061459 |
    0.080471 | 0.080474 |'
  prefs: []
  type: TYPE_TB
- en: '| din | 0.753086 | 0.110485 | 0.304961 | 0.444194 | 0.110485 | 0.191770 | 0.231055
    | 0.195825 |'
  prefs: []
  type: TYPE_TB
- en: '| gru4rec | 0.739768 | 0.088501 | 0.277903 | 0.424464 | 0.088501 | 0.168506
    | 0.209935 | 0.175074 |'
  prefs: []
  type: TYPE_TB
- en: '| HGN | 0.758495* | 0.127959* | 0.316798* | 0.473506* | 0.127959* | 0.208445*
    | 0.252562* | 0.213780* |'
  prefs: []
  type: TYPE_TB
- en: '| SASrec | 0.757952 | 0.122886 | 0.310598 | 0.467869 | 0.122886 | 0.199035
    | 0.243883 | 0.206036 |'
  prefs: []
  type: TYPE_TB
- en: '| S3rec | 0.621746 | 0.043968 | 0.135287 | 0.225479 | 0.043968 | 0.084166 |
    0.110406 | 0.102184 |'
  prefs: []
  type: TYPE_TB
- en: '| P5 | 0.668452 | 0.048653 | 0.068452 | 0.079520 | 0.048653 | 0.057892 | 0.059183
    | 0.069521 |'
  prefs: []
  type: TYPE_TB
- en: '| TALLRec | 0.691331 | 0.055784 | 0.175421 | 0.324157 | 0.055784 | 0.071487
    | 0.086995 | 0.074075 |'
  prefs: []
  type: TYPE_TB
- en: '| GANPrompt | 0.712376 | 0.213842 | 0.389260 | 0.540811 | 0.213842 | 0.256983
    | 0.274613 | 0.232908 |'
  prefs: []
  type: TYPE_TB
- en: '| Improvement (%) | - | +67.12% | +22.87% | +14.21% | +67.12% | +23.28% | +8.73%
    | +8.95% |'
  prefs: []
  type: TYPE_TB
- en: Table 4. Performance metrics on the Toys dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Metrics. To fairly evaluate the recommendation performance of GANPrompt
    and the baseline model, we utilized widely accepted evaluation metrics previously
    employed in recommendation methods. These metrics include the Area Under the Curve
    (AUC) for accuracy assessments, as well as the Normalized Discounted Cumulative
    Gain (NDCG), Hit Rate (HR) for specific cutoff values $k=1,5,10$ and MRR are more
    focused on the quality of the desired recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation Details.
  prefs: []
  type: TYPE_NORMAL
- en: All experiments were conducted in a Python 3.9 environment using the PyTorch
    framework and CUDA version 12.3, on NVIDIA’s A100 80G GPUs. Specifically, for
    the attribute generation segment, we utilized BERT as the attribute generator
    in a masked prediction task, selecting the top five attributes with the highest
    prediction frequency from each dataset as additional features. During the diversity
    encoder training phase, GANPrompt employed the T5 model’s encoder as the generator
    in a Generative Adversarial Network (GAN), with a custom-designed three-layer
    Multilayer Perceptron (MLP) serving as the discriminator; the batch size was set
    to 256, and both the encoder and discriminator were trained with a learning rate
    of 5e-5 using the Ada optimizer. For the sequential recommendation task, we used
    the pre-trained T5 language model as the foundational LLM for recommendations,
    setting the minimum length of historical interaction sequences at 3 and the maximum
    at 10, based on the P5\. The training batch size was 128 with a learning rate
    of 1e-5, using the AdamW optimizer. Differently from previous approaches, we fine-tuned
    only the T5’s decoder to adapt to the recommendation tasks while preserving the
    encoder’s capability to generate diverse data.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. Main Experiment Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c9e5c5215ebad806f25474605fc7fdc1.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Distribution of data before encoder adjustment on the Beauty dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1e3f7fc0266457f8ae0b7041a11a63d4.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Distribution of encoder-adjusted data on Beauty dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5819a5a05ceff6a71f37bdcfcd7dcdea.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Distribution of data before encoder adjustment on the Sports dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/395594ebf61c028474940ae83dbd21cf.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) Distribution of encoder-adjusted data on Sports dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a8b5ece95922b801c7418d26381e7770.png)'
  prefs: []
  type: TYPE_IMG
- en: (e) Distribution of data before encoder adjustment on the Toys dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/34cfdb61bc62a6c2a52d6b5df52f4b96.png)'
  prefs: []
  type: TYPE_IMG
- en: (f) Distribution of encoder-adjusted data on Toys dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3. Visualisation of data samples before and after Diversity encoder tuning,
    including the original sequence data of the three datasets and the attribute data
    after adding five attribute features.
  prefs: []
  type: TYPE_NORMAL
- en: In order to answer RQ1.The performance of GANPrompt and all baseline models
    on the three real-world datasets is shown in Tables LABEL:tab3,tab4,tab5, with
    the optimal results shown in bold and the best baseline results marked with an
    asterisk.The performance improvement of GANPrompt over the best baseline is shown
    as a percentage in the ‘Improvement’ row. The performance improvement of GANPrompt
    over the best baseline is shown in percentage in the ‘Improvement’ row.
  prefs: []
  type: TYPE_NORMAL
- en: The experimental results show that our proposed GANPrompt achieves leading recommendation
    performance on most evaluation metrics compared to the state-of-the-art baseline
    model. In particular, GANPrompt improves on the hit@1 metric in all datasets,
    especially on the Toys dataset, hit@1 metric improved by 67.12%. In addition,
    GANPrompt improves more than 20% on the hit@5 metric on both Beauty and Toys datasets,
    and more than 20% and 10% on the ndcg@5 and mrr metrics on all three datasets.
    While traditional sequential recommendation methods such as DIN excel in capturing
    users’ changing interests across advertisements or recommendation scenarios, and
    thus have superior performance in personalised recommendation accuracy, they perform
    poorly when dealing with diverse item features.GANPrompt learns how to deal with
    commonalities and differences between diverse data by training recommendation
    tasks on a diversity encoder. In contrast, LLM-based methods such as TALLRec,
    while taking advantage of LLM’s large number of parameters and pre-training knowledge,
    have good generalisation ability to diverse data, but are too sensitive to small
    differences between prompts. Compared to Llama2, the base model T5 used by GANPrompt,
    although having a smaller structure and parameters, exhibits more robust recommendation
    performance by adequately fine-tuning the diversity encoder for the recommendation
    task.
  prefs: []
  type: TYPE_NORMAL
- en: However, although GANPrompt performs better on the auc metric when comparing
    it to the LLM-based approach, overall the traditional model performs better overall
    on the auc metric, obtaining optimal results. This is because the auc metric focuses
    more on the overall model ranking ability, i.e., the model’s ability to distinguish
    between positive and negative samples. In contrast, hit@k, NDCG@k and MRR are
    more concerned with the specific position of the recommendation list, focusing
    on measuring the ranking ability of the front end of the model. This suggests
    that while LLM-based approaches are able to leverage semantic information to accurately
    place relevant items at the top of the list, achieving very superior performance
    locally; for the model’s overall sorting of positive and negative samples, traditional
    mini-models, on the other hand, perform better on the global recommendation task
    through a more streamlined structure and learning task.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. Diversity encoder visualisation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to verify whether the diversity encoder, which has been adapted by
    applying the GAN framework, can effectively differentiate data samples and enhance
    the diversity among data samples and answer the RQ2, we conducted detailed experimental
    analyses on three different datasets. These datasets all involve sequence recommendation
    tasks with rich and diverse feature data. In our experiments, we first considered
    the sequence recommendation data in each dataset as the original input, and then
    constructed a composite dataset with six attribute dimensions by combining the
    top 5 attribute data of each dataset for further visual presentation and analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'The visualisation results are shown in Figure [3](#S4.F3 "Figure 3 ‣ 4.2\.
    Main Experiment Results ‣ 4\. EXPERIMENTS ‣ GANPrompt: Enhancing Robustness in
    LLM-Based Recommendations with GAN-Enhanced Diversity Prompts"). From the figure,
    it can be observed that the encoder of the pre-trained language model T5 has difficulty
    in distinguishing datasets with different attributes before training, and all
    the sample points are almost clustered in the same data distribution region. However,
    after the diversity constraint adjustment by introducing the GAN framework and
    diversity encoder, the situation changes significantly. The datasets with different
    attributes are effectively differentiated in the feature space, and the data points
    of each collection start to scatter over a wider area.'
  prefs: []
  type: TYPE_NORMAL
- en: This obvious differentiation effect indicates that the diversity-adjusted encoder
    can effectively expand the distance between data with different attributes and
    change their distribution in the feature space. This not only illustrates the
    effectiveness of the diversity encoder under the GAN framework in improving data
    diversity, but also provides richer and more discriminative feature representations
    for the subsequent sequential recommendation task, enhancing the advantage of
    the subsequent model in dealing with complex and multi-attribute recommender system
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. CONCLUSIONS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we present an innovative framework called GANPrompt, which enhances
    the robustness of Large Language Models (LLMs) in recommender systems using Generative
    Adversarial Networks (GANs). Our research focuses on enhancing the model’s adaptability
    and stability to different cues through diverse cue generation. Specifically,
    GANPrompt first trains a multidimensional cue generator through GAN generation
    techniques, which is capable of generating highly diverse cues based on user behavioural
    data. These diverse cues are then used to train the LLM to improve its performance
    when faced with unseen cues. Furthermore, to ensure that the generated cues are
    both highly diverse and relevant, we introduce a diversity constraint mechanism
    based on mathematical theory to optimise cue generation and ensure that they semantically
    cover a wide range of user intentions.
  prefs: []
  type: TYPE_NORMAL
- en: Through extensive experiments on multiple datasets, we demonstrate the effectiveness
    of the proposed framework, especially in improving the adaptability and robustness
    of recommender systems in complex and dynamic environments. Experimental results
    show that GANPrompt provides significant improvements in accuracy and robustness
    compared to existing state-of-the-art approaches. These results not only demonstrate
    the effectiveness of diversity encoders under the GAN framework in enhancing the
    diversity of data, but also provide new technical paths and strong experimental
    evidence for processing complex and multi-attribute recommender system data. Advantages
    when processing complex and multi-attribute recommender system data.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems* 33 (2020), 1877–1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chowdhery et al. (2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways.
    *Journal of Machine Learning Research* 24, 240 (2023), 1–113.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai et al. (2023) Haixing Dai, Zhengliang Liu, Wenxiong Liao, Xiaoke Huang,
    Yihan Cao, Zihao Wu, Lin Zhao, Shaochen Xu, Wei Liu, Ninghao Liu, et al. 2023.
    Auggpt: Leveraging chatgpt for text data augmentation. *arXiv preprint arXiv:2302.13007*
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'de Souza Pereira Moreira et al. (2021) Gabriel de Souza Pereira Moreira, Sara
    Rabhi, Jeong Min Lee, Ronay Ak, and Even Oldridge. 2021. Transformers4rec: Bridging
    the gap between nlp and sequential/session-based recommendation. In *Proceedings
    of the 15th ACM conference on recommender systems*. 143–153.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu et al. (2021) Zuohui Fu, Yikun Xian, Shijie Geng, Gerard De Melo, and Yongfeng
    Zhang. 2021. Popcorn: Human-in-the-loop popularity debiasing in conversational
    recommender systems. In *Proceedings of the 30th ACM International Conference
    on Information & Knowledge Management*. 494–503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. (2022) Jiahui Gao, Renjie Pi, Yong Lin, Hang Xu, Jiacheng Ye, Zhiyong
    Wu, Weizhong Zhang, Xiaodan Liang, Zhenguo Li, and Lingpeng Kong. 2022. Self-guided
    noise-free data generation for efficient zero-shot learning. *arXiv preprint arXiv:2205.12679*
    (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Geng et al. (2022) Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and
    Yongfeng Zhang. 2022. Recommendation as language processing (rlp): A unified pretrain,
    personalized prompt & predict paradigm (p5). In *Proceedings of the 16th ACM Conference
    on Recommender Systems*. 299–315.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2014) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014.
    Generative adversarial nets. *Advances in neural information processing systems*
    27 (2014).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jannach et al. (2021) Dietmar Jannach, Ahtsham Manzoor, Wanling Cai, and Li
    Chen. 2021. A survey on conversational recommender systems. *ACM Computing Surveys
    (CSUR)* 54, 5 (2021), 1–36.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lester et al. (2021) Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The
    power of scale for parameter-efficient prompt tuning. *arXiv preprint arXiv:2104.08691*
    (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2022) Chunyuan Li, Haotian Liu, Liunian Li, Pengchuan Zhang, Jyoti
    Aneja, Jianwei Yang, Ping Jin, Houdong Hu, Zicheng Liu, Yong Jae Lee, et al. 2022.
    Elevater: A benchmark and toolkit for evaluating language-augmented visual models.
    *Advances in Neural Information Processing Systems* 35 (2022), 9287–9301.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2017) Li Li, Yi-Lun Lin, Dong-Pu Cao, Nan-Ning Zheng, and Fei-Yue
    Wang. 2017. Parallel learning-a new framework for machine learning. *Acta Automatica
    Sinica* 43, 1 (2017), 1–8.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023a) Lei Li, Yongfeng Zhang, and Li Chen. 2023a. Personalized prompt
    learning for explainable recommendation. *ACM Transactions on Information Systems*
    41, 4 (2023), 1–26.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023b) Lei Li, Yongfeng Zhang, and Li Chen. 2023b. Prompt distillation
    for efficient llm-based recommendation. In *Proceedings of the 32nd ACM International
    Conference on Information and Knowledge Management*. 1348–1357.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023c) Lei Li, Yongfeng Zhang, Dugang Liu, and Li Chen. 2023c. Large
    language models for generative recommendation: A survey and visionary discussions.
    *arXiv preprint arXiv:2309.01157* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2024) Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2024.
    Visual instruction tuning. *Advances in neural information processing systems*
    36 (2024).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023) Peng Liu, Lemei Zhang, and Jon Atle Gulla. 2023. Pre-train,
    Prompt, and Recommendation: A Comprehensive Survey of Language Modeling Paradigm
    Adaptations in Recommender Systems. *Transactions of the Association for Computational
    Linguistics* 11 (2023), 1553–1571.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lyu et al. (2023) Hanjia Lyu, Song Jiang, Hanqing Zeng, Yinglong Xia, and Jiebo
    Luo. 2023. Llm-rec: Personalized recommendation via prompting large language models.
    *arXiv preprint arXiv:2307.15780* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nash (1953) John Nash. 1953. Two-person cooperative games. *Econometrica: Journal
    of the Econometric Society* (1953), 128–140.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shanahan (2024) Murray Shanahan. 2024. Talking about large language models.
    *Commun. ACM* 67, 2 (2024), 68–79.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2019) Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu
    Ou, and Peng Jiang. 2019. BERT4Rec: Sequential recommendation with bidirectional
    encoder representations from transformer. In *Proceedings of the 28th ACM international
    conference on information and knowledge management*. 1441–1450.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Taylor et al. (2022) Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom,
    Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic.
    2022. Galactica: A large language model for science. *arXiv preprint arXiv:2211.09085*
    (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021) Lingzhi Wang, Huang Hu, Lei Sha, Can Xu, Kam-Fai Wong, and
    Daxin Jiang. 2021. RecInDial: A unified framework for conversational recommendation
    with pretrained language models. *arXiv preprint arXiv:2110.07477* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022b) Xiaolei Wang, Kun Zhou, Ji-Rong Wen, and Wayne Xin Zhao.
    2022b. Towards unified conversational recommender systems via knowledge-enhanced
    prompt learning. In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge
    Discovery and Data Mining*. 1929–1937.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022a) Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi
    Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, and Tomas Pfister. 2022a.
    Learning to prompt for continual learning. In *Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition*. 139–149.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2023) Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu,
    Tingjia Shen, Chuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, et al. 2023. A survey
    on large language models for recommendation. *arXiv preprint arXiv:2305.19860*
    (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xi et al. (2023) Yunjia Xi, Weiwen Liu, Jianghao Lin, Xiaoling Cai, Hong Zhu,
    Jieming Zhu, Bo Chen, Ruiming Tang, Weinan Zhang, Rui Zhang, et al. 2023. Towards
    open-world recommendation with knowledge augmentation from large language models.
    *arXiv preprint arXiv:2306.10933* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xia et al. (2015) Peipei Xia, Li Zhang, and Fanzhang Li. 2015. Learning similarity
    with cosine similarity ensemble. *Information sciences* 307 (2015), 39–52.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2023) Fan Yang, Zheng Chen, Ziyan Jiang, Eunah Cho, Xiaojiang
    Huang, and Yanbin Lu. 2023. Palr: Personalization aware llms for recommendation.
    *arXiv preprint arXiv:2305.07622* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yi et al. (2023) Zixuan Yi, Iadh Ounis, and Craig Macdonald. 2023. Contrastive
    graph prompt-tuning for cross-domain recommendation. *ACM Transactions on Information
    Systems* 42, 2 (2023), 1–28.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2024) Yue Yu, Yuchen Zhuang, Jieyu Zhang, Yu Meng, Alexander J Ratner,
    Ranjay Krishna, Jiaming Shen, and Chao Zhang. 2024. Large language model as attributed
    training data generator: A tale of diversity and bias. *Advances in Neural Information
    Processing Systems* 36 (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhai et al. (2023) Jianyang Zhai, Xiawu Zheng, Chang-Dong Wang, Hui Li, and
    Yonghong Tian. 2023. Knowledge prompt-tuning for sequential recommendation. In
    *Proceedings of the 31st ACM International Conference on Multimedia*. 6451–6461.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang (2023) Gangyi Zhang. 2023. User-centric conversational recommendation:
    Adapting the need of user with large language models. In *Proceedings of the 17th
    ACM Conference on Recommender Systems*. 1349–1354.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang and Wang (2023) Zizhuo Zhang and Bang Wang. 2023. Prompt learning for
    news recommendation. In *Proceedings of the 46th International ACM SIGIR Conference
    on Research and Development in Information Retrieval*. 227–237.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2020) Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang,
    Fuzheng Zhang, Zhongyuan Wang, and Ji-Rong Wen. 2020. S3-rec: Self-supervised
    learning for sequential recommendation with mutual information maximization. In
    *Proceedings of the 29th ACM international conference on information & knowledge
    management*. 1893–1902.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
