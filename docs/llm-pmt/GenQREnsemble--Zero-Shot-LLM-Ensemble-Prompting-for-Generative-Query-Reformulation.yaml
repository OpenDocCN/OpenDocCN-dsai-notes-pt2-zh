- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:45:08'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'GenQREnsemble: Zero-Shot LLM Ensemble Prompting for Generative Query Reformulation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.03746](https://ar5iv.labs.arxiv.org/html/2404.03746)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '¹¹institutetext: Department of Computer Science'
  prefs: []
  type: TYPE_NORMAL
- en: Emory University
  prefs: []
  type: TYPE_NORMAL
- en: Atlanta, USA
  prefs: []
  type: TYPE_NORMAL
- en: '¹¹email: {kaustubh.dhole, eugene.agichtein}@emory.eduKaustubh D. Dhole    Eugene
    Agichtein'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Query Reformulation(QR) is a set of techniques used to transform a user’s original
    search query to a text that better aligns with the user’s intent and improves
    their search experience. Recently, zero-shot QR has been shown to be a promising
    approach due to its ability to exploit knowledge inherent in large language models.
    By taking inspiration from the success of ensemble prompting strategies which
    have benefited many tasks, we investigate if they can help improve query reformulation.
    In this context, we propose an ensemble based prompting technique, GenQREnsemble
    which leverages paraphrases of a zero-shot instruction to generate multiple sets
    of keywords ultimately improving retrieval performance. We further introduce its
    post-retrieval variant, GenQREnsembleRF to incorporate pseudo relevant feedback.
    On evaluations over four IR benchmarks, we find that GenQREnsemble generates better
    reformulations with relative nDCG@10 improvements up to 18% and MAP improvements
    upto 24% over the previous zero-shot state-of-art. On the MSMarco Passage Ranking
    task, GenQREnsembleRF shows relative gains of 5% MRR using pseudo-relevance feedback,
    and 9% nDCG@10 using relevant feedback documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Query Reformulation Zero-Shot Prompting Relevance Feedback
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Users searching for relevant documents might not always be able to accurately
    express their information needs in their initial queries. This could result in
    queries being vague or ambiguous or lacking the necessary domain vocabulary. Query
    Reformulation (QR) is a set of techniques used to transform a user’s original
    search query to a text that better aligns with the user’s intent and improves
    their search experience. Such reformulation alleviates the vocabulary mismatch
    problem by expanding the query with related terms or paraphrasing it into a suitable
    form by incorporating additional context.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, with the success of large language models (LLMs) [[5](#bib.bib5),
    [23](#bib.bib23)], a plethora of QR approaches have been developed. The generative
    capabilities of LLMs have been exploited to produce novel queries [[22](#bib.bib22)],
    as well as useful keywords to be appended to the users’ original queries [[29](#bib.bib29)].
    By gaining exposure to enormous amounts of text during pre-training, prompting
    has become a promising avenue for utilizing knowledge inherent in an LLM for the
    benefit of subsequent downstream tasks [[27](#bib.bib27)] especially QR [[14](#bib.bib14),
    [32](#bib.bib32)].
  prefs: []
  type: TYPE_NORMAL
- en: Unlike training or few-shot learning, zero-shot prompting does not rely on any
    labeled examples. The advantage of a zero-shot approach is the ease with which
    a standalone generative model can be used to reformulate queries by prompting
    a templated piece of instruction along with the original query. Particularly,
    zero-shot QR can be used to generate keywords by prompting the user’s original
    query along with an instruction that defines the task of query reformulation in
    natural language like Generate useful search terms for the given query:‘List all
    the breweries in Austin’.
  prefs: []
  type: TYPE_NORMAL
- en: '| Instruction | Expansions Generated |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Increase the search efficacy by offering | age goldfish grow outsmart outlive
    |'
  prefs: []
  type: TYPE_TB
- en: '| beneficial expansion keywords for the query | ageing species… |'
  prefs: []
  type: TYPE_TB
- en: '| Enhance search outcomes by recommending beneficial | Goldfish breed sizes
    What kind of |'
  prefs: []
  type: TYPE_TB
- en: '| expansion terms to supplement the query | goldfish grows the fastest… |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 1: Keywords generated for the query (“do goldfish grow”) differ drastically
    when generated from two paraphrastic instructions prompted to flan-t5-xxl [[7](#bib.bib7)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, such a zero-shot prompting approach is still contingent on the exact
    instruction appearing in the prompt providing plenty of avenues of improvement.
    While LLMs have been known to vary significantly in performance across different
    prompts [[36](#bib.bib36), [8](#bib.bib8)] and generation settings [[33](#bib.bib33)],
    many natural language tasks have benefited by exploiting such variation via ensembling
    multiple prompts or generating diverse reasoning paths [[16](#bib.bib16), [3](#bib.bib3),
    [31](#bib.bib31)]. Whether such improvements also transfer to tasks like QR is
    yet to be determined. In Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ GenQREnsemble:
    Zero-Shot LLM Ensemble Prompting for Generative Query Reformulation"), a vast
    difference is noticed in the keywords generated when the input instruction is
    altered to a semantically similar variant. We hypothesize that QR might naturally
    benefit from such variation – An ensemble of zero-shot reformulators with paraphrastic
    instructions can be tasked to look at the input query in diverse ways so as to
    elicit different expansions. This work proposes the following contributions:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose a novel method,GenQREnsemble – a zero-shot Ensemble based Generative
    Query Reformulator which exploits multiple zero-shot instructions for QR to generate
    a more effective query reformulation than possible with an individual instruction.
    (Section 3)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We further introduce an extension GenQREnsembleRF to incorporate Relevance Feedback
    into the process. (Section 3)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We evaluate the proposed methods over four standard IR benchmarks, demonstrating
    significant relative improvements vs. recent state of the art, of up to 18% on
    nDCG@10 in pre-retrieval settings, and of up to 9% nDCG@10 on post-retrieval (feedback)
    settings, demonstrating increased generalizability of our approach.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, we summarize the prior work to place our contributions in context.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Query reformulation has been shown to be effective in many settings [[6](#bib.bib6)].
    It can be done pre-retrieval, or post-retrieval, via incorporating evidence from
    feedback, obtained either from a user or from top-ranked results in the sparse
    retrieval setting [[15](#bib.bib15)], and in the dense retrieval setting [[30](#bib.bib30),
    [35](#bib.bib35)].
  prefs: []
  type: TYPE_NORMAL
- en: Recently, zero-shot approaches to query reformulation have received considerable
    attention. Wang et al. [[29](#bib.bib29)] design a query reformulator by fine-tuning
    a sequence-to-sequence transformer, T5 [[25](#bib.bib25)] on pairs of raw and
    transformed queries. Their zero-shot prompting approach uses an instruction-tuned
    model, FlanT5 [[7](#bib.bib7)] to generate keywords for query expansion and incorporating
    PRF. Jagerman et al. [[14](#bib.bib14)] demonstrate LLMs can be more powerful
    than traditional methods for query expansion. Mo et al. [[19](#bib.bib19)] propose
    a framework to reformulate conversational search queries using LLMs. Gao et al. [[11](#bib.bib11)]’s
    framework performs retrieval through fake documents generated by prompting LLMs
    with user queries. Alaofi et al. [[2](#bib.bib2)] prompt LLMs with information
    descriptions to generate query variants.
  prefs: []
  type: TYPE_NORMAL
- en: However, using a single query reformulation can sometimes degrade performance
    compared to the original query. To address this drawback, prior efforts have incorporated
    ensemble strategies via keywords from numerous sources or fusing documents from
    different queries. Gao et al. [[10](#bib.bib10)], combine features derived from
    various translation models to generate better query rewrites. Si et al. [[26](#bib.bib26)]
    perform QR by utilizing multiple external biomedical resources. Hsu and Taksa [[13](#bib.bib13)]
    present a data fusion framework suggesting that diverse query formulations represent
    distinct evidence sources for inferring document relevance. Later, Mohankumar
    et al. [[20](#bib.bib20)] generated diverse queries by introducing a diversity-driven
    RL algorithm. For other tasks, recent works demonstrated the benefits of ensemble
    strategies for prompting LLMs, including self-consistency [[31](#bib.bib31)] for
    arithmetic and common sense tasks, Chain of Verification [[9](#bib.bib9)] for
    improving factuality, and Diverse [[16](#bib.bib16)] for question answering. However,
    zero-shot based ensemble methods for LLM have not been explored for the Query
    Reformulation task, as we propose in this paper.
  prefs: []
  type: TYPE_NORMAL
- en: '3 Proposed Approach: GenQREnsemble'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we describe two variations of our proposed approach, for the
    pre- and post-retrieval settings. In the pre-retrieval setting, a Query Reformulation
    $R$ to improve retrieval effectiveness for a given search task (e.g., passage
    or document retrieval). We also consider the post-retrieval setting, wherein the
    reformulator can incorporate additional contextual information like document or
    passage-level feedback.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2acda55b53552929f513ca6803ca5f0f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The complete flow and algorithm shown on the top right.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pre-retrieval: We propose GenQREnsemble – an ensemble prompting based query
    reformulator which uses $N$. The complete process and algorithm are shown in Figure [2](#S3.F2
    "Figure 2 ‣ 3 Proposed Approach: GenQREnsemble ‣ GenQREnsemble: Zero-Shot LLM
    Ensemble Prompting for Generative Query Reformulation").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Post-retrieval: To assess how well our method can incorporate additional context
    like document feedback, we introduce GenQREnsembleRF. Here, we prepend the $N$,
    obtained either as pseudo-relevance feedback from initial retrieval or manually
    chosen by the user.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| # | Instruction |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Improve the search effectiveness by suggesting expansion terms for the
    query |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Recommend expansion terms for the query to improve search results |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Improve the search effectiveness by suggesting useful expansion terms
    for the query |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Maximize search utility by suggesting relevant expansion phrases for
    the query |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | Enhance search efficiency by proposing valuable terms to expand the query
    |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | Elevate search performance by recommending relevant expansion phrases
    for the query |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | Boost the search accuracy by providing helpful expansion terms to enrich
    the query |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | Increase the search efficacy by offering beneficial expansion keywords
    for the query |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | Optimize search results by suggesting meaningful expansion terms to enhance
    the query |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | Enhance search outcomes by recommending beneficial expansion terms to
    supplement the query |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 3: Reformulation instructions generated ($N$=10).'
  prefs: []
  type: TYPE_NORMAL
- en: We now describe the experiments and analysis performed for different retrieval
    settings.
  prefs: []
  type: TYPE_NORMAL
- en: To instruct the LLM to generate query reformulations, we start with the instruction
    empirically chosen by Wang et al. [[29](#bib.bib29)] – as our base QR instruction
    $I_{1}$. These paraphrases serve as our instruction set for subsequent experiments.
  prefs: []
  type: TYPE_NORMAL
- en: For generating the actual query reformulations, we employ flan-t5-xxl [[7](#bib.bib7)],
    an instruction-tuned model. The FlanT5 set of models is created by fine-tuning
    the text-to-text transformer model, T5 [[25](#bib.bib25)] on instruction data
    of a variety of NL tasks. We use the checkpoint²²2[https://huggingface.co/google/flan-t5-xxl](https://huggingface.co/google/flan-t5-xxl)
    provided through HuggingFace’s Transformers library [[34](#bib.bib34)]. Nucleus
    sampling is performed with a cutoff probability of 0.92 keeping the top 200 tokens
    (top_k) and a repetition penalty of 1.2.
  prefs: []
  type: TYPE_NORMAL
- en: 'For evaluation, we use four popular benchmarks through IRDataset [[17](#bib.bib17)]’s
    interface: 1)TP19: TREC 19 Passage Ranking which uses the MSMarco dataset [[21](#bib.bib21),
    [14](#bib.bib14)] consisting of search engine queries. 2)TR04: TREC Robust 2004
    Track, a task intended for testing poorly performing topics. In our experiments,
    we use the Title as our choice of query. And two tasks from the BEIR [[28](#bib.bib28)]
    benchmark 3)WT: Webis Touche [[4](#bib.bib4)] for argument retrieval 4)DE: DBPedia
    Entity Retrieval [[12](#bib.bib12)].'
  prefs: []
  type: TYPE_NORMAL
- en: '4.1 Baselines:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We compare our work against the following using the Pyterrier [[18](#bib.bib18)]
    framework. For all the post-retrieval experiments, we use 5 documents as feedback.
  prefs: []
  type: TYPE_NORMAL
- en: 'With BM25 Retriever:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BM25: Here, we retrieve using raw queries without any reformulation'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'FlanQR [[29](#bib.bib29)]: We implement Wang et al’s single-instruction zero-shot
    QR [[29](#bib.bib29)] which is also a specific case of GenQREnsemble when N=1'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BM25+RM3 [[1](#bib.bib1)]: BM25 retrieval with RM3 expanded queries (#feedback
    terms=10)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BM25+FlanPRF [[29](#bib.bib29)]: BM25 retrieval with FlanPRF expanded queries'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'With Neural Reranking: Here, we re-evaluate the above settings in conjunction
    with a MonoT5 neural reranker [[24](#bib.bib24)] with all other parameters constant.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BM25+MonoT5: BM25 retrieval using raw queries, re-ranked with MonoT5 model [[24](#bib.bib24)]'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'FlanQR+MonoT5: BM25 retrieval with FlanQR reformulations, re-ranked with MonoT5
    model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BM25+RM3+MonoT5: BM25 retrieval with RM3 expanded queries, re-ranked with MonoT5
    model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BM25+FlanPRF+MonoT5: BM25 retrieval with FlanPRF expanded queries, re-ranked
    with MonoT5 model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 5 Results and Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We now report the results of query reformulation for pre- and post-retrieval
    settings.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Pre-Retrieval Performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c672ba76b441fa26bd0f391ad8073a94.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: nDCG@10 Scores of GenQREnsemble and FlanQR relative to BM25'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first compare the retrieval performances of raw queries and reformulations
    from FlanQR, and GenQREnsemble in Table [1](#S5.T1 "Table 1 ‣ 5.1 Pre-Retrieval
    Performance ‣ 5 Results and Analysis ‣ GenQREnsemble: Zero-Shot LLM Ensemble Prompting
    for Generative Query Reformulation").GenQREnsemble outperforms the raw queries
    as well as generates better reformulations than FlanQR’s reformulated queries
    across all the four benchmarks over a BM25 retriever, indicating the usefulness
    of paraphrasing initial instructions. On TP19, nDCG@10 and MAP improve significantly
    with relative improvements of 18% and 24% respectively. This is further validated
    through the querywise analysis shown in Figure [4](#S5.F4 "Figure 4 ‣ 5.1 Pre-Retrieval
    Performance ‣ 5 Results and Analysis ‣ GenQREnsemble: Zero-Shot LLM Ensemble Prompting
    for Generative Query Reformulation") – Relative to BM25, nDCG@10 scores of GenQREnsemble
    (shown in green) are overall better than FlanQR (shown in blue).GenQREnsemble
    seems more robust too as it avoids drastic degradation in at least 6 queries on
    which FlanQR fails.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Performance of GenQREnsemble on the four benchmarks. $\alpha$) over
    FlanQR. +% indicates % improvements relative to FlanQR (as whole numbers).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Evaluation Set | TREC Passage 19 | TREC Robust 04 | Webis Touche | DBpedia
    Entity |'
  prefs: []
  type: TYPE_TB
- en: '| Setting | nDCG@10 | MAP | MRR | P@10 | nDCG@10 | MRR | nDCG@10 | MAP | MRR
    | nDCG@10 | MAP | MRR |'
  prefs: []
  type: TYPE_TB
- en: '| BM25 | .480 | .286 | .642 | .426 | .434 | .154 | .260 | .206 | .454 | .321
    | .168 | .297 |'
  prefs: []
  type: TYPE_TB
- en: '| FlanQR | .477 | .302 | .593 | .473 | .483 | .151 | .315 | .241 | .511 | .342
    | .196 | .345 |'
  prefs: []
  type: TYPE_TB
- en: '| FlanQR[β=.05] | .511 | .323 | .621 | .469 | .477 | .150 | .276 | .221 | .476
    | .353 | .188 | .339 |'
  prefs: []
  type: TYPE_TB
- en: '| GenQREnsemble | .564^α+18% | .375^α+24% | .706+19% | .500^α+6% | .513^α+6%
    | .159+6% | .317+1% | .257+6% | .555+9% | .374^α+9% | .212^α+8% | .376^α+9% |'
  prefs: []
  type: TYPE_TB
- en: '| GenQREnsemble[β=.05] | .575^α | .377^α | .714 | .502^α | .512^α | .159 |
    .292 | .242 | .489 | .377^α | .212^α | .380^α |'
  prefs: []
  type: TYPE_TB
- en: '| BM25+MonoT5 | .718 | .477 | .881 | .492 | .513 | .173 | .299 | .216 | .525
    | .414 | .249 | .444 |'
  prefs: []
  type: TYPE_TB
- en: '| FlanQR+MonoT5 | .707 | .486 | .847 | .490 | .510 | .170 | .292 | .215 | .530
    | .415 | .255 | .446 |'
  prefs: []
  type: TYPE_TB
- en: '| GenQREnsemble+MonoT5 | .722+2% | .503+3% | .862+2% | .484-1% | .506-1% |
    .170 | .298+3% | .219+2% | .548+3% | .420+1% | .258+1% | .450+1% |'
  prefs: []
  type: TYPE_TB
- en: 'We further look at GenQREnsemble under the neural reranker setting shown at
    the bottom half of Table [1](#S5.T1 "Table 1 ‣ 5.1 Pre-Retrieval Performance ‣
    5 Results and Analysis ‣ GenQREnsemble: Zero-Shot LLM Ensemble Prompting for Generative
    Query Reformulation"). In three of the four settings, viz., TP19, WT, and DE,
    GenQREnsemble is preferable to its zero-shot variant, FlanQR. Evidently, the gains
    of both the zero-shot approaches in the traditional setting are stronger vis-à-vis
    the neural setting. We hypothesize this could be due to GenQREnsemble and FlanQR
    both expanding the query via incorporating semantically similar but lexically
    different keywords. Comparatively, neural models are adept at capturing notions
    of semantic similarity and might benefit less with query expansion. This also
    is in line with Weller et al.’s [[32](#bib.bib32)] recent analysis on the non-ensemble
    variant.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Post-Retrieval Performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 2: Comparison of PRF performance on the TREC 19 Passage Ranking Task'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | With BM25 Retriever | With Neural Reranking |'
  prefs: []
  type: TYPE_TB
- en: '| Setting | nDCG@10 | nDCG@20 | MAP | MRR | nDCG@10 | nDCG@20 | MAP | MRR |'
  prefs: []
  type: TYPE_TB
- en: '| BM25 | .480 | .473 | .286 | .642 | .718 | .696 | .477 | .881 |'
  prefs: []
  type: TYPE_TB
- en: '| RM3 | .504 | .496 | .311 | .595 | .716 | .699 | .480 | .858 |'
  prefs: []
  type: TYPE_TB
- en: '| FlanPRF | .576 | .553 | .363 | .715 | .722 | .703 | .486 | .874 |'
  prefs: []
  type: TYPE_TB
- en: '| GenQREnsembleRF | .585+2% | .560+1% | .373+3% | .753+5% | .729+1% | .706+1%
    | .501+3% | .894+2% |'
  prefs: []
  type: TYPE_TB
- en: '| FlanPRF (Oracle) | .753 | .728 | .501 | .936 | .742 | .734 | .545 | .881
    |'
  prefs: []
  type: TYPE_TB
- en: '| GenQREnsembleRF (Oracle) | .820^α+9% | .773+6% | .545+9% | .977+4% | .756+2%
    | .751+2% | .545 | .897+2% |'
  prefs: []
  type: TYPE_TB
- en: 'We now investigate if GenQREnsembleRF can effectively incorporate PRF in Table [2](#S5.T2
    "Table 2 ‣ 5.2 Post-Retrieval Performance ‣ 5 Results and Analysis ‣ GenQREnsemble:
    Zero-Shot LLM Ensemble Prompting for Generative Query Reformulation"). We find
    that GenQREnsembleRF improves retrieval performance as compared to other PRF approaches
    and is able to incorporate feedback from a BM25 retriever better than RM3 as well
    as its zero-shot counterpart. To assess if GenQREnsembleRF and FlanPRF can at
    all benefit from incorporating relevant documents, we perform oracle testing by
    providing the highest relevant gold documents as context. We find that GenQREnsembleRF
    is able to improve over GenQREnsemble (without feedback) showing that it is able
    to capture context effectively as well as benefit from it. Further, it can incorporate
    relevant feedback better than its single-instruction counterpart FlanPRF. We notice
    improvements even under the neural reranker setting as GenQREnsembleRF outperforms
    RM3 and FlanPRF. Besides, the oracle improvements are higher with only a BM25
    retriever as compared to when a neural reranker is introduced.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2cb66fc4b5e1c5635de275211f665847.png)![Refer to caption](img/c7b952e91fed0614daf1507c00f03228.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Effect of feedback documents under sparse (BM25) and neural (MonoT5)
    rankers'
  prefs: []
  type: TYPE_NORMAL
- en: We further evaluate the effect of varying the number of feedback documents from
    0 to 5\. We notice that resorting to an ensemble approach is highly beneficial.
    In the BM25 setting, the ensemble approach seems always preferable. Under the
    neural reranker setting too,GenQREnsembleRF almost always outperforms FlanPRF.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Zero-shot QR is advantageous since it does not rely on any labeled relevance
    judgements and allows eliciting pre-trained knowledge in the form of keywords
    by prompting the model with the original query and appropriate instruction. By
    introducing GenQREnsemble, we show that zero-shot performance can be further enhanced
    by using multiple views of the initial instruction. We also show that the extension
    GenQREnsembleRF is able to effectively incorporate relevance feedback, either
    automated or from users. While generative QR greatly benefits from our ensemble
    approach, the proposed methods come at a cost of potentially increased latency,
    but this is becoming less problematic with the increased availability of batch
    inference for LLMs. The proposed ensemble approach could also be applied to other
    settings, for example, to address different aspects of queries or metrics to optimize,
    or to better control the generated reformulations.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Abdul-Jaleel, N., Allan, J., Croft, W.B., Diaz, F., Larkey, L., Li, X.,
    Smucker, M.D., Wade, C.: Umass at trec 2004: Novelty and hard. Computer Science
    Department Faculty Publication Series p. 189 (2004)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Alaofi, M., Gallagher, L., Sanderson, M., Scholer, F., Thomas, P.: Can
    generative llms create query variants for test collections? an exploratory study.
    In: Proceedings of the 46th International ACM SIGIR Conference on Research and
    Development in Information Retrieval (SIGIR ’23). pp. 1869–1873\. Association
    for Computing Machinery, New York, NY, USA (2023). https://doi.org/10.1145/3539618.3591960'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Arora, S., Narayan, A., Chen, M.F., Orr, L., Guha, N., Bhatia, K., Chami,
    I., Re, C.: Ask me anything: A simple strategy for prompting language models.
    In: The Eleventh International Conference on Learning Representations (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Bondarenko, A., Fröbe, M., Beloucif, M., Gienapp, L., Ajjour, Y., Panchenko,
    A., Biemann, C., Stein, B., Wachsmuth, H., Potthast, M., et al.: Overview of touché
    2020: argument retrieval. In: Experimental IR Meets Multilinguality, Multimodality,
    and Interaction: 11th International Conference of the CLEF Association, CLEF 2020,
    Thessaloniki, Greece, September 22–25, 2020, Proceedings 11\. pp. 384–395\. Springer
    (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P.,
    Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are
    few-shot learners. Advances in neural information processing systems 33, 1877–1901
    (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Carpineto, C., Romano, G.: A survey of automatic query expansion in information
    retrieval. Acm Computing Surveys (CSUR) 44(1), 1–50 (2012)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Chung, H.W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y.,
    Wang, X., Dehghani, M., Brahma, S., et al.: Scaling instruction-finetuned language
    models. arXiv preprint arXiv:2210.11416 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Dhole, K., Gangal, V., Gehrmann, S., Gupta, A., Li, Z., Mahamood, S., Mahadiran,
    A., Mille, S., Shrivastava, A., Tan, S., et al.: Nl-augmenter: A framework for
    task-sensitive natural language augmentation. Northern European Journal of Language
    Technology 9(1) (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Dhuliawala, S.Z., Komeili, M., Xu, J., Raileanu, R., Li, X., Celikyilmaz,
    A., Weston, J.E.: Chain-of-verification reduces hallucination in large language
    models (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Gao, J., Xie, S., He, X., Ali, A.: Learning lexicon models from search
    logs for query expansion. In: Tsujii, J., Henderson, J., Paşca, M. (eds.) Proceedings
    of the 2012 Joint Conference on Empirical Methods in Natural Language Processing
    and Computational Natural Language Learning. pp. 666–676\. Association for Computational
    Linguistics, Jeju Island, Korea (Jul 2012), [https://aclanthology.org/D12-1061](https://aclanthology.org/D12-1061)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Gao, L., Ma, X., Lin, J., Callan, J.: Precise zero-shot dense retrieval
    without relevance labels. In: Proceedings of the 61st Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers). pp. 1762–1777 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Hasibi, F., Nikolaev, F., Xiong, C., Balog, K., Bratsberg, S.E., Kotov,
    A., Callan, J.: Dbpedia-entity v2: a test collection for entity search. In: Proceedings
    of the 40th International ACM SIGIR Conference on Research and Development in
    Information Retrieval. pp. 1265–1268 (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Hsu, F.D., Taksa, I.: Comparing rank and score combination methods for
    data fusion in information retrieval. Information Retrieval 8(3), 449–480 (2005)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Jagerman, R., Zhuang, H., Qin, Z., Wang, X., Bendersky, M.: Query expansion
    by prompting large language models. arXiv preprint arXiv:2305.03653 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Li, H., Mourad, A., Zhuang, S., Koopman, B., Zuccon, G.: Pseudo relevance
    feedback with deep language models and dense retrievers: Successes and pitfalls.
    ACM Transactions on Information Systems 41(3), 1–40 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Li, Y., Lin, Z., Zhang, S., Fu, Q., Chen, B., Lou, J.G., Chen, W.: Making
    language models better reasoners with step-aware verifier. In: Proceedings of
    the 61st Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers). pp. 5315–5333 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] MacAvaney, S., Yates, A., Feldman, S., Downey, D., Cohan, A., Goharian,
    N.: Simplified data wrangling with ir_datasets. In: Proceedings of the 44th International
    ACM SIGIR Conference on Research and Development in Information Retrieval. pp.
    2429–2436 (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Macdonald, C., Tonellotto, N., MacAvaney, S., Ounis, I.: Pyterrier: Declarative
    experimentation in python from bm25 to dense retrieval. In: Proceedings of the
    30th acm international conference on information & knowledge management. pp. 4526–4533
    (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Mo, F., Mao, K., Zhu, Y., Wu, Y., Huang, K., Nie, J.Y.: Convgqr: Generative
    query reformulation for conversational search. In: Proceedings of the 61st Annual
    Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).
    pp. 4998–5012 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Mohankumar, A.K., Begwani, N., Singh, A.: Diversity driven query rewriting
    in search advertising. In: Proceedings of the 27th ACM SIGKDD Conference on Knowledge
    Discovery & Data Mining. pp. 3423–3431 (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Nguyen, T., Rosenberg, M., Song, X., Gao, J., Tiwary, S., Majumder, R.,
    Deng, L.: Ms marco: A human-generated machine reading comprehension dataset (2016)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Nogueira, R., Lin, J., Epistemic, A.: From doc2query to doctttttquery.
    Online preprint 6(2) (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Peng, B., Li, C., He, P., Galley, M., Gao, J.: Instruction tuning with
    gpt-4\. arXiv preprint arXiv:2304.03277 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Pradeep, R., Nogueira, R., Lin, J.: The expando-mono-duo design pattern
    for text ranking with pretrained sequence-to-sequence models. arXiv preprint arXiv:2101.05667
    (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M.,
    Zhou, Y., Li, W., Liu, P.J.: Exploring the limits of transfer learning with a
    unified text-to-text transformer. Journal of machine learning research 21(140),
    1–67 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Si, L., Lu, J., Callan, J.: Combining multiple resources, evidences and
    criteria for genomic information retrieval. In: TREC (November 2006)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Srivastava, A., Rastogi, A., Rao, A., Shoeb, A.A.M., Abid, A., Fisch,
    A., Brown, A.R., Santoro, A., Gupta, A., Garriga-Alonso, A., et al.: Beyond the
    imitation game: Quantifying and extrapolating the capabilities of language models.
    Transactions on Machine Learning Research (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Thakur, N., Reimers, N., Rücklé, A., Srivastava, A., Gurevych, I.: Beir:
    A heterogeneous benchmark for zero-shot evaluation of information retrieval models.
    In: Thirty-fifth Conference on Neural Information Processing Systems Datasets
    and Benchmarks Track (Round 2) (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Wang, X., MacAvaney, S., Macdonald, C., Ounis, I.: Generative query reformulation
    for effective adhoc search. arXiv preprint arXiv:2308.00415 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Wang, X., Macdonald, C., Tonellotto, N., Ounis, I.: Colbert-prf: Semantic
    pseudo-relevance feedback for dense passage and document retrieval. ACM Transactions
    on the Web 17(1), 1–39 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Wang, X., Wei, J., Schuurmans, D., Le, Q.V., Chi, E.H., Narang, S., Chowdhery,
    A., Zhou, D.: Self-consistency improves chain of thought reasoning in language
    models. In: The Eleventh International Conference on Learning Representations
    (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Weller, O., Lo, K., Wadden, D., Lawrie, D., Van Durme, B., Cohan, A.,
    Soldaini, L.: When do generative query and document expansions fail? a comprehensive
    study across methods, retrievers, and datasets. In: Graham, Y., Purver, M. (eds.)
    Findings of the Association for Computational Linguistics: EACL 2024\. pp. 1987–2003\.
    Association for Computational Linguistics, St. Julian’s, Malta (Mar 2024), [https://aclanthology.org/2024.findings-eacl.134](https://aclanthology.org/2024.findings-eacl.134)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Wiher, G., Meister, C., Cotterell, R.: On decoding strategies for neural
    text generators. Transactions of the Association for Computational Linguistics
    10, 997–1012 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac,
    P., Rault, T., Louf, R., Funtowicz, M., et al.: Transformers: State-of-the-art
    natural language processing. In: Proceedings of the 2020 conference on empirical
    methods in natural language processing: system demonstrations. pp. 38–45 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Yu, H., Xiong, C., Callan, J.: Improving query representations for dense
    retrieval with pseudo relevance feedback. In: Proceedings of the 30th ACM International
    Conference on Information & Knowledge Management. pp. 3592–3596 (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Zhao, Z., Wallace, E., Feng, S., Klein, D., Singh, S.: Calibrate before
    use: Improving few-shot performance of language models. In: International conference
    on machine learning. pp. 12697–12706\. PMLR (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
