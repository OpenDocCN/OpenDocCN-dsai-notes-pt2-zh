- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:49:27'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Can LLMs Effectively Leverage Graph Structural Information: When and Why'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2309.16595](https://ar5iv.labs.arxiv.org/html/2309.16595)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Jin Huang¹ Xingjian Zhang¹ Qiaozhu Mei¹ Jiaqi Ma²
  prefs: []
  type: TYPE_NORMAL
- en: ¹University of Michigan  ²University of Illinois Urbana-Champaign
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This paper studies Large Language Models (LLMs) augmented with structured data–particularly
    graphs–a crucial data modality that remains underexplored in the LLM literature.
    We aim to understand when and why the incorporation of structural information
    inherent in graph data can improve the prediction performance of LLMs on node
    classification tasks with textual features. To address the “when” question, we
    examine a variety of prompting methods for encoding structural information, in
    settings where textual node features are either rich or scarce. For the “why”
    questions, we probe into two potential contributing factors to the LLM performance:
    data leakage and homophily. Our exploration of these questions reveals that (i)
    LLMs can benefit from structural information, especially when textual node features
    are scarce; (ii) there is no substantial evidence indicating that the performance
    of LLMs is significantly attributed to data leakage; and (iii) the performance
    of LLMs on a target node is strongly positively related to the local homophily
    ratio of the node¹¹1Codes and datasets are at: [https://github.com/TRAIS-Lab/LLM-Structured-Data](https://github.com/TRAIS-Lab/LLM-Structured-Data).'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Large Language Models (LLMs) have gained great popularity for a broad range
    of applications (Brown et al., [2020](#bib.bib4); OpenAI, [2023](#bib.bib35)).
    One important reason for their widespread adoption is the ability of an LLM to
    act as a versatile model, capable of solving a variety of tasks in a zero- or
    few-shot fashion. Recently, there is an increasing interest in enhancing the versatility
    of LLMs through multi-modal capabilities (Yin et al., [2023](#bib.bib50); Yang
    et al., [2023](#bib.bib47)). Several modalities, including images (Radford et al.,
    [2021](#bib.bib38)), videos (Li et al., [2023](#bib.bib23)), and even robotics (Brohan
    et al., [2023](#bib.bib3)), have been intensively explored; yet structured data,
    particularly in the form of graphs, remains largely underexplored. This leads
    us to an intriguing question: could the incorporation of structural information
    (such as graphs), when available, improve the predictive accuracy of LLMs?'
  prefs: []
  type: TYPE_NORMAL
- en: Directly answering this question turns to be tricky. Consider citation networks
    as an example, where each node represents a research paper, and each edge indicates
    a citation relationship between papers. While LLMs can make predictions based
    on node-level information alone, such as a paper’s title and abstract, there has
    not been a systematic understanding on whether LLMs can benefit from the neighborhood
    surrounding the target node. A few studies have touched on incorporating structured
    data with LLMs (Wang et al., [2023](#bib.bib44); He et al., [2023](#bib.bib15);
    Chen et al., [2023](#bib.bib7)). A recent work concurrent to this study, Chen
    et al. ([2023](#bib.bib7)), suggests that LLMs can, in some cases, benefit from
    neighborhood information, although the extent of this benefit can be dataset-dependent
    and the underlying mechanisms are not fully understood. Indeed, a notable concern
    arises as most node classification benchmarks have a data cut-off that predates
    the training data cut-off of LLMs like ChatGPT. This discrepancy raises concerns
    about data leakage–LLMs may have seen and memorized at least part of the test
    data of the common benchmark datasets–which could undermine the reliability of
    studies using earlier benchmark datasets.
  prefs: []
  type: TYPE_NORMAL
- en: To this end, this paper focuses on two concrete questions relevant to the incorporation
    of structural information into LLMs. Firstly, we seek to understand the conditions
    under which incorporating structural information improves the prediction accuracy
    of LLMs. Secondly, we examine potential factors contributing to the performance
    of LLMs (either desirable or not), particularly *data leakage* and *homophily* (McPherson
    et al., [2001](#bib.bib30)), the latter being the tendency of nodes with similar
    characteristics to connect. As an early attempt towards these questions, we focus
    on prompting methods for encoding structural information throughout this study,
    and leave the investigation of more advanced methods to future work.
  prefs: []
  type: TYPE_NORMAL
- en: Addressing the first question, we examine various methods to encode structural
    information into prompts, and using ChatGPT API (OpenAI, [2022](#bib.bib34)),
    we test them on node classification datasets with textual features. In particular,
    we transform the textual content of a target node and its neighboring nodes into
    natural language and instruct LLM to make predictions. By varying the richness
    of node-level textual information and the information incorporated from neighboring
    nodes, we reveal the conditions under which LLMs would benefit more from structural
    information.
  prefs: []
  type: TYPE_NORMAL
- en: For the second question, we first investigate the extent to which data leakage
    might artificially inflate the performance of LLMs. To rigorously measure the
    data leakage effect, we collect a new dataset, ensuring that the test nodes are
    sampled from time periods post the data cut-off of ChatGPT. Additionally, we examine
    the impact of homophily on the classification performance of LLMs. Through controlled
    experiments and correlation analyses, we establish a relationship between the
    local homophily ratio and the prediction accuracy of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Our key findings are summarized as follows. (i) LLMs benefit more from structural
    information when textual information of the target node is scarce. (ii) There
    is no strong evidence that data leakage is a major factor contributing to the
    performance of LLMs on node classification benchmark datasets. (iii) Homophily
    in the graph-structured data is a significant contributor to the improved accuracy
    observed in LLMs after incorporating structural information.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, this study marks an early attempt for the ambitious goal of enabling
    LLMs to be effectively augmented with structured data, an important data modality.
    By adapting node classification datasets with textual features, we establish a
    proper testbed for this goal. We have also examined various prompting methods
    for encoding the structural information with deeper understandings of their performance.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Literature
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs for graph learning.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We make a distinction between two lines of research: Using LLMs to solve graph
    learning tasks, and augmenting LLMs with structured data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first line has been examined by a few studies recently. He et al. ([2023](#bib.bib15))
    propose a method where LLMs perform zero-shot predictions along with generating
    explanations for their decisions, which are then used to enhance node features
    for training Message Passing Neural Networks (MPNNs) (Gilmer et al., [2017](#bib.bib11))
    to predict node categories. Chen et al. ([2023](#bib.bib7)) extend the work of
     He et al. ([2023](#bib.bib15)) by using LLMs both as feature enhancers and as
    predictors for node classification. They offer several observations such as Chain-of-thoughts
    is not contributing to performance gains. Wang et al. ([2023](#bib.bib44)) introduce
    NLGraph to benchmark LLMs on traditional graph tasks, while Guo et al. ([2023](#bib.bib12))
    perform an empirical study on using LLMs to solve structure and semantic understanding
    tasks. More recently, Ye et al. ([2023](#bib.bib49)) propose InstructGLM for the
    instruction tuning of LLMs, like LLaMA (Touvron et al., [2023](#bib.bib41)), for
    node classification tasks. One commonality for many of these methods is that they
    use LLMs as a sub-component (e.g., as a feature extractor) of conventional graph
    learning framework. Our study differs with this line of research in terms of the
    motivation: while we are using node classification datasets as a testbed, our
    primary goal is to understand LLMs’ capability of processing the graph modality,
    instead of leveraging LLMs to better solve node classification tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: On the other side, the line of research for augmenting LLMs with structured
    data, which our work belongs to, has also been explored in literature. Works by
    Zhang ([2023](#bib.bib51)) and Jiang et al. ([2023](#bib.bib18)) start to explore
    this space by interfacing LLMs with external tools and enhancing reasoning over
    structured data like knowledge graphs (KGs) or tables. Pan et al. ([2023](#bib.bib36))
    further investigate this by outlining a roadmap for integrating LLMs with KGs.
    However, structured data other than KGs and tables are still underexplored. Despite
    these initial efforts, a comprehensive understanding of the circumstances under
    which LLMs can efficiently leverage structural information in a zero-shot setting
    remains elusive. Our work contributes to this emerging field, seeking to provide
    more insights into the effective integration of LLMs with structured data.
  prefs: []
  type: TYPE_NORMAL
- en: Data leakage in LLMs.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Data leakage in LLMs has become a focal point of discussion due to the models’
    intrinsic ability to memorize training data. As demonstrated by Carlini et al.
    ([2022](#bib.bib6)), LLMs can emit memorized portions of their training data when
    appropriately prompted, a phenomenon that intensifies with increased model capacity
    and training data duplication. While memorization is inherent to their function,
    it raises serious security and privacy concerns. A study by Carlini et al. ([2021](#bib.bib5))
    shows that extraction attacks can recover sensitive information such as personally
    identifiable information (PII) from GPT-2 (Radford et al., [2019](#bib.bib37)).
    This capability to store and potentially leak personal data is further explored
    by Huang et al. ([2022](#bib.bib17)), confirming that although the risk is relatively
    low, there is a tangible potential for information leakage. Specifically,  Carlini
    et al. ([2022](#bib.bib6)) show that the 6 billion parameter GPT-J model (Wang
    & Komatsuzaki, [2021](#bib.bib43)) memorizes at least 1% of its training dataset.
    Furthermore, the issue of data leakage complicates the evaluation of these models.
    As highlighted by Aiyappa et al. ([2023](#bib.bib1)), the closed nature and continuous
    updates of models like ChatGPT make it challenging to prevent data contamination,
    affecting the reliability of evaluation on LLMs in various applications. In node
    classification tasks, a concurrent work by Chen et al. ([2023](#bib.bib7)) observe
    that a specific prompt alteration significantly improved performance on ogbn-arxiv,
    raising concerns about potential test data leakage. In this work, we take a rigorours
    approach by curating a new dataset for node classification tasks, which is explicitly
    designed to address the data leakage issues in existing benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: Homophily in graph learning.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The concept of homophily (McPherson et al., [2001](#bib.bib30)), which describes
    the tendency of nodes to form connections with similar nodes, plays an important
    role in the effectiveness of various graph learning methods (Zhu et al., [2020](#bib.bib52);
    Halcrow et al., [2020](#bib.bib13); Maurya et al., [2021](#bib.bib28); Lim et al.,
    [2021](#bib.bib24)). The principle of homophily enables MPNNs to smooth node representations
    by aggregating features from their likely similarly-labeled neighboring nodes.
    This aggregation process is particularly effective in various types of real-world
    graphs, such as political networks (Knoke, [1990](#bib.bib20)), and citation networks (Ciotti
    et al., [2016](#bib.bib8)). Despite its benefits, the reliance on homophily presents
    a challenge: MPNNs tend to underperform in graphs characterized by heterophily,
    where connected nodes are likely to differ in properties or labels (Zhu et al.,
    [2020](#bib.bib52)). Notably, the impact of homophily on the integration of structured
    data into LLMs remains an open area for exploration.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 When and Why Can LLMs Benefit from Structural Information?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Research Questions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we aim to gain a deeper understanding of two central questions.
    Firstly, under what circumstances can LLMs benefit from structural information
    inherent in the data (the “when” question)? Furthermore, what factors can be attributed
    to LLMs’s performance (the “why” question)? To ground our study, we experiment
    with the ChatGPT API on node classification datasets that have textual node features.
    We also decompose the questions into hypotheses of finer granularity, as described
    below.
  prefs: []
  type: TYPE_NORMAL
- en: The when question.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We hypothesize that the usefulness of structural information for LLMs on a
    node classification task depends on 1) the prompting methods used to encode the
    structural information; and 2) the richness of the textual information of each
    target node. To this end, we explore a variety of prompting methods under two
    distinct settings, one with *rich textual context* and another with *scarce textual
    context*. The detailed experimental design and results are discussed in Section [3.2](#S3.SS2
    "3.2 Influence of Structural Information on LLMs Under Varying Textual Contexts
    ‣ 3 When and Why Can LLMs Benefit from Structural Information? ‣ Can LLMs Effectively
    Leverage Graph Structural Information: When and Why").'
  prefs: []
  type: TYPE_NORMAL
- en: The why question.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Motivated by existing literature in LLM evaluation and graph learning, we hypothesize
    that *data leakage* and *homophily* are two potential contributing factors to
    the LLM performance on node classification tasks. While the latter is acceptable
    and even desirable, the former is not. We investigate the potential impact of
    data leakage in Section [3.3](#S3.SS3 "3.3 Data Leakage as a Potential Contributor
    of Performance ‣ 3 When and Why Can LLMs Benefit from Structural Information?
    ‣ Can LLMs Effectively Leverage Graph Structural Information: When and Why").
    In Section [3.4](#S3.SS4 "3.4 Impact of Homophily on LLMs Classification Accuracy
    ‣ 3 When and Why Can LLMs Benefit from Structural Information? ‣ Can LLMs Effectively
    Leverage Graph Structural Information: When and Why"), we examine the role of
    homophily in the performance of LLMs augmented with structural information.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Influence of Structural Information on LLMs Under Varying Textual Contexts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We study the impact of structural information on LLM predictions across four
    node classification benchmark datasets with textual node features: cora (McCallum
    et al., [2000](#bib.bib29); Lu & Getoor, [2003](#bib.bib26); Sen et al., [2008](#bib.bib39);
    Yang et al., [2016](#bib.bib48)), pubmed (Namata et al., [2012](#bib.bib33); Yang
    et al., [2016](#bib.bib48)), ogbn-arxiv (Hu et al., [2020](#bib.bib16)) and ogbn-product (Hu
    et al., [2020](#bib.bib16))²²2Please see Appendix [B.1](#A2.SS1 "B.1 Datasets
    Statistics and Splits ‣ Appendix B Datasets Information ‣ Can LLMs Effectively
    Leverage Graph Structural Information: When and Why") for the details of the datasets..
    We create prompts that encode both the textual features and the local graph structure
    of a target node in natural language, and then request ChatGPT API³³3We have used
    gpt-3.5-turbo for throughout the experiments. to make predictions for the target
    node. The prompt for each node is formulated in one of several styles, as we introduce
    in details below. Additionally, a fixed dataset-level instruction is attached
    to the prompt when the prompt is sent to the ChatGPT API. The dataset-level instructions
    are listed in Table [6](#A1.T6 "Table 6 ‣ Appendix A Details about Prompting Format
    and Settings ‣ Can LLMs Effectively Leverage Graph Structural Information: When
    and Why") in Appendix [A](#A1 "Appendix A Details about Prompting Format and Settings
    ‣ Can LLMs Effectively Leverage Graph Structural Information: When and Why").'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt styles.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here we introduce the design of prompt styles in our experiments. The exact
    prompt templates can be found in Table [1](#S3.T1 "Table 1 ‣ Prompt styles. ‣
    3.2 Influence of Structural Information on LLMs Under Varying Textual Contexts
    ‣ 3 When and Why Can LLMs Benefit from Structural Information? ‣ Can LLMs Effectively
    Leverage Graph Structural Information: When and Why").'
  prefs: []
  type: TYPE_NORMAL
- en: We first have a few prompt styles that do not encode structural information.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zero-shot: LLMs make zero-shot predictions based on the target node’s textual
    features only.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Few-shot: LLMs make predictions on nodes’ textual features only but with few-shot
    examples from the training set.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zero-shot Chain-of-Thought (CoT): Adding “Let’s think step by step” to the
    end of the zero-shot prompt (Kojima et al., [2022](#bib.bib21)). This simple change
    has been shown to boost LLMs’ performance on various tasks comparable to CoT prompts (Wei
    et al., [2022](#bib.bib46)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then we have two strategies for prompt design conceptually inspired by MPNNs,
    where information from neighboring nodes is aggregated to enhance the representation
    of the target node:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first strategy incorporates randomly selected neighbors into the prompt.
    The idea behind this strategy is to aggregate information from neighboring nodes,
    following the paradigms of GCN (Kipf & Welling, [2016](#bib.bib19)) and GraphSAGE
    (Hamilton et al., [2017](#bib.bib14)). The inclusion of 1-hop neighborhood information
    in the prompt can be seen as an analogous operation to a single-layer aggregation
    in GCN, where messages from direct neighbors are aggregated. Specifically, we
    have two styles:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '$k$-hop title: LLMs make predictions based on the target node’s textual features
    as well as titles of neighbors up to k-hop.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $k$-hop title, we include the labels for neighbors in training set or validation
    set .
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The second strategy is designed to weigh the influence of neighboring nodes
    during the prediction process. This strategy is inspired by Graph Attention Networks
    (GAT) (Veličković et al., [2017](#bib.bib42)), which employ attention mechanisms
    to dynamically allocate weights to neighboring nodes based on their task-specific
    importance. The strategy consists of two steps. a) Attention extraction: the LLM
    ranks neighbors based on their relevance to the target node. b) Attention prediction:
    the LLM makes predictions based on the target node and top-ranked neighbors. We
    name the whole strategy as $k$-hop attention in our experiment results.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Prompt styles and their corresponding templates. For the style “$k$-hop
    attention strategy.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt Style | Prompt Template |'
  prefs: []
  type: TYPE_TB
- en: '| Zero-shot | Abstract: <abstract>\nTitle: <title>\nDo not give any reasoning
    or logic for your answer. \nAnswer: \n\n |'
  prefs: []
  type: TYPE_TB
- en: '| Zero-shot CoT | Abstract: <abstract>\nTitle: <title>\nAnswer: \n\nLet’s think
    step by step. \n |'
  prefs: []
  type: TYPE_TB
- en: '| Few-shot | Abstract: <few-shot abstract>\n… \nAnswer: \n\n<few-shot label>\n…
    (more few-shot examples)\nAbstract: <abstract>… \nAnswer: \n\n |'
  prefs: []
  type: TYPE_TB
- en: '| $k$-hop title+label | Abstract: <abstract>\nTitle: <title>\nIt has following
    neighbor papers at hop 1:\nPaper 1 title: <paper 1 title>\nLabel: <paper 1 label>\n…
    (more 1-hop neighbors)\nIt has following neighbor papers at hop 2:\n… (more 2-hop
    neighbors)\nDo not give any reasoning or logic for your answer. \nAnswer: \n\n
    |'
  prefs: []
  type: TYPE_TB
- en: '| Attention extraction | The paper of interest is <title>. Please return a
    Python list of at most <k> indices of the most related papers among the following
    neighbors, ordered from most related to least related. If there are fewer than
    <k> neighbors, just rank the neighbors by relevance. The list should look like
    this: [1, 2, 3, …]\n1: <neighbor title 1>\n… (more 1-hop neighbors) \n |'
  prefs: []
  type: TYPE_TB
- en: '| Attention prediction | Abstract: <abstract>\nTitle: <title>\nIt has following
    important neighbors, from most related to least related:\n(more neighbors chosen
    by attention)\nDo not give any reasoning or logic for your answer. \nAnswer: \n\n
    |'
  prefs: []
  type: TYPE_TB
- en: Richness of textual node features.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To examine how the richness of the textual node features affects node classification,
    we compare two different settings:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rich textual context. In this setting, the nodes are associated with abundant
    textual features. Specifically, in citation networks (cora, pubmed and ogbn-arxiv),
    both the paper title and abstract are associated with each node as textual features.
    In the co-purchasing network (ogbn-product), both the product title and product
    content are associated with each node as textual features. This setting is adopted
    by several prior studies (Chen et al., [2023](#bib.bib7); Ye et al., [2023](#bib.bib49);
    Guo et al., [2023](#bib.bib12); Wang et al., [2023](#bib.bib44); He et al., [2023](#bib.bib15)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scarce textual context. In this setting, the nodes are associated with limited
    textual features. In citation networks (cora, pubmed and ogbn-arxiv), only the
    paper title is used as textual features. In product networks (ogbn-product), only
    the product name is associated with each node as textual features. While this
    setting is less explored in the literature, it is of great practical importance
    due to the prevalence of short texts in social networks (Alsmadi & Gan, [2019](#bib.bib2)).
    Such limited textual features present challenges like feature sparseness and non-standardization,
    reducing the effectiveness of traditional methods (Song et al., [2014](#bib.bib40)).
    In such scenarios, we expect the structural information becomes more useful for
    the predictions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Experimental results.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The experimental results of different prompting methods under the two settings
    with different richness of textual context are shown in Table [2](#S3.T2 "Table
    2 ‣ Experimental results. ‣ 3.2 Influence of Structural Information on LLMs Under
    Varying Textual Contexts ‣ 3 When and Why Can LLMs Benefit from Structural Information?
    ‣ Can LLMs Effectively Leverage Graph Structural Information: When and Why").
    We have the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incorporating structural information in prompts brings more gain when textual
    information about the target node is limited. In rich textual context, zero-shot
    predictions are very strong baselines because prompts with structural information
    yield marginal gains on ogbn-arxiv, pubmed, and ogbn-product (1.6% average increase).
    This suggests that abundant textual features often suffice for LLMs to make predictions
    even without structural information. However, in scarce textual contexts, LLMs
    gain significantly more improvement in accuracy by incorporating structural information
    compared to rich textual contexts, suggesting that structural information is more
    important when textual information is limited.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Few-shot and zero-shot CoT prompts do not yield significant performance gains.
    Sometimes, they even underperform zero-shot prompts.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In both rich and scarce textual contexts, the difference of performance between
    prompting styles that encode structural information ($k$-hop attention) is minimal.
    This underlines that the availability of textual information is a more critical
    factor of performance than the specific prompting style used.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In conclusion, structural information offers more benefits for node classification
    in scarce textual contexts than in rich textual contexts. Next, we further delve
    into potential factors contributing to the performance of LLMs on node classification
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Node classification accuracy for the ogbn-arxiv, cora, pubmed, and
    ogbn-product datasets. $\uparrow$ denotes the improvements of best prompt style
    that leverages structural information over zero-shot method. Best results are
    in bold.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Textual context | Prompt style | ogbn-arxiv | cora | pubmed | ogbn-product
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Rich | Zero-shot | 74.0 | 66.1 | 88.6 | 83.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Few-shot | 72.9 | 65.1 | 85.0 | 83.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Zero-shot CoT | 71.8 | 56.6 | 81.9 | 80.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 1-hop title+label | 75.1 | 72.5 | 89.1 | 85.2 |'
  prefs: []
  type: TYPE_TB
- en: '| 2-hop title+label | 74.5 | 74.7 | 89.7 | 86.2 |'
  prefs: []
  type: TYPE_TB
- en: '| 1-hop attention | 74.7 | 72.5 | 88.8 | 86.2 |'
  prefs: []
  type: TYPE_TB
- en: '| $\uparrow$ | 1.1 | 8.6 | 1.1 | 2.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Scarce | Zero-shot | 69.8 | 61.8 | 85.7 | 78.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 1-hop title | 72.3 | 69.6 | 84.8 | 80.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 1-hop title+label | 74.3 | 73.9 | 86.4 | 85.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 2-hop title | 71.3 | 69.9 | 86.2 | 80.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 2-hop title+label | 74.2 | 74.5 | 86.9 | 85.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 1-hop attention | 71.3 | 74.7 | 85.1 | 83.9 |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\uparrow$ | 4.5 | 12.9 | 1.2 | 6.9 |'
  prefs: []
  type: TYPE_TB
- en: 3.3 Data Leakage as a Potential Contributor of Performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'While LLMs have achieved decent performance on the node classification tasks,
    there is a risk that the performance of LLMs is artificially inflated by data
    leakage. Note that most node classification benchmark datasets have a data cut-off
    at 2019 (see Table [7](#A2.T7 "Table 7 ‣ B.1 Datasets Statistics and Splits ‣
    Appendix B Datasets Information ‣ Can LLMs Effectively Leverage Graph Structural
    Information: When and Why") in Appendix [B.1](#A2.SS1 "B.1 Datasets Statistics
    and Splits ‣ Appendix B Datasets Information ‣ Can LLMs Effectively Leverage Graph
    Structural Information: When and Why")), and ChatGPT was trained on data up to
    September 2021 (OpenAI, [2023](#bib.bib35)). While the training dataest of ChatGPT
    is not publicly available, given the widespread of these datasets on the internet
    and the enormous training corpus of ChatGPT, it is reasonable to worry about the
    data leakage issue on these datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: To this end, we curate a new node classification dataset, arxiv-2023, which
    is designed to resemble ogbn-arxiv as much as possible except that the test nodes
    are chosen as arXiv Computer Science (CS) papers published in 2023\. With the
    new dataset, we can rigorously investigate the influence of data leakage by comparing
    the LLM performance between arxiv-2023 and ogbn-arxiv.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset collection.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'While, ideally, we should curate the new dataset by simply extending ogbn-arxiv by
    including new papers, this is practically challenging for a couple of reasons.
    In particular, ogbn-arxiv  represents arXiv CS papers in the Microsoft Academic
    Graph (MAG) until 2019 (Hu et al., [2020](#bib.bib16)), where MAG is a heterogeneous
    graph representing scholarly communications (Wang et al., [2020](#bib.bib45)).
    Unfortunately, MAG and its APIs were retired in 2021 and no subsequent data is
    available⁴⁴4[https://www.microsoft.com/en-us/research/project/microsoft-academic-graph/](https://www.microsoft.com/en-us/research/project/microsoft-academic-graph/).
    Furthermore, the pipeline to collect and construct MAG is not publicly released.
    Consequently, we develop our own data collection pipeline to create arxiv-2023.
    Specifically, we first sample test nodes from arXiv CS papers published in 2023,
    and then gather papers within a 2-hop of these test nodes to create a citation
    network. More details about collection can be found in Appendix [B.2](#A2.SS2
    "B.2 Collection of arxiv-2023 ‣ Appendix B Datasets Information ‣ Can LLMs Effectively
    Leverage Graph Structural Information: When and Why").'
  prefs: []
  type: TYPE_NORMAL
- en: Comparison between arxiv-2023 and ogbn-arxiv.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As can be seen in Table [3](#S3.T3 "Table 3 ‣ Comparison between arxiv-2023
    and ogbn-arxiv. ‣ 3.3 Data Leakage as a Potential Contributor of Performance ‣
    3 When and Why Can LLMs Benefit from Structural Information? ‣ Can LLMs Effectively
    Leverage Graph Structural Information: When and Why"), arxiv-2023 and ogbn-arxiv share
    great similarities in their network characteristics, with consistent in-degree/out-degree
    pointing to analogous citation behaviors. arxiv-2023 shows a lower average in-degree
    in the test set, which is likely because the test papers in arxiv-2023 are new
    and have not had much time to accumulate citations. Additionally, Figure [1](#S3.F1
    "Figure 1 ‣ Comparison between arxiv-2023 and ogbn-arxiv. ‣ 3.3 Data Leakage as
    a Potential Contributor of Performance ‣ 3 When and Why Can LLMs Benefit from
    Structural Information? ‣ Can LLMs Effectively Leverage Graph Structural Information:
    When and Why") illustrates that the label distributions of the two datasets are
    comparable. A notable trend from arxiv-2023, in alignment with arXiv statistics⁵⁵5[https://info.arxiv.org/help/stats/2021_by_area/index.html](https://info.arxiv.org/help/stats/2021_by_area/index.html),
    indicates a rise in AI-related categories like ML, LG, CL, reflecting the current
    academic focus.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, we compare the performance of MPNNs on the two datasets. As can
    be seen from the two bottom rows in Table [4](#S3.T4 "Table 4 ‣ LLM performance
    on arxiv-2023 and ogbn-arxiv. ‣ 3.3 Data Leakage as a Potential Contributor of
    Performance ‣ 3 When and Why Can LLMs Benefit from Structural Information? ‣ Can
    LLMs Effectively Leverage Graph Structural Information: When and Why"), we observe
    that the performance metrics for MPNNs (GCN and SAGE) across both datasets are
    closely matched, suggesting that both datasets present comparable challenges for
    classification. For a more comprehensive setting of MPNNs, one can refer to Appendix [C](#A3
    "Appendix C MPNNs as Baselines ‣ Can LLMs Effectively Leverage Graph Structural
    Information: When and Why").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Statistics of ogbn-arxiv and arxiv-2023 datasets. Both represent directed
    citation networks where each node corresponds to a paper published on arXiv and
    each edge indicates one paper citing another. The metrics In-Degree/Out-Degree,
    Average Degree, and Published Year are presented for test nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Full Dataset | Test Set |'
  prefs: []
  type: TYPE_TB
- en: '| Dataset | #Nodes | #Edges | In-Degree/Out-Degree | Average Degree | Published
    Year |'
  prefs: []
  type: TYPE_TB
- en: '| ogbn-arxiv | 169343 | 1166243 | 1.33/11.1 | 12.43 | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| arxiv-2023 | 33868 | 305672 | 0.16/10.6 | 10.76 | 2023 |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/38be075a75b79c7e66e888f3a343439b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Proportional distribution of labels in ogbn-arxiv and arxiv-2023 datasets.
    Each label represents an arXiv Computer Science Category.'
  prefs: []
  type: TYPE_NORMAL
- en: LLM performance on arxiv-2023 and ogbn-arxiv.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If data leakage is a major contributor of performance on ogbn-arxiv, we would
    expect prompting methods based on LLMs to perform worse than MPNNs on arxiv-2023.
    This is because LLMs may benefit from their memory on ogbn-arxiv, but this advantage
    is not likely on arxiv-2023. However, as shown in Table [4](#S3.T4 "Table 4 ‣
    LLM performance on arxiv-2023 and ogbn-arxiv. ‣ 3.3 Data Leakage as a Potential
    Contributor of Performance ‣ 3 When and Why Can LLMs Benefit from Structural Information?
    ‣ Can LLMs Effectively Leverage Graph Structural Information: When and Why"),
    our findings indicate that prompting methods actually exceed MPNNs by 3% in rich
    textual contexts for arxiv-2023, and their accuracy in scarce textual contexts
    is nearly the same. This observation disproves our initial hypothesis that data
    leakage is a significant contributing factor to the LLM performance.'
  prefs: []
  type: TYPE_NORMAL
- en: To conclude, the observed results neither offer clear evidence in favor of data
    leakage nor does it advocate that data leakage predominantly improves LLM’s performance.
    Instead, LLM’s consistent performance across both datasets stresses its resilience
    and ability to generalize across varying distribution domains.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Comparison between LLM’s performance on ogbn-arxiv and arxiv-2023.
    Best results in prompting methods are in bold. 1-hop attention means attention
    extraction and prediction over 1-hop neighbors'
  prefs: []
  type: TYPE_NORMAL
- en: '| Rich context |  |  | Scarce context |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt style | ogbn-arxiv | arxiv-2023 | Prompt style | ogbn-arxiv | arxiv-2023
    |'
  prefs: []
  type: TYPE_TB
- en: '| Zero-shot | 74.0 | 73.5 | Zero-shot | 69.8 | 66.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Few-shot | 72.9 | 73.6 | 1-hop title | 72.3 | 70.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Zero-shot CoT | 71.8 | 73.7 | 1-hop title+label | 74.3 | 70.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 1-hop title+label | 75.1 | 73.8 | 2-hop title | 71.3 | 68.9 |'
  prefs: []
  type: TYPE_TB
- en: '| 2-hop title+label | 74.5 | 73.2 | 2-hop title+label | 74.2 | 68.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 1-hop attention | 74.7 | 73.7 | 1-hop attention | 71.3 | 69.6 |'
  prefs: []
  type: TYPE_TB
- en: '| GCN | 75.4 | 70.3 | GCN | 74.8 | 70.3 |'
  prefs: []
  type: TYPE_TB
- en: '| SAGE | 75.0 | 70.9 | SAGE | 74.4 | 69.1 |'
  prefs: []
  type: TYPE_TB
- en: 3.4 Impact of Homophily on LLMs Classification Accuracy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dc26abebc85aacb116a97925b30aab2e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Performance comparison of dropping neighbors using different strategies
    across arxiv-2023, cora, and ogbn-product datasets. Three dropping strategies
    are evaluated: “drop same” removes neighbors with the same label as the target
    node, “drop different” removes neighbors with different labels as the target node,
    and “drop random” randomly selects neighbors for removal. When percentage is $1$,
    “drop same” strategy drops all same-label neighbors but preserves all different-label
    neighbors, and “drop different” strategy drops all different-label neighbors but
    preserves all same-label neighbors. Details about the strategies are in Appendix [D](#A4.SS0.SSS0.Px1
    "Details about dropping experiments. ‣ Appendix D Additional Analysis for Data
    Leakage ‣ Can LLMs Effectively Leverage Graph Structural Information: When and
    Why")'
  prefs: []
  type: TYPE_NORMAL
- en: Homophily, the tendency of nodes with similar characteristics to connect, is
    foundational for many MPNNs. In fact, the degree of homophily in a dataset often
    correlates with the efficacy of MPNNs in classification tasks (Zhu et al., [2020](#bib.bib52);
    [2021](#bib.bib53); Lim et al., [2021](#bib.bib24); Maurya et al., [2021](#bib.bib28)).
    Given this significance, it becomes imperative to explore if and how homophily
    impacts the efficacy of LLMs in similar classification contexts, drawing potential
    parallels or contrasts with MPNN behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: Since LLM performs node-wise prediction over the neighborhood surrounding the
    target node, we use local homophily ratio (Loveland et al., [2023](#bib.bib25))
    to measure the degree of homophily with respect to the target node. For a prompt
    to predict the category of a target node, the local homophily ratio is defined
    as the fraction of neighbors sharing the same groundtruth label as the target
    node over the total number of neighbors included in the prompt. Intuitively, a
    higher local homophily ratio signals scenarios where a node is surrounded by a
    greater proportion of neighbors from the same category.
  prefs: []
  type: TYPE_NORMAL
- en: The neighbor dropping experiment.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We design a controlled experiment to demonstrate the effect of local homophily
    ratio on prediction accuracy. We gradually drop neighbors in three different ways:
    a) drop the neighbors with same label as the target node; b) drop the neighbors
    with different label as the target node; and c) drop neighbors randomly. We include
    details about the neighbor dropping strategies in Appendix [D](#A4.SS0.SSS0.Px1
    "Details about dropping experiments. ‣ Appendix D Additional Analysis for Data
    Leakage ‣ Can LLMs Effectively Leverage Graph Structural Information: When and
    Why"). The experimental results are shown in Figure [2](#S3.F2 "Figure 2 ‣ 3.4
    Impact of Homophily on LLMs Classification Accuracy ‣ 3 When and Why Can LLMs
    Benefit from Structural Information? ‣ Can LLMs Effectively Leverage Graph Structural
    Information: When and Why"), where we observe an evident trend: as we selectively
    remove neighbors sharing the same labels, there’s a decrease in prediction accuracy.
    Conversely, discarding neighbors with different labels leads to an increase in
    accuracy. This selective dropping inherently modifies the local homophily ratio
    within the prompts. The results show that accuracy of predictions made by LLMs
    is positively related to local homophily ratio.'
  prefs: []
  type: TYPE_NORMAL
- en: Correlation study.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table 5: Point biserial correlation between local homophily ratio and prediction
    correctness across five datasets (p-values in brackets). Point biserial correlation
    ranges between $[-1,1]$, where a value of 1 indicates a perfect positive relationship.
    A higher correlation value indicates that the local homophily ratio and prediction
    correctness are more positively related.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt Style | ogbn-arxiv | cora | pubmed | arxiv-2023 | ogbn-product |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Zero-shot | 0.440 (0.000) | 0.070 (0.106) | 0.278 (0.000) | 0.367 (0.000)
    | 0.387 (0.000) |'
  prefs: []
  type: TYPE_TB
- en: '| 1-hop title+label | 0.518 (0.000) | 0.222 (0.000) | 0.443 (0.000) | 0.481
    (0.000) | 0.560 (0.000) |'
  prefs: []
  type: TYPE_TB
- en: 'Building on the insights from the dropping neighbors experiment, we further
    investigate the relationship between local homophily ratio and the prediction
    correctness across different datasets. Each node possesses two key attributes:
    a) its local homophily ratio, which is a continuous random variable in $[0,1]$,
    and b) its prediction correctness, which is a binary random variable (0 indicating
    an incorrect prediction and 1 indicating a correct prediction). To quantify the
    correlation between these two attributes, we employ the point biserial correlation
    method (Kornbrot, [2014](#bib.bib22)). This correlation coefficient ranges between
    -1 and 1, where a value of 1 signifies a perfect positive relationship. The results
    of our analysis across five datasets are detailed in Table [5](#S3.T5 "Table 5
    ‣ Correlation study. ‣ 3.4 Impact of Homophily on LLMs Classification Accuracy
    ‣ 3 When and Why Can LLMs Benefit from Structural Information? ‣ Can LLMs Effectively
    Leverage Graph Structural Information: When and Why").'
  prefs: []
  type: TYPE_NORMAL
- en: For the cora dataset, we observe no significant correlation when only the title
    is used in prompts. However, a positive correlation emerges when neighbors are
    included alongside the title. This suggests that the more homophily is incorporated
    into the prompt, the more accurate the prediction becomes.
  prefs: []
  type: TYPE_NORMAL
- en: For the other datasets, a positive correlation is evident in both the zero-shot
    and 1-hop title+label settings. However, the strength of this positive correlation
    is amplified when neighborhood information is incorporated, highlighting the significance
    of homophily in prediction correctness. A plausible explanation for the observed
    positive relationship between local homophily ratio and prediction correctness
    in the zero-shot setting, could be the inherent difficulty in predicting nodes
    with lower homophily.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, our findings underline the critical role of homophily in influencing
    LLM’s node classification performance. The experiments and analyses consistently
    point to a positive relationship between local homophily ratio and prediction
    correctness, emphasizing the importance of understanding network structures and
    node relationships in enhancing classification outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Conclusions and Future Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This study marks an early step towards a broader research aim: enabling LLMs
    to process structured data, a crucial data modality commonly seen in practice.
    In this study, we have adapted node classfication datasets with textual features
    from graph learning benchmarks to establish a testbed for LLMs augmented with
    structured data. Our preliminary examination on prompting methods for encoding
    the structural information shows that LLMs benefit more from structural information
    when the textual features of the target node is scarce. We also delve into the
    impact of data leakage and homophily, which provides deeper insights about the
    LLM performance when augmented with graph-structured data.'
  prefs: []
  type: TYPE_NORMAL
- en: This study also opens several avenues for future research. Firstly, the findings
    of this study, as well as the new dataset curated by this work, establish a proper
    benchmark setup for more advanced methods to encode structural information for
    LLMs, such as finetuning or adapter training. Secondly, while we find that data
    leakage is not a major concern for the prompting methods examined in this paper,
    it is still possible that more advanced methods can elicit the memory of the LLMs
    from training corpus. We may need further investigation on the data leakage issue
    when proceeding with evaluating other methods. Finally, the fact that homophily
    plays a crucial role in the performance gain of LLMs with structured data suggests
    that LLMs may be utilizing superficial correlational information to aid the prediction
    tasks. It would be interesting to further investigate whether we can make LLMs
    grasp the deeper relational structure of the graph data.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Aiyappa et al. (2023) Rachith Aiyappa, Jisun An, Haewoon Kwak, and Yong-yeol
    Ahn. Can we trust the evaluation on ChatGPT? In *Proceedings of the 3rd Workshop
    on Trustworthy Natural Language Processing (TrustNLP 2023)*, pp.  47–54, Toronto,
    Canada, July 2023\. Association for Computational Linguistics. doi: 10.18653/v1/2023.trustnlp-1.5.
    URL [https://aclanthology.org/2023.trustnlp-1.5](https://aclanthology.org/2023.trustnlp-1.5).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alsmadi & Gan (2019) Issa Alsmadi and Keng Hoon Gan. Review of short-text classification.
    *International Journal of Web Information Systems*, 15(2):155–182, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Brohan et al. (2023) Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman,
    Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian,
    et al. Do as i can, not as i say: Grounding language in robotic affordances. In
    *Conference on Robot Learning*, pp.  287–318\. PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. Language models are few-shot learners, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carlini et al. (2021) Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew
    Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song,
    Ulfar Erlingsson, et al. Extracting training data from large language models.
    In *30th USENIX Security Symposium (USENIX Security 21)*, pp.  2633–2650, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carlini et al. (2022) Nicholas Carlini, Daphne Ippolito, Matthew Jagielski,
    Katherine Lee, Florian Tramer, and Chiyuan Zhang. Quantifying memorization across
    neural language models. *arXiv preprint arXiv:2202.07646*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2023) Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi
    Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, et al. Exploring the potential
    of large language models (llms) in learning on graphs. *arXiv preprint arXiv:2307.03393*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ciotti et al. (2016) Valerio Ciotti, Moreno Bonaventura, Vincenzo Nicosia, Pietro
    Panzarasa, and Vito Latora. Homophily and missing links in citation networks.
    *EPJ Data Science*, 5:1–14, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clement et al. (2019) Colin B. Clement, Matthew Bierbaum, Kevin P. O’Keeffe,
    and Alexander A. Alemi. On the use of arxiv as a dataset, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fey & Lenssen (2019) Matthias Fey and Jan Eric Lenssen. Fast graph representation
    learning with pytorch geometric. *arXiv preprint arXiv:1903.02428*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gilmer et al. (2017) Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley,
    Oriol Vinyals, and George E. Dahl. Neural message passing for quantum chemistry.
    In Doina Precup and Yee Whye Teh (eds.), *Proceedings of the 34th International
    Conference on Machine Learning*, volume 70 of *Proceedings of Machine Learning
    Research*, pp.  1263–1272\. PMLR, 06–11 Aug 2017. URL [https://proceedings.mlr.press/v70/gilmer17a.html](https://proceedings.mlr.press/v70/gilmer17a.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2023) Jiayan Guo, Lun Du, and Hengyu Liu. Gpt4graph: Can large
    language models understand graph structured data? an empirical evaluation and
    benchmarking. *arXiv preprint arXiv:2305.15066*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Halcrow et al. (2020) Jonathan Halcrow, Alexandru Mosoi, Sam Ruth, and Bryan
    Perozzi. Grale: Designing networks for graph learning. In *Proceedings of the
    26th ACM SIGKDD international conference on knowledge discovery & data mining*,
    pp.  2523–2532, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hamilton et al. (2017) Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive
    representation learning on large graphs. *Advances in neural information processing
    systems*, 30, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2023) Xiaoxin He, Xavier Bresson, Thomas Laurent, and Bryan Hooi.
    Explanations as features: Llm-based features for text-attributed graphs. *arXiv
    preprint arXiv:2305.19523*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2020) Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu
    Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets
    for machine learning on graphs. *Advances in neural information processing systems*,
    33:22118–22133, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2022) Jie Huang, Hanyin Shao, and Kevin Chen-Chuan Chang. Are
    large pre-trained language models leaking your personal information? *arXiv preprint
    arXiv:2205.12628*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (2023) Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin
    Zhao, and Ji-Rong Wen. Structgpt: A general framework for large language model
    to reason over structured data. *arXiv preprint arXiv:2305.09645*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kipf & Welling (2016) Thomas N Kipf and Max Welling. Semi-supervised classification
    with graph convolutional networks. *arXiv preprint arXiv:1609.02907*, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Knoke (1990) David Knoke. *Political networks: the structural perspective*,
    volume 4. Cambridge University Press, 1990.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. *Advances
    in neural information processing systems*, 35:22199–22213, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kornbrot (2014) Diana Kornbrot. Point biserial correlation. *Wiley StatsRef:
    Statistics Reference Online*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023) KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping
    Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding.
    *arXiv preprint arXiv:2305.06355*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lim et al. (2021) Derek Lim, Felix Hohne, Xiuyu Li, Sijia Linda Huang, Vaishnavi
    Gupta, Omkar Bhalerao, and Ser Nam Lim. Large scale learning on non-homophilous
    graphs: New benchmarks and strong simple methods. *Advances in Neural Information
    Processing Systems*, 34:20887–20902, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loveland et al. (2023) Donald Loveland, Jiong Zhu, Mark Heimann, Benjamin Fish,
    Michael T Shaub, and Danai Koutra. On performance discrepancies across local homophily
    levels in graph neural networks. *arXiv preprint arXiv:2306.05557*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lu & Getoor (2003) Qing Lu and Lise Getoor. Link-based classification. In *International
    Conference on Machine Learning (ICML)*, Washington, DC, USA, 2003.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. (2022) Jiaqi Ma, Xingjian Zhang, Hezheng Fan, Jin Huang, Tianyue
    Li, Ting Wei Li, Yiwen Tu, Chenshu Zhu, and Qiaozhu Mei. Graph learning indexer:
    A contributor-friendly and metadata-rich platform for graph learning benchmarks.
    In *Learning on Graphs Conference*, pp.  7–1\. PMLR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maurya et al. (2021) Sunil Kumar Maurya, Xin Liu, and Tsuyoshi Murata. Improving
    graph neural networks with simple architecture design. *arXiv preprint arXiv:2105.07634*,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McCallum et al. (2000) Andrew Kachites McCallum, Kamal Nigam, Jason Rennie,
    and Kristie Seymore. Automating the construction of internet portals with machine
    learning. *Information Retrieval*, 3(2):127–163, 2000.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'McPherson et al. (2001) Miller McPherson, Lynn Smith-Lovin, and James M Cook.
    Birds of a feather: Homophily in social networks. *Annual review of sociology*,
    27(1):415–444, 2001.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mikolov et al. (2013) Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado,
    and Jeff Dean. Distributed representations of words and phrases and their compositionality.
    *Advances in neural information processing systems*, 26, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Miller et al. (2009) Frederic P. Miller, Agnes F. Vandome, and John McBrewster.
    *Levenshtein Distance: Information Theory, Computer Science, String (Computer
    Science), String Metric, Damerau?Levenshtein Distance, Spell Checker, Hamming
    Distance*. Alpha Press, 2009. ISBN 6130216904.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Namata et al. (2012) Galileo Mark Namata, Ben London, Lise Getoor, and Bert
    Huang. Query-driven active surveying for collective classification. In *International
    Workshop on Mining and Learning with Graphs (MLG)*, Edinburgh, Scotland, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2022) OpenAI. Introducing chatgpt, 2022. URL [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. Gpt-4 technical report, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pan et al. (2023) Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang,
    and Xindong Wu. Unifying large language models and knowledge graphs: A roadmap.
    *arXiv preprint arXiv:2306.08302*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners.
    *OpenAI blog*, 1(8):9, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, et al. Learning transferable visual models from natural language supervision.
    In *International conference on machine learning*, pp.  8748–8763\. PMLR, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sen et al. (2008) Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor,
    Brian Galligher, and Tina Eliassi-Rad. Collective classification in network data.
    *AI magazine*, 29(3):93–93, 2008.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Song et al. (2014) Ge Song, Yunming Ye, Xiaolin Du, Xiaohui Huang, and Shifu
    Bie. Short text classification: a survey. *Journal of multimedia*, 9(5), 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Veličković et al. (2017) Petar Veličković, Guillem Cucurull, Arantxa Casanova,
    Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. *arXiv
    preprint arXiv:1710.10903*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang & Komatsuzaki (2021) Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion
    Parameter Autoregressive Language Model. [https://github.com/kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax),
    May 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023) Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang
    Han, and Yulia Tsvetkov. Can language models solve graph problems in natural language?
    *arXiv preprint arXiv:2305.10037*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020) Kuansan Wang, Zhihong Shen, Chiyuan Huang, Chieh-Han Wu,
    Yuxiao Dong, and Anshul Kanakia. Microsoft academic graph: When experts are not
    enough. *Quantitative Science Studies*, 1(1):396–413, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems*, 35:24824–24837, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2023) Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan
    Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react:
    Prompting chatgpt for multimodal reasoning and action. *arXiv preprint arXiv:2303.11381*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2016) Zhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting
    semi-supervised learning with graph embeddings. In *International conference on
    machine learning*, pp.  40–48\. PMLR, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ye et al. (2023) Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, and Yongfeng
    Zhang. Natural language is all a graph needs. *arXiv preprint arXiv:2308.07134*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yin et al. (2023) Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong
    Xu, and Enhong Chen. A survey on multimodal large language models. *arXiv preprint
    arXiv:2306.13549*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang (2023) Jiawei Zhang. Graph-toolformer: To empower llms with graph reasoning
    ability via prompt augmented by chatgpt. *arXiv preprint arXiv:2304.11116*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2020) Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman
    Akoglu, and Danai Koutra. Beyond homophily in graph neural networks: Current limitations
    and effective designs. *Advances in neural information processing systems*, 33:7793–7804,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2021) Jiong Zhu, Ryan A Rossi, Anup Rao, Tung Mai, Nedim Lipka,
    Nesreen K Ahmed, and Danai Koutra. Graph neural networks with heterophily. In
    *Proceedings of the AAAI conference on artificial intelligence*, volume 35, pp. 
    11168–11176, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Details about Prompting Format and Settings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our API call to ChatGPT utilize a two-part prompt structure, in line with the
    ChatGPT Chat Completions API⁶⁶6[https://platform.openai.com/docs/guides/gpt/chat-completions-api](https://platform.openai.com/docs/guides/gpt/chat-completions-api).
    Each API call involves a system prompt and a user prompt. The system prompt, detailed
    in Table [6](#A1.T6 "Table 6 ‣ Appendix A Details about Prompting Format and Settings
    ‣ Can LLMs Effectively Leverage Graph Structural Information: When and Why"),
    sets ChatGPT’s objective and return format. The user prompt, outlined in Table [1](#S3.T1
    "Table 1 ‣ Prompt styles. ‣ 3.2 Influence of Structural Information on LLMs Under
    Varying Textual Contexts ‣ 3 When and Why Can LLMs Benefit from Structural Information?
    ‣ Can LLMs Effectively Leverage Graph Structural Information: When and Why"),
    provides information on the target node and its neighborhood for prediction. To
    standardize ChatGPT’s output format, we append “Do not give any reasoning or logic
    for your answer” to the end of all prompts, except zero-shot CoT prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: System prompts for each dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Dataset | System Prompt |'
  prefs: []
  type: TYPE_TB
- en: '| ogbn-arxiv, arxiv-2023 | Please predict the most appropriate arXiv Computer
    Science (CS) sub-category for the paper. The predicted sub-category should be
    in the format ’cs.XX’. |'
  prefs: []
  type: TYPE_TB
- en: '| cora | Please predict the most appropriate category for the paper. Choose
    from the following categories:\nRule Learning\nNeural Networks\nCase Based\nGenetic
    Algorithms\nTheory\nReinforcement Learning\nProbabilistic Methods\n |'
  prefs: []
  type: TYPE_TB
- en: '| pubmed | Please predict the most likely type of the paper. Your answer should
    be chosen from:\nType 1 diabetes\nType 2 diabetes\nExperimentally induced diabetes.\n
    |'
  prefs: []
  type: TYPE_TB
- en: '| ogbn-product | Please predict the most likely category of this product from
    Amazon. Your answer should be chosen from the list:\nHome & Kitchen\nHealth &
    Personal Care\n… |'
  prefs: []
  type: TYPE_TB
- en: 'We outline the settings for each prompting method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Few-shot: Two correct example predictions from ChatGPT are added before the
    target node information.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Target node with neighbors: For datasets ogbn-arxiv, cora, pubmed and arxiv-2023,
    prompts include up to 20 one-hop and 5 two-hop neighbors. For ogbn-product, up
    to 40 one-hop and 10 two-hop neighbors are included.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Attention extraction: The maximum number of neighbors is the same as Target
    node with neighbors. We only consider one-hop attention in this study, setting
    the attention number $k$ to 5.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Common settings for all methods include a temperature of 0 and a maximum output
    token limit of 500\. If a neighbor belongs to the training or validation set,
    its label is included in the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Datasets Information
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section we detail the information about benchmark datasets and the collection
    pipeline of arxiv-2023.
  prefs: []
  type: TYPE_NORMAL
- en: B.1 Datasets Statistics and Splits
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [7](#A2.T7 "Table 7 ‣ B.1 Datasets Statistics and Splits ‣ Appendix B
    Datasets Information ‣ Can LLMs Effectively Leverage Graph Structural Information:
    When and Why") presents basic statistics for each dataset. For detailed information
    on datasets and methods to obtain raw text attributes, please see Appendix A in
     Chen et al. ([2023](#bib.bib7)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: Statistics of datasets. Data cut-off indicates the latest data coverage
    of the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | #Nodes | #Edges | #Task | Metric | #Test Nodes | Data Cut-Off |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| cora | 2,708 | 5,429 | 7 | Accuracy | 542 | 2000 |'
  prefs: []
  type: TYPE_TB
- en: '| pubmed | 19,717 | 44,338 | 3 | Accuracy | 1,000 | 2000 |'
  prefs: []
  type: TYPE_TB
- en: '| ogbn-arxiv | 169,343 | 1,166,243 | 40 | Accuracy | 1,000 | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| ogbn-product | 2,449,029 | 61,859,140 | 1 | Accuracy | 1,000 | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| arxiv-2023 | 33,868 | 305,672 | 40 | Accuracy | 668 | 2023 |'
  prefs: []
  type: TYPE_TB
- en: 'The dataset splits are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'cora: Training/Validation/Testing ratios are 0.1/0.2/0.2.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'pubmed: Training/Validation/Testing ratios are 0.6/0.2/0.2, following  He et al.
    ([2023](#bib.bib15)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'ogbn-arxiv: Original OGB (Hu et al., [2020](#bib.bib16)) splits are used, categorizing
    papers by their publication year: training (pre-2017), validation (2018), and
    testing (2019).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'ogbn-product: Original OGB splits are used based on sales ranking: top 8% for
    training, next 2% for validation, and the remainder for testing.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'arxiv-2023: Year-based splits similar to ogbn-arxivis adopted: training (pre-2019),
    validation (2020), and testing (2023).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Due to API cost and rate limits, we test on a random sample of 1,000 nodes for
    pubmed, ogbn-arxiv, and ogbn-product, using a fixed seed for reproducibility.
  prefs: []
  type: TYPE_NORMAL
- en: B.2 Collection of arxiv-2023
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The detailed pipeline is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample 668 test nodes from around 46,000 arXiv CS papers published from January
    1 to August 22, 2023.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract references to identify one-hop and two-hop neighbors. References were
    obtained by two steps. First, we search for valid arXiv IDs within each paper,
    following a method similar to  (Clement et al., [2019](#bib.bib9)). Second, we
    use AnyStyle⁷⁷7https://github.com/inukshuk/anystyle to extract the titles of the
    references, which we then search for via the arXiv API⁸⁸8https://info.arxiv.org/help/api/basics.html.
    Titles found on arXiv are considered valid citations if they have a small levenshtein
    distance (Miller et al., [2009](#bib.bib32)) from the searched title. To prevent
    duplicate searches, we skip any references that already have a matched arXiv ID.
    To comply with the arXiv API’s rate limit, each paper is restricted to a maximum
    of 30 searches. For papers published before 2019, we attempt to match them to
    nodes in the ogbn-arxiv based on titles. Unmatched pre-2019 nodes are excluded
    from our dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Construct a citation network using nodes from step 2\. Basically for each node
    we need a list of paper it cites. While references for test nodes and one-hop
    nodes are obtained through both arXiv ID matching and title searching, the references
    for two-hop nodes are solely determined by arXiv ID matching, due to rate limit
    constraints. Dataset statistics are in Table [3](#S3.T3 "Table 3 ‣ Comparison
    between arxiv-2023 and ogbn-arxiv. ‣ 3.3 Data Leakage as a Potential Contributor
    of Performance ‣ 3 When and Why Can LLMs Benefit from Structural Information?
    ‣ Can LLMs Effectively Leverage Graph Structural Information: When and Why").
    We have similar test node degrees between ogbn-arxiv and arxiv-2023.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Appendix C MPNNs as Baselines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Embedding generation.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We adapt the embedding generation pipeline from Hu et al. ([2020](#bib.bib16))
    to train a skip-gram model (Mikolov et al., [2013](#bib.bib31)) on corpus comprising
    titles and abstracts from both ogbn-arxiv and arxiv-2023. Each paper’s 128-dimensional
    feature vector is then obtained by averaging the word embeddings in its title.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tunning.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Baseline models GCN and SAGE are implemented with PyG (Fey & Lenssen, [2019](#bib.bib10)).
    For hyperparameter tunning, we perform a random search on the following hyperparameter
    tuning range for every model following Ma et al. ([2022](#bib.bib27)):'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hidden size: $\{32,64\}$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Learning rate: $\{.001,.005,.01,.1\}$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dropout rate: $\{.2,.4,.6,.8\}$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weight decay: $\{.0001,.001,.01,.1\}$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Each model is run on 100 random configurations and each random configuration
    is run for 3 times on ogbn-arxiv and arxiv-2023. The max training epoch number
    is 2000\. When training is finished, we use the model with highest average validation
    accuracy on the dataset for testing.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Additional Analysis for Data Leakage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Details about dropping experiments.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We have three different strategies: a) drop the neighbors with same label (drop
    same), b) drop the neighbors with different label (drop different), c) drop neighbors
    randomly (drop random). Let’s define $x$, we elaborate on the three strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'drop random: We randomly drop $(x+y)p$ neighbors.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'drop same: We retain $\max(x-(x+y)p,0)$ neighbors with different labels.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'drop different: We retain $\max(y-(x+y)p,0)$ neighbors with same labels.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We further explain this by an example. Assume node $A$ neighbors with same label.
  prefs: []
  type: TYPE_NORMAL
- en: Investigating data leakage through prompt variability.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Chen et al. ([2023](#bib.bib7)) reveal considerable fluctuations in Language
    Model (LLM) performance on ogbn-arxivwhen using three distinct prompt words: ”arXiv
    cs subcategory,” ”arXiv identifier,” and natural language. These variations have
    been interpreted as potential indicators of data leakage.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To delve deeper into this issue, we expand upon their experiments by testing
    additional prompt words. We also introduce two experimental settings: one with
    label options provided and another without. As displayed in Table [8](#A4.T8 "Table
    8 ‣ Investigating data leakage through prompt variability. ‣ Appendix D Additional
    Analysis for Data Leakage ‣ Can LLMs Effectively Leverage Graph Structural Information:
    When and Why"), the relative efficacy of various prompts on ogbn-arxiv mirrors
    their performance on arxiv-2023. Importantly, prompts with options underperform
    on both datasets, underscoring a consistent trend.'
  prefs: []
  type: TYPE_NORMAL
- en: Also, utilizing structural information in the prompts can somewhat mitigate
    the performance drop from less effective prompts. Indicate that LLMs can leverage
    structural information to improve predictions. This further supports that there
    is no conclusive evidence for data leakage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8: Performance across different prompt types between ogbn-arxiv and arxiv-2023.'
  prefs: []
  type: TYPE_NORMAL
- en: '| System Prompt | Zero-shot | 1-hop title+label |'
  prefs: []
  type: TYPE_TB
- en: '| ogbn-arxiv | arxiv-2023 | ogbn-arxiv | arxiv-2023 |'
  prefs: []
  type: TYPE_TB
- en: '| Please predict the most appropriate arXiv Computer Science (CS) sub-category
    for the paper. The predicted sub-category should be in the format ’cs.XX’. | 74.0
    | 73.7 | 74.3 | 70.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Please predict the most appropriate arXiv Computer Science (CS) sub-category
    for the paper. Your answer should be chosen from cs.AI, ..cs.SY. The predicted
    sub-category should be in the format ’cs.XX’. | 66.0 | 68.1 | 70.7 | 67.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Please predict the most appropriate original arXiv identifier for the paper.
    The predicted arxiv identifier should be in the format ’arxiv cs.xx’. | 71.3 |
    70.8 | 73.7 | 67.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Please predict the most appropriate original arXiv identifier for the paper.
    Your answer should be chosen from cs.ai,.. cs.sy. The predicted arxiv identifier
    should be in the format ’arxiv cs.xx’. | 58.4 | 57.2 | 71.7 | 64.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Please predict the most appropriate category for the paper. Your answer should
    be chosen from ”Artificial Intelligence”,.. ”Systems and Control”. | 54.6 | 53.4
    | 74.1 | 67.8 |'
  prefs: []
  type: TYPE_TB
