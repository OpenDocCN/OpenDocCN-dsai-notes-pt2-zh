- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:47:20'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Grounding-Prompter: Prompting LLM with Multimodal Information for Temporal
    Sentence Grounding in Long Videos'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2312.17117](https://ar5iv.labs.arxiv.org/html/2312.17117)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Houlun Chen
  prefs: []
  type: TYPE_NORMAL
- en: Tsinghua University    Xin Wang
  prefs: []
  type: TYPE_NORMAL
- en: Tsinghua University Corresponding author.    Hong Chen
  prefs: []
  type: TYPE_NORMAL
- en: Tsinghua University    Zihan Song
  prefs: []
  type: TYPE_NORMAL
- en: Tsinghua University    Jia Jia^∗
  prefs: []
  type: TYPE_NORMAL
- en: Tsinghua University    Wenwu Zhu^∗
  prefs: []
  type: TYPE_NORMAL
- en: Tsinghua University
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Temporal Sentence Grounding (TSG), which aims to localize moments from videos
    based on the given natural language queries, has attracted widespread attention.
    Existing works are mainly designed for short videos, failing to handle TSG in
    long videos, which poses two challenges: i) complicated contexts in long videos
    require temporal reasoning over longer moment sequences, and ii) multiple modalities
    including textual speech with rich information require special designs for content
    understanding in long videos. To tackle these challenges, in this work we propose
    a Grounding-Prompter method, which is capable of conducting TSG in long videos
    through prompting LLM with multimodal information. In detail, we first transform
    the TSG task and its multimodal inputs including speech and visual, into compressed
    task textualization. Furthermore, to enhance temporal reasoning under complicated
    contexts, a Boundary-Perceptive Prompting strategy is proposed, which contains
    three folds: i) we design a novel multiscale denoising Chain-of-Thought (CoT)
    to combine global and local semantics with noise filtering step by step, ii) we
    set up validity principles capable of constraining LLM to generate reasonable
    predictions following specific formats, and iii) we introduce one-shot In-Context-Learning
    (ICL) to boost reasoning through imitation, enhancing LLM in TSG task understanding.
    Experiments demonstrate the state-of-the-art performance of our Grounding-Prompter
    method, revealing the benefits of prompting LLM with multimodal information for
    TSG in long videos.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b25078f8969407b7d84ae72e71ce5514.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Comparison on technical roadmaps on TSG task.'
  prefs: []
  type: TYPE_NORMAL
- en: Temporal Sentence Grounding (TSG) [[1](#bib.bib1), [16](#bib.bib16)] aims to
    localize a moment from an untrimmed video to match the given query, requiring
    methods to understand the temporal boundaries and contexts across videos and texts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nevertheless, existing literature [[4](#bib.bib4), [16](#bib.bib16), [50](#bib.bib50),
    [14](#bib.bib14), [28](#bib.bib28), [31](#bib.bib31), [27](#bib.bib27), [23](#bib.bib23)]
    are mainly designed for short videos, failing to handle TSG in long videos which
    is very prevalent in practical scenarios covering movies, news, and courses etc.
    However, exploring TSG in long videos encounters the following challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Long videos normally contain complicated contexts, which require temporal reasoning
    over longer moment sequences.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Long videos such as movies may contain multiple modalities including textual
    speech with rich information, requiring special designs for content understanding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'On the one hand, it’s difficult to extend traditional TSG methods [[4](#bib.bib4),
    [16](#bib.bib16), [50](#bib.bib50), [14](#bib.bib14), [28](#bib.bib28), [31](#bib.bib31),
    [27](#bib.bib27), [23](#bib.bib23)] to long videos, as shown in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Grounding-Prompter: Prompting LLM with Multimodal
    Information for Temporal Sentence Grounding in Long Videos")(a). With huge amounts
    of parameters required, they involve high computational costs when fully trained
    on long-video datasets. Besides, traditional TSG methods suffer from fitting bias
    of specific datasets [[22](#bib.bib22), [52](#bib.bib52)], thereby exhibiting
    poor generalization to long moment sequences. Additionally, the incapability of
    capturing rich semantics from textual speeches prevent them from conducting TSG
    on long speech-intensive videos. On the other hand, recent trials on employing
    multimodal large language models for videos (MLLM-V) [[5](#bib.bib5), [55](#bib.bib55),
    [25](#bib.bib25), [32](#bib.bib32)] have sprung up with remarkable performance
    gain across several video tasks. However, these LLM based approaches fail to align
    the TSG task well with LLMs and thus show poor temporal reasoning ability, especially
    in long videos, as shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Grounding-Prompter:
    Prompting LLM with Multimodal Information for Temporal Sentence Grounding in Long
    Videos")(b).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To handle the challenges above, we reformulate TSG into a long-textual task
    and propose to empower large language models (LLMs) with the ability to conduct
    temporal reasoning under complicated context in long videos. Concretely, we propose
    a novel Grounding-Prompter method via prompting LLMs with speech and visual information
    to solve the TSG task, as shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Grounding-Prompter: Prompting LLM with Multimodal Information for Temporal Sentence
    Grounding in Long Videos")(c). First, to align LLMs with the TSG task, we transcribe
    speeches and caption the sparsely sampled frames that align speeches and scenes
    with temporal information in order to obtain compressed task textualization. Additionally,
    to enhance temporal reasoning, we propose a Boundary-Perceptive Prompting strategy,
    which consists of i) a multiscale denoising Chain-of-Thought (CoT) that combines
    global and local semantics with noise filtering step by step, ii) validity principles
    that constrain LLMs to generate reasonable predictions following specific formats,
    and iii) one-shot In-Context-Learning (ICL) that enhances LLMs in TSG understanding
    and temporal reasoning through imitation.'
  prefs: []
  type: TYPE_NORMAL
- en: To verify the superiority of the proposed Grounding-Prompter, we establish a
    VidChapters-mini dataset for experiments from VidChapters-7M [[48](#bib.bib48)].
    Empirical results show that our Grounding-Prompter strategy achieves the state-of-the-art
    performance with great margins compared to other baseline methods. Ablation studies
    further validate the effectiveness of our designs, demonstrating that our Grounding-Prompter
    benefits from both textual speech and visual modalities when handling complicated
    and noisy long contexts around 10k tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'To conclude, our contributions lie in the following folds:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We propose Grounding-Prompter, the first trial to address TSG in long videos
    through LLM, to the best of our knowledge.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We integrate textual speech and visual modalities to LLMs with compressed task
    textualization to handle TSG in long videos, where each modality significantly
    benefits the predictions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We propose a novel Boundary-Perceptive Prompting strategy which enables LLMs
    to conduct temporal reason over time boundaries correctly under complicated and
    noisy long contexts around 10k tokens.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We establish a VidChapters-mini dataset and conduct extensive experiments to
    demonstrate the advantages of Grounding-Prompter over existing baseline methods.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f88e6b504a6f806695f0f518ac593b09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Framework of our Grounding-Prompter. The task inputs are transformed
    into a compressed textualized representation to feed LLM. To enhance the temporal
    perception capability, a boundary-perceptive prompting strategy is proposed. Then,
    the task inputs are rephrased into a fluent prompt under this prompting strategy.
    LLM is activated to regulate its answer into JSON format and the predictions can
    be parsed automatically.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Temporal Sentence Grounding (TSG). Temporal Sentence Grounding [[1](#bib.bib1),
    [16](#bib.bib16)] aims to localize a moment from an untrimmed video that matches
    the given query. Most early TSG literature [[4](#bib.bib4), [16](#bib.bib16),
    [50](#bib.bib50), [54](#bib.bib54), [56](#bib.bib56), [59](#bib.bib59), [26](#bib.bib26),
    [34](#bib.bib34), [51](#bib.bib51), [53](#bib.bib53)] solves it in a supervised
    manner. Since they heavily rely on labor-intensive manual annotations, a few works [[10](#bib.bib10),
    [17](#bib.bib17), [33](#bib.bib33), [42](#bib.bib42), [7](#bib.bib7), [14](#bib.bib14),
    [28](#bib.bib28), [41](#bib.bib41), [31](#bib.bib31)] apply weak-supervised techniques
    on TSG, where locations of ground truth moments are unavailable during the training
    stage [[22](#bib.bib22)]. However, these specialized methods trained on specific
    datasets consume high computational resources and suffer from fitting bias [[22](#bib.bib22),
    [52](#bib.bib52)]. To alleviate these problems, many works [[27](#bib.bib27),
    [30](#bib.bib30), [47](#bib.bib47), [23](#bib.bib23)] resort to pretraining with
    diverse video-language tasks in a more unified framework to boost TSG. However,
    existing TSG methods mainly focus on short videos, leaving TSG in long videos
    largely unexplored, which usually involve temporal reasoning under much more complicated
    contexts.
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, recent TSG works introduce more modalities from videos for better
    localization. A few literatures incorporate modalities, such as optical flows [[1](#bib.bib1),
    [9](#bib.bib9), [29](#bib.bib29)] and audio [[8](#bib.bib8), [30](#bib.bib30),
    [6](#bib.bib6)] besides the RGB frames. However, the speech modality has not been
    comprehensively explored, which sometimes contributes the most to localization,
    especially in long videos like news, courses, *etc*.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, in this paper, we solve TSG in long videos via LLM with both visual
    and speech modalities integrated.
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models (LLM). Recent advancement in Large Language Models (LLM) [[35](#bib.bib35),
    [3](#bib.bib3), [43](#bib.bib43), [11](#bib.bib11), [13](#bib.bib13)] has made
    a great difference for its remarkable abilities in Chain-of-Thought (CoT) [[44](#bib.bib44)],
    In-Context Learning (ICL) [[3](#bib.bib3)], *etc*. CoT decomposes complex tasks
    into a series of intermediate reasoning steps [[49](#bib.bib49)], while ICL encourages
    LLM to learn from analogy [[12](#bib.bib12)], empowering LLM with the capability
    of few-shot learning. For the purpose of handling multimodal tasks with its powerful
    reasoning ability, Multimodal Large Language Models (MLLM) [[49](#bib.bib49),
    [18](#bib.bib18), [58](#bib.bib58)] emerge, which take both multimodal features
    and texts as inputs. Some MLLM for videos (MLLM-V) literature [[5](#bib.bib5),
    [55](#bib.bib55), [25](#bib.bib25), [32](#bib.bib32)] project video feature sequences
    into the token embedding space of LLM to have LLM to understand videos. However,
    they fail to understand the TSG task well and are deficient in temporal perception.
    In contrast, we take an alternative approach in that we textualize video inputs
    with temporal marks and activate LLM to conduct TSG with our Boundary-Perceptive
    Prompting strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Long Video Understanding. Handling long videos imposes substantial demands on
    computational resources and memories. Efficient sparse sampling is often a valid
    mechanism to better cover long video inputs with controllable memory usage. Such
    sampling strategies are mainly based on saliency [[20](#bib.bib20), [57](#bib.bib57)],
    adaptability [[46](#bib.bib46), [15](#bib.bib15)], or stochastics [[38](#bib.bib38)].
    Since there is much more redundancy in long videos, [[2](#bib.bib2)] removes a
    great number of irrelevant video segments via multimodal guidance. Since sampling
    inevitably loses some information, some works [[39](#bib.bib39), [45](#bib.bib45)]
    process several video slices and integrate them for global perception. Besides,
    information compression improves the perception coverage of videos with condensed
    representations. [[40](#bib.bib40)] designs a memory mechanism to combine short
    and long memories for long video question answering. We argue that textualization
    is also an efficient compression of videos in TSG, especially for the visual modality,
    thus making it possible to localize moments by LLM.
  prefs: []
  type: TYPE_NORMAL
- en: '3 Proposed Method: Grounding-Prompter'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We elaborate on the details of our proposed Grounding-Prompter method in this
    section (Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Grounding-Prompter: Prompting
    LLM with Multimodal Information for Temporal Sentence Grounding in Long Videos")).
    After formulating this problem (Section [3.1](#S3.SS1 "3.1 Problem Formulation
    ‣ 3 Proposed Method: Grounding-Prompter ‣ Grounding-Prompter: Prompting LLM with
    Multimodal Information for Temporal Sentence Grounding in Long Videos")), we first
    transform the TSG task and its multimodal inputs into compressed representations
    to feed LLM (Section [3.2](#S3.SS2 "3.2 Compressed Task Textualization ‣ 3 Proposed
    Method: Grounding-Prompter ‣ Grounding-Prompter: Prompting LLM with Multimodal
    Information for Temporal Sentence Grounding in Long Videos")). Then, to further
    activate LLM in temporal reasoning, the Boundary-Perceptive Prompting strategy
    is designed (Section [3.3](#S3.SS3 "3.3 Boundary-Perceptive Prompting ‣ 3 Proposed
    Method: Grounding-Prompter ‣ Grounding-Prompter: Prompting LLM with Multimodal
    Information for Temporal Sentence Grounding in Long Videos")), where we feed LLM
    with additional Multiscale Denoising Chain-of-Thought (CoT) (Section [3.3.1](#S3.SS3.SSS1
    "3.3.1 Multiscale Denoising Chain-of-Thought ‣ 3.3 Boundary-Perceptive Prompting
    ‣ 3 Proposed Method: Grounding-Prompter ‣ Grounding-Prompter: Prompting LLM with
    Multimodal Information for Temporal Sentence Grounding in Long Videos")), Validity
    Principles (Section [3.3.2](#S3.SS3.SSS2 "3.3.2 Validity Principles ‣ 3.3 Boundary-Perceptive
    Prompting ‣ 3 Proposed Method: Grounding-Prompter ‣ Grounding-Prompter: Prompting
    LLM with Multimodal Information for Temporal Sentence Grounding in Long Videos")),
    and One-Shot In-Context-Learning (ICL) (Section [3.3.3](#S3.SS3.SSS3 "3.3.3 One-Shot
    In-Context-Learning ‣ 3.3 Boundary-Perceptive Prompting ‣ 3 Proposed Method: Grounding-Prompter
    ‣ Grounding-Prompter: Prompting LLM with Multimodal Information for Temporal Sentence
    Grounding in Long Videos")). With the above, the LLM gives the prediction via
    step-by-step reasoning.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Problem Formulation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given a video $V$. Usually, $N_{s},N_{c}$.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Compressed Task Textualization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To transform the TSG task and its multimodal inputs into texts that LLM can
    take in, we design the compressed task textualization pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'To align the behaviors of LLM with the TSG task, we explain the meaning of
    TSG and formulate the format of its inputs and outputs to LLM, shown in Section [3.4](#S3.SS4
    "3.4 Prompt Example ‣ 3 Proposed Method: Grounding-Prompter ‣ Grounding-Prompter:
    Prompting LLM with Multimodal Information for Temporal Sentence Grounding in Long
    Videos")(1). Then, to have LLM understand multimodal inputs, shown in Section [3.4](#S3.SS4
    "3.4 Prompt Example ‣ 3 Proposed Method: Grounding-Prompter ‣ Grounding-Prompter:
    Prompting LLM with Multimodal Information for Temporal Sentence Grounding in Long
    Videos")(2-4), we textualize speeches and visual modalities into transcriptions
    and captions with temporal marks. We argue that this textualization preserves
    enough semantics for localization, thereby an efficiently compressed representation
    of long videos, which is verified in our experiments (Section [4](#S4 "4 Experiments
    ‣ Grounding-Prompter: Prompting LLM with Multimodal Information for Temporal Sentence
    Grounding in Long Videos")).'
  prefs: []
  type: TYPE_NORMAL
- en: Speeches are transcribed into non-overlapped sentences that are temporally partitioned
    via Automatic Speech Recognition (ASR). Since huge amounts of frames in long videos
    contain much redundancy, a straightforward but effective sampling strategy is
    adopted, where we pre-detect scene transformations and only sample frames in alignment
    with scenes and speeches. The intermediate frame for each piece of transcriptions
    and each scene is selected, *i.e*. the frame at time ${(t_{s(s)}^{(i)}+t_{s(e)}^{(i)})}/{2}$.
    Captions are only generated on these sampled frames. After that, we feed transcriptions
    and captions with the query text to LLM as the task inputs.
  prefs: []
  type: TYPE_NORMAL
- en: We note that it’s possible that the visual captions could be quite noisy, however,
    the information of the captions is still able to benefit moment localization,
    which is proved in our experiments.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Boundary-Perceptive Prompting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since mainstream generation tasks in LLM hardly consider temporal boundary perception
    under complicated long contexts, it’s difficult for LLM to accurately follow the
    TSG task. Additionally, LLM outputs free-form responses, making it possible to
    generate unreasonable predictions or even fail to provide predictions. To handle
    them, we propose the Boundary-Perceptive Prompting strategy. First, we design
    the Multiscale Denoising Chain-of-Thought to enhance temporal boundary perception
    by reasoning step-by-step under long and noisy contexts. Moreover, several validity
    principles are introduced to regularize the answer of LLM. Besides, we integrate
    one-shot ICL as LLMs are few-shot learners [[3](#bib.bib3)].
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1 Multiscale Denoising Chain-of-Thought
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We decompose our Multiscale Denoising CoT design into the following steps to
    combine global and local semantics with noise resistance when generating predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Global Understanding. We have LLM to summarize the whole video, just
    shown in Section [3.4](#S3.SS4 "3.4 Prompt Example ‣ 3 Proposed Method: Grounding-Prompter
    ‣ Grounding-Prompter: Prompting LLM with Multimodal Information for Temporal Sentence
    Grounding in Long Videos")(5), to filter detailed redundancy and unnecessary repetition
    with global high-level semantics preserved.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Noise Evaluation. Since visual captions usually contain larger amounts
    of noise compared to speech transcriptions, thus, as illustrated in Section [3.4](#S3.SS4
    "3.4 Prompt Example ‣ 3 Proposed Method: Grounding-Prompter ‣ Grounding-Prompter:
    Prompting LLM with Multimodal Information for Temporal Sentence Grounding in Long
    Videos")(6), we prompt LLM this observation and instruct LLM to evaluate how the
    captions assist in moment localization and balance visual-speech information gaps
    adaptively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Partition Understanding. To encourage LLM to understand local details
    in both matched and mismatched moments, as shown in Section [3.4](#S3.SS4 "3.4
    Prompt Example ‣ 3 Proposed Method: Grounding-Prompter ‣ Grounding-Prompter: Prompting
    LLM with Multimodal Information for Temporal Sentence Grounding in Long Videos")(7),
    we have LLM to partition the video by the yet-to-be-predicted start-and-end timestamps
    and summarize each partition with the given query as conditions, to inspire LLM
    to capture differences relevant to query among several parts.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: Prediction. Finally, we have LLM to predict the timestamps $(\hat{t}_{s},\hat{t}_{e})$
    according to the reasoning steps above.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2 Validity Principles
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We set up the following three validity principles to constrain the behaviors
    of LLM and ensure that a reasonable prediction can be obtained.
  prefs: []
  type: TYPE_NORMAL
- en: 'Format Compliance. To better improve the LLM’s compliance with our multiscale
    denoising CoT, we devise an answering template in JSON, whose keys cover the reasoning
    steps in Section [3.3.1](#S3.SS3.SSS1 "3.3.1 Multiscale Denoising Chain-of-Thought
    ‣ 3.3 Boundary-Perceptive Prompting ‣ 3 Proposed Method: Grounding-Prompter ‣
    Grounding-Prompter: Prompting LLM with Multimodal Information for Temporal Sentence
    Grounding in Long Videos"), demonstrated in Section [3.4](#S3.SS4 "3.4 Prompt
    Example ‣ 3 Proposed Method: Grounding-Prompter ‣ Grounding-Prompter: Prompting
    LLM with Multimodal Information for Temporal Sentence Grounding in Long Videos")(8).
    With format compliance principles, the answers of LLM would be parsed automatically.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer Regularization. To avoid improper predictions, as Section [3.4](#S3.SS4
    "3.4 Prompt Example ‣ 3 Proposed Method: Grounding-Prompter ‣ Grounding-Prompter:
    Prompting LLM with Multimodal Information for Temporal Sentence Grounding in Long
    Videos")(9) shows, we inform LLM to regularize its answer. First, there exists
    only one appropriate moment given a query. Furthermore, $\hat{t}_{s}<\hat{t}_{e}$
    must be ensured for each prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Plagiarism Prohibition. To prevent LLM from copying the prediction of the given
    example, therefore, we emphasize that LLM should imitate the format and reasoning
    steps from the example, rather than just copying the prediction, referring to
    Section [3.4](#S3.SS4 "3.4 Prompt Example ‣ 3 Proposed Method: Grounding-Prompter
    ‣ Grounding-Prompter: Prompting LLM with Multimodal Information for Temporal Sentence
    Grounding in Long Videos")(10).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.3 One-Shot In-Context-Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'With just a one-shot example introduced, illustrated in Section [3.4](#S3.SS4
    "3.4 Prompt Example ‣ 3 Proposed Method: Grounding-Prompter ‣ Grounding-Prompter:
    Prompting LLM with Multimodal Information for Temporal Sentence Grounding in Long
    Videos")(11), it is proven to significantly enhance temporal reasoning and format
    compliance. It’s fixed during inference for convenience.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | DiDeMo [[1](#bib.bib1)] | Charades-STA [[16](#bib.bib16)] | ActivityNet
    Captions [[21](#bib.bib21)] | TACoS [[16](#bib.bib16)] | VidChapters-mini |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Ave Duration (min) | 0.49 | 0.51 | 1.96 | 4.78 | 13.96 |'
  prefs: []
  type: TYPE_TB
- en: '| Modalities | V+A | V+A | V+A | V | V+A+S |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Statistical Comparison between VidChapters-mini and other TSG benchmark
    datasets, where V, A, and S denote visual, audio, and speech modalities.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Prompt Example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We present a prompt example in this section to better explain the details of
    our prompt design.
  prefs: []
  type: TYPE_NORMAL
- en: '(1) Task Description & Formulation: You can analyze the correlations between
    a video and query, and locate the video segment that matches the query. You are
    given: (1) Video title (2) Query (3) Speech transcription, with temporal information
    in the format of: [START-TIMESTAMP]-[END-TIMESTAMP]:[TRANSCRIPTION] (4) Visual
    caption, with temporal information in the format of: [TIMESTAMP]:[CAPTION]. You
    should give the answer in [X, Y] format where X, Y are the start and end timestamps
    of the matching segment.'
  prefs: []
  type: TYPE_NORMAL
- en: '(2) Query: Habit 2: Build other people up'
  prefs: []
  type: TYPE_NORMAL
- en: '(3) Speech Transcriptions: 0-7: While watching clips from my last Game of Thrones
    video…'
  prefs: []
  type: TYPE_NORMAL
- en: '(4) Visual Captions: 5: A woman with long blonde hair…'
  prefs: []
  type: TYPE_NORMAL
- en: '(5) Global Understanding: You summarize the video.'
  prefs: []
  type: TYPE_NORMAL
- en: '(6) Noise Evaluation: We note that the visual caption might be quite NOISY.
    Now you comment if the visual captions are helpful enough for localization. You
    can give up information from captions if you think some of them are wrong.'
  prefs: []
  type: TYPE_NORMAL
- en: '(7) Partition Understanding: You analyze the video content before X, between
    X and Y, and after Y, respectively. After that, you give the answer [X, Y].'
  prefs: []
  type: TYPE_NORMAL
- en: '(8) Format Compliance: Please use JSON format of {“summary”:“…”(you summarize
    the whole video),“comment”: “…”(you evaluate effectiveness of visual captions),
    “query”:“…”(the query input), “before X”: “…”(you summarize video before X), “between
    X and Y”: “…”(you summarize video between X and Y), “after Y”: “…”(you summarize
    video after Y), “answer”: [X, Y]}.'
  prefs: []
  type: TYPE_NORMAL
- en: '(9) Answer Regularization: We ensure there does exist ONE moment matching the
    query and X is no more than Y.'
  prefs: []
  type: TYPE_NORMAL
- en: '(10) Plagiarism Prohibition: You MUST NOT just copy the answer given by the
    example! X and Y should be replaced by the real start and end timestamps of the
    moment you find in videos.'
  prefs: []
  type: TYPE_NORMAL
- en: '(11) One-Shot In-Context-Learning: $<$'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Datasets and Settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Datasets. We adopt the VidChapters-7M [[48](#bib.bib48)] dataset for TSG in
    long videos, which contains 817K open-domain YouTube videos with 7M user-annotated
    chapters, featuring longer duration and richer modalities with speeches compared
    with other TSG benchmark datasets. Considering the huge amount of videos with
    various durations in VidChapters-7M, we randomly select 3 chapter annotations
    for each video whose duration is 13-15 minutes in its test split, resulting in
    1830 query-moment pairs in the final test, called VidChapters-mini. Statistical
    comparison is shown in Table [1](#S3.T1 "Table 1 ‣ 3.3.3 One-Shot In-Context-Learning
    ‣ 3.3 Boundary-Perceptive Prompting ‣ 3 Proposed Method: Grounding-Prompter ‣
    Grounding-Prompter: Prompting LLM with Multimodal Information for Temporal Sentence
    Grounding in Long Videos").'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Metrics. We follow the metrics “r@{$m$ seconds from the corresponding
    ground truth start-time. To measure the instruction-following ability of LLM in
    TSG, a new metric, collapse rate “cr”(%), is introduced, which is defined as the
    proportion of predictions failing to generate answers adhering to the prescribed
    format. If a collapse happens, the model is regarded as giving an invalid answer,
    with its IoU being 0.0 and the distance to the ground truth start-time being +inf.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation Details. We apply GPT-3.5-turbo-16k ¹¹1[https://platform.openai.com/docs/models/gpt-3-5](https://platform.openai.com/docs/models/gpt-3-5)
    as the LLM model. The video scenes are detected by PySceneDetect ²²2[https://www.scenedetect.com/](https://www.scenedetect.com/)
    tool based on content transition, and the sampled frames are captioned by BLIP [[24](#bib.bib24)]
    model. For speech transcriptions, we take the Whisper-based [[37](#bib.bib37)]
    tools to finish ASR following [[48](#bib.bib48)]. The temperature of LLM is set
    to 0.0 in all experiments for reproducibility and we set the minimum number of
    sampled frames $\overline{N_{c}}$ to 100.
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | Modalities | r@0.3 | r@0.5 | r@0.7 | r@0.9 | mIoU | r@1s | r@3s
    | r@5s | r@10s | cr |'
  prefs: []
  type: TYPE_TB
- en: '| Random | - | 13.10 | 5.36 | 1.67 | 0.19 | 10.31 | 0.37 | 0.83 | 1.38 | 2.52
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| Complete | - | 12.62 | 3.77 | 1.04 | 0.16 | 15.94 | 10.11 | 10.16 | 10.27
    | 11.04 | - |'
  prefs: []
  type: TYPE_TB
- en: '| CLIP Zero-shot | V | 12.08 | 6.01 | 2.13 | 0.60 | 10.15 | 1.09 | 3.17 | 5.08
    | 7.65 | - |'
  prefs: []
  type: TYPE_TB
- en: '| BERT Zero-shot | S | 15.25 | 6.94 | 2.95 | 0.49 | 12.80 | 0.77 | 2.13 | 3.17
    | 4.59 | - |'
  prefs: []
  type: TYPE_TB
- en: '| VideoChat | V+S | 1.20 | 0.38 | 0.05 | 0.00 | 1.48 | 3.61 | 4.27 | 4.76 |
    6.24 | 43.84 |'
  prefs: []
  type: TYPE_TB
- en: '| Video-ChatGPT | V+S | 1.20 | 0.60 | 0.05 | 0.05 | 1.21 | 2.57 | 3.01 | 3.55
    | 4.32 | 46.72 |'
  prefs: []
  type: TYPE_TB
- en: '| Video-LLaMA | A+V+S | 0.60 | 0.24 | 0.12 | 0.06 | 1.16 | 2.60 | 3.50 | 3.98
    | 5.25 | 52.81 |'
  prefs: []
  type: TYPE_TB
- en: '| M-DETR Zero-shot | V | 14.59 | 6.34 | 2.30 | 0.33 | 11.68 | 1.15 | 2.51 |
    3.28 | 5.08 | - |'
  prefs: []
  type: TYPE_TB
- en: '| M-DETR | V | 49.48 | 36.85 | 26.41 | 10.93 | 25.41 | 10.33 | 15.53 | 18.97
    | 27.99 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Ours | V+S | 34.81 | 22.95 | 14.92 | 6.28 | 26.81 | 17.60 | 25.41 | 32.02
    | 39.73 | 10.27 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Comparison with other technical roadmaps on VidChapters-mini, where
    Moment-DETR (M-DETR) Zero-shot is pretrained with visuals. A, V, and S are short
    for audio, visual, and speech modalities, which are the input modalities from
    videos. The best and second are highlighted by bold and underline.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/df65e980c0cbef8cb4c908ce87e96e18.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e86a7e432c72eba1bbcf23c9e59202ca.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f67b678a7f98fb8819fd568c427659bc.png)'
  prefs: []
  type: TYPE_IMG
- en: (c)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: Comparisons between moment-DETR and its variants, where the green
    and orange bars are overlapped in the bottom to clearly illustrate the performance
    gaps. Moment-DETR still demonstrates comparable performance even some inputs are
    removed, showing that it largely suffers from fitting bias.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Technical Roadmap Baselines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We compare our method with three types of technical baselines: the rule-based,
    Multimodal Large Language Models for Videos (MLLM-V), and state-of-the-art TSG
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Rule-Based. We implement four ones. (i) “Random”: We randomly select a
    pair $(\hat{t}_{s},\hat{t}_{e})$ to 0.05 in accordance with [[48](#bib.bib48)].'
  prefs: []
  type: TYPE_NORMAL
- en: MLLM-V. We compare with three methods, *i.e*. VideoChat [[25](#bib.bib25)],
    Video-ChatGPT [[32](#bib.bib32)], and Video-LLaMA [[55](#bib.bib55)] in this category.
    For a fair comparison, these MLLM-Vs take the whole video and its speech transcriptions
    as input. We sample as many frames as possible from videos to integrate more visual
    information and also feed them with a similar one-shot example if supported by
    the MLLM-V. For automatic purposes, format control is adopted as well in order
    to obtain regularized answers.
  prefs: []
  type: TYPE_NORMAL
- en: TSG Models. Furthermore, we also compare our method with the state-of-the-art
    TSG method, moment-DETR [[23](#bib.bib23)]. We test it in both training-free and
    training-based fashions. In the training-free fashion, we apply the checkpoint
    pretrained on QVHighlights [[23](#bib.bib23)] dataset and transfer it to VidChapters-mini.
    Additionally, in the training-based fashion, a moment-DETR is fully trained on
    the training split of VidChapters-7M. Additionally, we report three other results
    of this trained model, by clearing the query, setting visual features to zero,
    and implementing both of them to check the bias.
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods |  |  | r@0.3 | r@0.5 | r@0.7 | r@0.9 | mIoU | r@1s | r@3s | r@5s
    | r@10s | cr |'
  prefs: []
  type: TYPE_TB
- en: '| Random |  |  | 13.10 | 5.36 | 1.67 | 0.19 | 10.31 | 0.37 | 0.83 | 1.38 |
    2.52 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Complete |  |  | 12.62 | 3.77 | 1.04 | 0.16 | 15.94 | 10.11 | 10.16 | 10.27
    | 11.04 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Ours | CoT | ICL |  |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| ✘ | ✘ | 2.40 | 1.26 | 0.38 | 0.05 | 4.37 | 12.84 | 18.85 | 23.50 | 29.89
    | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '| ✓ | ✘ | 12.73 | 6.99 | 3.33 | 0.98 | 11.56 | 16.39 | 23.61 | 29.62 | 37.27
    | 3.66 |'
  prefs: []
  type: TYPE_TB
- en: '| ✘ | ✓ | 21.69 | 13.06 | 7.32 | 2.24 | 17.24 | 16.39 | 23.44 | 27.6 | 34.32
    | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '| Speech | Visual |  |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| ✓ | ✘ | 28.25 | 18.42 | 11.37 | 4.70 | 21.84 | 16.78 | 23.93 | 29.34 | 36.12
    | 18.03 |'
  prefs: []
  type: TYPE_TB
- en: '| ✘ | ✓ | 20.22 | 10.49 | 5.14 | 1.15 | 15.17 | 5.90 | 10.27 | 12.40 | 15.68
    | 5.08 |'
  prefs: []
  type: TYPE_TB
- en: '| ✓ | ✓ | 34.81 | 22.95 | 14.92 | 6.28 | 26.81 | 17.60 | 25.41 | 32.02 | 39.73
    | 10.27 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Ablation Studies on VidChapters-mini. The best and the second are
    highlighted by bold and underline, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Empirical Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [2](#S4.T2 "Table 2 ‣ 4.1 Datasets and Settings ‣ 4 Experiments ‣ Grounding-Prompter:
    Prompting LLM with Multimodal Information for Temporal Sentence Grounding in Long
    Videos") presents the performance on VidChapters-mini, which indicates that our
    method achieves the state-of-the-art in training-free settings. We could even
    beat the fully-trained moment-DETR on more than half of the metrics, especially
    on r@{$n$}s.'
  prefs: []
  type: TYPE_NORMAL
- en: Rows 5-7 show the MLLM-Vs completely collapse when handling TSG. They fail to
    provide answers that follow specific formats on around half of the samples and
    are even beaten by “random” and “complete” on IoU-based metrics, indicating the
    drawbacks of these MLLM-Vs to understand the TSG task and perceive temporal boundaries
    across multiple modalities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Observing rows 8-12, though moment-DETR achieves the highest performance on
    most IoU-based metrics, however, we observe that the “state-of-the-art” performance
    largely derives from fitting distribution bias of both queries and video moments,
    indicating that it mainly learns spurious correlations other than real semantics,
    as shown in Figure [3](#S4.F3 "Figure 3 ‣ 4.1 Datasets and Settings ‣ 4 Experiments
    ‣ Grounding-Prompter: Prompting LLM with Multimodal Information for Temporal Sentence
    Grounding in Long Videos"). This may explain the reason why moment-DETR only shows
    marginal improvements compared to CLIP and BERT in zero-shot settings. In contrast,
    without training, our method shows a high level of generalization capability.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Ablation Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We conduct the following ablation studies in Table [3](#S4.T3 "Table 3 ‣ 4.2
    Technical Roadmap Baselines ‣ 4 Experiments ‣ Grounding-Prompter: Prompting LLM
    with Multimodal Information for Temporal Sentence Grounding in Long Videos") to
    evaluate the crucial components in our method. For rows 3-5, we remove the three
    steps of CoT in Section [3.3.1](#S3.SS3.SSS1 "3.3.1 Multiscale Denoising Chain-of-Thought
    ‣ 3.3 Boundary-Perceptive Prompting ‣ 3 Proposed Method: Grounding-Prompter ‣
    Grounding-Prompter: Prompting LLM with Multimodal Information for Temporal Sentence
    Grounding in Long Videos") and prompt LLM to merely generate a pair of timestamp
    predictions in column w/o CoT, and instruct LLM to give answers without one-shot
    example in column w/o ICL. For rows 6-7, we remove the speech transcriptions or
    visual captions in both the one-shot example and the test input, to evaluate the
    effectiveness of multimodal information.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.1 Effect of Boundary-Perceptive Prompting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For rows 3-5, our findings indicate the effectiveness of our Boundary-Perception
    Prompting strategy, with around 40- and 125-fold improvements on tighter r@0.7
    and r@0.9 metrics when both CoT and ICL are implemented. In addition, there are
    more in-depth observations.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CoT and ICL mainly boost performance by improving temporal coverage of predictions,
    *i.e*. better predicting the start and end timestamps jointly other than improving
    the accuracy of point localization. LLM is capable of predicting a single timestamp
    without careful prompt designs on r@{$n$} and mIoU without CoT and ICL (row 3),
    indicating both CoT and ICL are indispensable for LLM to truly understand the
    TSG task.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compared to learning from CoT, LLM is better at imitating examples to understand
    this novel and complicated temporal task. The implementation of ICL yields performance
    on r@{$m$}s metrics (rows 4-5).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More reasoning steps activate LLM to understand the novel TSG task but hinder
    format compliance. One-shot ICL activates imitating ability well with significant
    performance advancement, but it seems anti-intuitive that there are more format
    collapses when it’s implemented. Interestingly, it has been observed that LLM
    usually conveys meanings such as “The answer cannot be determined due to the absence
    of any existing matches” when format collapses occur, showing interpretation for
    failure cases to some extent.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.4.2 Effect of Multimodal Information
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Rows 6-8 show our method comprehensively combines speech and visual modalities,
    outperforming itself with great margins when any of them is removed. (i) Speeches
    make more contributions to localization with a 5.13% improvement on r@0.9, and
    performance also improves significantly when captions are added, with a 1.58%
    update on r@0.9\. (ii) The start point prediction accuracy is close to the optimal
    only with ASR, but captions work well in improving the IoU accuracy. (iii) Although
    some key information is lost and much noise is introduced when translating videos
    into sparse captions, LLM still benefits from visual information, showing its
    strong ability to resist noise. (iv) Speeches contribute to more performance improvement
    but also cause a higher likelihood of collapse occurrences. The collapses are
    mitigated when visual captions are integrated, indicating that integrating multimodal
    information is advantageous for the robustness of our method.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Qualitative Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Compared to traditional TSG methods, our method not only provides structural
    predictions, but more explanatory remarks as well. In this section, we conduct
    qualitative analysis on GPT-3.5-turbo-16k.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/90046560fa64bdb8b3f9e3c239a3c92b.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6546b7726c7c20f4859a28c11b3f63d0.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/45c7487660dd9094b009be5252cbfee9.png)'
  prefs: []
  type: TYPE_IMG
- en: (c)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Case demonstration on the multimodal settings. For clarity and brevity,
    we present them in multi-round dialogs to show how we vary the input we feed the
    LLM, but in fact, our method only requires one round interaction with LLM. we
    only demonstrate relevant words with multimodal from LLM’s responses. Orange inputs,
    green outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8a3e144e6853160499a0b64fb9b6894b.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b4ad6cbad8b5cf755665303fba6f04f8.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7a4a2f3eea4bcf6e39b7ee65b1898eb9.png)'
  prefs: []
  type: TYPE_IMG
- en: (c)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: Case demonstration on the prompting strategy, where LLM correctly
    captures the boundaries. Orange inputs, green outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [4](#S4.F4 "Figure 4 ‣ 4.5 Qualitative Analysis ‣ 4 Experiments ‣ Grounding-Prompter:
    Prompting LLM with Multimodal Information for Temporal Sentence Grounding in Long
    Videos") demonstrates how our method benefits from multimodal information. In
    Figure [4](#S4.F4 "Figure 4 ‣ 4.5 Qualitative Analysis ‣ 4 Experiments ‣ Grounding-Prompter:
    Prompting LLM with Multimodal Information for Temporal Sentence Grounding in Long
    Videos")(a), we find our method able to resist noise. LLM properly evaluates the
    noise in visual captions and filters noisy information in captions adaptively,
    thereby giving a more precise prediction. In Figure [4](#S4.F4 "Figure 4 ‣ 4.5
    Qualitative Analysis ‣ 4 Experiments ‣ Grounding-Prompter: Prompting LLM with
    Multimodal Information for Temporal Sentence Grounding in Long Videos")(b), it’s
    observed that our method can refine predictions through valid visual captions.
    LLM concludes that visual captions are beneficial to localization and it refines
    the end-time prediction. In Figure [4](#S4.F4 "Figure 4 ‣ 4.5 Qualitative Analysis
    ‣ 4 Experiments ‣ Grounding-Prompter: Prompting LLM with Multimodal Information
    for Temporal Sentence Grounding in Long Videos")(c), the LLM fails to provide
    answers solely based on speeches. However, a reasonable prediction is obtained
    when incorporating visual captions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate the temporal reasoning ability of our method with the Boundary-Perceptive
    Prompting strategy, a few examples are exhibited in Figure [5](#S4.F5 "Figure
    5 ‣ 4.5 Qualitative Analysis ‣ 4 Experiments ‣ Grounding-Prompter: Prompting LLM
    with Multimodal Information for Temporal Sentence Grounding in Long Videos"),
    showing that our method provides appropriate temporal partitions and correct moment
    summaries correspondingly.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we propose a Grounding-Prompter method to solve the Temporal
    Sentence Grounding (TSG) task in long videos. With the compressed task textualization,
    we effectively activate LLM to understand the TSG task and its multimodal inputs,
    speeches and visual content. After that, we propose the Boundary-Perceptive Prompting
    strategy, which is proven to enhance temporal reasoning and boundary perception
    under complicated and noisy long contexts. Experiments prove that our proposed
    Grounding-Prompter can effectively generate answers with consistent format compliance
    and achieve state-of-the-art performance with great margins compared to other
    baseline methods.
  prefs: []
  type: TYPE_NORMAL
- en: To the best of our knowledge, we are the first to explore LLMs for TSG in long
    videos by reformulating TSG into a long-textual task. With novel prompt designs,
    the competitive performance of our method indicates the potential of LLM to conduct
    temporal video tasks in a novel training-free manner. It will be interesting to
    investigate tuning tailored LLMs for TSG related tasks in future research.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Anne Hendricks et al. [2017] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman,
    Josef Sivic, Trevor Darrell, and Bryan Russell. Localizing moments in video with
    natural language. In *Proceedings of the IEEE international conference on computer
    vision*, pages 5803–5812, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Barrios et al. [2023] Wayner Barrios, Mattia Soldan, Alberto Mario Ceballos-Arroyo,
    Fabian Caba Heilbron, and Bernard Ghanem. Localizing moments in long video via
    multimodal guidance. In *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*, pages 13667–13678, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877–1901, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao et al. [2021] Meng Cao, Long Chen, Mike Zheng Shou, Can Zhang, and Yuexian
    Zou. On pursuit of designing multi-modal transformer for video grounding. In *Proceedings
    of the 2021 Conference on Empirical Methods in Natural Language Processing*, pages
    9810–9823, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2023a] Guo Chen, Yin-Dong Zheng, Jiahao Wang, Jilan Xu, Yifei
    Huang, Junting Pan, Yi Wang, Yali Wang, Yu Qiao, Tong Lu, et al. Videollm: Modeling
    video sequence with large language models. *arXiv preprint arXiv:2305.13292*,
    2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2023b] Houlun Chen, Xin Wang, Xiaohan Lan, Hong Chen, Xuguang
    Duan, Jia Jia, and Wenwu Zhu. Curriculum-listener: Consistency-and complementarity-aware
    audio-enhanced temporal sentence grounding. In *Proceedings of the 31st ACM International
    Conference on Multimedia*, pages 3117–3128, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen and Jiang [2021] Shaoxiang Chen and Yu-Gang Jiang. Towards bridging event
    captioner and sentence localizer for weakly supervised dense event captioning.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    pages 8425–8435, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2020a] Shaoxiang Chen, Wenhao Jiang, Wei Liu, and Yu-Gang Jiang.
    Learning modality interaction for temporal sentence localization and event captioning
    in videos. In *Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK,
    August 23–28, 2020, Proceedings, Part IV 16*, pages 333–351\. Springer, 2020a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2021] Yi-Wen Chen, Yi-Hsuan Tsai, and Ming-Hsuan Yang. End-to-end
    multi-modal video temporal grounding. *Advances in Neural Information Processing
    Systems*, 34:28442–28453, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2020b] Zhenfang Chen, Lin Ma, Wenhan Luo, Peng Tang, and Kwan-Yee K
    Wong. Look closer to ground better: Weakly-supervised temporal grounding of sentence
    in video. *arXiv preprint arXiv:2001.09308*, 2020b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chiang et al. [2023] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez,
    et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.
    *See https://vicuna. lmsys. org (accessed 14 April 2023)*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. [2022] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao
    Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey for in-context learning.
    *arXiv preprint arXiv:2301.00234*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Du et al. [2021] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu,
    Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive
    blank infilling. *arXiv preprint arXiv:2103.10360*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duan et al. [2018] Xuguang Duan, Wenbing Huang, Chuang Gan, Jingdong Wang, Wenwu
    Zhu, and Junzhou Huang. Weakly supervised dense event captioning in videos. *Advances
    in Neural Information Processing Systems*, 31, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fayyaz et al. [2022] Mohsen Fayyaz, Soroush Abbasi Koohpayegani, Farnoush Rezaei
    Jafari, Sunando Sengupta, Hamid Reza Vaezi Joze, Eric Sommerlade, Hamed Pirsiavash,
    and Jürgen Gall. Adaptive token sampling for efficient vision transformers. In
    *European Conference on Computer Vision*, pages 396–414\. Springer, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. [2017] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. Tall:
    Temporal activity localization via language query. In *Proceedings of the IEEE
    international conference on computer vision*, pages 5267–5275, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. [2019] Mingfei Gao, Larry Davis, Richard Socher, and Caiming Xiong.
    Wslln: Weakly supervised natural language localization networks. In *Proceedings
    of the 2019 Conference on Empirical Methods in Natural Language Processing and
    the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*,
    pages 1481–1487, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. [2023] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng,
    Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2:
    Parameter-efficient visual instruction model. *arXiv preprint arXiv:2304.15010*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kenton and Toutanova [2019] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina
    Toutanova. Bert: Pre-training of deep bidirectional transformers for language
    understanding. In *Proceedings of naacL-HLT*, page 2, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Korbar et al. [2019] Bruno Korbar, Du Tran, and Lorenzo Torresani. Scsampler:
    Sampling salient clips from video for efficient action recognition. In *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, pages 6232–6242,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krishna et al. [2017] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei,
    and Juan Carlos Niebles. Dense-captioning events in videos. In *Proceedings of
    the IEEE international conference on computer vision*, pages 706–715, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lan et al. [2023] Xiaohan Lan, Yitian Yuan, Xin Wang, Zhi Wang, and Wenwu Zhu.
    A survey on temporal sentence grounding in videos. *ACM Transactions on Multimedia
    Computing, Communications and Applications*, 19(2):1–33, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lei et al. [2021] Jie Lei, Tamara L Berg, and Mohit Bansal. Detecting moments
    and highlights in videos via natural language queries. *Advances in Neural Information
    Processing Systems*, 34:11846–11858, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2022a] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip:
    Bootstrapping language-image pre-training for unified vision-language understanding
    and generation. In *International Conference on Machine Learning*, pages 12888–12900\.
    PMLR, 2022a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2023] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping
    Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding.
    *arXiv preprint arXiv:2305.06355*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2022b] Mengze Li, Tianbao Wang, Haoyu Zhang, Shengyu Zhang, Zhou
    Zhao, Jiaxu Miao, Wenqiao Zhang, Wenming Tan, Jin Wang, Peng Wang, et al. End-to-end
    modeling via information tree for one-shot natural language spatial video grounding.
    In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 8707–8717, 2022b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. [2023] Kevin Qinghong Lin, Pengchuan Zhang, Joya Chen, Shraman Pramanick,
    Difei Gao, Alex Jinpeng Wang, Rui Yan, and Mike Zheng Shou. Univtg: Towards unified
    video-language temporal grounding. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, pages 2794–2804, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. [2020] Zhijie Lin, Zhou Zhao, Zhu Zhang, Qi Wang, and Huasheng Liu.
    Weakly-supervised video moment retrieval via semantic completion network. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, pages 11539–11546, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2023] Daizong Liu, Xiang Fang, Wei Hu, and Pan Zhou. Exploring optical-flow-guided
    motion and detection-based appearance for temporal sentence grounding. *IEEE Transactions
    on Multimedia*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2022] Ye Liu, Siyuan Li, Yang Wu, Chang-Wen Chen, Ying Shan, and
    Xiaohu Qie. Umt: Unified multi-modal transformers for joint video moment retrieval
    and highlight detection. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, pages 3042–3051, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lv et al. [2023] Zezhong Lv, Bing Su, and Ji-Rong Wen. Counterfactual cross-modality
    reasoning for weakly supervised video moment localization. In *Proceedings of
    the 31st ACM International Conference on Multimedia*, pages 6539–6547, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Maaz et al. [2023] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz
    Khan. Video-chatgpt: Towards detailed video understanding via large vision and
    language models. *arXiv preprint arXiv:2306.05424*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mithun et al. [2019] Niluthpol Chowdhury Mithun, Sujoy Paul, and Amit K Roy-Chowdhury.
    Weakly supervised video moment retrieval from text queries. In *Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 11592–11601,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mun et al. [2020] Jonghwan Mun, Minsu Cho, and Bohyung Han. Local-global video-text
    interactions for temporal grounding. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, pages 10810–10819, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI [2023] OpenAI. Gpt-4 technical report, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, et al. Learning transferable visual models from natural language supervision.
    In *International conference on machine learning*, pages 8748–8763\. PMLR, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. [2023] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine
    McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision.
    In *International Conference on Machine Learning*, pages 28492–28518\. PMLR, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rodriguez et al. [2023] Cristian Rodriguez, Edison Marrese-Taylor, Basura Fernando,
    Hiroya Takamura, and Qi Wu. Memory-efficient temporal moment localization in long
    videos. In *Proceedings of the 17th Conference of the European Chapter of the
    Association for Computational Linguistics*, pages 1901–1916, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Soldan et al. [2022] Mattia Soldan, Alejandro Pardo, Juan León Alcázar, Fabian
    Caba, Chen Zhao, Silvio Giancola, and Bernard Ghanem. Mad: A scalable dataset
    for language grounding in videos from movie audio descriptions. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages
    5026–5035, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Song et al. [2023] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang
    Zhou, Feiyang Wu, Xun Guo, Tian Ye, Yan Lu, Jenq-Neng Hwang, et al. Moviechat:
    From dense token to sparse memory for long video understanding. *arXiv preprint
    arXiv:2307.16449*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song et al. [2020] Yijun Song, Jingwen Wang, Lin Ma, Zhou Yu, and Jun Yu. Weakly-supervised
    multi-level attentional reconstruction network for grounding textual queries in
    videos. *arXiv preprint arXiv:2003.07048*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tan et al. [2021] Reuben Tan, Huijuan Xu, Kate Saenko, and Bryan A Plummer.
    Logan: Latent graph co-attention network for weakly-supervised video moment retrieval.
    In *Proceedings of the IEEE/CVF Winter Conference on Applications of Computer
    Vision*, pages 2083–2092, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems*, 35:24824–24837, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. [2019a] Chao-Yuan Wu, Christoph Feichtenhofer, Haoqi Fan, Kaiming
    He, Philipp Krahenbuhl, and Ross Girshick. Long-term feature banks for detailed
    video understanding. In *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, pages 284–293, 2019a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. [2019b] Zuxuan Wu, Caiming Xiong, Chih-Yao Ma, Richard Socher, and
    Larry S Davis. Adaframe: Adaptive frame selection for fast video recognition.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    pages 1278–1287, 2019b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yan et al. [2023] Shen Yan, Xuehan Xiong, Arsha Nagrani, Anurag Arnab, Zhonghao
    Wang, Weina Ge, David Ross, and Cordelia Schmid. Unloc: A unified framework for
    video localization tasks. In *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*, pages 13623–13633, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. [2023] Antoine Yang, Arsha Nagrani, Ivan Laptev, Josef Sivic, and
    Cordelia Schmid. Vidchapters-7m: Video chapters at scale. *arXiv preprint arXiv:2309.13952*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yin et al. [2023] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong
    Xu, and Enhong Chen. A survey on multimodal large language models. *arXiv preprint
    arXiv:2306.13549*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yuan et al. [2019a] Yitian Yuan, Lin Ma, Jingwen Wang, Wei Liu, and Wenwu Zhu.
    Semantic conditioned dynamic modulation for temporal sentence grounding in videos.
    *Advances in Neural Information Processing Systems*, 32, 2019a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yuan et al. [2019b] Yitian Yuan, Tao Mei, and Wenwu Zhu. To find where you
    talk: Temporal sentence localization in video with attention based location regression.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, pages 9159–9166,
    2019b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yuan et al. [2021] Yitian Yuan, Xiaohan Lan, Xin Wang, Long Chen, Zhi Wang,
    and Wenwu Zhu. A closer look at temporal sentence grounding in videos: Dataset
    and metric. In *Proceedings of the 2nd international workshop on human-centric
    multimedia analysis*, pages 13–21, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zeng et al. [2020] Runhao Zeng, Haoming Xu, Wenbing Huang, Peihao Chen, Mingkui
    Tan, and Chuang Gan. Dense regression network for video grounding. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages
    10287–10296, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2019] Da Zhang, Xiyang Dai, Xin Wang, Yuan-Fang Wang, and Larry S
    Davis. Man: Moment alignment network for natural language moment retrieval via
    iterative graph adjustment. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, pages 1247–1257, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2023] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned
    audio-visual language model for video understanding. *arXiv preprint arXiv:2306.02858*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2020] Songyang Zhang, Houwen Peng, Jianlong Fu, and Jiebo Luo.
    Learning 2d temporal adjacent networks for moment localization with natural language.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, pages 12870–12877,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhi et al. [2021] Yuan Zhi, Zhan Tong, Limin Wang, and Gangshan Wu. Mgsampler:
    An explainable sampling strategy for video action recognition. In *Proceedings
    of the IEEE/CVF International conference on Computer Vision*, pages 1513–1522,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. [2023a] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed
    Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large
    language models. *arXiv preprint arXiv:2304.10592*, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. [2023b] Jiahao Zhu, Daizong Liu, Pan Zhou, Xing Di, Yu Cheng, Song
    Yang, Wenzheng Xu, Zichuan Xu, Yao Wan, Lichao Sun, et al. Rethinking the video
    sampling and reasoning strategies for temporal sentence grounding. *arXiv preprint
    arXiv:2301.00514*, 2023b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
