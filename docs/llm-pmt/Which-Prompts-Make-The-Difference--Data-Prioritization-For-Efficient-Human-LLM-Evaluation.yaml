- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:48:29'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Which Prompts Make The Difference? Data Prioritization For Efficient Human LLM
    Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.14424](https://ar5iv.labs.arxiv.org/html/2310.14424)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: name=Meriem Boubdir    affiliation=Cohere for AI    email=meri.boubdir@gmail.com
       name=Edward Kim    affiliation=Cohere    email=edward@cohere.com    name=Beyza
    Ermis    affiliation=Cohere for AI    email=beyza@cohere.com    name=Marzieh Fadaee
       affiliation=Cohere for AI    email=marzieh@cohere.com    name=Sara Hooker   
    affiliation=Cohere for AI    email=sarahooker@cohere.com
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Human evaluation is increasingly critical for assessing large language models,
    capturing linguistic nuances, and reflecting user preferences more accurately
    than traditional automated metrics. However, the resource-intensive nature of
    this type of annotation process poses significant challenges. The key question
    driving our work: is it feasible to minimize human-in-the-loop feedback by prioritizing
    data instances which most effectively distinguish between models? We evaluate
    several metric-based methods and find that these metrics enhance the efficiency
    of human evaluations by minimizing the number of required annotations, thus saving
    time and cost, while ensuring a robust performance evaluation. We show that our
    method is effective across widely used model families, reducing instances of indecisive
    (or “tie”) outcomes by up to 54% compared to a random sample when focusing on
    the top-20 percentile of prioritized instances. This potential reduction in required
    human effort positions our approach as a valuable strategy in future large language
    model evaluations.'
  prefs: []
  type: TYPE_NORMAL
- en: \pdfcolInitStack
  prefs: []
  type: TYPE_NORMAL
- en: tcb@breakable
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large language models (LLMs) have produced notable breakthroughs in downstream
    performance [[61](#bib.bib61); [11](#bib.bib11); [19](#bib.bib19); [62](#bib.bib62);
    [91](#bib.bib91); [49](#bib.bib49); [8](#bib.bib8); [78](#bib.bib78)], but have
    also introduced new challenges in model evaluation. The success of LLMs has initiated
    a fundamental paradigm shift away from small specialized models designed for single
    tasks to universal models expected to perform well across a wide range of tasks.
    This shift has also posed an existential challenge for evaluation, with a need
    to move away from solely task-specific automatic metrics of evaluation and increasing
    reliance on human evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: While automatic metrics offer a degree of objectivity and reproducibility, alongside
    the benefits of speed and cost-effectiveness, they often fall short in fully capturing
    the complexities and nuances of natural language [[48](#bib.bib48); [68](#bib.bib68)].
    Moreover, automatic metrics often rely on auxiliary models which introduce potential
    points of failure and unexpected challenges over time [[58](#bib.bib58)]. For
    example, reference-based metrics such as BLEU [[54](#bib.bib54)] and ROUGE [[45](#bib.bib45)]
    are usually poor indicators of human judgment, as they emphasize lexical overlap
    and struggle to account for the diverse expressions inherent in semantic representation
     [[34](#bib.bib34); [84](#bib.bib84); [9](#bib.bib9)]. This limitation is particularly
    prominent in open-ended conversations [[47](#bib.bib47)], open-response question
    answering [[41](#bib.bib41)], summarization [[66](#bib.bib66)], and code generation
    [[30](#bib.bib30)], where there isn’t a single reference ground truth for comparison,
    but rather diverse yet equivalent responses.
  prefs: []
  type: TYPE_NORMAL
- en: Human evaluation has emerged as an indispensable method in assessing LLMs’ performance.
    It provides direct feedback on understandability, usability, correctness, and
    safety [[79](#bib.bib79)] from the end-user’s perspective. Furthermore, human
    feedback plays an essential role in aligning model behavior through either supervised
    finetuning on human annotated completions [[3](#bib.bib3); [60](#bib.bib60); [89](#bib.bib89)]
    or reinforcement learning with reward models learned from human preferences  [[92](#bib.bib92);
    [70](#bib.bib70); [2](#bib.bib2)].
  prefs: []
  type: TYPE_NORMAL
- en: However, this growing dependence on human evaluation has introduced certain
    trade-offs. The evaluation process itself is resource-intensive requiring substantial
    investments in human resources and domain-specific expertise [[22](#bib.bib22);
    [6](#bib.bib6)]. Notably, in the context of safety, human annotation can expose
    evaluators to highly toxic content, which carries a significant risk to their
    mental health and leaves them vulnerable to lasting psychological harm [[69](#bib.bib69)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/63930935506fe0de49407d2e4ec5d134.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Evaluation Methodology: Human evaluators assess N prompts, each with
    completions from two different models, A and B (left). Completions are ranked
    by dissimilarity, from the most distinct to potential ties (right). The dashed
    line marks a threshold at ‘k’ evaluations beyond which the likelihood of a tied
    outcome increases, making further evaluations less informative at assessing models
    performance discrepancies.'
  prefs: []
  type: TYPE_NORMAL
- en: In this work, we aim to reduce the number of required human annotations while
    mitigating tie outcomes in a comparison setting, thereby diminishing both time
    and resource expenditures. We ask Which prompts should be prioritized to most
    efficiently differentiate and rank relative model quality amongst a pool of models?
    To achieve this, we introduce prompt prioritization as a strategy to reduce the
    need for extensive human feedback and effectively decrease the time and cost complexities
    associated with the annotation collection process. By informatively mitigating
    tie outcomes in a comparison setting, our aim is to maintain robust performance
    results.
  prefs: []
  type: TYPE_NORMAL
- en: 'We benchmark several metrics, such as Cross-Entropy and KL divergence, to assess
    the dissimilarity between model completions. These metrics, efficient to compute
    as derivatives of a model’s generation probabilities, guide our prompt ranking
    strategy. Across several popular and widely used model families such as Flan-T5
    [[20](#bib.bib20)], Dolly-V2 [[23](#bib.bib23)] families, as well as MPT-7B-instruct
    [[74](#bib.bib74)] and Falcon-7B-instruct [[56](#bib.bib56)], we arrive at consistent
    results. Our contributions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Systematic Offline Ranking: We introduce a systematic, offline ranking method
    to expedite the evaluation process. This approach leverages KL divergence and
    Cross-Entropy to prioritize prompts and completion pairs based on their predicted
    potential to yield decisive preference outcomes.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Reduction in Ties: We show that our use of KL divergence for ranking completion
    pairs results in a remarkable 54% reduction in ties within the top 20% of prioritized
    prompts when compared to random selection. This finding highlights the potential
    of our approach in improving the efficiency of human-in-the-loop evaluations.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Robustness of Elo Scores: We highlight that our ranking strategy enhances the
    stability of Elo scores, a widely used evaluation metric for players in zero-sum
    games, across various model comparisons. This improvement allows us to reduce
    our dependence on extensive human annotations while maintaining reliable and conclusive
    models rankings.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many human evaluation pipelines use a pairwise comparison format where model
    A is compared to model B. Pairwise evaluation is often considered to be less subjective
    and more consistent compared to other evaluation methods because it focuses on
    relative judgments rather than absolute judgments. This involves comparing the
    responses of two models to a single prompt, a strategy referred to as a head-to-head
    pairwise block comparison, dating back to the work of  Thurstone [[77](#bib.bib77)].
    When a comparison for a given prompt favors one completion over the other, we
    gain insight into the relative performance of the two models (e.g., Sample 1 in
    Figure [2](#S2.F2 "Figure 2 ‣ 2 Methodology ‣ Which Prompts Make The Difference?
    Data Prioritization For Efficient Human LLM Evaluation")). If both models yield
    high-quality and equally valid responses for a prompt, discerning the superiority
    of one over the other becomes inherently challenging, as demonstrated in Figure [2](#S2.F2
    "Figure 2 ‣ 2 Methodology ‣ Which Prompts Make The Difference? Data Prioritization
    For Efficient Human LLM Evaluation") (Sample 2). In this work, our objective involves
    strategically selecting prompts that amplify the informativeness of each comparison,
    thereby streamlining and optimizing the evaluation process.
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="S2.F2.1.p1.pic1" class="ltx_picture" height="247.94" overflow="visible"
    version="1.1" width="570"><g transform="translate(0,247.94) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 2.77 9.26)"><foreignobject width="564.46" height="229.42"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Sample
    1: Comparison breaks tie – effective for ranking models Clear preference for completion
    A. This comparison has low likelihood of ties. <svg id="S2.F2.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.pic1"
    class="ltx_picture" height="65.71" overflow="visible" version="1.1" width="603.54"><g
    transform="translate(0,65.71) matrix(1 0 0 -1 0 0) translate(0,3.54)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.76 44.01)"><foreignobject width="556.48" height="12.15" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Prompt</foreignobject></g> <g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.76 13.89)"><foreignobject width="556.48"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Put the concepts
    together to form a sentence: kitchen, sit, table.</foreignobject></g></g></svg>
    <svg id="S2.F2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.p2.pic1" class="ltx_picture"
    height="63.17" overflow="visible" version="1.1" width="603.54"><g transform="translate(0,63.17)
    matrix(1 0 0 -1 0 0) translate(0,3.54)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.76 41.32)"><foreignobject
    width="556.48" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Completion
    A</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0
    21.76 13.89)"><foreignobject width="556.48" height="9.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">A man sits at a table in a kitchen.</foreignobject></g></g></svg>
    <svg id="S2.F2.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.p3.pic1" class="ltx_picture"
    height="63.17" overflow="visible" version="1.1" width="603.54"><g transform="translate(0,63.17)
    matrix(1 0 0 -1 0 0) translate(0,3.54)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.76 41.32)"><foreignobject
    width="556.48" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Completion
    B</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0
    21.76 13.89)"><foreignobject width="556.48" height="9.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">The kitchen sits at the table.</foreignobject></g></g></svg></foreignobject></g></g></svg><svg
    id="S2.F2.2.p1.pic1" class="ltx_picture" height="253.32" overflow="visible" version="1.1"
    width="570"><g transform="translate(0,253.32) matrix(1 0 0 -1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 2.77 9.26)"><foreignobject width="564.46" height="234.8" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Sample 2: Comparison results
    in a tie – not effective for ranking models Both completions present equivalently
    suitable coherent and rational responses. <svg id="S2.F2.2.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.pic1"
    class="ltx_picture" height="65.71" overflow="visible" version="1.1" width="603.54"><g
    transform="translate(0,65.71) matrix(1 0 0 -1 0 0) translate(0,3.54)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.76 44.01)"><foreignobject width="556.48" height="12.15" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Prompt</foreignobject></g> <g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.76 13.89)"><foreignobject width="556.48"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Put the concepts
    together to form a sentence: jump, pool, swimsuit.</foreignobject></g></g></svg>
    <svg id="S2.F2.2.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.p2.pic1" class="ltx_picture"
    height="65.86" overflow="visible" version="1.1" width="603.54"><g transform="translate(0,65.86)
    matrix(1 0 0 -1 0 0) translate(0,3.54)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.76 44.01)"><foreignobject
    width="556.48" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Completion
    A</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0
    21.76 13.89)"><foreignobject width="556.48" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">A girl in a swimsuit jumps into a pool.</foreignobject></g></g></svg>
    <svg id="S2.F2.2.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.p3.pic1" class="ltx_picture"
    height="65.86" overflow="visible" version="1.1" width="603.54"><g transform="translate(0,65.86)
    matrix(1 0 0 -1 0 0) translate(0,3.54)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.76 44.01)"><foreignobject
    width="556.48" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Completion
    B</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0
    21.76 13.89)"><foreignobject width="556.48" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Jumping into a swimming pool without a swimsuit
    is a bad idea.</foreignobject></g></g></svg></foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Comparison of a prompt, used to rank completions from models A and
    B, Left: which is strong at distinguishing which model is better (low likelihood
    of ties in human eval), Right: which is weak at distinguishing which model is
    better (high likelihood of ties in human eval). Our goal is to automatically identify
    and rank prompts which are similar to sample 1 higher than the ones similar to
    sample 2.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally, let us consider an evaluation set $P=\{p_{i}\}_{i=1}^{N}$, defined
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\begin{cases}Score_{i}^{A}=1\text{ and }Score_{i}^{B}=0&amp;\text{if
    $c_{i}^{A}$ is preferred}\\ Score_{i}^{A}=0\text{ and }Score_{i}^{B}=1&amp;\text{if
    $c_{i}^{B}$ is preferred}\\'
  prefs: []
  type: TYPE_NORMAL
- en: Score_{i}^{A}=Score_{i}^{B}=1&amp;\text{if both are of similar quality}\\
  prefs: []
  type: TYPE_NORMAL
- en: \end{cases}$$ |  | (1) |
  prefs: []
  type: TYPE_NORMAL
- en: We propose an offline approach that automatically ranks prompts in $P$ using
    model completions. While we assess responses from two models, our primary focus
    is on highlighting their dissimilarity. Although widely used, conventional string
    matching techniques such as BLEU and ROUGE are not well-suited for this problem.
    These metrics may indicate a high lexical overlap between two completions, but
    they might differ significantly in meaning or quality. Coversely, completions
    may exhibit minimal lexical overlap while delivering equally valid answers to
    the same prompt (see Figure [2](#S2.F2 "Figure 2 ‣ 2 Methodology ‣ Which Prompts
    Make The Difference? Data Prioritization For Efficient Human LLM Evaluation")).
    Such a focus is instrumental in minimizing tie outcomes in human evaluations,
    which can arise when both completions are viewed by annotators as similarly good
    or bad.
  prefs: []
  type: TYPE_NORMAL
- en: Our goal is to rearrange the prompts and present them to the annotators in an
    order that starts with a low likelihood of a tie outcome and progresses to a higher
    likelihood. To achieve this, we reorder the prompt and completion pairs within
    an evaluation set $P$ (see Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Which
    Prompts Make The Difference? Data Prioritization For Efficient Human LLM Evaluation")).
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Quantifying the "A vs. B" dissimilarity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To enhance our evaluation process effectively, it is essential to understand
    how two model outputs diverge when given the same prompt. This section outlines
    our strategy to systematically prioritize prompts that tend to diminish tie outcomes
    in the top percentile, while simultaneously maximizing them in the bottom percentile.
  prefs: []
  type: TYPE_NORMAL
- en: In this context, we quantify the variation between the responses of different
    models $\{c_{i}^{M_{1}},...,c_{i}^{M_{n}}\}$ is the number of models within our
    evaluation pool.
  prefs: []
  type: TYPE_NORMAL
- en: To achieve this, we investigate scoring metrics that utilize the log probabilities
    or probabilities of the completions, as computed by their respective generating
    models. This serves as an efficient proxy metric for quality and has been demonstrated
    to be more calibrated as an indicator of model certainty at larger model sizes
    [[16](#bib.bib16)].
  prefs: []
  type: TYPE_NORMAL
- en: Let $S^{A}=(s_{1}^{A},s_{2}^{A},\ldots,s_{t}^{A})$. To accomplish our objective,
    we employ both KL Divergence (Equation [2](#S2.E2 "Equation 2 ‣ 2.1 Quantifying
    the "A vs. B" dissimilarity ‣ 2 Methodology ‣ Which Prompts Make The Difference?
    Data Prioritization For Efficient Human LLM Evaluation")) and Cross-Entropy (Equation [3](#S2.E3
    "Equation 3 ‣ 2.1 Quantifying the "A vs. B" dissimilarity ‣ 2 Methodology ‣ Which
    Prompts Make The Difference? Data Prioritization For Efficient Human LLM Evaluation"))
    as thesemetrics quantify the difference between two probability distributions.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $KL=\sum_{i}\mathbf{p}_{i}^{A}\log\frac{\mathbf{p}_{i}^{A}}{\mathbf{p}_{i}^{B}}$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $CE=-\sum_{i}\mathbf{p}_{i}^{A}\log\mathbf{p}_{i}^{B}$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: KL Divergence measures how one probability distribution diverges from a second,
    expected distribution, making it useful for capturing differences between model
    completions, $S^{A}$. We normalize each sequence by the sum of its probabilities
    before calculating the divergence. Cross-Entropy indicates the dissimilarity between
    the two distributions, serving as an indicator of the model’s relative performance.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have model completions for each model in $\mathbb{M}$ respectively.
    The prompts in each ordered set span from pairs with the most dissimilar completions
    to those with the least dissimilar completions according to the metric. We anticipate
    that this prompt ranking will lead to a more efficient differentiation of model
    performance during human evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Applying our ranking in Elo rating computation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Originally introduced by  Elo [[28](#bib.bib28)] for ranking chess players,
    the Elo rating system has emerged as a pivotal tool in evaluating and comparing
    models performance, particularly in pairwise preference-based setups  [[70](#bib.bib70);
    [1](#bib.bib1); [50](#bib.bib50); [2](#bib.bib2); [51](#bib.bib51)]. By computing
    Elo ratings for each model within a pool, we can establish a global ranking tier
    based solely on pairwise comparisons. In this section, we explore how our proposed
    evaluation methodology can benefit the convergence and robustness of Elo-based
    evaluations, especially when dealing with limited resources.
  prefs: []
  type: TYPE_NORMAL
- en: The appeal of the Elo rating system in evaluating LLMs lies in its ability to
    effectively capture the performance dynamics within a constantly changing pool
    of models. Furthermore, it provides an intuitive interpretation of the ratings
    as a measure of expected performance in a direct head-to-head comparison of any
    two models.
  prefs: []
  type: TYPE_NORMAL
- en: 'By definition, the expected Elo scores for each model are calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle E_{\mathsf{A}}$ |  | (4a) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle E_{\mathsf{B}}$ |  | (4b) |'
  prefs: []
  type: TYPE_TB
- en: 'Here $R_{\mathsf{A}}$ is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $R_{\mathsf{A}}^{\prime}=R_{\mathsf{A}}+K\cdot(S_{\mathsf{A}}-E_{\mathsf{A}})$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: In this equation, $R_{\mathsf{A}}$.
  prefs: []
  type: TYPE_NORMAL
- en: After $N$ evaluations, the final Elo ratings, referred to as Elo scores are
    used to rank models. Higher scores signify stronger models, and the scores difference
    between models serves as a valuable indicator of performance disparities. As indicated
    in Equation [5](#S2.E5 "Equation 5 ‣ 2.2 Applying our ranking in Elo rating computation
    ‣ 2 Methodology ‣ Which Prompts Make The Difference? Data Prioritization For Efficient
    Human LLM Evaluation"), Elo ratings are updated through pairwise comparisons.
    The arrangement of evaluation prompts and, subsequently, the outcome of each comparison,
    can influence the behavior of the Elo ratings. We use this evaluation framework
    to investigate how our prompt rankings impact the robustness of Elo scores and
    consequently model performance rankings.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Experimental Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Pool of prompts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our objective is to evaluate model quality across a range of tasks, spanning
    from unstructured to more structured formats. To achieve this, we sample prompts
    from several different sources. The social *chit-chat* dataset, Soda [[39](#bib.bib39)],
    is utilized to assess the models’ performance in handling informal conversations.
    To evaluate the models’ proficiency in structured and goal-oriented settings,
    we incorporate prompts from the Public Pool of Prompts (P3) dataset [[64](#bib.bib64)].
    Prompts from the datasets CommonsenseQA [[72](#bib.bib72)] and CommonGen [[44](#bib.bib44)]
    serve to assess the models’ capability of reasoning and coherent narratives generation
    given some context. We also include AdversarialQA, which consists of complex prompts
    specifically designed to mislead the models. We randomly sample 100 examples from
    each dataset to create our target pool of prompts. In the case of open-domain
    dialog corpora, we process the dialogue into a sequence of utterances and randomly
    sample a turn index. The prompt is then constructed up to that turn, and the model’s
    task is to generate the next turn(s).
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 1: Details of the models used for evaluation in this work.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Base model | Architecture | Size | Finetuning data |  |'
  prefs: []
  type: TYPE_TB
- en: '| Flan-t5 | T5 | encoder-decoder | [3B, 11B] | formal instruct |  |'
  prefs: []
  type: TYPE_TB
- en: '| Dolly-v2 | pythia | decoder-only | [7B, 12B] | colloquial instruct |  |'
  prefs: []
  type: TYPE_TB
- en: '| Falcon-instruct | falcon | decoder-only | [7B] | instruct/chat |  |'
  prefs: []
  type: TYPE_TB
- en: '| MPT-instruct | mpt | decoder-only | [7B] | colloquial instruct/preference
    |  |'
  prefs: []
  type: TYPE_TB
- en: We assemble an evaluation pool comprised of several state-of-the-art and widely
    used language models including Flan-T5 [[20](#bib.bib20)] and Dolly-V2 [[23](#bib.bib23)]
    families, as well as MPT-7b-instruct [[74](#bib.bib74)] and falcon-7b-instruct
    [[56](#bib.bib56)]. Each of these models, ranging from 3 billion to 12 billion
    parameters, has been selected for their demonstrated capabilities across various
    NLP tasks and the diversity they bring to our pool regarding architectural differences
    and training methodologies. While Flan-T5 is an encoder-decoder model, all other
    models are decoder only. The base models also differ between models, ranging from
    Pythia [[8](#bib.bib8)] to T5 [[21](#bib.bib21)]. Table [1](#S3.T1 "Table 1 ‣
    3.2 Models ‣ 3 Experimental Setup ‣ Which Prompts Make The Difference? Data Prioritization
    For Efficient Human LLM Evaluation") provides additional information about each
    of the models.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Completion generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To ensure fairness in comparing models, it is essential to generate high-quality
    completions for each model in our evaluation pool. We briefly describe below the
    parameters we used during inference time for each model, chosen in accordance
    with the guidelines specified by the respective model repositories or HuggingFace
    model card.
  prefs: []
  type: TYPE_NORMAL
- en: For all our models, we generate completions in a zero-shot manner. For flan-t5-xxl
    and flan-t5-xl models, inference was run exclusively on a CPU device using the
    JAX framework [[10](#bib.bib10)], configured for deterministic outputs via $\tau=0$.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the absence of a specific padding token for these models, we used the
    "<eos>" token as a substitute. Completions were generated in variable batches,
    ranging from 8 to 50, depending on the model sizes, on an Nvidia A100 GPU with
    40GB memory, to maximize computational efficiency. To reduce memory usage, we
    run inference in a Bfloat16 setting  [[37](#bib.bib37)].
  prefs: []
  type: TYPE_NORMAL
- en: It is important to highlight that due to the differences in training and fine-tuning
    data among models and the diverse nature of our evaluation tasks, certain models
    faced challenges when dealing with specific datasets. In particular MPT-7b-instruct
    and falcon-7b-instruct struggled with generating completions for prompts from
    the AdversarialQA. To address this, we appended the phrase "Answer:" to the end
    of these prompts, enabling models to generate reasonable completions. However,
    the quality of completions from MPT-7b-instruct on both AdversarialQA and Soda
    remained subpar, necessitating the omission of these datasets in experiments involving
    these models.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Human annotation collection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Annotator Task We incorporate an outcome-level relative assessment (ORA) for
    human evaluations. This comparative method, as described by [[29](#bib.bib29)],
    operates on assumptions about the rationality of human judgment, commonly employed
    in LLM evaluations [[2](#bib.bib2)]: Generate pairs with outputs from two systems,
    have annotators select their preferred choice from each pair, and calculate a
    preference-based score to evaluate system performance.'
  prefs: []
  type: TYPE_NORMAL
- en: In this work, we create random pairs containing completions from models $\mathsf{A}$
    in the interface. This random assignment prevents any potential bias that might
    arise if annotators consistently favor one position over the other.
  prefs: []
  type: TYPE_NORMAL
- en: Random Baseline The procedure we follow to establish a baseline for comparison
    between two models involves presenting prompts $P=\{p_{i}\}_{i=1}^{N}$, against
    rankings based on both KL divergence (KL) and Cross-Entropy (CE) at varying top
    k percentiles. An effective metric that aptly measures the dissimilarity between
    two completions should manifest as reduced tie rates, particularly in the earliest,
    highest-ranked annotations.
  prefs: []
  type: TYPE_NORMAL
- en: Aggregation of Preferences For each unique “A vs. B” model comparison experiment,
    we collect $N_{evals}$ when comparing models from different families.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Results and Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An efficient ranking will aim to minimize tie outcomes among the highest-ranked
    instances. By prioritizing instances with a lower likelihood of inducing a tie,
    we can obtain early signals to determine the superior model. Ideally, by employing
    a robust ranking system, one can draw the same conclusions about model performance
    using a specific subset of prompts as they would have by annotating the entire
    dataset. This is particularly valuable since human annotations can be costly,
    and budget constraints may make it unfeasible to manually annotate all instances
    within a given prompt pool. Our goal is to arrive at a ranking where annotating
    only a subset of the top k% of evaluation instances, where $k<N_{\text{prompts}}$,
    is sufficient to have reliable results.
  prefs: []
  type: TYPE_NORMAL
- en: 'We consider two settings for evaluating the effectiveness of our proposed metrics
    in reducing tie outcomes: 1) within the same model family (intra-family), and
    2) across *different* model families (inter-family). In each of these settings,
    we conducted two distinct experiments. For a comprehensive view of both intra-family
    and inter-family experiments, including detailed win rates and tie outcomes, we
    refer to Appendix [B](#A2 "Appendix B Pairwise Comparison Results ‣ Which Prompts
    Make The Difference? Data Prioritization For Efficient Human LLM Evaluation").'
  prefs: []
  type: TYPE_NORMAL
- en: Intra-family comparisons refer to models that share analogous architectural
    foundations and are governed by comparable training paradigms, such as varying
    sizes of the same model. In our observations, such alignment typically results
    in their generated log probabilities being confined to similar numerical ranges,
    facilitating a straightforward comparative analysis without necessitating normalization
    techniques. On the other hand, in the inter-family comparisons, we observe significant
    variations in the range of log probabilities across models, given the same evaluation
    prompts set. These disparities can be attributed to divergent training sources,
    architectural intricacies, or underlying methodologies. A normalization is imperative
    to ensure an objective and unbiased comparison between models across diverse sources.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Comparing Models Within the Same Family
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We present tie rates for two sets of model comparisons: flan-t5-xxl vs. flan-t5-xl
    and dolly-v2-12b vs. dolly-v2-7b, across different top k percentages of collected
    human annotations in Figure [3](#S4.F3 "Figure 3 ‣ 4.1 Comparing Models Within
    the Same Family ‣ 4 Results and Analysis ‣ Which Prompts Make The Difference?
    Data Prioritization For Efficient Human LLM Evaluation"). In our baseline, which
    represents a random ordering, tie rates remain consistent, showing only minor
    deviations around the results for 100% annotated data. This consistency emerges
    from our approach of averaging across $N_{perms}$ sequences, significantly outperform
    the random ordering, especially between the top-20 and top-30 percentiles.'
  prefs: []
  type: TYPE_NORMAL
- en: KL Divergence successfully reduces tie outcomes in the first half of the ranked
    set for both flan and dolly families. We observe reduction in ties in the top-20
    percentile for models compared within the dolly family with ties reducing $6.83\%$,
    indicating a consistent metric performance across distinct model families. In
    both experiments, we observe a significant difference between the metric-based
    and random rankings in the first half of the prompt list. These significant results
    confirm that a greater dissimilarity between completions, as indicated by higher
    values, corresponds to a reduced rate of ties at the top of the ranking, thereby
    facilitating more decisive preferences from the annotators.
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/6df00363a08521e4a5644dc46479e87d.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) flan-t5-xxl vs. flan-t5-xl
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b4121a80ef02ce02002787a96e7c0fa7.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) dolly-v2-12b vs. dolly-v2-7b
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: Setting 1: Ranking Models from the Same Families – Comparison of
    tie ratios between KL Divergence and Cross-Entropy metrics against the average
    outcome of $100$ samples of randomly ordered sequences for two distinct models
    at various data selection percentages. The error bars represent the 95% confidence
    intervals for the ‘Random’ baseline.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Percentage of decrease in number of ties for each metric relative
    to a random selection of prompts for comparison “flan-t5-xxl vs. flan-t5-xl”.'
  prefs: []
  type: TYPE_NORMAL
- en: '| % Decrease in Ties Relative to Random Selection |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Scoring Metric | Percentile Ranking |'
  prefs: []
  type: TYPE_TB
- en: '| 5% | 10% | 20% | 30% | 50% |'
  prefs: []
  type: TYPE_TB
- en: '| KL Divergence | 57.81 | 51.12 | 54.64 | 51.16 | 48.72 |'
  prefs: []
  type: TYPE_TB
- en: '| Cross-Entropy | 29.68 | 30.17 | 51.15 | 53.49 | 50.10 |'
  prefs: []
  type: TYPE_TB
- en: 4.2 Comparing Models from Different Families
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A natural question that arises following our analysis of model specific instance
    prioritization is whether this analysis is transferable to comparing models from
    two different model settings. This setting is similar to the one described in
    Section [4.1](#S4.SS1 "4.1 Comparing Models Within the Same Family ‣ 4 Results
    and Analysis ‣ Which Prompts Make The Difference? Data Prioritization For Efficient
    Human LLM Evaluation") but involves ranking models from different families.
  prefs: []
  type: TYPE_NORMAL
- en: To compare models across different families, we first normalize probability
    ranges by adopting a min-max scaling approach as expressed in Equation [6](#S4.E6
    "Equation 6 ‣ 4.2 Comparing Models from Different Families ‣ 4 Results and Analysis
    ‣ Which Prompts Make The Difference? Data Prioritization For Efficient Human LLM
    Evaluation"). $P_{\text{min}}$ represent the minimum and maximum probabilities
    across both models in comparison, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $P_{\text{norm}}=\frac{P-P_{\text{min}}}{P_{\text{max}}-P_{\text{min}}}$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: This standardization method enables the computation of our metrics on a comparable
    scale across different model families. Once these adjustments are implemented,
    we follow the same ranking and comparison procedures as in the inter-family case.
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/82e3ba78a9cb306d365e422ab8cb3b89.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) flan-t5-xxl vs. dolly-v2-12b
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/47e418dfbddcc52951072ccf7c1ea946.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) MPT-7b vs. falcon-7b
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Setting 2: Ranking Models from Different Families – Comparison of
    tie ratios between KL Divergence and Cross-Entropy metrics against the average
    outcome of $100$ samples of randomly ordered sequences for two distinct models
    at various data selection percentages. The error bars represent the 95% confidence
    intervals for the ‘Random’ baseline.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure [4](#S4.F4 "Figure 4 ‣ 4.2 Comparing Models from Different Families ‣
    4 Results and Analysis ‣ Which Prompts Make The Difference? Data Prioritization
    For Efficient Human LLM Evaluation") illustrates our intra-family main results.
    for the model comparison between “flan-t5-xxl and dolly-v2-12b” at 20% annotated
    data, the Cross-Entropy metric reduces the tie rates by $9.85\%$. At 30%, both
    metrics conform to the anticipated trend, with Cross-Entropy marginally outperforming
    KL Divergence.
  prefs: []
  type: TYPE_NORMAL
- en: In the comparison between “MPT-7b and falcon-7b”, the most significant reduction
    in tie outcomes is observed at 20% annotated data, followed by a gradual increase
    up to 50%. Here, KL outperforms CE, in agreement with the results of the inter-family
    experiments presented in section [4.1](#S4.SS1 "4.1 Comparing Models Within the
    Same Family ‣ 4 Results and Analysis ‣ Which Prompts Make The Difference? Data
    Prioritization For Efficient Human LLM Evaluation"). It is imperative to note
    that while the improvements in this setting might not be as pronounced as in the
    same family models, it is primarily due to the inherently lower tie rates in the
    different family comparisons. Nonetheless, even in this challenging scenario,
    our metrics evidently provide a noticeable advantage when examining the top percentile.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Comparing the two settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We observe more efficiency gains in setting 1 where we compare models from the
    same family as opposed to intra-family comparisons in setting 2. Typically models
    in setting 1 tend to exhibit similar generalization behaviors due to similar optimization,
    finetuning data, and base models, and it is challenging to distinguish improvements
    between them. Here, we observe the most significant gains in efficiency by leveraging
    our method.
  prefs: []
  type: TYPE_NORMAL
- en: However, when comparing models from different families (setting 2), the generated
    outputs can vary significantly based on specific tasks or prompts [[71](#bib.bib71)].
    When models are from different model families, there is typically a more pronounced
    performance spread which make it easier for human annotators to distinguish between
    model outputs. Therefore, there are fewer tie outcomes, leading to fewer expected
    benefits from prioritization. Our findings indicate that our method can still
    bring meaningful efficiency improvements even in this challenging setting. It
    is crucial to underscore the real-world implications of these findings. In practical
    scenarios, especially within production environments, it is more common to study
    comparisons within model families. The primary objective in such instances is
    to rank and evaluate various iterations of a single model, aiming to select the
    most optimal version for deployment. In such cases, using our proposed metrics
    to rank model completions substantially diminishes tie occurrences in top comparisons,
    resulting in cost savings in terms of annotation.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Elo Score Robustness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b0e8918eea21beeb197d9ad9a500188c.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) flan-t5-xxl vs. flan-t5-xl
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2a23f26dec8d2cefed0fb877a129557b.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) dolly-v2-7b vs. dolly-v2-12b
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: Elo scores of intra-family comparisons using the top 20% and top
    30% subsets of evaluation data. Dashed lines represent the Elo scores, determined
    using 100% of human annotation data, establishing our ‘Gold Standard’ models ranking.
    For further details, we refer to Table [3](#S4.T3 "Table 3 ‣ 4.4 Elo Score Robustness
    ‣ 4 Results and Analysis ‣ Which Prompts Make The Difference? Data Prioritization
    For Efficient Human LLM Evaluation").'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we investigate how our findings impact the robustness of Elo
    rating system, previously introduced in Section [2.2](#S2.SS2 "2.2 Applying our
    ranking in Elo rating computation ‣ 2 Methodology ‣ Which Prompts Make The Difference?
    Data Prioritization For Efficient Human LLM Evaluation"). To update the Elo ratings
    of two LLMs, the models engage in a head-to-head match for each prompt and annotators
    express their preference of the model completions. Tie outcomes can occur due
    to individual annotator indecision as well as inter-annotator disagreement. Selecting
    one model’s output as superior in a pairwise comparison provides a robust signal
    for the Elo rating to adjust accordingly. However, in the case of a tie, the adjustment
    in rating points is typically smaller, as ties are considered less informative
    about the relative strengths of the models. As a result, both models end up gaining
    or losing the same number of rating points. A tie treats both models’ quality
    as equally uncertain, which does not provide valuable information for ranking
    the models. The order of prompts affects how quickly and accurately the ratings
    converge to reflect the true skill levels of the models.
  prefs: []
  type: TYPE_NORMAL
- en: To evaluate the impact of our proposed prompt order on Elo scores, we begin
    by establishing a consistent baseline. We achieve this by averaging the Elo scores
    over 100 permutations for each of our four experiments, comparing intra- and inter-family
    models. These averages serve as our ‘Gold Standard’ model rankings, as summarized
    in Table [3](#S4.T3 "Table 3 ‣ 4.4 Elo Score Robustness ‣ 4 Results and Analysis
    ‣ Which Prompts Make The Difference? Data Prioritization For Efficient Human LLM
    Evaluation").
  prefs: []
  type: TYPE_NORMAL
- en: We next evaluate the efficacy of our ranking metrics by calculating Elo scores
    over prompts ordered by CE and KL metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Figure [5](#S4.F5 "Figure 5 ‣ 4.4 Elo Score Robustness ‣ 4 Results and Analysis
    ‣ Which Prompts Make The Difference? Data Prioritization For Efficient Human LLM
    Evaluation"), we present Elo scores, focusing on the top percentile of the evaluation
    set. When using randomly sampled prompts, the Elo scores display unpredictable
    behavior, switching model rankings frequently in the initial stages. For instance,
    we can observe that with a random sequence, relying on the Elo ratings with only
    30% of the data is insufficient to assess the relative performance of the two
    models: the random sequence Elo scores suggest that flan-t5-xl beats flan-t5-xxl
    (Figure [5](#S4.F5 "Figure 5 ‣ 4.4 Elo Score Robustness ‣ 4 Results and Analysis
    ‣ Which Prompts Make The Difference? Data Prioritization For Efficient Human LLM
    Evaluation") (a)) and dolly-v2-12b beats dolly-v2-7b (Figure [5](#S4.F5 "Figure
    5 ‣ 4.4 Elo Score Robustness ‣ 4 Results and Analysis ‣ Which Prompts Make The
    Difference? Data Prioritization For Efficient Human LLM Evaluation") (b)). However
    the dashed lines in the figures indicates that the correct ranking is the opposite
    for both cases. This is mainly due to the appearance of prompts with tie outcomes
    in the top percentile. In contrast, our metrics demonstrate a more reliable and
    consistent performance. Both KL Divergence and Cross-Entropy effectively rank
    the two models from the early stages of evaluation and maintain this consistency
    throughout the entire process. As a result, we can confidently employ a smaller
    subset of the evaluation set sorted by our metrics, and achieve Elo scores equivalent
    to the Gold Standard obtained using the full dataset. This is shown to be not
    the case when using a random sequence of prompts. Our proposed evaluation provides
    a fast and thorough strategy to assess models performance relative to each other,
    enhancing trust in results even when working with smaller datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Experiment | Elo Score $S$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Flan-t5-xxl | $1432.69$ p m 2.34 |'
  prefs: []
  type: TYPE_TB
- en: '| Flan-t5-xl | $1367.31$ p m 2.34 |'
  prefs: []
  type: TYPE_TB
- en: '| Dolly-v2-7b | $1402.82$ p m 2.06 |'
  prefs: []
  type: TYPE_TB
- en: '| Dolly-v2-12b | $1397.18$ p m 2.06 |'
  prefs: []
  type: TYPE_TB
- en: '| Flan-t5-xxl | $1481.70$ p m 2.06 |'
  prefs: []
  type: TYPE_TB
- en: '| Dolly-v2-12b | $1318.30$ p m 2.06 |'
  prefs: []
  type: TYPE_TB
- en: '| Falcon-7b-instruct | $1513.45$ p m 1.85 |'
  prefs: []
  type: TYPE_TB
- en: '| MPT-7b-instruct | $1286.55$ p m 1.85 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: ‘Gold Standard’ Elo scores, accompanied by their respective Standard
    Errors of the Mean (SEM), derived from our human evaluation experiments. These
    scores are computed from an average over $N_{\text{perms}}=100$ permutations.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Limitations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our methodology focuses on prioritizing evaluation instances showcasing distinct
    model behaviors, with the goal of minimizing tie outcomes. While this strategy
    is aimed at optimizing the evaluation process, especially when resources are limited,
    it could inherently favor certain data points, possibly leading to some biases.
    By prioritizing pronounced differences, we risk over-representing certain challenges
    and under-representing areas where models have consistent outputs. In particular,
    prompts that help to establish the lower and upper performance bounds of models
    may be underrepresented in our ranking methodology. It’s important to note that
    our proposed methodology is designed to prioritize annotation within the constraints
    of one’s budget, rather than using it to make decisions about sample exclusion.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Related work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1 Evaluation of LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Automatic evaluation methods of NLP systems has been a longstanding goal for
    researchers and practitioners [[42](#bib.bib42); [53](#bib.bib53); [25](#bib.bib25)].
    Recently, using language models as Natural Language Generation (NLG) evaluation
    metric, such as embedding-based metrics [[87](#bib.bib87); [88](#bib.bib88)] that
    evaluate the semantic similarity via pre-trained BERT model, LLM-based metrics
    [[85](#bib.bib85); [31](#bib.bib31)] that evaluate the quality of the NLG model
    outputs, learning-metric [[15](#bib.bib15)] that mimics human preference when
    judging a story, receives increasing attention since it offers a proxy for human-related
    judgement. Lin & Chen [[46](#bib.bib46)] introduced a method called LLM-Eval that
    assesses multiple dimensions of conversation quality, including content, grammar
    and relevance, without the need for human references or prompts.
  prefs: []
  type: TYPE_NORMAL
- en: However, these metrics still struggle to fully encode the quality aspects of
    generated text [[14](#bib.bib14); [13](#bib.bib13)]. Furthermore, large language
    modelling has led to models which are widely deployed and often used for many
    different tasks. This has posed challenges for traditional evaluation. Instead
    of only evaluating for a single task, models are often expected to perform well
    at many tasks. This has prompted a shift towards relying on automated benchmark
    approaches that collate many tasks measuring different narrow properties [[33](#bib.bib33);
    [82](#bib.bib82)]. Recent benchmarks [[43](#bib.bib43); [90](#bib.bib90); [35](#bib.bib35)]
    are increasingly supplementing automated metrics with human judgement. Compared
    to automatic evaluation, human evaluation closely resembles real-life scenarios
    and offers more thorough and precise feedback [[12](#bib.bib12); [93](#bib.bib93);
    [5](#bib.bib5)].
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Improving quality of human-in-the-loop evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The importance of analyzing and standardizing human evaluation methods in text
    generation tasks has been gaining more attention due to a lack of consensus on
    how to qualitatively evaluate NLG systems [[80](#bib.bib80); [40](#bib.bib40)]
    and the renewed importance of annotations in preference learning and alignment
    optimization. Previously, various annotation frameworks have been developed to
    make the annotation process robust and reusable [[36](#bib.bib36); [18](#bib.bib18);
    [63](#bib.bib63)]. Annotating datasets requires attention to various aspects of
    the problem, including nuances of the language [[7](#bib.bib7)], user adaptivity
    [[26](#bib.bib26)], annotator bias [[76](#bib.bib76)], and domain knowledge [[83](#bib.bib83)].
    While our primary focus is to minimize human-in-the-loop feedback to effectively
    distinguish between models, our prompt prioritization approach also ensures robust
    performance evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Different task designs and data collection methods impact the consistency of
    collected judgments [[52](#bib.bib52); [65](#bib.bib65)]. Comparative approaches
    have proved successful in contrast to direct evaluation [[52](#bib.bib52)], but
    tend to require multiple head-to-head comparisons to achieve statistical significance
    [[13](#bib.bib13)]. Initiatives like GENIE [[38](#bib.bib38)] strives to automate
    and standardize the human evaluation of various NLG systems, with the goal of
    creating a human evaluation leaderboard to track LLM performance. Another approach
    has introduced by Gehrmann et al. [[32](#bib.bib32)], uses GENIE’s infrastructure
    for conducting human evaluations while adopting its own human evaluation techniques.
    [[57](#bib.bib57)] focuses on aggregation methods of human feedback in pairwise
    comparisons of NLG systems. While these works concentrate on the format of annotation
    that leads to the highest quality, our focus is instead to provide a general and
    scalable framework to prioritize instances for feedback in a manner that is agnostic
    to the annotation tool.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Improving efficiency of human-in-the-loop evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Collecting human judgments remains a core component of evaluation of NLP systems.
    However, this process can be both expensive and time-consuming. Efforts have been
    made to address this issue and devise more efficient evaluation strategies. Thorleiksdóttir
    et al. [[75](#bib.bib75)] conducted a comprehensive analysis of common labelling
    strategies, considering varying difficulties for model comparisons. Their analysis
    aiming to commence the design of new evaluation methods that require a reduced
    number of annotations. Another approach to improving efficiency in evaluation
    involves employing Item Response Theory (IRT) for assessing NLG systems. Sedoc
    & Ungar [[67](#bib.bib67)] employed IRT by collecting binary comparisons of system
    responses and reduced the overall number of samples required for model assessment.
    In contrast, the focus of our work is on optimizing the annotation effort when
    comparing two models, ensuring a confident model decision by ranking prompts and
    completion pairs to be used in human evaluation. By strategically selecting and
    prioritizing the most informative samples for evaluation, our proposed approach
    aims to make the evaluation process more efficient and resource-effective.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Prioritizing data instances
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Much of NLP development has involved making heuristic choices about which data
    to prioritize for training and optimization. The selection processes to determine
    what goes into large scale datasets have centered on rule based filters and heuristics
    [[4](#bib.bib4)], such as removing Reddit threads with fewer than three responses
    [[86](#bib.bib86)], removing entire components of the PILE based upon perceived
    quality or blocklists [[27](#bib.bib27)] or filtering out documents that fall
    outside specific word count or vocabulary criteria [[59](#bib.bib59)]. In contrast
    to these heuristics, our focus is on scalable and rigorous techniques to prioritize
    a subset of prompt and completions for human annotation during the evaluation
    stage. While recent research has concentrated on the prioritizing datasets during
    the training and fine-tuning stages, typically through the application of quality
    filters as selection criteria, our emphasis is on the prioritization of evaluation
    instances. For instance, models like Alpaca [[73](#bib.bib73)], Vicuna [[17](#bib.bib17)],
    and Koala [[81](#bib.bib81)] all have as a base the LLaMA model [[78](#bib.bib78)]
    combined with instruction data generated from existing large language models.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work was motivated by the problem of language model preference evaluation,
    with the primary objective of improving the efficiency of the manual annotation
    process. We proposed a data prioritization approach that identifies prompts in
    the evaluation set with minimal information gain when annotated. Through the early
    elimination of prompts resulting in tied outcomes during the annotation process,
    we acquire robust signals regarding model preference, enabling us to determine
    such preferences with fewer annotations. Our ranking method achieved an impressive
    reduction of up to $54\%$ in tied outcomes among the top-20 percentile of the
    evaluation set compared to random ranking. Moreover, when evaluating subsets of
    our prioritized data, we found that Elo scores, a measure of model performance,
    showed clearer distinctions between models in the early thresholds like the top
    20% to 30%. These results are consistent with our ‘Gold Standard’ models ranking,
    underscoring the efficacy of our method over random sequencing, especially in
    resource-limited scenarios. Our findings have a crucial impact on the manual preference
    evaluation of language models, and carries broader implications for increasing
    the overall efficiency of the human annotation process.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Askell et al. [2021] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep
    Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson
    Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine
    Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, and Jared
    Kaplan. A general language assistant as a laboratory for alignment, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bai et al. [2022] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna
    Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas
    Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson
    Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna
    Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack
    Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training a helpful
    and harmless assistant with reinforcement learning from human feedback. *arXiv
    preprint arXiv:2204.05862*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bakker et al. [2022] Michiel A. Bakker, Martin J. Chadwick, Hannah R. Sheahan,
    Michael Henry Tessler, Lucy Campbell-Gillingham, Jan Balaguer, Nat McAleese, Amelia
    Glaese, John Aslanides, Matthew M. Botvinick, and Christopher Summerfield. Fine-tuning
    language models to find agreement among humans with diverse preferences, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bane et al. [2022] Fred Bane, Celia Soler Uguet, Wiktor Stribiżew, and Anna
    Zaretskaya. A comparison of data filtering methods for neural machine translation.
    In *Proceedings of the 15th Biennial Conference of the Association for Machine
    Translation in the Americas (Volume 2: Users and Providers Track and Government
    Track)*, pp.  313–325, Orlando, USA, September 2022\. Association for Machine
    Translation in the Americas. URL [https://aclanthology.org/2022.amta-upg.22](https://aclanthology.org/2022.amta-upg.22).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bang et al. [2023] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai,
    Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al.
    A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination,
    and interactivity. *arXiv preprint arXiv:2302.04023*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Beck et al. [2020] Christin Beck, Hannah Booth, Mennatallah El-Assady, and
    Miriam Butt. Representation problems in linguistic annotations: Ambiguity, variation,
    uncertainty, error and bias. In *Proceedings of the 14th Linguistic Annotation
    Workshop*, pp.  60–73, Barcelona, Spain, December 2020\. Association for Computational
    Linguistics. URL [https://aclanthology.org/2020.law-1.6](https://aclanthology.org/2020.law-1.6).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bergman & Diab [2022] A. Bergman and Mona Diab. Towards responsible natural
    language annotation for the varieties of Arabic. In *Findings of the Association
    for Computational Linguistics: ACL 2022*, pp.  364–371, Dublin, Ireland, May 2022\.
    Association for Computational Linguistics. [10.18653/v1/2022.findings-acl.31](https:/doi.org/10.18653/v1/2022.findings-acl.31).
    URL [https://aclanthology.org/2022.findings-acl.31](https://aclanthology.org/2022.findings-acl.31).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Biderman et al. [2023] Stella Biderman, Hailey Schoelkopf, Quentin Anthony,
    Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit,
    USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der
    Wal. Pythia: A suite for analyzing large language models across training and scaling,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blagec et al. [2022] Kathrin Blagec, Georg Dorffner, Milad Moradi, Simon Ott,
    and Matthias Samwald. A global analysis of metrics used for measuring performance
    in natural language processing. *ArXiv*, abs/2204.11574, 2022. URL [https://api.semanticscholar.org/CorpusID:248377502](https://api.semanticscholar.org/CorpusID:248377502).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bradbury et al. [2018] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James
    Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas,
    Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy
    programs, 2018. URL [http://github.com/google/jax](http://github.com/google/jax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. [2020] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. Language models are few-shot learners, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bubeck et al. [2023] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes
    Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,
    et al. Sparks of artificial general intelligence: Early experiments with gpt-4.
    *arXiv preprint arXiv:2303.12712*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Celikyilmaz et al. [2020] Asli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao.
    Evaluation of text generation: A survey. *arXiv preprint arXiv:2006.14799*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chaganty et al. [2018] Arun Tejasvi Chaganty, Stephen Mussman, and Percy Liang.
    The price of debiasing automatic metrics in natural language evaluation. *arXiv
    preprint arXiv:1807.02202*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2022] Hong Chen, Duc Vo, Hiroya Takamura, Yusuke Miyao, and Hideki
    Nakayama. StoryER: Automatic story evaluation via ranking, rating and reasoning.
    In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language
    Processing*, pp.  1739–1753, Abu Dhabi, United Arab Emirates, December 2022\.
    Association for Computational Linguistics. URL [https://aclanthology.org/2022.emnlp-main.114](https://aclanthology.org/2022.emnlp-main.114).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2023] Yangyi Chen, Lifan Yuan, Ganqu Cui, Zhiyuan Liu, and Heng
    Ji. A close look into the calibration of pre-trained language models, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chiang et al. [2023] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez,
    et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.
    *See https://vicuna. lmsys. org (accessed 14 April 2023)*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chiarcos et al. [2008] Christian Chiarcos, Stefanie Dipper, Michael Götze, Ulf
    Leser, Anke Lüdeling, Julia Ritz, and Manfred Stede. A flexible framework for
    integrating annotations from different tools and tag sets. *Trait. Autom. des
    Langues*, 49(2):217–246, 2008. URL [http://www.atala.org/IMG/pdf/TAL-2008-49-2-08-Chiarcos.pdf](http://www.atala.org/IMG/pdf/TAL-2008-49-2-08-Chiarcos.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chowdhery et al. [2022] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
    Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily
    Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael
    Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
    Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam
    Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander
    Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M.
    Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
    Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,
    Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern,
    Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling
    with pathways, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chung et al. [2022a] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay,
    William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert
    Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery,
    Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai,
    Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny
    Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models,
    2022a. URL [https://arxiv.org/abs/2210.11416](https://arxiv.org/abs/2210.11416).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chung et al. [2022b] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay,
    William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert
    Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery,
    Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav
    Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov,
    Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and
    Jason Wei. Scaling instruction-finetuned language models, 2022b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cohen et al. [2018] K. Bretonnel Cohen, Jingbo Xia, Pierre Zweigenbaum, Tiffany
    Callahan, Orin Hargraves, Foster Goss, Nancy Ide, Aurélie Névéol, Cyril Grouin,
    and Lawrence E. Hunter. Three dimensions of reproducibility in natural language
    processing. In *Proceedings of the Eleventh International Conference on Language
    Resources and Evaluation (LREC 2018)*, Miyazaki, Japan, May 2018. European Language
    Resources Association (ELRA). URL [https://aclanthology.org/L18-1025](https://aclanthology.org/L18-1025).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Conover et al. [2023] Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie,
    Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin.
    Free dolly: Introducing the world’s first truly open instruction-tuned llm, 2023.
    URL [https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Davani et al. [2022] Aida Mostafazadeh Davani, Mark Díaz, and Vinodkumar Prabhakaran.
    Dealing with disagreements: Looking beyond the majority vote in subjective annotations.
    *Transactions of the Association for Computational Linguistics*, 10:92–110, 01
    2022. ISSN 2307-387X. [10.1162/tacl_a_00449](https:/doi.org/10.1162/tacl_a_00449).
    URL [https://aclanthology.org/2022.tacl-1.6](https://aclanthology.org/2022.tacl-1.6).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deriu et al. [2020] Jan Deriu, Don Tuggener, Pius Von Däniken, Jon Ander Campos,
    Alvaro Rodrigo, Thiziri Belkacem, Aitor Soroa, Eneko Agirre, and Mark Cieliebak.
    Spot the bot: A robust and efficient framework for the evaluation of conversational
    dialogue systems. *arXiv preprint arXiv:2010.02140*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dipper et al. [2004] Stefanie Dipper, Michael Götze, and Stavros Skopeteas.
    Towards user-adaptive annotation guidelines. In *Proceedings of the 5th International
    Workshop on Linguistically Interpreted Corpora*, pp.  23–30, Geneva, Switzerland,
    aug 29 2004\. COLING. URL [https://aclanthology.org/W04-1904](https://aclanthology.org/W04-1904).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dodge et al. [2021] Jesse Dodge, Maarten Sap, Ana Marasović, William Agnew,
    Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. Documenting
    large webtext corpora: A case study on the colossal clean crawled corpus, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elo [1978] Arpad E. Elo. *The Rating of Chessplayers, Past and Present*. Arco
    Pub., New York, 1978. ISBN 0668047216 9780668047210. URL [http://www.amazon.com/Rating-Chess-Players-Past-Present/dp/0668047216](http://www.amazon.com/Rating-Chess-Players-Past-Present/dp/0668047216).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ethayarajh & Jurafsky [2022] Kawin Ethayarajh and Dan Jurafsky. The authenticity
    gap in human evaluation. In *Proceedings of the 2022 Conference on Empirical Methods
    in Natural Language Processing*, pp.  6056–6070, Abu Dhabi, United Arab Emirates,
    December 2022\. Association for Computational Linguistics. URL [https://aclanthology.org/2022.emnlp-main.406](https://aclanthology.org/2022.emnlp-main.406).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Evtikhiev et al. [2022] Mikhail Evtikhiev, Egor Bogomolov, Yaroslav Sokolov,
    and Timofey Bryksin. Out of the bleu: how should we assess quality of the code
    generation models? *J. Syst. Softw.*, 203:111741, 2022. URL [https://api.semanticscholar.org/CorpusID:251371647](https://api.semanticscholar.org/CorpusID:251371647).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu et al. [2023] Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu.
    Gptscore: Evaluate as you desire. *arXiv preprint arXiv:2302.04166*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gehrmann et al. [2021] Sebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal,
    Pawan Sasanka Ammanamanchi, Aremu Anuoluwapo, Antoine Bosselut, Khyathi Raghavi
    Chandu, Miruna Clinciu, Dipanjan Das, Kaustubh D Dhole, et al. The gem benchmark:
    Natural language generation, its evaluation and metrics. *arXiv preprint arXiv:2102.01672*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gehrmann et al. [2022] Sebastian Gehrmann, Abhik Bhattacharjee, Abinaya Mahendiran,
    Alex Wang, Alexandros Papangelis, Aman Madaan, Angelina McMillan-Major, Anna Shvets,
    Ashish Upadhyay, Bingsheng Yao, Bryan Wilie, Chandra Bhagavatula, Chaobin You,
    Craig Thomson, Cristina Garbacea, Dakuo Wang, Daniel Deutsch, Deyi Xiong, Di Jin,
    Dimitra Gkatzia, Dragomir Radev, Elizabeth Clark, Esin Durmus, Faisal Ladhak,
    Filip Ginter, Genta Indra Winata, Hendrik Strobelt, Hiroaki Hayashi, Jekaterina
    Novikova, Jenna Kanerva, Jenny Chim, Jiawei Zhou, Jordan Clive, Joshua Maynez,
    João Sedoc, Juraj Juraska, Kaustubh Dhole, Khyathi Raghavi Chandu, Laura Perez-Beltrachini,
    Leonardo F. R. Ribeiro, Lewis Tunstall, Li Zhang, Mahima Pushkarna, Mathias Creutz,
    Michael White, Mihir Sanjay Kale, Moussa Kamal Eddine, Nico Daheim, Nishant Subramani,
    Ondrej Dusek, Paul Pu Liang, Pawan Sasanka Ammanamanchi, Qi Zhu, Ratish Puduppully,
    Reno Kriz, Rifat Shahriyar, Ronald Cardenas, Saad Mahamood, Salomey Osei, Samuel
    Cahyawijaya, Sanja Štajner, Sebastien Montella, Shailza, Shailza Jolly, Simon
    Mille, Tahmid Hasan, Tianhao Shen, Tosin Adewumi, Vikas Raunak, Vipul Raheja,
    Vitaly Nikolaev, Vivian Tsai, Yacine Jernite, Ying Xu, Yisi Sang, Yixin Liu, and
    Yufang Hou. Gemv2: Multilingual nlg benchmarking in a single line of code, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graham [2015] Yvette Graham. Re-evaluating automatic summarization with bleu
    and 192 shades of rouge. In *Conference on Empirical Methods in Natural Language
    Processing*, 2015. URL [https://api.semanticscholar.org/CorpusID:8651371](https://api.semanticscholar.org/CorpusID:8651371).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gu et al. [2023] Zhouhong Gu, Xiaoxuan Zhu, Haoning Ye, Lin Zhang, Jianchen
    Wang, Sihang Jiang, Zhuozhi Xiong, Zihan Li, Qianyu He, Rui Xu, et al. Xiezhi:
    An ever-updating benchmark for holistic domain knowledge evaluation. *arXiv preprint
    arXiv:2306.05783*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ide et al. [2003] Nancy Ide, Laurent Romary, and Eric de la Clergerie. International
    standard for a linguistic annotation framework. In *Proceedings of the HLT-NAACL
    2003 Workshop on Software Engineering and Architecture of Language Technology
    Systems (SEALTS)*, pp. 25–30, 2003. URL [https://aclanthology.org/W03-0804](https://aclanthology.org/W03-0804).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kalamkar et al. [2019] Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi,
    Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturiy, Nataraj
    Jammalamadakaz, Jianyu Huang, Hector Yuen, Jiyan Yang, Jongsoo Park, Alexander
    Heinecke, Evangelos Georganas, Sudarshan Srinivasan, Abhisek Kundu, Misha Smelyanskiy,
    Bharat Kaul, and Pradeep Dubey. A study of bfloat16 for deep learning training.
    *arXiv preprint arXiv:1905.12322*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Khashabi et al. [2021] Daniel Khashabi, Gabriel Stanovsky, Jonathan Bragg,
    Nicholas Lourie, Jungo Kasai, Yejin Choi, Noah A Smith, and Daniel S Weld. Genie:
    A leaderboard for human-in-the-loop evaluation of text generation. *arXiv preprint
    arXiv:2101.06561*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. [2022] Hyunwoo Kim, Jack Hessel, Liwei Jiang, Peter West, Ximing
    Lu, Youngjae Yu, Pei Zhou, Ronan Le Bras, Malihe Alikhani, Gunhee Kim, Maarten
    Sap, and Yejin Choi. Soda: Million-scale dialogue distillation with social commonsense
    contextualization. *ArXiv*, abs/2212.10465, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowles [2021] Rebecca Knowles. On the stability of system rankings at WMT.
    In *Proceedings of the Sixth Conference on Machine Translation*, pp.  464–477,
    Online, November 2021\. Association for Computational Linguistics. URL [https://aclanthology.org/2021.wmt-1.56](https://aclanthology.org/2021.wmt-1.56).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kočiský et al. [2018] Tomáš Kočiský, Jonathan Schwarz, Phil Blunsom, Chris Dyer,
    Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. The NarrativeQA reading
    comprehension challenge. *Transactions of the Association for Computational Linguistics*,
    6:317–328, 2018. [10.1162/tacl_a_00023](https:/doi.org/10.1162/tacl_a_00023).
    URL [https://aclanthology.org/Q18-1023](https://aclanthology.org/Q18-1023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lan et al. [2020] Tian Lan, Xian-Ling Mao, Wei Wei, Xiaoyan Gao, and Heyan
    Huang. Pone: A novel automatic evaluation metric for open-domain generative dialogue
    systems. *ACM Transactions on Information Systems (TOIS)*, 39(1):1–37, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang et al. [2022] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras,
    Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya
    Kumar, et al. Holistic evaluation of language models. *arXiv preprint arXiv:2211.09110*,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. [2020] Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra
    Bhagavatula, Yejin Choi, and Xiang Ren. CommonGen: A constrained text generation
    challenge for generative commonsense reasoning. In *Findings of the Association
    for Computational Linguistics: EMNLP 2020*, pp.  1823–1840, Online, November 2020\.
    Association for Computational Linguistics. [10.18653/v1/2020.findings-emnlp.165](https:/doi.org/10.18653/v1/2020.findings-emnlp.165).
    URL [https://aclanthology.org/2020.findings-emnlp.165](https://aclanthology.org/2020.findings-emnlp.165).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin [2004] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries.
    In *Text Summarization Branches Out*, pp.  74–81, Barcelona, Spain, July 2004\.
    Association for Computational Linguistics. URL [https://aclanthology.org/W04-1013](https://aclanthology.org/W04-1013).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin & Chen [2023] Yen-Ting Lin and Yun-Nung Chen. LLM-Eval: Unified multi-dimensional
    automatic evaluation for open-domain conversations with large language models,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2016] Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Noseworthy,
    Laurent Charlin, and Joelle Pineau. How NOT to evaluate your dialogue system:
    An empirical study of unsupervised evaluation metrics for dialogue response generation.
    In *Proceedings of the 2016 Conference on Empirical Methods in Natural Language
    Processing*, pp.  2122–2132, Austin, Texas, November 2016. Association for Computational
    Linguistics. [10.18653/v1/D16-1230](https:/doi.org/10.18653/v1/D16-1230). URL
    [https://aclanthology.org/D16-1230](https://aclanthology.org/D16-1230).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mathur et al. [2020] Nitika Mathur, Timothy Baldwin, and Trevor Cohn. Tangled
    up in BLEU: Reevaluating the evaluation of automatic machine translation evaluation
    metrics. In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pp.  4984–4997, Online, July 2020. Association for Computational
    Linguistics. [10.18653/v1/2020.acl-main.448](https:/doi.org/10.18653/v1/2020.acl-main.448).
    URL [https://aclanthology.org/2020.acl-main.448](https://aclanthology.org/2020.acl-main.448).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Maynez et al. [2023] Joshua Maynez, Priyanka Agrawal, and Sebastian Gehrmann.
    Benchmarking large language model capabilities for conditional generation. In
    *Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics
    (Volume 1: Long Papers)*, pp.  9194–9213, Toronto, Canada, July 2023\. Association
    for Computational Linguistics. URL [https://aclanthology.org/2023.acl-long.511](https://aclanthology.org/2023.acl-long.511).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Menick et al. [2022] Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides,
    Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham,
    Geoffrey Irving, and Nat McAleese. Teaching language models to support answers
    with verified quotes, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nakano et al. [2022] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
    Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju,
    William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin
    Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt: Browser-assisted
    question-answering with human feedback, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Novikova et al. [2018] Jekaterina Novikova, Ondřej Dušek, and Verena Rieser.
    Rankme: Reliable human ratings for natural language generation. *arXiv preprint
    arXiv:1803.05928*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pang et al. [2020] Bo Pang, Erik Nijkamp, Wenjuan Han, Linqi Zhou, Yixian Liu,
    and Kewei Tu. Towards holistic and automatic evaluation of open-domain dialogue
    generation. In *Proceedings of the 58th Annual Meeting of the Association for
    Computational Linguistics*, pp.  3619–3629, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Papineni et al. [2002] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. Bleu: a method for automatic evaluation of machine translation. In *Proceedings
    of the 40th Annual Meeting of the Association for Computational Linguistics*,
    pp.  311–318, Philadelphia, Pennsylvania, USA, July 2002\. Association for Computational
    Linguistics. [10.3115/1073083.1073135](https:/doi.org/10.3115/1073083.1073135).
    URL [https://aclanthology.org/P02-1040](https://aclanthology.org/P02-1040).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Paszke et al. [2019] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James
    Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca
    Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zachary DeVito, Martin Raison,
    Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and
    Soumith Chintala. Pytorch: An imperative style, high-performance deep learning
    library. In *Advances in Neural Information Processing Systems 32*, pp. 8024–8035,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Penedo et al. [2023] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra
    Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,
    and Julien Launay. The RefinedWeb dataset for Falcon LLM: outperforming curated
    corpora with web data, and web data only. *arXiv preprint arXiv:2306.01116*, 2023.
    URL [https://arxiv.org/abs/2306.01116](https://arxiv.org/abs/2306.01116).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peyrard et al. [2021] Maxime Peyrard, Wei Zhao, Steffen Eger, and Robert West.
    Better than average: Paired evaluation of NLP systems. In *Proceedings of the
    59th Annual Meeting of the Association for Computational Linguistics and the 11th
    International Joint Conference on Natural Language Processing (Volume 1: Long
    Papers)*, pp.  2301–2315, Online, August 2021\. Association for Computational
    Linguistics. [10.18653/v1/2021.acl-long.179](https:/doi.org/10.18653/v1/2021.acl-long.179).
    URL [https://aclanthology.org/2021.acl-long.179](https://aclanthology.org/2021.acl-long.179).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pozzobon et al. [2023] Luiza Pozzobon, Beyza Ermis, Patrick Lewis, and Sara
    Hooker. On the challenges of using black-box apis for toxicity evaluation in research,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rae et al. [2022] Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican,
    Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah
    Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell,
    George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia
    Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John
    Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant
    Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela
    Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida
    Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch,
    Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault
    Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d’Autume,
    Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego
    de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake
    Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero,
    Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne
    Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language
    models: Methods, analysis & insights from training gopher, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rafailov et al. [2023] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano
    Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization:
    Your language model is secretly a reward model, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring
    the limits of transfer learning with a unified text-to-text transformer, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Roy et al. [2022] Subhro Roy, Sam Thomson, Tongfei Chen, Richard Shin, Adam
    Pauls, Jason Eisner, and Benjamin Van Durme. Benchclamp: A benchmark for evaluating
    language models on semantic parsing, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sabou et al. [2014] Marta Sabou, Kalina Bontcheva, Leon Derczynski, and Arno
    Scharl. Corpus annotation through crowdsourcing: Towards best practice guidelines.
    In *Proceedings of the Ninth International Conference on Language Resources and
    Evaluation (LREC’14)*, pp.  859–866, Reykjavik, Iceland, May 2014\. European Language
    Resources Association (ELRA). URL [http://www.lrec-conf.org/proceedings/lrec2014/pdf/497_Paper.pdf](http://www.lrec-conf.org/proceedings/lrec2014/pdf/497_Paper.pdf).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sanh et al. [2021] Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach,
    Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao,
    Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma
    Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti
    Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen,
    Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos
    Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan
    Teehan, Stella Biderman, Leo Gao, Tali Bers, Thomas Wolf, and Alexander M. Rush.
    Multitask prompted training enables zero-shot task generalization, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Santhanam & Shaikh [2019] Sashank Santhanam and Samira Shaikh. Towards best
    experiment design for evaluating dialogue system output. *arXiv preprint arXiv:1909.10122*,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schluter [2017] Natalie Schluter. The limits of automatic summarisation according
    to ROUGE. In *Proceedings of the 15th Conference of the European Chapter of the
    Association for Computational Linguistics: Volume 2, Short Papers*, pp.  41–45,
    Valencia, Spain, April 2017\. Association for Computational Linguistics. URL [https://aclanthology.org/E17-2007](https://aclanthology.org/E17-2007).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sedoc & Ungar [2020] João Sedoc and Lyle Ungar. Item response theory for efficient
    human evaluation of chatbots. In *Proceedings of the First Workshop on Evaluation
    and Comparison of NLP Systems*, pp.  21–33, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shen et al. [2022] Lingfeng Shen, Lemao Liu, Haiyun Jiang, and Shuming Shi.
    On the evaluation metrics for paraphrase generation. In *Proceedings of the 2022
    Conference on Empirical Methods in Natural Language Processing*, pp.  3178–3190,
    Abu Dhabi, United Arab Emirates, December 2022\. Association for Computational
    Linguistics. URL [https://aclanthology.org/2022.emnlp-main.208](https://aclanthology.org/2022.emnlp-main.208).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Steiger et al. [2021] Miriah Steiger, Timir J Bharucha, Sukrit Venkatagiri,
    Martin J Riedl, and Matthew Lease. The psychological well-being of content moderators:
    the emotional labor of commercial moderation and avenues for improving support.
    In *Proceedings of the 2021 CHI conference on human factors in computing systems*,
    pp.  1–14, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stiennon et al. [2020] Nisan Stiennon, Long Ouyang Wu, Jeffrey Ziegler, Paul
    Christiano, Dario Amodei, and Buck Shlegeris. Learning to summarize from human
    feedback. *arXiv preprint arXiv:2009.01325*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Strobelt et al. [2021] Hendrik Strobelt, Benjamin Hoover, Arvind Satyanaryan,
    and Sebastian Gehrmann. LMdiff: A visual diff tool to compare language models.
    In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations*, pp.  96–105, Online and Punta Cana, Dominican
    Republic, November 2021\. Association for Computational Linguistics. [10.18653/v1/2021.emnlp-demo.12](https:/doi.org/10.18653/v1/2021.emnlp-demo.12).
    URL [https://aclanthology.org/2021.emnlp-demo.12](https://aclanthology.org/2021.emnlp-demo.12).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Talmor et al. [2019] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan
    Berant. CommonsenseQA: A question answering challenge targeting commonsense knowledge.
    In *Proceedings of the 2019 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
    Short Papers)*, pp.  4149–4158, Minneapolis, Minnesota, June 2019\. Association
    for Computational Linguistics. [10.18653/v1/N19-1421](https:/doi.org/10.18653/v1/N19-1421).
    URL [https://aclanthology.org/N19-1421](https://aclanthology.org/N19-1421).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Taori et al. [2023] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpaca: A
    strong, replicable instruction-following model. *Stanford Center for Research
    on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html*, 3(6):7,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Team [2023] MosaicML NLP Team. Introducing mpt-7b: A new standard for open-source,
    commercially usable llms, 2023. URL [www.mosaicml.com/blog/mpt-7b](www.mosaicml.com/blog/mpt-7b).
    Accessed: 2023-03-28.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thorleiksdóttir et al. [2021] Thórhildur Thorleiksdóttir, Cedric Renggli, Nora
    Hollenstein, and Ce Zhang. Dynamic human evaluation for relative model comparisons.
    *arXiv preprint arXiv:2112.08048*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thorn Jakobsen et al. [2022] Terne Sasha Thorn Jakobsen, Maria Barrett, Anders
    Søgaard, and David Lassen. The sensitivity of annotator bias to task definitions
    in argument mining. In *Proceedings of the 16th Linguistic Annotation Workshop
    (LAW-XVI) within LREC2022*, pp.  44–61, Marseille, France, June 2022. European
    Language Resources Association. URL [https://aclanthology.org/2022.law-1.6](https://aclanthology.org/2022.law-1.6).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thurstone [1927] L. L. Thurstone. A law of comparative judgment. *Psychological
    Review*, 34(4):273–286, 1927. [10.1037/h0070288](https:/doi.org/10.1037/h0070288).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
    Guillaume Lample. Llama: Open and efficient foundation language models, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ung et al. [2022] Megan Ung, Jing Xu, and Y-Lan Boureau. SaFeRDialogues: Taking
    feedback gracefully after conversational safety failures. In *Proceedings of the
    60th Annual Meeting of the Association for Computational Linguistics (Volume 1:
    Long Papers)*, pp.  6462–6481, Dublin, Ireland, May 2022\. Association for Computational
    Linguistics. [10.18653/v1/2022.acl-long.447](https:/doi.org/10.18653/v1/2022.acl-long.447).
    URL [https://aclanthology.org/2022.acl-long.447](https://aclanthology.org/2022.acl-long.447).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Van Der Lee et al. [2019] Chris Van Der Lee, Albert Gatt, Emiel Van Miltenburg,
    Sander Wubben, and Emiel Krahmer. Best practices for the human evaluation of automatically
    generated text. In *Proceedings of the 12th International Conference on Natural
    Language Generation*, pp.  355–368, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vu et al. [2023] Thuy-Trang Vu, Xuanli He, Gholamreza Haffari, and Ehsan Shareghi.
    Koala: An index for quantifying overlaps with pre-training corpora. *arXiv preprint
    arXiv:2303.14770*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2022] Boxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu Cheng,
    Jianfeng Gao, Ahmed Hassan Awadallah, and Bo Li. Adversarial glue: A multi-task
    benchmark for robustness evaluation of language models, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yada et al. [2020] Shuntaro Yada, Ayami Joh, Ribeka Tanaka, Fei Cheng, Eiji
    Aramaki, and Sadao Kurohashi. Towards a versatile medical-annotation guideline
    feasible without heavy medical knowledge: Starting from critical lung diseases.
    In *Proceedings of the Twelfth Language Resources and Evaluation Conference*,
    pp.  4565–4572, Marseille, France, May 2020\. European Language Resources Association.
    ISBN 979-10-95546-34-4. URL [https://aclanthology.org/2020.lrec-1.561](https://aclanthology.org/2020.lrec-1.561).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. [2018] An Yang, Kai Liu, Jing Liu, Yajuan Lyu, and Sujian Li. Adaptations
    of rouge and bleu to better evaluate machine reading comprehension task. In *QA@ACL*,
    2018. URL [https://api.semanticscholar.org/CorpusID:47016928](https://api.semanticscholar.org/CorpusID:47016928).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yuan et al. [2021] Weizhe Yuan, Graham Neubig, and Pengfei Liu. Bartscore:
    Evaluating generated text as text generation. *Advances in Neural Information
    Processing Systems*, 34:27263–27277, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2022] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh
    Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained
    transformer language models, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2019] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger,
    and Yoav Artzi. Bertscore: Evaluating text generation with bert. *arXiv preprint
    arXiv:1904.09675*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. [2019] Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M
    Meyer, and Steffen Eger. Moverscore: Text generation evaluating with contextualized
    embeddings and earth mover distance. *arXiv preprint arXiv:1909.02622*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. [2023] Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad
    Saleh, and Peter J. Liu. Slic-hf: Sequence likelihood calibration with human feedback,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. [2023] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al.
    Judging llm-as-a-judge with mt-bench and chatbot arena. *arXiv preprint arXiv:2306.05685*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. [2023] Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian
    Huang, Lingpeng Kong, Jiajun Chen, and Lei Li. Multilingual machine translation
    with large language models: Empirical results and analysis, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ziegler et al. [2020] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B.
    Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning
    language models from human preferences, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ziems et al. [2023] Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao
    Zhang, and Diyi Yang. Can large language models transform computational social
    science? *arXiv preprint arXiv:2305.03514*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Human Feedback Collection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Evaluations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This subsection showcases random evaluation instances to highlight performance
    disparities between models within the same family: flan-t5-xxl vs. flan-t5-xl,
    and dolly-v2-7b vs. dolly-v2-12b, across diverse datasets. Drawing samples from
    both the top and bottom 20% according to our ranking metrics, we illustrate the
    capability of these metrics to systematically arrange evaluations based on the
    dissimilarity of completions in “A vs. B” comparisons. Conversely, examples from
    the bottom percentile display instances where the models generate outputs of similar
    quality, increasing the likelihood of tied outcomes. Examples from the top 20%
    highlight dissimilar completions, demonstrating the discriminative power of our
    ranking metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Comparison of flan-t5-xxl vs. flan-t5-xl'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ranked by KL Divergence |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Soda - Top 20% |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt | Rosalina went to the ticket counter at the tennis stadium and bought
    two tickets to the match that was going to start in an hour. She was excited to
    see her favorite player, Rafael Nadal, compete against his longtime rival, Roger
    Federer. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Rosalina: Hi, I would like to buy two tickets to the match that’s happening
    in an hour. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Ticket Seller: That will be $180$. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Rosalina: Okay, here you go. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Ticket Seller: Thank you. Enjoy the match! |'
  prefs: []
  type: TYPE_TB
- en: '|  | Rosalina: Thanks! I’m really excited to see it. Nadal is my favorite player.
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Ticket Seller: |'
  prefs: []
  type: TYPE_TB
- en: '| Completion A | Ticket Seller: Have a good time! |'
  prefs: []
  type: TYPE_TB
- en: '| Completion B | Rosalina: I’m sure he’ll win. |'
  prefs: []
  type: TYPE_TB
- en: '| Soda - Bottom 20% |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt | Hashim never refuses when someone asks her for a favor because she
    wants to be seen as a good person. She knows that by saying "yes" to everything,
    people will appreciate her more and think of her as being helpful. Hashim also
    feels that it is her duty to help others, even if it means going out of her way
    or inconveniencing herself. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Hashim: Hey, how are you doing? |'
  prefs: []
  type: TYPE_TB
- en: '|  | Friend: I’m good, thanks. How about you? |'
  prefs: []
  type: TYPE_TB
- en: '|  | Hashim: I’m good. Hey, do you need any help with anything? |'
  prefs: []
  type: TYPE_TB
- en: '|  | Friend: No, I don’t think so. Thanks for offering though. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Hashim: No problem. I just wanted to help out if I could. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Friend: That’s really nice of you. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Hashim: Yeah, I just feel like it’s the right thing to do. Helping others
    is important to me. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Friend: I can see that. It’s one of the things I like about you. You’re
    always willing to help out, no matter what. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Hashim: Thanks. I just think that it’s important to be there for people
    when they need help. We all have to stick together and help each other out. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Friend: |'
  prefs: []
  type: TYPE_TB
- en: '| Completion A | Yeah, I agree. |'
  prefs: []
  type: TYPE_TB
- en: '| Completion B | That’s a great philosophy. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Comparison of flan-t5-xxl vs. flan-t5-xl'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ranked by KL Divergence |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| CommonsenseQA - Top 20% |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt | Here’s a question: Where would one find a snake in a cage? |'
  prefs: []
  type: TYPE_TB
- en: '|  | Here are possible answers to this question: |'
  prefs: []
  type: TYPE_TB
- en: '|  | - tropical forest |'
  prefs: []
  type: TYPE_TB
- en: '|  | - oregon |'
  prefs: []
  type: TYPE_TB
- en: '|  | - woods |'
  prefs: []
  type: TYPE_TB
- en: '|  | - pet store |'
  prefs: []
  type: TYPE_TB
- en: '|  | - louisiana |'
  prefs: []
  type: TYPE_TB
- en: '|  | I believe the correct choice is "pet store", here’s why: |'
  prefs: []
  type: TYPE_TB
- en: '| Completion A | pet stores sell animals |'
  prefs: []
  type: TYPE_TB
- en: '| Completion B | pet store would one find a snake in a cage |'
  prefs: []
  type: TYPE_TB
- en: '| CommonsenseQA - Bottom 20% |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt | Here’s a question: What uses a ribbon to put words on paper? |'
  prefs: []
  type: TYPE_TB
- en: '|  | Here are possible answers to this question: |'
  prefs: []
  type: TYPE_TB
- en: '|  | - wrapping paper |'
  prefs: []
  type: TYPE_TB
- en: '|  | - girl’s hair |'
  prefs: []
  type: TYPE_TB
- en: '|  | - bath |'
  prefs: []
  type: TYPE_TB
- en: '|  | - floral arrangement |'
  prefs: []
  type: TYPE_TB
- en: '|  | - typewriter |'
  prefs: []
  type: TYPE_TB
- en: '|  | I believe the correct choice is "typewriter", here’s why: |'
  prefs: []
  type: TYPE_TB
- en: '| Completion A | typewriters are mechanical devices for writing |'
  prefs: []
  type: TYPE_TB
- en: '| Completion B | typewriters use ribbons to write |'
  prefs: []
  type: TYPE_TB
- en: '| CommonGen - Top 20% |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt | Put the concepts together to form a sentence: cart, road, ride.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Completion A | A man riding a cart down a road. |'
  prefs: []
  type: TYPE_TB
- en: '| Completion B | A man riding a horse and cart down a road. |'
  prefs: []
  type: TYPE_TB
- en: '| CommonGen - Bottom 20% |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt | Put the concepts together to form a sentence: sit, car, backseat.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Completion A | A man sits in the backseat of a car. |'
  prefs: []
  type: TYPE_TB
- en: '| Completion B | A man sits in the backseat of a car. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Comparison of flan-t5-xxl vs. flan-t5-xl'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ranked by KL Divergence |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| AdversarialQA - Top 20% |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt | Extract the answer to the question from the following context. Question:
    What broadcast a signal? Context: The city is served by the Tyne and Wear Metro,
    a system of suburban and underground railways covering much of Tyne and Wear.
    It was opened in five phases between 1980 and 1984, and was Britain’s first urban
    light rail transit system; two extensions were opened in 1991 and 2002\. It was
    developed from a combination of existing and newly built tracks and stations,
    with deep-level tunnels constructed through Newcastle city centre. A bridge was
    built across the Tyne, between Newcastle and Gateshead, and opened by Queen Elizabeth
    II in 1981\. The network is operated by DB Regio on behalf of Nexus and carries
    over 37 million passengers a year, extending as far as Newcastle Airport, Tynemouth,
    South Shields and South Hylton in Sunderland. In 2004, the company Marconi designed
    and constructed the mobile radio system to the underground Metro system. The Metro
    system was the first in the UK to have mobile phone antennae installed in the
    tunnels. |'
  prefs: []
  type: TYPE_TB
- en: '| Completion A | Marconi |'
  prefs: []
  type: TYPE_TB
- en: '| Completion B | antennae |'
  prefs: []
  type: TYPE_TB
- en: '| AdversarialQA - Bottom 20% |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt | Extract the answer to the question from the following context. Question:
    Area shortages meant a shortage of what? Context: In 1939, c. 1,300,000 people
    lived in Warsaw, but in 1945 – only 420,000\. During the first years after the
    war, the population growth was c. 6%, so shortly the city started to suffer from
    the lack of flats and of areas for new houses. The first remedial measure was
    the Warsaw area enlargement (1951) – but the city authorities were still forced
    to introduce residency registration limitations: only the spouses and children
    of the permanent residents as well as some persons of public importance (like
    renowned specialists) were allowed to get the registration, hence halving the
    population growth in the following years. It also bolstered some kind of conviction
    among Poles that Varsovians thought of themselves as better only because they
    lived in the capital. Unfortunately this belief still lives on in Poland (although
    not as much as it used to be) – even though since 1990 there are no limitations
    to residency registration anymore. |'
  prefs: []
  type: TYPE_TB
- en: '| Completion A | flats |'
  prefs: []
  type: TYPE_TB
- en: '| Completion B | new houses |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Comparison of dolly-v2-7b vs. dolly-v2-12b'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ranked by Cross-Entropy |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Soda - Top 20% |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt | Benzion joins an activity because Benzion wants to meet new people
    and get out of the house. Joining an activity will help Benzion connect with other
    people who have similar interests. Benzion: Hi, I’m Benzion. Other members: |'
  prefs: []
  type: TYPE_TB
- en: '| Completion A | Hi, I’m Joe, Slack, and Eric. Welcome to the group activity!
    |'
  prefs: []
  type: TYPE_TB
- en: '| Completion B | Hi, I’m Benzion. I’m looking to meet other people who are
    interested in the same activities as me. |'
  prefs: []
  type: TYPE_TB
- en: '| Soda - Bottom 20% |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt | Asael has a friend over. His friend is a boy named Tim. Asael and
    Tim are playing video games together in Asael’s room. Asael: Yeah, I just got
    this game yesterday and I’ve been dying to play it with someone. Tim: No problem,
    dude. I’m always up for some video games. Asael: Alright, cool. So, what do you
    think of this game so far? Tim: It’s actually really fun! I like the graphics
    and the gameplay is solid. Asael: Yeah, I know! I was worried it was going to
    be too easy, but it’s definitely challenging. Tim: Yeah, definitely. But that’s
    what makes it more fun in my opinion. If a game is too easy, it gets boring pretty
    quickly. But this one is just hard enough to keep you engaged. Asael: Exactly!
    And there are so many different levels to play through too. Have you gotten very
    far? Tim: Yeah, I’m on the fourth level right now. Asael: Oh wow, you’re way ahead
    of me! I’m still on the second level. Tim: Well, it’s not too difficult once you
    get the hang of it. Asael: Yeah, I think I’m starting to get a little better.
    So, how do you like the new controller? Tim: It’s okay, I guess. The buttons are
    a little too close together for my liking though. Asael: |'
  prefs: []
  type: TYPE_TB
- en: '| Completion A | Asael and Tim are playing video games together in Asael’s
    room. |'
  prefs: []
  type: TYPE_TB
- en: '| Completion B | Asael has a friend over. His friend is a boy named Tim. Asael
    and Tim are playing video games together in Asael’s room. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: Comparison of dolly-v2-7b vs. dolly-v2-12b'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ranked by Cross-Entropy |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| CommonsenseQA - Top 20% |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt | Here’s a question: A mountie got off at a subway stop. What city
    might he be in? Here are possible answers to this question: - urban area'
  prefs: []
  type: TYPE_NORMAL
- en: '- metropolis'
  prefs: []
  type: TYPE_NORMAL
- en: '- chicago'
  prefs: []
  type: TYPE_NORMAL
- en: '- new york city'
  prefs: []
  type: TYPE_NORMAL
- en: '- toronto I believe the correct choice is "toronto", here’s why: |'
  prefs: []
  type: TYPE_NORMAL
- en: '| Completion A | The subway stop the mountie got off at is called St. George
    station. St. George is a suburb of Toronto, Ontario, Canada. |'
  prefs: []
  type: TYPE_TB
- en: '| Completion B | A mountie is a member of the national police force of Canada,
    so he is probably from Toronto. Toronto is the largest city in Canada, so it is
    a good choice. |'
  prefs: []
  type: TYPE_TB
- en: '| CommonsenseQA - Bottom 20% |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt | Here’s a question: What does a gambler do that causes him or her
    to be unhappy? Here are possible answers to this question: - play cards'
  prefs: []
  type: TYPE_NORMAL
- en: '- double winnings'
  prefs: []
  type: TYPE_NORMAL
- en: '- lose money'
  prefs: []
  type: TYPE_NORMAL
- en: '- play poker'
  prefs: []
  type: TYPE_NORMAL
- en: '- to win the prize I believe the correct choice is "lose money", here’s why:
    |'
  prefs: []
  type: TYPE_NORMAL
- en: '| Completion A | A gambler who plays cards for a living might be unhappy if
    they consistently win money, because they have to go back to work. A gambler who
    plays poker might be unhappy if they lost all of their money, because they lost
    a game. A gambler who plays poker for a prize might be unhappy if they won the
    prize, because they would have to give up their prize to the prize provider. A
    gambler who plays cards for money might be unhappy if they win too much money,
    because they would have to spend their winnings and then go back to work. A gambler
    who plays cards for fun might be unhappy because they are not having fun. |'
  prefs: []
  type: TYPE_TB
- en: '| Completion B | Playing cards, poker, and other games of chance can be very
    enjoyable, but they can also be very stressful. Stressful situations are often
    unpleasant, and can lead to negative emotions such as anger, anxiety, and depression.
    If you are a gambler and any of these things happen to you, you may be experiencing
    unpleasant emotions that are caused by your gambling. If this is the case, you
    should seek help from a therapist or other mental health professional. They can
    help you identify and work through the causes of these negative emotions. If you
    continue to experience negative emotions despite working through these causes,
    you may want to consider changing your gambling habits. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: Comparison of dolly-v2-7b vs. dolly-v2-12b'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ranked by Cross-Entropy |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| AdversarialQA - Top 20% |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt | Extract the answer to the question from the following context. Question:
    What happens to those that pay too little? Context: In a purely capitalist mode
    of production (i.e. where professional and labor organizations cannot limit the
    number of workers) the workers wages will not be controlled by these organizations,
    or by the employer, but rather by the market. Wages work in the same way as prices
    for any other good. Thus, wages can be considered as a function of market price
    of skill. And therefore, inequality is driven by this price. Under the law of
    supply and demand, the price of skill is determined by a race between the demand
    for the skilled worker and the supply of the skilled worker. "On the other hand,
    markets can also concentrate wealth, pass environmental costs on to society, and
    abuse workers and consumers." "Markets, by themselves, even when they are stable,
    often lead to high levels of inequality, outcomes that are widely viewed as unfair."
    Employers who offer a below market wage will find that their business is chronically
    understaffed. Their competitors will take advantage of the situation by offering
    a higher wage the best of their labor. For a businessman who has the profit motive
    as the prime interest, it is a losing proposition to offer below or above market
    wages to workers. |'
  prefs: []
  type: TYPE_TB
- en: '| Completion A | In a purely capitalist mode of production, the workers wages
    are not controlled by the professional and labor organizations, but rather by
    the market. Wages are a function of market price of skill. In this context, inequality
    is driven by this price. Under the law of supply and demand, the price of skill
    is determined by a race between the demand and supply of the skilled worker. Markets
    can also concentrate wealth, pass environmental costs on to society, and abuse
    workers and consumers. By themselves, even when they are stable, often lead to
    high levels of inequality, outcomes that are widely viewed as unfair. Employers
    who offer a below market wage will find that their business is chronically understaffed.
    Their competitors will take advantage of the situation by offering a higher wage
    the best of their labor. For a businessman who has the profit motive as the prime
    interest, it is a losing proposition to offer below or above market wages to workers.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Completion B | The subway stop the mountie got off at is called St. George
    station. St. George is a suburb of Toronto, Ontario, Canada. |'
  prefs: []
  type: TYPE_TB
- en: '| AdversarialQA - Bottom 20% |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt | Here’s a question: What could happen if someone is seeing new presents
    at a birthday party? Here are possible answers to this question: - envy'
  prefs: []
  type: TYPE_NORMAL
- en: '- jealousy'
  prefs: []
  type: TYPE_NORMAL
- en: '- education'
  prefs: []
  type: TYPE_NORMAL
- en: '- fear'
  prefs: []
  type: TYPE_NORMAL
- en: '- excitement I believe the correct choice is "excitement", here’s why: |'
  prefs: []
  type: TYPE_NORMAL
- en: '| Completion A | Someone seeing new presents at a birthday party means that
    there is new and exciting things happening. This is exciting for everyone at the
    party, especially the birthday person, because they are likely to receive new
    toys that they have not seen before. New toys mean new possibilities, and this
    is exciting for everyone at the party because they can imagine what this new possibility
    could be. |'
  prefs: []
  type: TYPE_TB
- en: '| Completion B | New presents at a birthday party can be a source of excitement
    for some people, while for others it can be a source of fear, jealousy or even
    envy. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10: Comparison of dolly-v2-7b vs. dolly-v2-12b'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ranked by Cross-Entropy |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| CommonGen - Top 20% |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt | Put the concepts together to form a sentence: raft, pull, water.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Completion A | Rafts are pulled by water. |'
  prefs: []
  type: TYPE_TB
- en: '| Completion B | Rafting is a fun form of outdoor recreation that combines
    the thrill of rafting with the beauty of nature. Rafting involves floating down
    a river in a rubber raft, with guides pulling you by rope over whitewater. When
    done correctly, rafting can be quite relaxing, as you are pulled over gentle riffles
    by the current, with the peacefulness interrupted only by the sound of the river
    and the crack of the guides rope. Sometimes people call out "rapids" when they
    are in a whitewater rafting trip, and these are rapid-fire words that the guides
    yell out to help the rafters know what to expect in the river. Whitewater rafting
    is also available in the form of a "Class III+" trip, which means the rapids are
    more intense than those in a "Class II" trip, and are suitable for expert rafters
    only. |'
  prefs: []
  type: TYPE_TB
- en: '| CommonGen - Bottom 20% |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt | Put the concepts together to form a sentence: groom, cake, cut,
    bride. |'
  prefs: []
  type: TYPE_TB
- en: '| Completion A | Groom and bride are both terms used to describe the head of
    a household. A groom will typically celebrate his wedding with a cake, but a bride
    will typically celebrate her wedding with a cutting of the cake. A cutting is
    a term used in Buddhism to describe the ceremony where monks receive alms food
    in return for teaching meditation lessons. |'
  prefs: []
  type: TYPE_TB
- en: '| Completion B | Grooming is a vital part of being a bride. It shows that you
    care for your appearance and are taking care of your own hygiene. It also shows
    that you are taking the time to prepare for your big day. A grooming kit is a
    great way to show your preparation and love of detail. A grooming kit can include
    things like hairbrushes, hair ties, hair spray, hair gel, hair scissors, etc.
    Grooming is also a symbol of new beginnings, and a bride should be excited for
    her big day. A grooming kit can be a great way to make the day special and prepare
    for the big celebration. |'
  prefs: []
  type: TYPE_TB
- en: A.2 Annotation Guidelines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We strive to maintain a consistent evaluation framework for our human feedback
    experiments conducted within our work. The following guidelines have been established
    to assist the annotators in conducting systematic and standardized pairwise comparisons
    of model completions.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an annotator, you have the exciting task of selecting the best completion
    by evaluating how well each one covers the given guidelines in relation to the
    other completion. Take a thorough look at both completions, and the winner should
    be the one that truly stands out compared to its counterpart. Remember, it’s not
    necessary to check off all rules perfectly; instead, consider which completion
    adheres to the guidelines to the highest extent. If both completions demonstrate
    a similar level of adherence, feel free to choose the ‘Both good’ option. On the
    other hand, if neither of the options can fulfill the task or adhere to these
    rules, feel free to select the ‘Both bad‘ option. Rules are: 1. Task fulfilment:
    The most important rule is whether completions fulfill the task. For example in
    case of ‘common_gen_Put_together‘ prompts, all concept words should be included
    in one, and only one, sentence as stated in the prompt. Completions that separate
    the content into multiple sentences might not be in line with the original task.
    However, if a completion has more than one sentence, and one of those sentences
    fulfills the task requirement, it is also considered acceptable. 2. Grammar: If
    both completions respect the first rule, grammar would be the next important deciding
    factor. 3. Semantic: If the sentence makes sense at all should be considered.
    4. Creativity: Personal preference can play a part in your evaluation process
    when other criteria are fulfilled.'
  prefs: []
  type: TYPE_NORMAL
- en: A.3 Dataset-Specific Instructions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section provides instructions tailored to each dataset, complementing the
    main annotation guidelines to further ensure consistent assessments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Soda — A multi-turn conversation with some context in the intro truncated at
    a random next turn utterance: • No specific task is explicitly described • The
    completion is expected to include the next turn in the conversation, preferably
    by incorporating the given context • A completion may also include the whole prompt
    (context and/or dialogue), then the next turn of the conversation. It can also
    extend to multiple turns completion. As long as the dialogue is fluent and within
    context, this is also acceptable. AdversarialQA — Information extractive task
    given some context and a related question: • The completion should include information
    from given context. • The completion can be a one word answer or a whole paragraph.
    • The labels for this dataset can be found shared if needed for reference to speed
    up the annotation process, as some questions tend to be “tricky". CommonGen —
    Construct a sentence with all provided concept words: • Concept words examples:
    sit —> sat, ball —> football, swim —> swimmer • A completion should include at
    least one sentence with all concept words. • A completion may start with “The
    sentence is” • A completion may also include a description of the example sentence.
    • A completion with concept words split between two or more sentences is not Ok.
    CommonsenseQA — Explain why one choice among others is the correct answer: • Completions
    may include the prompt itself or part of it. This is Ok. • The completion should
    have a decent level of common sense, logical or moral reasoning, depending on
    the situation. • Some completions will start hallucinating scenarios as a possible
    explanation, these are per-case acceptable. If you are unsure, please share the
    example on the slack channel!'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Pairwise Comparison Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We detail the outcome rates of each pairwise comparison discussed in Section
    [4](#S4 "4 Results and Analysis ‣ Which Prompts Make The Difference? Data Prioritization
    For Efficient Human LLM Evaluation"). The summarized results are depicted in Figure [6](#A2.F6
    "Figure 6 ‣ Appendix B Pairwise Comparison Results ‣ Which Prompts Make The Difference?
    Data Prioritization For Efficient Human LLM Evaluation"). We observe that models
    within the same family, particularly those of proximate number or parameters,
    such as dolly-v2-12b vs. dolly-v2-7b, display analogous responses attributed to
    their shared architectures and overlapping training data, resulting in higher
    tie rates. On the other hand, comparisons between models from different families
    yield lower tie rates. Such models exhibit varied writing styles, stemming from
    their unique training backgrounds, making evaluations more decisive. Our evaluation
    prompts set, curated to encompass diverse tasks, accentuates these output disparities,
    with certain tasks proving more intuitive for some models and challenging for
    others.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6b3d1763e5ef63965739ca884a02dba0.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) flan-t5-xxl vs. flan-t5-xl
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/eff9fc58987fbfa8b2287378633d87a7.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) dolly-v2-12b vs. dolly-v2-7b
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dc6dda92a488467662532be185245f6c.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) flan-t5-xxl vs. dolly-v2-12b
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e521a3fdbe5089dfec743c4646484de8.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) MPT-7b-instruct vs. falcon-7b-instruct
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Barplots representing the win rates per model for each of our four
    experiments, alongside the rate of tie outcomes.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Elo Scores Analysis Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Building upon section [2.2](#S2.SS2 "2.2 Applying our ranking in Elo rating
    computation ‣ 2 Methodology ‣ Which Prompts Make The Difference? Data Prioritization
    For Efficient Human LLM Evaluation"), where we examine the impact of our ranking
    metrics on the Robustness of Elo Scores used to relatively rank models, we provide
    a direct comparison between inter- and intra-family results (see Fig. [7](#A3.F7
    "Figure 7 ‣ Appendix C Elo Scores Analysis Results ‣ Which Prompts Make The Difference?
    Data Prioritization For Efficient Human LLM Evaluation")). Additionally, we chart
    the progression of Elo ratings throughout the evaluation process. Compared to
    a random sequence (refer to Fig. [8](#A3.F8 "Figure 8 ‣ Appendix C Elo Scores
    Analysis Results ‣ Which Prompts Make The Difference? Data Prioritization For
    Efficient Human LLM Evaluation")), our ranking metrics amplify the divergence
    of Elo scores, especially within the early top k% of evaluation instances, up
    to top 40 percentile.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b0e8918eea21beeb197d9ad9a500188c.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) flan-t5-xxl vs. flan-t5-xl
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2a23f26dec8d2cefed0fb877a129557b.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) dolly-v2-7b vs. dolly-v2-12b
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f7b423ab742b82ba1dc44295c73d6622.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) flan-t5-xxl vs. dolly-v2-12b
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/49364e1c114c72948aec1714dd5c75d0.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) MPT-7b-instruct vs. falcon-7b-instruct
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7: Elo Scores for inter- and intra-family experiments. Dashed lines
    indicate the ‘Gold Standard’ values for each model; refer to Table [3](#S4.T3
    "Table 3 ‣ 4.4 Elo Score Robustness ‣ 4 Results and Analysis ‣ Which Prompts Make
    The Difference? Data Prioritization For Efficient Human LLM Evaluation") for details.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a3d5e8dd5bf333fc74dd05e2308e63de.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) flan-t5-xxl vs. flan-t5-xl
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f8925b747b509641345dbdb05a056ac1.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) dolly-v2-7b vs. dolly-v2-12b
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/95e9e989bd9921e5bddc1e67217abff9.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) flan-t5-xxl vs. dolly-v2-12b
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/606df771a42a19cc0535661ab6a95dec.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) MPT-7b-instruct vs. falcon-7b-instruct
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8: Elo Ratings Analysis for Inter- and Intra-Family Model Comparisons.
    We contrast the ranking methods, KL Divergence and Cross-Entropy, against a Random
    Sequence strategy to demonstrate their impact on ratings update per evaluation
    instance.'
  prefs: []
  type: TYPE_NORMAL
