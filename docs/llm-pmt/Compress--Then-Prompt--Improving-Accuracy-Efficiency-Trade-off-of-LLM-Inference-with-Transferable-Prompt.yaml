- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:51:20'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference
    with Transferable Prompt'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2305.11186](https://ar5iv.labs.arxiv.org/html/2305.11186)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Zhaozhuo Xu Equal contribution. The order of authors is determined by flipping
    a coin. Department of Computer Science, Rice University Zirui Liu^* Department
    of Computer Science, Rice University Beidi Chen Department of Electrical and Computer
    Engineering, Carnegie Mellon University Yuxin Tang Department of Computer Science,
    Rice University Jue Wang ETH Zürich, Switzerland Kaixiong Zhou Department of Computer
    Science, Rice University Xia Hu Department of Computer Science, Rice University
    Anshumali Shrivastava Department of Computer Science, Rice University
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: While the numerous parameters in Large Language Models (LLMs) contribute to
    their superior performance, this massive scale makes them inefficient and memory-hungry.
    Thus, they are hard to deploy on commodity hardware, such as one single GPU. Given
    the memory and power constraints of such devices, model compression methods are
    widely employed to reduce both the model size and inference latency, which essentially
    trades off model quality in return for improved efficiency. Thus, optimizing this
    accuracy-efficiency trade-off is crucial for the LLM deployment on commodity hardware.
    In this paper, we introduce a new perspective to optimize this trade-off by prompting
    compressed models. Specifically, we first observe that for certain questions,
    the generation quality of a compressed LLM can be significantly improved by adding
    carefully designed hard prompts, though this isn’t the case for all questions.
    Based on this observation, we propose a soft prompt learning method where we expose
    the compressed model to the prompt learning process, aiming to enhance the performance
    of prompts. Our experimental analysis suggests our soft prompt strategy greatly
    improves the performance of the $8\times$ compressed LLaMA-7B model (with a joint
    4-bit quantization and 50% weight pruning compression), allowing them to match
    their uncompressed counterparts on popular benchmarks. Also, we demonstrate that
    these learned prompts can be transferred across various datasets, tasks, and compression
    levels. Hence with this transferability, we can stitch the soft prompt to a newly
    compressed model to improve the test-time accuracy in an “in-situ” way.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) [[27](#bib.bib27), [28](#bib.bib28), [2](#bib.bib2),
    [40](#bib.bib40), [33](#bib.bib33)] has revolutionized the field of Natural Language
    Processing (NLP). Notably, LLMs are known for their in-context learning ability,
    allowing them to generalize to unseen tasks without additional fine-tuning [[2](#bib.bib2)].
    Specifically, LLMs are controlled through user-provided natural language specifications
    of the task, or *prompts*, which illustrate how to complete a task. Equipped with
    the in-context learning ability, we only need to serve a single large model to
    efficiently handle different tasks. Despite of their remarkable adaptability,
    LLMs are very expensive to deploy [[3](#bib.bib3), [35](#bib.bib35)]. The inference
    process of LLMs, such as LLaMA 2 [[34](#bib.bib34)], may require multiple powerful
    GPUs, which is prohibitively expensive for the general community. Consequently,
    it is crucial to facilitate LLM inference on more accessible hardware, such as
    a single GPU, which inherently has limited computational and memory resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'To address this problem, model compression methods are widely employed to reduce
    the model size and inference latency, such as quantization [[26](#bib.bib26),
    [4](#bib.bib4), [36](#bib.bib36), [7](#bib.bib7)] and pruning [[6](#bib.bib6)].
    These methods essentially trade off model quality in return for reduced latency
    and model size. Thus, there is an inevitable trade-off between accuracy and efficiency,
    resulting in a noticeable reduction in the model’s accuracy and, consequently,
    the overall performance benefits of LLMs. To get a sense, as shown in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Compress, Then Prompt: Improving Accuracy-Efficiency
    Trade-off of LLM Inference with Transferable Prompt"), the full model (LLaMA-7B)
    is able to provide accurate answers to all three questions. However, the pruned
    model generates unrelated and off-topic answers to the same questions.'
  prefs: []
  type: TYPE_NORMAL
- en: Both model compression and prompts can influence the generation quality of LLMs.
    Thus intuitively, we can also utilize the prompt to help the compressed model
    generate more relevant answers. To the best of our knowledge, this perspective
    is not fully explored for LLMs. Thus one natural question is, *for a compressed
    model, can we design a prompt that helps it correct its predictions accordingly?*
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, we provide the first affirmative answer to the above question.
    As shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Compress, Then Prompt:
    Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt"),
    we manually attach the prompt “*Please carefully examine the weight matrix within
    the model, as it may contain errors. It is crucial to verify its accuracy and
    make any necessary adjustments to ensure optimal performance*” to the original
    question. The prompted pruned model, i.e., “LLaMA-7B (62.5% sparsity) w./ Hard
    Prompt” in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Compress, Then Prompt:
    Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt"),
    shows a significant improvement in its responses, although not all of them are
    accurate or complete. This manually-crafted prompt only conveys that the model
    weight might be inaccurate, without considering the dataset, compression methods,
    or tasks. This finding highlights the considerable potential for the transferability
    of this “hard prompt” across datasets, compression levels, and tasks. Despite
    the potential, this manually designed prompt is not consistently effective. Inspired
    by previous learnable prompt works [[19](#bib.bib19), [18](#bib.bib18)], we hypothesize
    that by involving the compressed weight in the prompt learning process, a learnable
    prompt could potentially surpass the performance of the manually-designed prompt,
    while maintaining the transferability. Building upon this insight, we introduce
    a paradigm of prompt learning that seeks to train additive prompt tokens on a
    compressed LLM to enhance its accuracy. We underscore that the primary distinction
    between our prompt learning approach and previous prompt tuning frameworks [[19](#bib.bib19),
    [18](#bib.bib18), [32](#bib.bib32)] is that earlier methods mainly utilized the
    prompt to adapt the model for specific downstream tasks. In contrast, the learned
    prompt in this paper resembles the hard prompt in Figure [1](#S1.F1 "Figure 1
    ‣ 1 Introduction ‣ Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off
    of LLM Inference with Transferable Prompt"), as it can be transferred between
    various datasets, compression methods, and tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/feb7c0645cd18f5e1a431e7305ba3a6c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The hard prompt enables compressed LLMs to regain commonsense. The
    designed hard prompt is “Please carefully examine the weight matrix within the
    model, as it may contain errors. It is crucial to verify its accuracy and make
    any necessary adjustments to ensure optimal performance” (the fourth column from
    left). We highlight the improved answers with green color.'
  prefs: []
  type: TYPE_NORMAL
- en: Our experimental analysis suggests our method greatly improves the performance
    of the $8\times$ compressed LLaMA-7B model (with a joint 4-bit quantization and
    50% weight pruning compression), allowing them to match their uncompressed counterparts
    on several standard benchmarks. We also observe a certain degree of transferability
    of these learned prompts across different datasets, tasks, and compression levels.
    Hence with this transferability, we can stitch the soft prompt to a newly compressed
    model to improve the test-time accuracy in an “in-situ” way.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Problem Statement and Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will begin by introducing the efficiency bottleneck of LLM
    inference. Then we will introduce current approximation approaches that are designed
    to reduce the computation and memory overhead and improve LLM inference latency.
    Finally, we will provide a review of recent progress that has been made in the
    development of prompts for LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Efficiency Bottleneck of LLM Inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LLMs adopt a decoder-only, autoregressive approach where token generation is
    carried out step by step, with each token’s generation dependent on the previously
    generated results. For instance, models such as GPT [[27](#bib.bib27), [28](#bib.bib28),
    [2](#bib.bib2)] follow this paradigm. A recent study by [[20](#bib.bib20)] investigates
    the inference process of OPT-175B models and finds that (1) token generation is
    the dominant factor contributing to the inference latency, and (2) Multilayer
    Perceptron (MLP) incurs higher I/O and computation latency compared to attention
    blocks during token generation. While system-level optimizations [[30](#bib.bib30),
    [9](#bib.bib9), [10](#bib.bib10)] can enhance the inference time of LLMs, they
    do not directly mitigate the computation and memory I/Os involved in the LLM inference
    process.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Approximation in LLM Inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In addition to optimizing at the system level, there are two primary approaches
    for reducing both computation and memory I/O to minimize the latency in inference.
    (1) Sparse modeling: the general idea is to choose a particular set of weights
    in certain layers to minimize both computation and memory I/O [[6](#bib.bib6),
    [20](#bib.bib20)]. These techniques are also closely related to pruning [[12](#bib.bib12),
    [15](#bib.bib15), [17](#bib.bib17), [14](#bib.bib14)] in the literature. Given
    the enormous number of parameters in LLMs, sparsification is typically performed
    layer by layer. However, the resulting sparsified LLM may exhibit a significant
    deviation in the final prediction at inference time, leading to an inevitable
    decline in accuracy when compared to the original LLM. (2) Quantization: it refers
    to the process of compressing trained weight values in LLMs into lower bits [[26](#bib.bib26),
    [4](#bib.bib4), [36](#bib.bib36), [7](#bib.bib7)]. Empirical evaluations have
    shown that int8 quantization can provide a great approximation of the predictive
    performance of the original LLMs [[4](#bib.bib4)]. However, there is a significant
    decline in accuracy when attempting to reduce the number of bits even further.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Prompt for LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LLMs are known for their in-context learning ability, allowing them to generalize
    to unseen tasks without additional fine-tuning [[2](#bib.bib2)]. Specifically,
    LLMs are controlled through user-provided natural language specifications of the
    task, or *prompts*, which illustrate how to complete a task. In this paradigm,
    we do not enforce modifications on the LLMs themselves. Instead, we focus on adapting
    the inputs to the LLMs for better predictive performance in downstream tasks.
    A typical strategy is to insert tokens before the input sequence to affect the
    attention mechanism. It has been shown in [[2](#bib.bib2)] that prompt engineering
    enables LLMs to match the performance of fine-tuned language models on a variety
    of language understanding tasks. Moreover, [[18](#bib.bib18)] empirically indicate
    that there is an equivalence between modifying the input and fine-tuning the model.
    Furthermore, [[31](#bib.bib31)] studies the transferability of prompts across
    similar datasets or even tasks. Since then, we have witnessed the growth of prompt
    tuning infrastructure [[5](#bib.bib5)]. However, we would like to emphasize that
    most of the current demonstrations of prompt tuning are task-specific [[19](#bib.bib19),
    [18](#bib.bib18)]. When considering efficiency, it is desirable for a prompt to
    exhibit transferability across various settings.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Motivation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The compression methods reduce the computational complexity at the cost of giving
    less accurate outputs. Thus, there naturally exists an accuracy-efficiency trade-off.
    In this section, we first empirically evaluate the trade-off of compressed LLMs.
    Then we found that for a compressed model, we can manually design a hard prompt
    that informs the model of its compressed state and helps it correct its predictions
    accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Performance of the Existing Approaches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/21e61511b80faecdf4405077c3ea76c5.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Quantization
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e4ba1d43b04e23c730c5bf2b5084fd25.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Pruning
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: The validation perplexity of LLaMA-7B on C4 dataset at different
    compression level. The green line is the PPL of the original model.'
  prefs: []
  type: TYPE_NORMAL
- en: Experimental Setup. We assess the trade-off using LLaMA [[33](#bib.bib33)] on
    C4 dataset [[29](#bib.bib29)]. Here we adopt two representative post-training
    compression methods, i.e., GPTQ [[7](#bib.bib7)] and SparseGPT [[6](#bib.bib6)],
    to analyze the trade-off across various compression levels. We note that we choose
    post-training compression methods primarily for their ease of deployment. For
    the quantization method, we apply GPTQ to compress the model weights into 2, 3,
    and 4 bits integer numbers. As for the pruning method, we employ SparseGPT to
    eliminate 50%, 62.5%, and 75% of the model parameters. We would like to note that
    the post-training compression is conducted using the training set of C4, and subsequently,
    we evaluate the performance of the compression with the validation set of C4.
  prefs: []
  type: TYPE_NORMAL
- en: 'Quantitative Results. As shown in Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Performance
    of the Existing Approaches ‣ 3 Motivation ‣ Compress, Then Prompt: Improving Accuracy-Efficiency
    Trade-off of LLM Inference with Transferable Prompt"), we visualize the evaluation
    perplexity (PPL) [[16](#bib.bib16)] versus the compression level. When we prune
    50% of the parameters or quantize the parameters to 4 bits, the PPL remains closer
    to that of the full LLaMA model. The PPL consistently increases as we decrease
    the allocated resource (e.g., bit-width/sparsity). Notably, the PPL will explode
    when the resource is below a certain threshold. For instance, the PPL shifts from
    14 to 53 as sparsity increases from 62.5% to 75%. Moreover, the PPL grows significantly
    from around 11 to around 691 when we lower the quantization bits from 3-bit to
    2-bit.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Qualitative Results. As shown in the left part of Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off
    of LLM Inference with Transferable Prompt"), besides PPL, we also do a case study
    to understand how compression affects model generation results. In this example,
    the full model is able to provide accurate and relevant answers to all three simple
    questions. Specifically, it correctly identifies Long Beach as a city in Los Angeles
    County, California, pinpoints Tulsa in northeastern Oklahoma, and describes asparagus
    as a spring vegetable belonging to the lily family. However, the pruned model
    with 62.5% weight sparsity struggles to generate meaningful responses. Instead
    of providing the requested information, its answers seem unrelated and tangential.
    For example, the pruned model responds with a statement about seeking a job when
    asked about Long Beach, mentions being a student at the University of Tulsa when
    asked about Tulsa’s location, and admits uncertainty about Asparagus. This case
    study demonstrates that aggressive model compression, such as the 62.5% weight
    sparsity applied to the pruned model, can lead to a significant degradation in
    the quality of generated responses.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Prompt Compressed Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In-context learning refers to the ability of adapting to the context provided
    within the input data through user-provided natural language specifications [[37](#bib.bib37),
    [25](#bib.bib25)], often referred to as *prompts*. Prompts serve to guide LLMs
    toward generating desired predictions by offering useful contextual information.
    As shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Compress, Then Prompt:
    Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt"),
    the compressed model generates answers that are unrelated and off-topic when responding
    to these simple questions. Thus one natural question is, *for a compressed model,
    can we design a specific prompt that helps it correct its predictions accordingly?*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Following the question, we manually design the hard prompt as “*Please carefully
    examine the weight matrix within the model, as it may contain errors. It is crucial
    to verify its accuracy and make any necessary adjustments to ensure optimal performance*”.
    The results are shown in the fourth column of Figure [1](#S1.F1 "Figure 1 ‣ 1
    Introduction ‣ Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off
    of LLM Inference with Transferable Prompt"). The observations are summarized as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The prompted pruned model, i.e., “LLaMA-7B (62.5% sparsity) w./ Hard Prompt”
    in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Compress, Then Prompt: Improving
    Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt"), shows
    a significant improvement in its responses, although not all of them are accurate
    or complete. Specifically, (1) when explicitly told about its compressed state,
    the prompted pruned model correctly identifies that Long Beach is located in the
    United States. However, it does not provide further information about the city,
    such as its presence in Los Angeles County, California. (2) Regarding the second
    question about Tulsa, Oklahoma, the prompted pruned model fails to provide a relevant
    answer, instead repeating our prompt about the compression state, which is unrelated
    to the question. (3) When asked about asparagus, the prompted pruned model correctly
    identifies it as a plant used for cooking.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Insights. By explicitly informing the model of its compressed state, LLMs can
    generate more relevant responses for certain questions. The success of the designed
    prompt implies three great potentials:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cross-Dataset Transferability. This human-designed prompt only provides the
    information that model weight is inaccurate. So intuitively, irrespective of the
    specific dataset being used, we hypothesize that the LLMs can generate more relevant
    responses with the same prompt.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cross-Compression Transferability. Similarly, the human-designed prompt only
    mentions that the weight is inaccurate, without specifying the exact compression
    level or method. We hypothesize that LLMs can generate more relevant responses
    with the same prompt across different compression levels and methods.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cross-Task Transferability. If LLMs can understand their compressed state and
    adjust accordingly, this adaptability is not limited to specific tasks or problem
    domains. Instead, it can be extended to a wide range of tasks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: However, despite the potential, as we analyzed at the beginning of this section,
    the manually designed prompt is not consistently effective. In other words, it
    only works for some problems, and not all answers generated are accurate or complete.
    Inspired by previous learnable prompt work [[19](#bib.bib19), [18](#bib.bib18)],
    we hypothesize that by involving the compressed weight in the prompt learning
    process, a learnable prompt could potentially surpass the performance of the hard
    prompt while still retaining the transferability aspects of the hard prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Learning Prompt for Efficient LLM Inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will begin by introducing the formulation of the prompt
    learning paradigm. Then, we will shift our focus to the maximum likelihood objective
    of learning the prompt. Finally, we will delve into the transferability of the
    learned prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Formulation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Section [3.2](#S3.SS2 "3.2 Prompt Compressed Models ‣ 3 Motivation ‣ Compress,
    Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable
    Prompt") has shown that incorporating prompts can enhance the predictive performance
    of compressed LLMs. However, discovering effective language-based prompts through
    trial and error is a cumbersome and inefficient process that requires exploring
    a vast vocabulary space. Therefore, this paper aims to develop a data-driven approach
    to learning a soft prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: Typically an LLM would have a tokenizer that maps each input sentence into a
    sequence of integers $[x_{0},x_{1},\cdots,x_{n}]$.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Learning Objectives
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this study, we present a prompt learning strategy that can be utilized as
    a post-training process for compressed LLMs. Given an LLM model with parameters
    denoted as $\theta$ before it. Next, we optimize the following objective.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'We note that the model parameter $\widetilde{\theta}$. Specifically, the Eq ([1](#S4.E1
    "In 4.2 Learning Objectives ‣ 4 Learning Prompt for Efficient LLM Inference ‣
    Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference
    with Transferable Prompt")) aims to maximize the likelihood of correctly predicting
    the next token in the sequence, given the preceding tokens. In this way, the learned
    prompt is aware of the compressed weights, as the gradient flows through these
    compressed weights during the optimization process. This allows the model to adapt
    its behavior to account for the compression effects while generating responses,
    potentially leading to improved performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Transferability of Learned Prompt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The findings derived from Section [3.2](#S3.SS2 "3.2 Prompt Compressed Models
    ‣ 3 Motivation ‣ Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off
    of LLM Inference with Transferable Prompt") have provided us with a compelling
    impetus to delve into the exploration of the transferability of prompt tokens
    acquired through Eq ([1](#S4.E1 "In 4.2 Learning Objectives ‣ 4 Learning Prompt
    for Efficient LLM Inference ‣ Compress, Then Prompt: Improving Accuracy-Efficiency
    Trade-off of LLM Inference with Transferable Prompt")). The representation of
    these prompt tokens, as well as their acquisition through one dataset, could have
    a significant impact on other NLP applications. Specifically, we have chosen to
    concentrate on the scenarios below.'
  prefs: []
  type: TYPE_NORMAL
- en: Cross-Dataset Transferability. We aim to investigate whether prompt tokens trained
    from one dataset are applicable to other datasets. Prompt learning, while more
    efficient than fine-tuning, necessitates significant computational power and memory.
    With a single Nvidia-A100 possessing 40GB of memory, only the prompt learning
    of the LLaMA-7B model using a batch size of 1, sequence length of 1024, and 100
    prompt tokens can be supported. If we perform a single round of prompt learning
    for a compressed LLM and achieve favorable outcomes across various datasets, we
    can substantially enhance the accuracy-efficiency trade-offs of the LLM during
    inference.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-Compression Transferability. We aim to investigate the feasibility of
    utilizing learned prompts trained from a compressed LLM to another compressed
    LLM with different compression levels. For instance, we assess whether a prompt
    trained on a sparse LLM with a 75% sparsity can effectively boost the performance
    of an LLM with a 50% weight sparsity. Additionally, we also examine the applicability
    of prompts trained on a sparse LLM when used with a quantized LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cross-Task Transferability. We aim to investigate whether the learned prompt
    trained from Eq ([1](#S4.E1 "In 4.2 Learning Objectives ‣ 4 Learning Prompt for
    Efficient LLM Inference ‣ Compress, Then Prompt: Improving Accuracy-Efficiency
    Trade-off of LLM Inference with Transferable Prompt")) on token generation tasks
    can be applied to other NLP tasks. This exploration will prove the effectiveness
    of prompts in improving the accuracy-efficiency trade-offs in the zero-shot generalization
    of LLMs in downstream tasks such as question answering.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experiment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we assess the effectiveness of our prompt strategy in enhancing
    the trade-off between accuracy and efficiency during LLM inference. We commence
    by outlining the experimental setup, followed by presenting the results of token
    generation. Furthermore, we investigate the transferability of prompts across
    different datasets and compression levels. For additional experiments related
    to transferability and efficiency, please refer to Appendix [A](#A1 "Appendix
    A More Experiments ‣ Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off
    of LLM Inference with Transferable Prompt"), where we have included further details.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Experiment Setting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In our experimental framework, we incorporated the use of an Nvidia V100 GPU
    to conduct inference and prompt learning in LLMs. The datasets we utilized for
    token generation were comprehensive, including the Common Crawl’s web corpus (C4) [[29](#bib.bib29)],
    Wikitext-2 [[23](#bib.bib23)], and the Penn Treebank (PTB) [[22](#bib.bib22)]
    databases. We set the sequence length for these datasets to 1024\. For the token
    generation task, we use perplexity (PPL) [[16](#bib.bib16)] as the evaluation
    metric. We also introduce some downstream tasks to evaluate the cross-task transferability
    of the learned prompt. We will introduce the task information in the specific
    section. At the core of our modeling approach, we adopted the Open Pre-trained
    Transformer (OPT) Language Models [[40](#bib.bib40)] and Large Language Model
    Architecture (LLaMA) [[33](#bib.bib33)]. To compress the OPT and LLaMA model,
    we employed techniques from both SparseGPT [[6](#bib.bib6)] and GPTQ [[7](#bib.bib7)]
    methodologies. We refer the readers to Appendix [A.1](#A1.SS1 "A.1 Experiment
    Details ‣ Appendix A More Experiments ‣ Compress, Then Prompt: Improving Accuracy-Efficiency
    Trade-off of LLM Inference with Transferable Prompt") for more experimental details.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Token Generation Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'On the C4 training set, we compress the OPT-1.3B, OPT-2.7B, OPT-6.7B, and LLaMA-7B
    using SparseGPT [[6](#bib.bib6)]. We utilize sparsity levels of 50%, 62.5%, and
    75% for compression. Additionally, we employ GPTQ [[7](#bib.bib7)] for 2-bit,
    3-bit, and 4-bit quantization. Furthermore, prompt learning is applied to each
    compressed model using the methodology introduced in Eq ([1](#S4.E1 "In 4.2 Learning
    Objectives ‣ 4 Learning Prompt for Efficient LLM Inference ‣ Compress, Then Prompt:
    Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt")).
    We set $k$ in Eq. [1](#S4.E1 "In 4.2 Learning Objectives ‣ 4 Learning Prompt for
    Efficient LLM Inference ‣ Compress, Then Prompt: Improving Accuracy-Efficiency
    Trade-off of LLM Inference with Transferable Prompt") to 100, i.e., incorporating
    100 learnable prompt tokens. In Table [1](#S5.T1 "Table 1 ‣ 5.2 Token Generation
    Results ‣ 5 Experiment ‣ Compress, Then Prompt: Improving Accuracy-Efficiency
    Trade-off of LLM Inference with Transferable Prompt"), we also conduct the ablation
    study on the impact of the number of soft tokens using 3-bit quantized LLaMA-7B
    on PTB dataset. We observe that there is still a significant improvement with
    25 prompt tokens, and we can improve the performance by increasing the prompt
    size.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Ablation study on the impact of the number of soft tokens using 3-bit
    quantized LLama-7B on PTB dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| # tokens | Perplexity |'
  prefs: []
  type: TYPE_TB
- en: '| Baseline (0 tokens) | 15.74 |'
  prefs: []
  type: TYPE_TB
- en: '| 25 tokens | 9.26 |'
  prefs: []
  type: TYPE_TB
- en: '| 50 tokens | 8.61 |'
  prefs: []
  type: TYPE_TB
- en: '| 75 tokens | 8.17 |'
  prefs: []
  type: TYPE_TB
- en: '| 100 tokens | 7.76 |'
  prefs: []
  type: TYPE_TB
- en: 'Figure [3](#S5.F3 "Figure 3 ‣ 5.2 Token Generation Results ‣ 5 Experiment ‣
    Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference
    with Transferable Prompt") demonstrates the impact of our approach on the validation
    set of C4\. We observe a significant improvement in PPL across all compression
    levels. Firstly, by employing soft prompt tokens, the compressed LLMs using SparseGPT
    with 50% sparsity even outperform the full model counterparts, exhibiting lower
    PPL. This trend is also observed in the 4-bit quantization of LLMs using GPTQ.
    Secondly, even with further enhanced compression, the compressed LLMs with soft
    prompt tokens learned from Eq ([1](#S4.E1 "In 4.2 Learning Objectives ‣ 4 Learning
    Prompt for Efficient LLM Inference ‣ Compress, Then Prompt: Improving Accuracy-Efficiency
    Trade-off of LLM Inference with Transferable Prompt")) still maintain comparable
    PPL to their original counterparts. Notably, prompts learned from each of the
    four 3-bit quantized models aid in surpassing the performance of their respective
    full model counterparts. We also observe a similar effect in sparse models with
    62.5% sparsity for OPT-1.3B and OPT-2.7B. Conversely, prompts learned from both
    OPT-6.7B and LLaMA-7B assist in achieving the same PPL as their full model counterparts.
    Lastly, our approach significantly enhances the predictive performance of extreme
    scale compression. In both SparseGPT with 75% sparsity and GPTQ with 2-bit quantization,
    we find that the prompt learning strategy substantially improves the PPL across
    all four models. For example, prompts learned over the 2-bit GPTQ compression
    of OPT-1.3B reduce the PPL from 2337.8 to 59.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/54845cbb8b0a35cbc11663b46318c277.png)![Refer to caption](img/d815b17e952b49fbe10ffd4039c6acce.png)![Refer
    to caption](img/871112434c354959a0083005090d1f3e.png)![Refer to caption](img/2f2ed94e12ee95703b7f16d9eefe9ace.png)![Refer
    to caption](img/f637f09b1c9071616dadc3471093ab54.png)![Refer to caption](img/8814daa37b68cc20a1d781156d8feb7f.png)![Refer
    to caption](img/486d311142e2839cd798913cd4d6d21e.png)![Refer to caption](img/218fa6e146804b23e912a8a362222c6c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: OPT-1.3B, OPT-2.7B, OPT-6.7B, and LLaMA-7B on C4 dataset, validation
    set at different bit-width and sparsity. Here the “Baseline” (green line) represents
    the uncompressed model.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Cross-Dataset Transferability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Intuitively, a model compressed using one dataset should achieve decent predictive
    performance when transferred to other datasets [[7](#bib.bib7), [6](#bib.bib6)].
    Here we assess whether the prompt tokens learned from one dataset exhibit similar
    transferability across different datasets. Specifically, we first compress a model
    with SparseGPT or GPTQ using C4 training set. We then learn the prompt with the
    compressed model on C4 training set. Finally, we evaluate the performance of this
    compressed model with and without the learned prompts on other datasets, e.g.,
    Wikitext-2 and PTB dataset. We emphasize the entire process does not involve any
    task-specific data, and our results thus remain “zero-shot”.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [4](#S5.F4 "Figure 4 ‣ 5.3 Cross-Dataset Transferability ‣ 5 Experiment
    ‣ Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference
    with Transferable Prompt") presents the performance of OPT-1.3B, OPT-2.7B, OPT-6.7B,
    and LLaMA-7B on the test set of Wikitext-2 and the PTB dataset. For each LLM model,
    we also include the performance of its compressed versions with 50%, 62.5%, and
    75% sparsity using SparseGPT. Additionally, we include the performance of each
    model’s compressed version with 2-bit, 3-bit, and 4-bit quantization using GPTQ.
    The figures demonstrate the consistent advantages of prompt tokens across the
    two datasets. For every model with 50% sparsity or 4-bit quantization, learning
    prompts from the C4 dataset result in a lower PPL compared to the full model counterpart.
    Moreover, we observe a substantial improvement in PPL when using learned prompt
    tokens as the model becomes more compressed. This phenomenon validates that the
    prompts learned on top of compressed models can be effectively transferred across
    datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Due to the page limits, we also conduct the ablation experiments on the transferability
    in Appendix [A.2](#A1.SS2 "A.2 Ablation on the Transferability ‣ Appendix A More
    Experiments ‣ Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of
    LLM Inference with Transferable Prompt"). Specifically, we compare the transferred
    soft prompts against the soft prompts that are trained on the downstream dataset,
    which serve as the top-line counterpart. We also observe that with learned soft
    prompt, the gap between the full model and quantized model is greatly reduced'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a37604363d72cb4bc3fbbeb90764fa9c.png)![Refer to caption](img/30023b13bfe0a86e02d25d0092d07b20.png)![Refer
    to caption](img/b5b5a9ce09dd5957c6a421b8f4a125a3.png)![Refer to caption](img/cb4c8b3203502c36765bc9216a37e56d.png)![Refer
    to caption](img/56f48705ea099928db36e3d197216e7b.png)![Refer to caption](img/9421fa41c53c6eff38f7f0e63e96a090.png)![Refer
    to caption](img/663ee1912cdcc3644de0ac38a007ad29.png)![Refer to caption](img/edccb2bf54b8098e9c6046b2cdeb7c3c.png)![Refer
    to caption](img/ab41f3e7dab7a5b42403dded1a512534.png)![Refer to caption](img/7c246064d76beedaaef68d81d33a0761.png)![Refer
    to caption](img/7709a039188a9d86af4fbbb34b10eade.png)![Refer to caption](img/fc9fd246230c507502b395392011f495.png)![Refer
    to caption](img/0bd89a4fd86659df94e2b534694d2f20.png)![Refer to caption](img/8dde0f51f17b6fb10e098cfe71a26c01.png)![Refer
    to caption](img/3de97c8cab5600ab2e4f896763bbe81d.png)![Refer to caption](img/13646aa56e39f660c3e7750e58e5bb41.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: OPT-1.3B, OPT-2.7B, OPT-6.7B, and LLaMA-7B on Wikitext-2 and PTB
    test set at different bit-width and sparsity. Here the “Baseline” (green line)
    represents the uncompressed model.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Cross-Compression Transferability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we assess the transferability of learned prompts across various
    compression levels. Specifically, we aim to address the following questions: (1)
    Can the prompt learned from an LLM compressed through sparsification at a specific
    sparsity level be applied to other sparse LLMs with different sparsities? (2)
    Can the prompt learned from an LLM quantized to a particular bit level be applied
    to other quantized LLMs with different bits? (3) Is it possible to transfer prompts
    learned from sparse LLMs to quantized LLMs, or vice versa, in order to enhance
    predictive accuracy?'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Figure [5](#S5.F5 "Figure 5 ‣ 5.4 Cross-Compression Transferability ‣ 5
    Experiment ‣ Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of
    LLM Inference with Transferable Prompt"), we assess the performance of employing
    prompts derived from a compressed LLM on other compressed LLMs, employing various
    compression approaches and levels. As an example, we utilize LLaMA-7B and present
    the PPL results on the validation set of C4, as well as the test sets of Wikitext-2
    and PTB. In this context, the “target” refers to the compression type and level
    for the compressed model, while the “source” represents the type and level of
    the compressed model from which the prompt is learned. For example, “source 4-bit”
    indicates that the prompt is learned from a compressed model with 4-bit quantization.
    Based on the figures, we address the raised questions from three perspectives:
    (1) Regarding sparse LLMs, prompts learned from higher sparsity can be effectively
    transferred to models with lower sparsity. For instance, prompts learned from
    62.5% and 75% sparsity can be applied to a sparse LLaMA-7B model with 50% sparsity,
    resulting in a better PPL compared to the original LLaMA-7B model. (2) For quantized
    LLMs, prompts learned from lower bit quantization levels can be successfully applied
    to models with higher bit quantization, while achieving comparable performance.
    (3) There is a certain degree of transferability of prompts learned between different
    compression types, especially when the compression level is less. For instance,
    a prompt learned from a LLaMA-7B model with 4-bit quantization can be transferred
    to a LLaMA-7B model with 50% sparsity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a1e4f2d7b5197ce74a92f8de6b1edaac.png)![Refer to caption](img/80e76cf3ed521ae780fda6e4e4474bea.png)![Refer
    to caption](img/e44448a5f498f1f639be3c384ff5fd55.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: LLaMA-7B transfer between different sparsity and bit-width. The “target”
    refers to the compression type and level for the compressed model, while the“source”
    represents the type and level of the compressed model from which the prompt is
    learned. For example, “4-bit” in source indicates that the prompt is learned from
    a compressed model with 4-bit quantization.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Combination of Sparsification and Quantization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 2: The PPL of joint 50% sparsity + 4-bit quantization with learned prompts
    on the validation set of C4 and a test set of Wikitext-2 and PTB. The prompt is
    learned on C4 training set.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Models | C4 | Wikitext-2 | PTB |'
  prefs: []
  type: TYPE_TB
- en: '| Full | 7.59 | 6.34 | 11.02 |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 50% + 4-bit &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (w./o. prompt) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 10.94 | 9.67 | 17.39 |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 50% + 4-bit &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (w./ prompt) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 7.38 | 7.31 | 10.64 |'
  prefs: []
  type: TYPE_TB
- en: 'In this section, we explore the effectiveness of the prompt strategy in the
    combination of sparsification and quantization for compressing LLM. Since sparsification
    and quantization target different aspects of compression, it is natural to combine
    them to achieve better efficiency. Table [2](#S5.T2 "Table 2 ‣ 5.5 Combination
    of Sparsification and Quantization ‣ 5 Experiment ‣ Compress, Then Prompt: Improving
    Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt") presents
    the PPL before and with, and without the learned prompt on the validation set
    of C4, as well as the test sets of Wikitext-2 and PTB. We choose the LLaMA-7B
    model compressed using 50% sparsity and 4-bit quantization from the training set
    of C4\. We should note that the prompt learning process also takes place on the
    training set of C4\. Our results demonstrate that the prompt learning strategy
    remains effective when combining sparsification and quantization. Additionally,
    with the prompt, the 50% sparse and 4-bit compressed model still performs comparably
    to the original LLaMA-7B.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This research showcases an innovative approach to optimize the trade-off between
    computational efficiency and accuracy in Large Language Models (LLMs). The study
    demonstrates that utilizing a distinct input format and strategically chosen prompts
    can significantly improve the performance of compressed LLMs. The introduction
    of a prompt learning paradigm, which emphasizes the addition of precise prompts
    over a compressed LLM, has shown to enhance their accuracy, often matching and
    even surpassing that of the original models. The research also highlights the
    transferability of these learned prompts across different datasets, tasks, and
    compression levels, revealing promising avenues for further advancements in scaling
    LLMs on common hardware. The results underline the significance of prudent input
    editing to a compressed large model, potentially revolutionizing the way we approach
    LLM inference on standard hardware platforms.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Bisk et al. [2020] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao,
    and Yejin Choi. PIQA: reasoning about physical commonsense in natural language.
    In *The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The
    Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI
    2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence,
    EAAI 2020, New York, NY, USA, February 7-12, 2020*, pages 7432–7439\. AAAI Press,
    2020. URL [https://ojs.aaai.org/index.php/AAAI/article/view/6239](https://ojs.aaai.org/index.php/AAAI/article/view/6239).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877–1901, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2023] Lingjiao Chen, Matei Zaharia, and James Zou. Frugalgpt:
    How to use large language models while reducing cost and improving performance.
    *arXiv preprint arXiv:2305.05176*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. [2022] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    Llm. int8 (): 8-bit matrix multiplication for transformers at scale. *arXiv preprint
    arXiv:2208.07339*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ding et al. [2022] Ning Ding, Shengding Hu, Weilin Zhao, Yulin Chen, Zhiyuan
    Liu, Haitao Zheng, and Maosong Sun. Openprompt: An open-source framework for prompt-learning.
    In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics: System Demonstrations*, pages 105–113, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar and Alistarh [2023] Elias Frantar and Dan Alistarh. Sparsegpt: Massive
    language models can be accurately pruned in one-shot, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar et al. [2022] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. *arXiv preprint arXiv:2210.17323*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. [2021] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony
    DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
    Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and
    Andy Zou. A framework for few-shot language model evaluation, September 2021.
    URL [https://doi.org/10.5281/zenodo.5371628](https://doi.org/10.5281/zenodo.5371628).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GitHub [2023a] GitHub. [https://github.com/mlc-ai/mlc-llm](https://github.com/mlc-ai/mlc-llm),
    2023a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GitHub [2023b] GitHub. [https://github.com/mlc-ai/web-llm](https://github.com/mlc-ai/web-llm),
    2023b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gugger et al. [2022] S Gugger, L Debut, T Wolf, P Schmid, Z Mueller, and S Mangrulkar.
    Accelerate: Training and inference at scale made simple, efficient and adaptable.
    [https://github.com/huggingface/accelerate](https://github.com/huggingface/accelerate),
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. [2018] Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and
    Song Han. Amc: Automl for model compression and acceleration on mobile devices.
    In *Proceedings of the European conference on computer vision (ECCV)*, pages 784–800,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. [2020] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language
    understanding. *arXiv preprint arXiv:2009.03300*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hubara et al. [2021a] Itay Hubara, Brian Chmiel, Moshe Island, Ron Banner,
    Joseph Naor, and Daniel Soudry. Accelerated sparse neural training: A provable
    and efficient method to find n: m transposable masks. *Advances in Neural Information
    Processing Systems*, 34:21099–21111, 2021a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hubara et al. [2021b] Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and
    Daniel Soudry. Accurate post training quantization with small calibration sets.
    In *International Conference on Machine Learning*, pages 4466–4475\. PMLR, 2021b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jelinek et al. [1977] Fred Jelinek, Robert L Mercer, Lalit R Bahl, and James K
    Baker. Perplexity—a measure of the difficulty of speech recognition tasks. *The
    Journal of the Acoustical Society of America*, 62(S1):S63–S63, 1977.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kwon et al. [2022] Woosuk Kwon, Sehoon Kim, Michael W Mahoney, Joseph Hassoun,
    Kurt Keutzer, and Amir Gholami. A fast post-training pruning framework for transformers.
    *arXiv preprint arXiv:2204.09656*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lester et al. [2021] Brian Lester, Rami Al-Rfou, and Noah Constant. The power
    of scale for parameter-efficient prompt tuning. In *Proceedings of the 2021 Conference
    on Empirical Methods in Natural Language Processing*, pages 3045–3059, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li and Liang [2021] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing
    continuous prompts for generation. *arXiv preprint arXiv:2101.00190*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2023] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan,
    Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Ré, and
    Beidi Chen. Deja vu: Contextual sparsity for efficient llms at inference time.
    In *International Conference on Machine Learning*. PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loshchilov and Hutter [2019] Ilya Loshchilov and Frank Hutter. Decoupled weight
    decay regularization. In *7th International Conference on Learning Representations,
    ICLR 2019, New Orleans, LA, USA, May 6-9, 2019*. OpenReview.net, 2019. URL [https://openreview.net/forum?id=Bkg6RiCqY7](https://openreview.net/forum?id=Bkg6RiCqY7).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Marcus et al. [1994] Mitch Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert
    MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. The penn
    treebank: Annotating predicate argument structure. In *Human Language Technology:
    Proceedings of a Workshop held at Plainsboro, New Jersey, March 8-11, 1994*, 1994.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Merity et al. [2017] Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. Pointer sentinel mixture models. In *5th International Conference on Learning
    Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track
    Proceedings*. OpenReview.net, 2017. URL [https://openreview.net/forum?id=Byj72udxe](https://openreview.net/forum?id=Byj72udxe).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mihaylov et al. [2018] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
    Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book
    question answering. In *Proceedings of the 2018 Conference on Empirical Methods
    in Natural Language Processing*, pages 2381–2391, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Min et al. [2022] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis,
    Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations:
    What makes in-context learning work? *arXiv preprint arXiv:2202.12837*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nagel et al. [2020] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos
    Louizos, and Tijmen Blankevoort. Up or down? adaptive rounding for post-training
    quantization. In *International Conference on Machine Learning*, pages 7197–7206\.
    PMLR, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. [2018] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever,
    et al. Improving language understanding by generative pre-training. 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. [2019] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners.
    *OpenAI blog*, 1(8):9, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *The
    Journal of Machine Learning Research*, 21(1):5485–5551, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sheng et al. [2023] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max
    Ryabinin, Daniel Y Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E Gonzalez,
    and othersi. High-throughput generative inference of large language models with
    a single gpu. In *International Conference on Machine Learning*. PMLR, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Su et al. [2022] Yusheng Su, Xiaozhi Wang, Yujia Qin, Chi-Min Chan, Yankai
    Lin, Huadong Wang, Kaiyue Wen, Zhiyuan Liu, Peng Li, Juanzi Li, et al. On transferability
    of prompt tuning for natural language processing. In *Proceedings of the 2022
    Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies*, pages 3949–3969, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tang [2023] Yuxin Tang. Chain-of-thought prompting under streaming batch: A
    case study. *arXiv preprint arXiv:2306.00550*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. [2023a] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. [2023b] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. [2023] Bingyang Wu, Yinmin Zhong, Zili Zhang, Gang Huang, Xuanzhe
    Liu, and Xin Jin. Fast distributed inference serving for large language models.
    *arXiv preprint arXiv:2305.05920*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al. [2022] Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth,
    and Song Han. Smoothquant: Accurate and efficient post-training quantization for
    large language models. *arXiv preprint arXiv:2211.10438*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie et al. [2022] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu
    Ma. An explanation of in-context learning as implicit bayesian inference. In *The
    Tenth International Conference on Learning Representations, ICLR 2022, Virtual
    Event, April 25-29, 2022*. OpenReview.net, 2022. URL [https://openreview.net/forum?id=RdJVFCHjUMI](https://openreview.net/forum?id=RdJVFCHjUMI).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yuan et al. [2022] Binhang Yuan, Yongjun He, Jared Davis, Tianyi Zhang, Tri
    Dao, Beidi Chen, Percy S Liang, Christopher Re, and Ce Zhang. Decentralized training
    of foundation models in heterogeneous environments. *Advances in Neural Information
    Processing Systems*, 35:25464–25477, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zellers et al. [2019] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In *Proceedings
    of the 57th Annual Meeting of the Association for Computational Linguistics*,
    pages 4791–4800, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2022] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    et al. Opt: Open pre-trained transformer language models. *arXiv preprint arXiv:2205.01068*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix
  prefs: []
  type: TYPE_NORMAL
- en: Appendix A More Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Experiment Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the experiment, we employed the AdamW [[21](#bib.bib21)] optimizer as our
    chosen optimizer. We conducted iterative prompt updates using a batch size of
    4, a weight decay of $10^{-5}$. We set the total optimization steps as 30,000
    and use the model corresponding to the best validation perplexity as the final
    model. To facilitate mix-precision training and system-level optimization, we
    leveraged the accelerate library [[11](#bib.bib11)].
  prefs: []
  type: TYPE_NORMAL
- en: 'All experiments are conducted on a server with eight Nvidia V100 (32GB) GPUs,
    1.5T main memory, and two Intel Xeon CPU E5-2699A. The software and package version
    is specified below:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Package configurations of our experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Package | Version |'
  prefs: []
  type: TYPE_TB
- en: '| CUDA | 11.6 |'
  prefs: []
  type: TYPE_TB
- en: '| pytorch | 2.0.1 |'
  prefs: []
  type: TYPE_TB
- en: '| transformers | 4.30.0.dev0 |'
  prefs: []
  type: TYPE_TB
- en: '| accelerate | 0.18.0 |'
  prefs: []
  type: TYPE_TB
- en: A.2 Ablation on the Transferability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Table [4](#A1.T4 "Table 4 ‣ A.2 Ablation on the Transferability ‣ Appendix
    A More Experiments ‣ Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off
    of LLM Inference with Transferable Prompt"), we conduct the ablation study on
    the transferability of the learned soft prompts using quantized LLaMA-7B on Wikitext2
    and PTB dataset. Specifically, we compare the transferred soft prompts against
    the soft prompts that are trained on the downstream dataset, which serve as the
    top-line counterpart. We observe that directly trained prompts perform better
    than our transferred prompts. However, we note that models with our transferred
    prompts are much closer to the top-line compared to the compressed model without
    prompts, especially for extremely compressed models. This suggests the effectiveness
    of our transferable prompts. We also observe that with learned soft prompt, the
    gap between the full model and quantized model is greatly reduced. For example,
    without learned prompts, the gaps between the full model and 3bit model are 4.72
    (PTB, 11.02 versus 15.74) and 3.12 (Wikitext2, 6.33 versus 9.45). However, after
    adding the learned prompt, the gap was reduced to 0.9 (PTB, 6.86 versus 7.76)
    and 0.75 (Wikitext-2, 5.58 versus 6.33). Also, after adding learned prompts, 4-bit
    quantized can almost match the full model with negligible perplexity drop, which
    highlights the importance of learned prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Perplexity comparison between full model and quantized models with
    different prompts, where we report test perplexity on PTB and Wikitext-2 dataset.
    “w./o. prompt” refers to the quantized model without soft prompts.“w./ direct
    prompt” means the soft prompts are directly trained on the target dataset.“w./
    transferred prompt” means the prompt is trained on C4 dataset and then transferred
    to the target dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | PTB | Wikitext2 |'
  prefs: []
  type: TYPE_TB
- en: '| Full Model | 11.02 | 6.33 |'
  prefs: []
  type: TYPE_TB
- en: '| Full Model w./ direct prompt | 6.86 | 5.57 |'
  prefs: []
  type: TYPE_TB
- en: '| 4-bit |'
  prefs: []
  type: TYPE_TB
- en: '&#124; w./o. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; prompt &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 11.65 | 6.92 |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; w./ direct &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; prompt &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 7.04 | 5.88 |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; w./ transferred &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; prompt &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 9.25 | 6.26 |'
  prefs: []
  type: TYPE_TB
- en: '| 3-bit |'
  prefs: []
  type: TYPE_TB
- en: '&#124; w./o. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; prompt &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 15.74 | 9.45 |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; w./ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; direct prompt &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 7.76 | 6.33 |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; w./  transferred &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; prompt &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 10.81 | 6.90 |'
  prefs: []
  type: TYPE_TB
- en: '| 2-bit |'
  prefs: []
  type: TYPE_TB
- en: '&#124; w./o. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; prompt &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 5883.13 | 2692.81 |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; w./ direct &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; prompt &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 14.98 | 16.67 |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; w./ transferred &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; prompt &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 29.82 | 20.56 |'
  prefs: []
  type: TYPE_TB
- en: A.3 Cross-Task Transferability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we explore the transferability of learned prompts across different
    tasks. Specifically, we aim to assess the effectiveness of prompts learned from
    token generation tasks, as indicated by Eq ([1](#S4.E1 "In 4.2 Learning Objectives
    ‣ 4 Learning Prompt for Efficient LLM Inference ‣ Compress, Then Prompt: Improving
    Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt")), in
    downstream tasks of LLM. As an illustrative example, we consider the zero-shot
    generalization tasks of LLaMA-7B [[33](#bib.bib33)]. For evaluation purposes,
    we have chosen OpenbookQA [[24](#bib.bib24)], Hellaswag [[39](#bib.bib39)], PIQA [[1](#bib.bib1)],
    and the high school European history task from [[13](#bib.bib13)]. The European
    history task is particularly interesting due to its inclusion of a lengthy context
    sentence for each question. We employ the lm-evaluation-hardness framework [[8](#bib.bib8)],
    incorporating adapters from [[38](#bib.bib38)], for the purpose of conducting
    the experiment.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [5](#A1.T5 "Table 5 ‣ A.4 Efficiency Profiling ‣ Appendix A More Experiments
    ‣ Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference
    with Transferable Prompt") presents the results in terms of normalized accuracy,
    and we also include the standard deviation, as indicated by [[8](#bib.bib8)].
    The table clearly demonstrates that the learned prompt significantly enhances
    the accuracy of these tasks. These findings imply that prompts acquired through
    token generation tasks can effectively enhance the accuracy-efficiency trade-off
    of compressed LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: A.4 Efficiency Profiling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/40dad02bab150b86fe9e9b3d94e8ff32.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Caption'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we analyze how the inclusion of prompt tokens impacts the
    latency of LLM inference. Figure [6](#A1.F6 "Figure 6 ‣ A.4 Efficiency Profiling
    ‣ Appendix A More Experiments ‣ Compress, Then Prompt: Improving Accuracy-Efficiency
    Trade-off of LLM Inference with Transferable Prompt") illustrates the latency
    of three OPT models and the LLaMA-7B model utilized in this paper, considering
    the insertion of additional prompt tokens with varying lengths. For token generation,
    we set the sequence length to 1024\. The figure demonstrates that the addition
    of prompt tokens does not significantly increase the latency of LLM inference,
    particularly when the inserted tokens account for less than 10% of the original
    sequence length. Furthermore, our observations indicate that the latency does
    not exhibit a linear correlation with the length of the inserted tokens, highlighting
    the effectiveness of the prompt in facilitating efficient LLM inference.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: The zero-shot results on transforming the learned prompt to OpenBookQA,
    Hellaswag, PIQA, and High School European History dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Models |  | OpenbookQA | Hellaswag | PIQA | High School European History
    |'
  prefs: []
  type: TYPE_TB
- en: '| Full |  | 0.410±0.022 | 0.497±0.005 | 0.702±0.011 | 0.364±0.038 |'
  prefs: []
  type: TYPE_TB
- en: '| 50% | w./o. Prompt | 0.412±0.022 | 0.449±0.005 | 0.682±0.011 | 0.364±0.038
    |'
  prefs: []
  type: TYPE_TB
- en: '| + Learned Prompt | 0.400±0.022 | 0.469±0.005 | 0.689±0.011 | 0.358±0.037
    |'
  prefs: []
  type: TYPE_TB
- en: '| 62.5% | w./o. Prompt | 0.396±0.022 | 0.380±0.005 | 0.638±0.011 | 0.345±0.037
    |'
  prefs: []
  type: TYPE_TB
- en: '| + Learned Prompt | 0.402±0.022 | 0.433±0.005 | 0.668±0.011 | 0.345±0.037
    |'
  prefs: []
  type: TYPE_TB
- en: '| 75% | w./o. Prompt | 0.366±0.022 | 0.280±0.004 | 0.549±0.012 | 0.315±0.036
    |'
  prefs: []
  type: TYPE_TB
- en: '| + Learned Prompt | 0.358±0.021 | 0.344±0.005 | 0.614±0.011 | 0.358±0.037
    |'
  prefs: []
  type: TYPE_TB
- en: '| 4-bit | w./o. Prompt | 0.410±0.022 | 0.487±0.005 | 0.690±0.011 | 0.358±0.037
    |'
  prefs: []
  type: TYPE_TB
- en: '| + Learned Prompt | 0.418±0.022 | 0.487±0.005 | 0.692±0.011 | 0.352±0.037
    |'
  prefs: []
  type: TYPE_TB
- en: '| 3-bit | w./o. Prompt | 0.378±0.022 | 0.446±0.005 | 0.674±0.011 | 0.358±0.037
    |'
  prefs: []
  type: TYPE_TB
- en: '| + Learned Prompt | 0.404±0.022 | 0.459±0.005 | 0.688±0.011 | 0.358±0.037
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2-bit | w./o. Prompt | 0.354±0.021 | 0.240±0.004 | 0.491±0.012 | 0.315±0.036
    |'
  prefs: []
  type: TYPE_TB
- en: '| + Learned Prompt | 0.350±0.021 | 0.294±0.005 | 0.563±0.012 | 0.333±0.037
    |'
  prefs: []
  type: TYPE_TB
- en: Appendix B More Visualization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we present further visualizations of compression-aware prompts,
    as demonstrated in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Compress, Then
    Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable
    Prompt") in Section [1](#S1 "1 Introduction ‣ Compress, Then Prompt: Improving
    Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt"). The
    results unveil a significant improvement achieved by utilizing a hard, task-independent
    prompt on compressed LLMs. Additionally, we showcase the visualization of responses
    generated using our prompt derived from the C4 training set. It is worth noting
    that, in certain instances, the task-independent and learned prompt outperforms
    the hard prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ced5f317593922f7b67fb5e72a6fd603.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Case study for the effect of prompts on a pruned LLaMA-7B with a
    62.5% weight sparsity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/79f7ade8f36b17ce587ad6d3370f13d3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Case study for the effect of prompts on a pruned LLaMA-7B with a
    4-bit quantization.'
  prefs: []
  type: TYPE_NORMAL
