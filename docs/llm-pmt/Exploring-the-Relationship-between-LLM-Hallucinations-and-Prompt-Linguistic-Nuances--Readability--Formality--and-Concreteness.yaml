- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:49:37'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Exploring the Relationship between LLM Hallucinations and Prompt Linguistic
    Nuances: Readability, Formality, and Concreteness'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2309.11064](https://ar5iv.labs.arxiv.org/html/2309.11064)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Vipula Rawte¹, Prachi Priya², S.M Towhidul Islam Tonmoy³, S M Mehedi Zaman³,
  prefs: []
  type: TYPE_NORMAL
- en: Amit Sheth¹, Amitava Das¹
  prefs: []
  type: TYPE_NORMAL
- en: ¹AI Institute, University of South Carolina, USA
  prefs: []
  type: TYPE_NORMAL
- en: ²Indian Institute of Technology, Kharagpur
  prefs: []
  type: TYPE_NORMAL
- en: ³Islamic University of Technology {vrawte}@mailbox.sc.edu corresponding author
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As Large Language Models (LLMs) have advanced, they have brought forth new challenges,
    with one of the prominent issues being LLM hallucination. While various mitigation
    techniques are emerging to address hallucination, it is equally crucial to delve
    into its underlying causes. Consequently, in this preliminary exploratory investigation,
    we examine how linguistic factors in prompts, specifically readability, formality,
    and concreteness, influence the occurrence of hallucinations. Our experimental
    results suggest that prompts characterized by greater formality and concreteness
    tend to result in reduced hallucination. However, the outcomes pertaining to readability
    are somewhat inconclusive, showing a mixed pattern.
  prefs: []
  type: TYPE_NORMAL
- en: '1 Hallucination in LLMs: An introduction'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The remarkable advantages offered by extensive generative AI models like GPT-4
    (Brown et al., [2020](#bib.bib2); OpenAI, [2023](#bib.bib18)), Stable Diffusion
    Rombach et al. ([2022](#bib.bib26)), DALL-E Ramesh et al. ([2021](#bib.bib25),
    [2022](#bib.bib24)), and Midjourney Midjourney ([2022](#bib.bib14)) are accompanied
    by a significant potential for misuse.
  prefs: []
  type: TYPE_NORMAL
- en: The recent and rapid developments in the field of LLMs have been gaining significant
    attention and use in various applications. This included natural language understanding
    and generation for chatbots, content generation, translation, summarization, and
    more. They were also being applied in fields like healthcare, finance, and education.
    Nevertheless, these LLMs encounter significant hurdles, with one prominent issue
    being termed hallucination. This term describes a situation in which the LLM generates
    responses that contain factual inaccuracies or fabrications.
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S1.F1.pic1" class="ltx_picture ltx_figure_panel" height="53.72" overflow="visible"
    version="1.1" width="258"><g transform="translate(0,53.72) matrix(1 0 0 -1 0 0)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 11.07 31.18)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 9.06 7.81)"><foreignobject width="100.2" height="12.3"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Original
    Prompt</foreignobject></g></g></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 7.29 7.29)"><foreignobject width="243.42" height="16.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Who owned the ships in the
    Boston Tea Party?</foreignobject></g></g></svg><svg id="S1.F1.pic2" class="ltx_picture
    ltx_figure_panel" height="78.07" overflow="visible" version="1.1" width="277.68"><g
    transform="translate(0,78.07) matrix(1 0 0 -1 0 0) translate(19.69,0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 7.29 7.29)"><foreignobject width="243.42" height="63.5" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">The ships involved in the Boston
    Tea Party were British-owned. The three ships—Dartmouth, Eleanor, and Beaver—that
    carried the tea cargo were merchant vessels operated by British companies. They
    were docked in Boston Harbor and were targeted by American colonists who protested
    against the British-imposed Tea Act by boarding the ships and dumping the tea
    into the harbor.</foreignobject></g></g></svg><svg id="S1.F1.pic3" class="ltx_picture
    ltx_figure_panel" height="53.72" overflow="visible" version="1.1" width="258"><g
    transform="translate(0,53.72) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 11.07 31.18)"><g class="ltx_nestedsvg"
    transform="matrix(1 0 0 1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 9.06 7.81)"><foreignobject
    width="132.68" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Reformulated Prompt</foreignobject></g></g></g> <g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 7.29 7.29)"><foreignobject width="243.42" height="16.6"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Which American
    merchant owned the ship Dartmouth in the Boston Tea Party?</foreignobject></g></g></svg><svg
    id="S1.F1.pic4" class="ltx_picture ltx_figure_panel" height="127.89" overflow="visible"
    version="1.1" width="277.68"><g transform="translate(0,127.89) matrix(1 0 0 -1
    0 0) translate(19.69,0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 7.29 7.29)"><foreignobject
    width="243.42" height="113.31" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">The ship Dartmouth, one of the vessels involved in the Boston
    Tea Party, was owned by a partnership of American merchants. The principal owner
    of the Dartmouth was Francis Rotch, a wealthy American merchant and prominent
    figure in the New England whaling industry. Joseph Rotch, his brother, and William
    Rotch, their cousin, were also involved in the ownership of the Dartmouth. These
    merchants were associated with the tea trade and had interests in shipping and
    commerce. Their ship, the Dartmouth, carried a shipment of British tea to Boston
    Harbor, where it was targeted by American colonists during the Boston Tea Party
    protest against British taxation.</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: An illustration of how a “reformulated prompt” can aid in addressing
    the hallucination issue by providing pertinent context. Here, the hallucinated
    text is highlighted in red. By introducing additional context highlighted in blue,
    such as “who” and “what”, we modify the prompt to be more formal and concrete.
    Thus, the newly generated response now incorporates the factually correct (dehallucinated)
    text, highlighted in green.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Several mitigation techniques have emerged to address and reduce the occurrence
    of hallucinations. These techniques can be broadly categorized into two groups:
    i) Black-box Mündler et al. ([2023](#bib.bib15)), which operates without depending
    on external grounded knowledge, and ii) Gray-box Zhang et al. ([2023](#bib.bib33));
    Peng et al. ([2023](#bib.bib21)); Li et al. ([2023](#bib.bib11)), which incorporates
    external knowledge to a certain extent.'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering can play a crucial role in mitigating hallucinations in generative
    AI models. By providing clear and specific prompts, users can steer the AI model
    toward generating content that aligns with their intended context or requirements.
    This can reduce the chances of the model producing hallucinated or inaccurate
    information. Prompts can include contextual cues that help the AI model understand
    the context of the request. This additional context can guide the model in generating
    responses that are more contextually accurate and less prone to hallucination.
    Complex prompts can be used to guide the model through a series of steps, ensuring
    that it follows a logical sequence of thought and produces coherent responses.
  prefs: []
  type: TYPE_NORMAL
- en: 'The state-of-the-art LLMs have the capability to process lengthy prompts as
    input. However, findings in Liu et al. ([2023b](#bib.bib13)) indicate (see [Fig. 2](#S1.F2
    "In 1 Hallucination in LLMs: An introduction ‣ Exploring the Relationship between
    LLM Hallucinations and Prompt Linguistic Nuances: Readability, Formality, and
    Concreteness")) that these models tend to perform best when pertinent information
    is located at the beginning or end of the input context. their performance significantly
    diminishes when they need to access relevant information in the middle of lengthy
    contexts. Moreover, as the input context becomes more extended, even models explicitly
    designed for longer contexts experience a substantial decrease in performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ec05b018c314a23eb18d3e85d3dce758.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Empirical results in Liu et al. ([2023b](#bib.bib13)) show that the
    models tend to excel at utilizing pertinent information found at the very start
    or end of their input context, but their performance notably declines when they
    need to access and utilize information situated in the middle of their input context.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, our primary objective is to explore the impact of the key linguistic
    attributes of prompts on hallucinations generated in LLMs. The contributions are
    as follows: 1) We delineate the broad categories of hallucinations observed in
    LLMs, as discussed in [Section 2](#S2 "2 Types of Hallucination ‣ Exploring the
    Relationship between LLM Hallucinations and Prompt Linguistic Nuances: Readability,
    Formality, and Concreteness"). 2) We construct and provide annotations for our
    dataset, which is derived from tweets related to New York Times events, as detailed
    in [Section 3](#S3 "3 Dataset and Annotation ‣ Exploring the Relationship between
    LLM Hallucinations and Prompt Linguistic Nuances: Readability, Formality, and
    Concreteness"). 3) We analyze the relationship between the primary linguistic
    aspects of prompts, such as their readability, formality, and concreteness, and
    the occurrence of hallucinations in LLMs, as discussed in [Section 4](#S4 "4 Linguistic
    Properties of the prompt ‣ Exploring the Relationship between LLM Hallucinations
    and Prompt Linguistic Nuances: Readability, Formality, and Concreteness").'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Types of Hallucination
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this study, we explore the following four different categories of hallucination.
    Additionally, we offer examples for each case in which the hallucinated text is
    marked in red.
  prefs: []
  type: TYPE_NORMAL
- en: '1\. Person (P):'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The issue of generating fictional characters is discussed in Ladhak et al.
    ([2023](#bib.bib10)) and [Table 1](#S2.T1 "In 2\. Location (L): ‣ 2 Types of Hallucination
    ‣ Exploring the Relationship between LLM Hallucinations and Prompt Linguistic
    Nuances: Readability, Formality, and Concreteness").'
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Location (L):'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The case of generating fictional places is addressed in Ladhak et al. ([2023](#bib.bib10))
    and [Table 1](#S2.T1 "In 2\. Location (L): ‣ 2 Types of Hallucination ‣ Exploring
    the Relationship between LLM Hallucinations and Prompt Linguistic Nuances: Readability,
    Formality, and Concreteness").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Original | Antoine Richard is a former athlete from France who mainly competed
    in the 100 metres. He was French 100 metre champion on 5 occasions, and also 200
    metre winner in 1985\. He also won the French 60 metres title 5 times as well.
    |'
  prefs: []
  type: TYPE_TB
- en: '| AI-generated | Athlete Naoki Tsukahara was born in Tokyo, Japan to a Japanese
    father and French mother. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: An example showing how imaginary places such as Tokyo and persons
    such as father and mother are hallucinated Ladhak et al. ([2023](#bib.bib10)).'
  prefs: []
  type: TYPE_NORMAL
- en: '3\. Number (N):'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Similarly, Varshney et al. ([2023](#bib.bib30)) delves into the generation
    of imaginary numbers, as shown in [Table 2](#S2.T2 "In 3\. Number (N): ‣ 2 Types
    of Hallucination ‣ Exploring the Relationship between LLM Hallucinations and Prompt
    Linguistic Nuances: Readability, Formality, and Concreteness").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Original | Freddie Frith. |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| AI-generated | He was born in London in 1929 and began his racing career
    in 1951. |'
  prefs: []
  type: TYPE_TB
- en: '| Fact | He was born in Grimsby in 1909 and began his career in 1930. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Both years 1929 and 1951 are hallucinated.'
  prefs: []
  type: TYPE_NORMAL
- en: '4\. Acronym (A):'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Additionally, we investigate the potential role of acronyms in prompting the
    generation of inaccurate responses as illustrated in [Table 3](#S2.T3 "In 4\.
    Acronym (A): ‣ 2 Types of Hallucination ‣ Exploring the Relationship between LLM
    Hallucinations and Prompt Linguistic Nuances: Readability, Formality, and Concreteness").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Original | Robert used PDF for his math homework. |'
  prefs: []
  type: TYPE_TB
- en: '| AI-generated | In the context you provided, “PDF” most likely refers to “Portable
    Document Format,” which is a common file format used for documents, including
    math homework assignments. PDF files are widely used for sharing documents because
    they preserve the formatting and layout across different devices and operating
    systems. Robert likely used a PDF file for his math homework to ensure that it
    could be easily shared and viewed by others without compatibility issues. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: In the mathematical context, “PDF” stands for “Probability Density
    Function.”.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Dataset and Annotation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To conduct our empirical analysis, where we examine how linguistic properties
    affect hallucination, we create and annotate a hallucination dataset using the
    NYT tweets detailed in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 New York Times News Tweets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We utilize a news dataset, specifically the New York Times (NYT) news events
    tweets [NYT](#bib.bib16) . We selected a total of 2,500 tweets. These news tweets
    serve as our source of factually accurate prompts, which are then presented to
    the fifteen Large Language Models (LLMs) described in [Section 3.2](#S3.SS2 "3.2
    Selection of LLMs ‣ 3 Dataset and Annotation ‣ Exploring the Relationship between
    LLM Hallucinations and Prompt Linguistic Nuances: Readability, Formality, and
    Concreteness").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Selection of LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have selected 15 contemporary LLMs that have consistently demonstrated outstanding
    performance across a wide spectrum of NLP tasks. These models include: (i) GPT-4
    OpenAI ([2023](#bib.bib18)) (ii) GPT-3.5 OpenAI ([2022](#bib.bib17)) (iii) GPT-3
    Brown et al. ([2020](#bib.bib2)) (iv) GPT-2 Radford et al. ([2019](#bib.bib22))
    (v) MPT Wang et al. ([2023](#bib.bib31)) (vi) OPT Zhang et al. ([2022](#bib.bib34))
    (vii) LLaMA Touvron et al. ([2023](#bib.bib29)) (viii) BLOOM Scao et al. ([2022](#bib.bib27))
    (ix) Alpaca Taori et al. ([2023](#bib.bib28)) (x) Vicuna Chiang et al. ([2023](#bib.bib4))
    (xi) Dolly databricks ([2023](#bib.bib5)) (xii) StableLM Liu et al. ([2023a](#bib.bib12))
    (xiii) XLNet Yang et al. ([2019](#bib.bib32)) (xiv) T5 Raffel et al. ([2020](#bib.bib23))
    (xv) T0 Deleu et al. ([2022](#bib.bib6)).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Annotation guidelines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the purpose of annotating the 2,500 text snippets, we leveraged the services
    of Amazon Mechanical Turk (AMT) [Amazon](#bib.bib1) . Through this platform, we
    obtained annotations at the sentence level to identify the different four categories
    of hallucination.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Dataset statistics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Following the annotation process, our dataset statistics for the hallucination
    categories are presented in [Table 4](#S3.T4 "In 3.4 Dataset statistics ‣ 3 Dataset
    and Annotation ‣ Exploring the Relationship between LLM Hallucinations and Prompt
    Linguistic Nuances: Readability, Formality, and Concreteness").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Hallucinated sentences |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Person | 14850 |'
  prefs: []
  type: TYPE_TB
- en: '| Location | 13050 |'
  prefs: []
  type: TYPE_TB
- en: '| Number | 7275 |'
  prefs: []
  type: TYPE_TB
- en: '| Acronym | 1225 |'
  prefs: []
  type: TYPE_TB
- en: '| Total | 36910 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Hallucination dataset statistics'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Linguistic Properties of the prompt
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Linguistic properties refer to the various characteristics and attributes of
    language and its components. These properties encompass a wide range of aspects
    that help define and understand a language. Some fundamental linguistic properties
    include: syntactic, semantic, pragmatic, and lexical. Considering these characteristics,
    we will delve more deeply into the three primary linguistic subtleties in the
    forthcoming [Sections 4.1](#S4.SS1 "4.1 Readability ‣ 4 Linguistic Properties
    of the prompt ‣ Exploring the Relationship between LLM Hallucinations and Prompt
    Linguistic Nuances: Readability, Formality, and Concreteness"), [4.2](#S4.SS2
    "4.2 Formality ‣ 4 Linguistic Properties of the prompt ‣ Exploring the Relationship
    between LLM Hallucinations and Prompt Linguistic Nuances: Readability, Formality,
    and Concreteness") and [4.3](#S4.SS3 "4.3 Concreteness ‣ 4 Linguistic Properties
    of the prompt ‣ Exploring the Relationship between LLM Hallucinations and Prompt
    Linguistic Nuances: Readability, Formality, and Concreteness").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Readability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Readability quantifies the ease with which a text can be comprehended. Several
    factors, including the text’s complexity, familiarity, legibility, and typography,
    collectively contribute to its readability.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Flesch Reading Ease Score (FRES) Flesch ([1948](#bib.bib7)) (see [Eq. 1](#S4.E1
    "In 4.1 Readability ‣ 4 Linguistic Properties of the prompt ‣ Exploring the Relationship
    between LLM Hallucinations and Prompt Linguistic Nuances: Readability, Formality,
    and Concreteness")) is a measure of the readability of a text. It was developed
    to assess how easy or difficult a piece of text is to read and understand. The
    score is calculated based on two factors: (a) Sentence Length and (b) Word Complexity.'
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the following example, in the first sentence, the language is straightforward,
    and the sentence is easy to understand, resulting in a high readability score.
    In contrast, the second sentence contains complex vocabulary and lengthy phrasing,
    making it more challenging to comprehend, resulting in a lower readability score.
  prefs: []
  type: TYPE_NORMAL
- en: Easy Readability (High Flesch Reading Ease Score)
  prefs: []
  type: TYPE_NORMAL
- en: 'Sentence: The sun rises in the east every morning.'
  prefs: []
  type: TYPE_NORMAL
- en: Difficult Readability (Low Flesch Reading Ease Score)
  prefs: []
  type: TYPE_NORMAL
- en: 'Sentence: The intricacies of quantum mechanics, as expounded upon by renowned
    physicists, continue to baffle even the most astute scholars.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  $\text{FRES}=206.835-1.015\left(\frac{\text{ total words }}{\text{ total
    sentences }}\right)-84.6\left(\frac{\text{ total syllables }}{\text{ total words
    }}\right)$  |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'To investigate the impact of the readability of the prompt, we pose the following
    research questions:'
  prefs: []
  type: TYPE_NORMAL
- en: RQ 1
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does the complexity of a prompt’s language or vocabulary affect the likelihood
    of hallucination in LLM-generated responses?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: RQ 2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the length of a prompt impact the potential for hallucination, and how
    does the readability of a long versus a short prompt affect LLM behavior?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: RQ 3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do different LLM architectures (e.g., GPT-3, GPT-4, etc.) respond to prompts
    of varying linguistic readability, and do they exhibit differences in hallucination
    tendencies?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.2 Formality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The formality of language refers to the degree of sophistication, decorum, or
    politeness conveyed by the choice of words, sentence structure, and overall tone
    in communication. It is a way to indicate the level of etiquette, respect, or
    professionalism in a given context.
  prefs: []
  type: TYPE_NORMAL
- en: In the example given below, both sentences convey an identical message, yet
    the initial one carries significantly more formality. Such stylistic distinctions
    frequently exert a more significant influence on the reader’s comprehension of
    the sentence than the literal meaning itself Hovy ([1987](#bib.bib9)).
  prefs: []
  type: TYPE_NORMAL
- en: Example of formality in sentences Pavlick and Tetreault ([2016](#bib.bib20))
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Those recommendations were unsolicited and undesirable.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: that’s the stupidest suggestion EVER.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Formality (defined in Heylighen and Dewaele ([1999](#bib.bib8))) is calculated
    as given in [Eq. 2](#S4.E2 "In 4.2 Formality ‣ 4 Linguistic Properties of the
    prompt ‣ Exploring the Relationship between LLM Hallucinations and Prompt Linguistic
    Nuances: Readability, Formality, and Concreteness"):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\text{F}=(\text{noun frequency}+\text{adjective freq.}$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle+\text{preposition freq.}+\text{article freq.}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | - pronoun freq. - verb freq. |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\text{- adverb freq.}\text{ - interjection freq.}+100)/2$
    |  |'
  prefs: []
  type: TYPE_TB
- en: To examine how the formality of the prompt influences the outcome, we ask the
    following research inquiries.
  prefs: []
  type: TYPE_NORMAL
- en: RQ 1
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does the level of formality in prompts influence the likelihood of hallucination
    in responses generated by LLMs?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: RQ 2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Are there specific categories of hallucination that are more prevalent in responses
    prompted with formal versus informal language?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.3 Concreteness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Concreteness assesses the extent to which a word represents a tangible or perceptible
    concept. As per the theory in Paivio ([2013](#bib.bib19)), it is suggested that
    concrete words are easier to process compared to abstract words. The degree of
    concreteness associated with each word is expressed using a 5-point rating scale
    that ranges from abstract to concrete.
  prefs: []
  type: TYPE_NORMAL
- en: A concrete word receives a higher rating and pertains to something that physically
    exists in reality, i.e. one can directly experience it through senses (smell,
    taste, touch, hear, see) and actions. An abstract word receives a lower rating
    and refers to something that isn’t directly accessible through your senses or
    actions. Its meaning is dependent on language and is usually elucidated by employing
    other words since there’s no straightforward method for direct demonstration.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of concrete words
  prefs: []
  type: TYPE_NORMAL
- en: Apple, Dog, Chair, Book, Water, Mountain, Car
  prefs: []
  type: TYPE_NORMAL
- en: Examples of abstract words
  prefs: []
  type: TYPE_NORMAL
- en: Justice, Love, Happiness, Courage, Friendship, Wisdom, Equality, Democracy
  prefs: []
  type: TYPE_NORMAL
- en: 'Concreteness ratings for 37,058 individual English words and 2,896 two-word
    expressions (i.e., a total of 39,954) are provided in Brysbaert et al. ([2014](#bib.bib3)).
    Since these ratings are at the word level, we compute the concreteness of a sentence
    by taking an average as described in [Eq. 3](#S4.E3 "In 4.3 Concreteness ‣ 4 Linguistic
    Properties of the prompt ‣ Exploring the Relationship between LLM Hallucinations
    and Prompt Linguistic Nuances: Readability, Formality, and Concreteness").'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\text{concreteness of a sentence containing $n$ tokens}=$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\frac{\sum_{i=1}^{n}\text{concreteness rating}_{i}}{n}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: In order to explore the influence of the prompt’s concreteness on the study,
    we present the following research questions.
  prefs: []
  type: TYPE_NORMAL
- en: RQ 1
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does the level of linguistic concreteness in a prompt impact the probability
    of hallucination in LLMs?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: RQ 2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do LLMs tend to hallucinate less when provided with prompts that include specific
    details and constraints?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: RQ 3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Are LLMs more prone to hallucination when given abstract or vague prompts compared
    to concrete and specific prompts?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 5 Our findings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To investigate how the linguistic characteristics of prompts affect the generation
    of hallucinations in LLMs, we initially define the ranges for three specific scores,
    as outlined in [Table 5](#S5.T5 "In 5 Our findings ‣ Exploring the Relationship
    between LLM Hallucinations and Prompt Linguistic Nuances: Readability, Formality,
    and Concreteness"). A comprehensive analysis of these findings is presented in
    the following sections.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Range $\rightarrow$ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Linguistic Aspect $\downarrow$ | Low | Mid | High |'
  prefs: []
  type: TYPE_TB
- en: '| Readability | 0-30 | 31-70 | 71-100 |'
  prefs: []
  type: TYPE_TB
- en: '| Formality | 0-30 | 31-70 | 71-100 |'
  prefs: []
  type: TYPE_TB
- en: '| Concreteness | 1-2.5 | 2.5-3.5 | 3.5-5 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Range(s) for three linguistic aspects of the prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Effects of readability on hallucination in LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The figure (see [Fig. 3](#S5.F3 "In 5.3 Effects of concreteness on hallucination
    in LLMs ‣ 5 Our findings ‣ Exploring the Relationship between LLM Hallucinations
    and Prompt Linguistic Nuances: Readability, Formality, and Concreteness")) illustrates
    our empirical findings and the following are the main insights that address the
    research questions posed earlier in [Section 4.1](#S4.SS1 "4.1 Readability ‣ 4
    Linguistic Properties of the prompt ‣ Exploring the Relationship between LLM Hallucinations
    and Prompt Linguistic Nuances: Readability, Formality, and Concreteness").'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S5.SS1.p2.pic1" class="ltx_picture" height="94.33" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,94.33) matrix(1 0 0 -1 0 0)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 76.12)"><foreignobject width="556.69" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">Effects of readability on LLM’s
    hallucination:</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69" height="44.62" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">➠ Prompts that are easier to
    read tend to have fewer instances of hallucinations. ➠ Some difficult-to-read
    prompts, but more formal also hallucinate less. ➠ Hence, the results regarding
    readability are somewhat uncertain, displaying a combination of findings.</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Effects of formality on hallucination in LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Fig. 4](#S5.F4 "In 5.3 Effects of concreteness on hallucination in LLMs ‣
    5 Our findings ‣ Exploring the Relationship between LLM Hallucinations and Prompt
    Linguistic Nuances: Readability, Formality, and Concreteness") represents our
    empirical findings.The following points outline the primary insights that respond
    to the research queries introduced in [Section 4.2](#S4.SS2 "4.2 Formality ‣ 4
    Linguistic Properties of the prompt ‣ Exploring the Relationship between LLM Hallucinations
    and Prompt Linguistic Nuances: Readability, Formality, and Concreteness").'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S5.SS2.p2.pic1" class="ltx_picture" height="127.9" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,127.9) matrix(1 0 0 -1 0 0)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 109.69)"><foreignobject width="556.69" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">Effects of formality on LLM’s
    hallucination:</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69" height="78.2" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">➠ Formal language prompts typically
    exhibit a lower propensity for generating hallucinatory content. ➠ Our findings
    demonstrate how utilizing more formal prompts can address hallucinations in the
    Name and Location categories. ➠ The linguistic impacts of the prompts become more
    evident in LLMs such as GPT-4, OPT, and subsequent versions.</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Effects of concreteness on hallucination in LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Fig. 5](#S5.F5 "In 5.3 Effects of concreteness on hallucination in LLMs ‣
    5 Our findings ‣ Exploring the Relationship between LLM Hallucinations and Prompt
    Linguistic Nuances: Readability, Formality, and Concreteness") shows our experimental
    results. The following section highlights the core insights that address the research
    inquiries introduced in [Section 4.3](#S4.SS3 "4.3 Concreteness ‣ 4 Linguistic
    Properties of the prompt ‣ Exploring the Relationship between LLM Hallucinations
    and Prompt Linguistic Nuances: Readability, Formality, and Concreteness").'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S5.SS3.p2.pic1" class="ltx_picture" height="108.97" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,108.97) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 93.45)"><foreignobject width="556.69"
    height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">Effects
    of concreteness on LLM’s hallucination:</foreignobject></g> <g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="61.96" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">➠
    Prompts that use clearer and more specific language tend to generate fewer hallucinations.
    ➠ Our results show that incorporating more specific and concrete terms into the
    prompts effectively reduces hallucinations in the Number and Acronym categories.
    ➠ Just as we observed with our formality findings, the impact of concrete prompts
    becomes increasingly apparent in advanced LLMs like GPT-4, OPT, and their later
    iterations.</foreignobject></g></g></svg>![Refer to caption](img/91bba7c0cbd65e2f9ee7c86f6364a4ae.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: Hallucination vs Readability'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c902cdba6c42545024182f93dd339ed9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Hallucination vs Formality'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8ac4d2052f67912d7e8d2b47c0c96549.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Hallucination vs Concreteness'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this preliminary research study, we begin by categorizing the primary types
    of hallucinations present in LLMs. Subsequently, we compile our dataset by utilizing
    New York Times news tweets, aligning with these established categories. Language
    intricacies assume a crucial role in the comprehension of language. Therefore,
    we delve into the examination of three significant linguistic dimensions: readability,
    formality, and concreteness, and their potential influence on the occurrence of
    hallucinations in LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1) Amazon. [Amazon mechanical turk](https://www.mturk.com/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. [Language models are few-shot learners](https://arxiv.org/abs/2005.14165).
    *Advances in neural information processing systems*, 33:1877–1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brysbaert et al. (2014) Marc Brysbaert, Amy Beth Warriner, and Victor Kuperman.
    2014. Concreteness ratings for 40 thousand generally known english word lemmas.
    *Behavior research methods*, 46:904–911.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica, and Eric P. Xing. 2023. [Vicuna: An open-source chatbot impressing
    gpt-4 with 90%* chatgpt quality](https://vicuna.lmsys.org).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: databricks (2023) databricks. 2023. [Dolly](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deleu et al. (2022) Tristan Deleu, David Kanaa, Leo Feng, Giancarlo Kerg, Yoshua
    Bengio, Guillaume Lajoie, and Pierre-Luc Bacon. 2022. [Continuous-time meta-learning
    with forward mode differentiation](https://openreview.net/forum?id=57PipS27Km).
    In *The Tenth International Conference on Learning Representations, ICLR 2022,
    Virtual Event, April 25-29, 2022*. OpenReview.net.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Flesch (1948) R Flesch. 1948. A new readability yardstick journal of applied
    psychology 32: 221–233.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Heylighen and Dewaele (1999) Francis Heylighen and Jean-Marc Dewaele. 1999.
    Formality of language: definition, measurement and behavioral determinants. *Interner
    Bericht, Center “Leo Apostel”, Vrije Universiteit Brüssel*, 4(1).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hovy (1987) Eduard Hovy. 1987. Generating natural language under pragmatic constraints.
    *Journal of Pragmatics*, 11(6):689–719.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ladhak et al. (2023) Faisal Ladhak, Esin Durmus, Mirac Suzgun, Tianyi Zhang,
    Dan Jurafsky, Kathleen Mckeown, and Tatsunori B Hashimoto. 2023. [When do pre-training
    biases propagate to downstream tasks? a case study in text summarization](https://aclanthology.org/2023.eacl-main.234.pdf).
    In *Proceedings of the 17th Conference of the European Chapter of the Association
    for Computational Linguistics*, pages 3198–3211.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023) Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Lidong
    Bing, Shafiq Joty, and Soujanya Poria. 2023. Chain of knowledge: A framework for
    grounding large language models with structured knowledge bases. *arXiv preprint
    arXiv:2305.13269*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2023a) Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming
    Zhang. 2023a. [Is your code generated by chatgpt really correct? rigorous evaluation
    of large language models for code generation](https://arxiv.org/abs/2305.01210).
    *arXiv preprint arXiv:2305.01210*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023b) Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape,
    Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023b. Lost in the middle:
    How language models use long contexts. *arXiv preprint arXiv:2307.03172*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Midjourney (2022) Midjourney. 2022. [https://www.midjourney.com](https://www.midjourney.com).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mündler et al. (2023) Niels Mündler, Jingxuan He, Slobodan Jenko, and Martin
    Vechev. 2023. [Self-contradictory hallucinations of large language models: Evaluation,
    detection and mitigation](http://arxiv.org/abs/2305.15852).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (16) NYT. [https://www.nytimes.com/topic/company/twitter](https://www.nytimes.com/topic/company/twitter).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2022) OpenAI. 2022. [Introducing chatgpt](https://openai.com/blog/chatgpt).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. 2023. [Gpt-4 technical report](http://arxiv.org/abs/2303.08774).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Paivio (2013) Allan Paivio. 2013. Dual coding theory, word abstractness, and
    emotion: a critical review of kousta et al.(2011).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pavlick and Tetreault (2016) Ellie Pavlick and Joel Tetreault. 2016. [An empirical
    analysis of formality in online communication](https://doi.org/10.1162/tacl_a_00083).
    *Transactions of the Association for Computational Linguistics*, 4:61–74.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng et al. (2023) Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia
    Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, et al. 2023. Check
    your facts and try again: Improving large language models with external knowledge
    and automated feedback. *arXiv preprint arXiv:2302.12813*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. 2019. [Language models are unsupervised multitask
    learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf).
    *OpenAI blog*, 1(8):9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. [Exploring
    the limits of transfer learning with a unified text-to-text transformer](https://www.jmlr.org/papers/volume21/20-074/20-074.pdf).
    *The Journal of Machine Learning Research*, 21(1):5485–5551.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramesh et al. (2022) Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
    and Mark Chen. 2022. [Hierarchical text-conditional image generation with clip
    latents](https://arxiv.org/abs/2204.06125). *arXiv preprint arXiv:2204.06125*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramesh et al. (2021) Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
    Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. [Zero-shot text-to-image
    generation](https://arxiv.org/abs/2102.12092). In *International Conference on
    Machine Learning*, pages 8821–8831\. PMLR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick
    Esser, and Björn Ommer. 2022. [High-resolution image synthesis with latent diffusion
    models](https://arxiv.org/abs/2112.10752). In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, pages 10684–10695.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scao et al. (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick,
    Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François
    Yvon, Matthias Gallé, et al. 2022. [Bloom: A 176b-parameter open-access multilingual
    language model](https://arxiv.org/abs/2211.05100). *arXiv preprint arXiv:2211.05100*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. [Stanford
    alpaca: An instruction-following llama model](https://crfm.stanford.edu/2023/03/13/alpaca.html).
    [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
    Guillaume Lample. 2023. [Llama: Open and efficient foundation language models](https://arxiv.org/abs/2302.13971).
    *arXiv preprint arXiv:2302.13971*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Varshney et al. (2023) Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu
    Chen, and Dong Yu. 2023. A stitch in time saves nine: Detecting and mitigating
    hallucinations of llms by validating low-confidence generation. *arXiv preprint
    arXiv:2307.03987*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023) Zhen Wang, Rameswar Panda, Leonid Karlinsky, Rogerio Feris,
    Huan Sun, and Yoon Kim. 2023. [Multitask prompt tuning enables parameter-efficient
    transfer learning](https://openreview.net/forum?id=Nk2pDtuhTq). In *The Eleventh
    International Conference on Learning Representations*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2019) Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R
    Salakhutdinov, and Quoc V Le. 2019. [Xlnet: Generalized autoregressive pretraining
    for language understanding](https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf).
    *Advances in neural information processing systems*, 32.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2023) Shuo Zhang, Liangming Pan, Junzhou Zhao, and William Yang
    Wang. 2023. [Mitigating language model hallucination with interactive question-knowledge
    alignment](http://arxiv.org/abs/2305.13669).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh
    Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. [Opt: Open pre-trained
    transformer language models](http://arxiv.org/abs/2205.01068).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
