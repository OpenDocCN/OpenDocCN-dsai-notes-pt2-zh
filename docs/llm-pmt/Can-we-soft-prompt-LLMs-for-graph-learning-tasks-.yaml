- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:46:46'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Can we soft prompt LLMs for graph learning tasks?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.10359](https://ar5iv.labs.arxiv.org/html/2402.10359)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Zheyuan Liu University of Notre Dame  [zliu29@nd.edu](mailto:zliu29@nd.edu)
    ,  Xiaoxin He National University of Singapore  [he.xiaoxin@u.nus.edu](mailto:he.xiaoxin@u.nus.edu)
    ,  Yijun Tian University of Notre Dame  [yijun.tian@nd.edu](mailto:yijun.tian@nd.edu)
     and  Nitesh V. Chawla University of Notre Dame  [nchawla@nd.edu](mailto:nchawla@nd.edu)(2023)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Graph plays an important role in representing complex relationships in real-world
    applications such as social networks, biological data and citation networks. In
    recent years, Large Language Models (LLMs) have achieved tremendous success in
    various domains, which makes applying LLMs to graphs particularly appealing. However,
    directly applying LLMs to graph modalities presents unique challenges due to the
    discrepancy and mismatch between the graph and text modalities. Hence, to further
    investigate LLMs’ potential for comprehending graph information, we introduce
    GraphPrompter, a novel framework designed to align graph information with LLMs
    via soft prompts. Specifically, GraphPrompter consists of two main components:
    a graph neural network to encode complex graph information and an LLM that effectively
    processes textual information. Comprehensive experiments on various benchmark
    datasets under node classification and link prediction tasks demonstrate the effectiveness
    of our proposed method. The GraphPrompter framework unveils the substantial capabilities
    of LLMs as predictors in graph-related tasks, enabling researchers to utilize
    LLMs across a spectrum of real-world graph scenarios more effectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Large Language Models, Graph Neural Networks, Natural Language Processing,
    Graph Representation Learning^†^†copyright: acmcopyright^†^†journalyear: 2023^†^†copyright:
    none^†^†conference: The Web Conference 2024; May 13 - 17, 2024; Singapore'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large Language Models (LLMs)  (Brown et al., [2020](#bib.bib3); Chowdhery et al.,
    [2023](#bib.bib6); Touvron et al., [2023](#bib.bib15); Radford et al., [2019](#bib.bib14);
    Ouyang et al., [2022](#bib.bib13); Lewkowycz et al., [2022](#bib.bib9)) has demonstrated
    significant success across various domains, largely attributed to their extensive
    knowledge memorized during the pretraining phase and their exceptional ability
    to generalize during the fine-tuning process on diverse textual datasets  (Hoffmann
    et al., [2022](#bib.bib8); Webson and Pavlick, [2021](#bib.bib17); Liang et al.,
    [2022](#bib.bib10); Carlini et al., [2022](#bib.bib4)). This success has spurred
    interest in combining graph neural networks (GNNs) with LLMs to enhance their
    capabilities in understanding and modeling graphs (Yang et al., [2021](#bib.bib19);
    Zhang et al., [2022a](#bib.bib21), [b](#bib.bib23); Ostendorff et al., [2022](#bib.bib12)),
    including implementing LLMs as encoders to process features within GNNs (Chai
    et al., [2023](#bib.bib5); Mavromatis et al., [2023](#bib.bib11); He et al., [2023](#bib.bib7);
    Yu et al., [2023](#bib.bib20)), and employing LLMs as aligners with GNNs to enhance
    performance (Zhang et al., [2021](#bib.bib22); Zhao et al., [2022](#bib.bib24);
    Wen and Fang, [2023](#bib.bib18); Brannon et al., [2023](#bib.bib2)).
  prefs: []
  type: TYPE_NORMAL
- en: However, directly applying LLMs to graph modalities presents unique challenges
    due to the discrepancy and mismatch between the graph and text modalities. For
    example, existing works primarily map the graph into text, ignoring the irrelevant
    information and noises pertinent to the graphs. This oversight results in an inadequate
    understanding of crucial structural knowledge within the graphs. Furthermore,
    the challenge is amplified when dealing with large graphs containing thousands
    or millions of nodes and edges, as this complexity hinders LLMs’ ability to grasp
    intricate structural information.
  prefs: []
  type: TYPE_NORMAL
- en: 'These limitations motivate our investigation into using a graph as a soft prompt
    encoded by GNNs for LLMs in graph learning tasks. The intuition behind this approach
    is that GNN are more proficient in aggregating and transforming information from
    the neighbourhood information, which could provide topological information to
    LLMs. Additionally, it can guide the LLM in selecting relevant information from
    textual input and control the generation process for token generation. Specifically,
    in this work, we aim to examine the efficacy of a soft graph prompt in informing
    the LLM’s predictions. Hence, a natural yet pivotal research question arise: Can
    we soft prompt LLMs for graph leanring tasks?'
  prefs: []
  type: TYPE_NORMAL
- en: To answer this question, we introduce GraphPrompter, a novel framework that
    combines the strength of GNNs and LLMs to process and comprehend graph-structured
    data. In particular, GraphPrompter capitalizes on the frozen LLM as a robust feature
    extractor that taps into its vast pretraining knowledge, thus enabling us to avoid
    extensive task-specific fine-tuning. In parallel, the GNN works on the graph to
    produce node embeddings, which are later concatenated with a prompt instruction
    to guide LLMs for graph learning tasks. The LLM, due to its powerful autoregressive
    nature, generates a language response based on the fused graph and text information,
    effectively turning the LLM into a powerful tool for graph understanding tasks.
    See Figure [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ Can we soft prompt LLMs for
    graph learning tasks?") for an illustration.
  prefs: []
  type: TYPE_NORMAL
- en: 'This hybrid approach is particularly suitable for graphs with textual attributes
    (i.e., textual graphs), which requires an understanding of both the textual content
    and the graph structure, such as identifying the subcategory of an academic paper
    based on its citation network and abstract. We experimentally demonstrate that
    our framework is capable of prompting LLMs for graph learning tasks on five benchmark
    datasets under node classification and link prediction tasks. It showcases the
    potential for significant advancements in the use of LLMs for complex data structures
    beyond traditional text, opening up new avenues for research and application in
    the realm of AI assistants capable of intricate graph comprehension. Our main
    contributions are as follows ¹¹1The code is avilable at [https://github.com/franciscoliu/graphprompter](https://github.com/franciscoliu/graphprompter).:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To the best of our knowledge, this is the very first work investigating whether
    LLMs can understand graph learning tasks via soft prompting.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose GraphPrompter, a novel plug-and-play framework that first employ
    GNN to get node representations from the textual graph. Then the obtained embeddings
    are concatenated with a prompt instruction to guide LLMs for graph learning tasks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extensive experiments demonstrate the effectiveness of our proposed framework
    under both node classification and link prediction tasks across various graph
    benchmarks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9c822a050e85632b9064b633963e6390.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1. Illustration of the proposed GraphPrompter for node classification
    task. The process involves extracting a k-hop subgraph for each node, feeding
    it into a GNN followed by a projection layer. Simultaneously, the textual attributes
    associated with each node are processed by the text embedder. The resulting node
    embedding is then concatenated with text embeddings, serving as a soft promt to
    guide the LLM for graph learning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To enhance the alignment of graph knowledge with LLM, we introduce GraphPrompter,
    a plug-and-play pipeline that fuses post-processed node embeddings with LLMs.
    This integration is designed to leverage the rich semantic context LLMs provide,
    enhancing the interpretability and utility of graph representations, as illustrated
    in Figure  [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ Can we soft prompt LLMs for
    graph learning tasks?").
  prefs: []
  type: TYPE_NORMAL
- en: 2.1\. Graph Section
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A graph can be represented as $G=(V,E)$, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (1) |  | $X_{i}=\text{GNN}(G_{s_{i}})\in\mathbb{R}^{d_{g}},$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $d_{g}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'We employ a projection layer, specifically, a multilayer perceptron (MLP),
    to align the node embedding with the vector space of the LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (2) |  | $\hat{X}_{i}=\text{MLP}(X_{i})\in\mathbb{R}^{d_{l}},$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $d_{l}$ is the hidden dimension of the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both the GNN and the projection layer are trained in an end-to-end manner,
    ensuring that the embeddings $X_{i}$ are discriminative for the downstream task.
    The rationale behind this approach is twofold: first, to leverage the GNN’s inherent
    capacity to aggregate and transform local neighborhood information into meaningful
    embeddings; and second, to enable the subsequent integration with the LLM, which
    interprets these embeddings as a soft prompt in conjunction with textual data.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. LLM Section
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After encoding the graph structure, the GraphPrompter proceeds to process the
    textual information associated with each node. This is where the power of the
    LLM comes into play, as it is kept frozen for efficient fine-tuning while preserving
    the LLM’s pre-trained knowledge. For each node $v_{i}$ using the frozen tokenizer
    of the LLM as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (3) |  | $\displaystyle T_{\text{tokens}}=\text{Tokenizer}(T_{i}).$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'The tokenizer converts the text into a sequence of discrete tokens $T_{\text{tokens}}$
    in the LLM’s vocabulary. Subsequently, these tokens are then embedded into a continuous
    space:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (4) |  | $\displaystyle T_{\text{emb}}=\text{Embed}(T_{\text{tokens}})\in\mathbb{R}^{M\times
    d_{l}},$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $M$ is the hidden dimension of the LLM. These embeddings are designed
    to capture the semantic meaning of the text, complementing the structural information
    provided by the GNN.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we concatenate the node embeddings and the text embeddings, denoted as
    $[\hat{X}_{i},T_{\text{emb}}]$, which will go through the self-attention layers
    in the LLM as usual.
  prefs: []
  type: TYPE_NORMAL
- en: 'The motivation for this step is to ensure that the LLM can process the rich
    semantic content of the text alongside the graph embeddings. By encoding the text
    attributes into a compatible format, we aim to capitalize on the LLM’s advanced
    understanding of natural language, which is crucial for tasks that require a combination
    of structural and textual data interpretation, such as node classification in
    citation networks. Here, we point out the key distinction between node classification
    and link prediction: node classification evaluates information from a single node,
    whereas link prediction considers the attributes of two nodes, specifically their
    origin and destination.'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/537960850a2d241a4ff0dce73e1e3497.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Dense (Title + Abstract)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/205553c280e6c6de7fa71849e109774f.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Sparse (Title Only)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2. Node classification experiments comparing the proposed GraphPrompter
    with baseline methods (i.e., soft prompt tuning and fine-tuning). The $x$ axis
    displays accuracy scores. Figure [2(a)](#S3.F2.sf1 "In Figure 2 ‣ 3\. Experiments
    ‣ Can we soft prompt LLMs for graph learning tasks?") illustrates a dense semantic
    setting including both the title and abstract of a paper within the node embeddings,
    while Figure [2(b)](#S3.F2.sf2 "In Figure 2 ‣ 3\. Experiments ‣ Can we soft prompt
    LLMs for graph learning tasks?") illustrates a sparse semantic setting with title
    only.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we conduct extensive experiments to validate the effectiveness
    of the GraphPrompter. Specifically, our experiments aim to address the central
    question: Can we soft prompt LLMs for graph learning tasks?'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Experiment setups
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 3.1.1\. Datasets and baseline models.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Our experiments mainly focus on node classification and link prediction using
    various graph benchmarks: Cora, Citeseer, Pubmed, Ogbn-arxiv, and Ogbn-products.
    For Ogbn-products, given its substantial scale of 2 million nodes and 61 million
    edges, we have employed a node sampling strategy to obtain a subgraph containing
    54k nodes and 74k edges. We compare our approach with three kinds of baseline
    models: 1) Pure GNN; 2) Frozen LLM, which includes zero shot and soft prompt tuning;
    and 3) Tuned LLM with LoRA. We employ the LLAMA2-7B (Touvron et al., [2023](#bib.bib15))
    as the LLM backbone. For the pure GNN method, we use GAT (Veličković et al., [2017](#bib.bib16)).
    In the LLM-frozen setting, for zero-shot, we use a frozen LLM for direct node
    classification/edge prediction using textual attributes of the nodes. In soft
    prompt tuning, we keep the parameters of the LLM frozen and adjust only the prompt.
    In LLM-tuned, the original LLM parameters are updated for downstream tasks by
    utilizing LoRA.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2\. Implementation Details.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We report the mean of five independent runs with different seeds. The experiments
    are conducted on 2 A100 GPUs (80GB).
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Experiments Result
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To answer the central question, we conduct experiments across various graph
    benchmark to comprehensively evaluate the effectiveness of GraphPrompter. The
    performance is reported in Table  [1](#S4.T1 "Table 1 ‣ 4.1\. Node Classification
    Task. ‣ 4\. Experiments Result ‣ Can we soft prompt LLMs for graph learning tasks?")
    and Table  [2](#S4.T2 "Table 2 ‣ 4.1\. Node Classification Task. ‣ 4\. Experiments
    Result ‣ Can we soft prompt LLMs for graph learning tasks?").
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. Node Classification Task.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In Table [1](#S4.T1 "Table 1 ‣ 4.1\. Node Classification Task. ‣ 4\. Experiments
    Result ‣ Can we soft prompt LLMs for graph learning tasks?"), we present the results
    of node classification experiments conducted on a variety of benchmark datasets,
    where both title and abstract information are provided to LLM alongside detailed
    instructions. As it can be seen from the table, the integration of the proposed
    method (GraphPrompter) with LoRA consistently surpasses the performance of other
    baseline methods across diverse benchmarks. In particular, GraphPrompter with
    LoRA in the PubMed and Citeseer datasets with top accuracies of 94.80 % and 73.61
    %, respectively. The naive fine-tuning with LoRA approach typically appears as
    the runner-up and has small performance gap with GraphPrompter. Though subgraph
    prompt tuning approach has relatively close accuracy score with naive fine-tuning,
    it still not as effective as fine-tuning on node classification task. Furthermore,
    we observe that zero-shot generally ranks lowest in performance across all benchmark
    datasets, with the exception of PubMed dataset. This reflects the limitation of
    LLMs in capturing graph knowledge without the auxiliary processing provided by
    GNNs. Additionally, two distinct experimental setups are illustrated in Figure
    [2](#S3.F2 "Figure 2 ‣ 3\. Experiments ‣ Can we soft prompt LLMs for graph learning
    tasks?"), where Figure [2(a)](#S3.F2.sf1 "In Figure 2 ‣ 3\. Experiments ‣ Can
    we soft prompt LLMs for graph learning tasks?") encompasses both title and abstract
    within the node embeddings, and Figure [2(b)](#S3.F2.sf2 "In Figure 2 ‣ 3\. Experiments
    ‣ Can we soft prompt LLMs for graph learning tasks?") restricts the node embeddings
    to only the title. The comparative analysis illustrated in the figures further
    illustrates the superior average performance of the proposed method (GraphPrompter),
    which combines subgraph prompt tuning and GraphPrompter, in comparison to other
    baseline approaches, specifically soft prompt tuning and fine-tuning methodologies.
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Cora | Citeseer | Pubmed | Ogbn-arxiv | Ogbn-products |'
  prefs: []
  type: TYPE_TB
- en: '| Node Classification Accuracy (%) ${\color[rgb]{0,0,1}\uparrow}$ |'
  prefs: []
  type: TYPE_TB
- en: '| GAT | 84.69 | 70.78 | 84.09 | 71.82 | 70.52 |'
  prefs: []
  type: TYPE_TB
- en: '| Zero-Shot | 43.31 | 29.22 | 91.39 | 44.23 | 15.05 |'
  prefs: []
  type: TYPE_TB
- en: '| Soft Prompt Tuning | 70.31 | 70.97 | 91.45 | 71.99 | 75.14 |'
  prefs: []
  type: TYPE_TB
- en: '| Fine-tuning + LoRA | 75.97 | 73.45 | 94.68 | 74.58 | 78.99 |'
  prefs: []
  type: TYPE_TB
- en: '| Subgraph Prompt Tuning | 80.17 | 72.29 | 93.84 | 75.04 | 75.30 |'
  prefs: []
  type: TYPE_TB
- en: '| GraphPrompter + LoRA | 80.26 | 73.61 | 94.80 | 75.61 | 79.54 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1. Node classification result of our proposed GraphPrompter, with a number
    of baselines under various graph benchmarks: Cora, Citeseer, Pubmed, Ogbn-arxiv
    and Ogbn-products. For each model, we present the mean accuracy from five independent
    runs, with both the title and abstract of papers provided as input along with
    specific instructions. Bold indicates the best performance and underline indicates
    the runner-up.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Cora | Citeseer | Pubmed | Ogbn-arxiv | Ogbn-products |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Link Prediction Accuracy (%) ${\color[rgb]{0,0,1}\uparrow}$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| GAT | 90.71 | 87.40 | 86.18 | 72.93 | 65.67 |'
  prefs: []
  type: TYPE_TB
- en: '| Zero-Shot | 21.94 | 7.69 | 10.23 | 34.41 | 7.47 |'
  prefs: []
  type: TYPE_TB
- en: '| Soft Prompt Tuning | 86.77 | 88.87 | 83.98 | 69.13 | 62.78 |'
  prefs: []
  type: TYPE_TB
- en: '| Fine-tuning + LoRA | 87.58 | 87.91 | 81.33 | 70.10 | 67.31 |'
  prefs: []
  type: TYPE_TB
- en: '| Subgraph Prompt Tuning | 89.15 | 93.49 | 87.20 | 75.28 | 68.18 |'
  prefs: []
  type: TYPE_TB
- en: '| GraphPrompter + LoRA | 90.10 | 91.67 | 86.49 | 73.21 | 69.55 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2. Link prediction result of our proposed GraphPrompter, with a number
    of baselines under various graph benchmarks: Cora, Citeseer, Pubmed, Ogbn-arxiv
    and Ogbn-products. For each model, we present the mean accuracy derived from five
    independent runs conducted on the link prediction task, with only the abstract
    of papers are provided as input. Bold indicates the best performance and underline
    indicates the runner-up.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. Link Prediction Task.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In Table [2](#S4.T2 "Table 2 ‣ 4.1\. Node Classification Task. ‣ 4\. Experiments
    Result ‣ Can we soft prompt LLMs for graph learning tasks?"), we present the results
    of link prediction experiments conducted on a variety of benchmark datasets, where
    only the title is provided to LLM. As seen in Table  [2](#S4.T2 "Table 2 ‣ 4.1\.
    Node Classification Task. ‣ 4\. Experiments Result ‣ Can we soft prompt LLMs for
    graph learning tasks?"), subgraph prompt tuning approach consistently surpasses
    baseline models in link prediction tasks on various benchmarks. It shows remarkable
    performance especially on the Citeseer dataset, achieving a 93.49 % accuracy rate.
    GraphPrompter + LoRA is usually the runner-up model for the majority case. Notably,
    similar to node classification task, the Zero-Shot approach generally scores lowest,
    underscoring the necessity of GNN integration for LLMs to effectively process
    and understand graph data. It is notable that under both node classification and
    link prediction, GraphPrompter can continue outperforming traditional GNN and
    soft prompting techniques. This observation indicates that GraphPrompter, is particularly
    adept at capturing and utilizing the complex intersection between graph components
    and textual information, leading to a better performance on various graph related
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The experiments on benchmark datasets for node classification and link prediction
    tasks demonstrate LLMs’ adaptability and efficiency in graph learning, enabled
    by our proposed soft graph prompting strategies. Here, we summarize the key findings:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Performance Benchmark: Overall, our proposed method ranks either as the best
    or runner-up across all benchmark datasets for both tasks. This consistently high
    performance affirms a positive response to our central question: we can effectively
    soft prompt for LLMs in graph learning tasks.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Methodological Superiority: The fine-tuning approach, while competitive, displayed
    a marginal performance gap when compared to GraphPrompter. This indicates that
    while traditional fine-tuning remains effective, the specialized soft graph prompt
    tuning strategy designed for graph data offers a more potent solution for enhancing
    LLM’s graph learning capabilities.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Challenges with Zero-Shot Learning: The zero-shot approach consistently ranked
    the lowest across all benchmarks for both tasks. This reflects the challenges
    LLMs face in comprehending graph structure and semantics, highlighting the necessity
    for tailored approaches like GraphPrompter in graph learning tasks.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 6\. Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In conclusion, our study reveals the significant potential of leveraging LLMs
    for interpreting graph structures through a prompt tuning methodology. To facilitate
    this study, we presents GraphPrompter, a novel plug-and-play framework that integrates
    the capabilities of Large Language Models with the structural insights of Graph
    Neural Networks for the task of node classification and link prediction in graph
    data. Specifically, GraphPrompter consists of two sections: the graph section
    utilizes GNN for structural insights, while the text section applies LLM to interpret
    node-related textual data. Extensive experiments and in-depth studies demonstrate
    the superiority of GraphPrompter across multiple benchmark datasets. Future work
    could focus on extending our pipeline to other more complex graph level tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Brannon et al. (2023) William Brannon, Suyash Fulay, Hang Jiang, Wonjune Kang,
    Brandon Roy, Jad Kabbara, and Deb Roy. 2023. ConGraT: Self-Supervised Contrastive
    Pretraining for Joint Graph and Text Embeddings. *arXiv preprint arXiv:2305.14321*
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems* 33 (2020), 1877–1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carlini et al. (2022) Nicholas Carlini, Daphne Ippolito, Matthew Jagielski,
    Katherine Lee, Florian Tramer, and Chiyuan Zhang. 2022. Quantifying memorization
    across neural language models. *arXiv preprint arXiv:2202.07646* (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chai et al. (2023) Ziwei Chai, Tianjie Zhang, Liang Wu, Kaiqiao Han, Xiaohai
    Hu, Xuanwen Huang, and Yang Yang. 2023. Graphllm: Boosting graph reasoning ability
    of large language model. *arXiv preprint arXiv:2310.05845* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chowdhery et al. (2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways.
    *Journal of Machine Learning Research* 24, 240 (2023), 1–113.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2023) Xiaoxin He, Xavier Bresson, Thomas Laurent, and Bryan Hooi.
    2023. Explanations as Features: LLM-Based Features for Text-Attributed Graphs.
    *arXiv preprint arXiv:2305.19523* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hoffmann et al. (2022) Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena
    Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks,
    Johannes Welbl, Aidan Clark, et al. 2022. Training compute-optimal large language
    models. *arXiv preprint arXiv:2203.15556* (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lewkowycz et al. (2022) Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan
    Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag,
    Theo Gutman-Solo, et al. 2022. Solving quantitative reasoning problems with language
    models. *Advances in Neural Information Processing Systems* 35 (2022), 3843–3857.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang et al. (2022) Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras,
    Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya
    Kumar, et al. 2022. Holistic evaluation of language models. *arXiv preprint arXiv:2211.09110*
    (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mavromatis et al. (2023) Costas Mavromatis, Vassilis N Ioannidis, Shen Wang,
    Da Zheng, Soji Adeshina, Jun Ma, Han Zhao, Christos Faloutsos, and George Karypis.
    2023. Train Your Own GNN Teacher: Graph-Aware Distillation on Textual Graphs.
    *arXiv preprint arXiv:2304.10668* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ostendorff et al. (2022) Malte Ostendorff, Nils Rethmeier, Isabelle Augenstein,
    Bela Gipp, and Georg Rehm. 2022. Neighborhood contrastive learning for scientific
    document representations with citation embeddings. *arXiv preprint arXiv:2202.06671*
    (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. 2022. Training language models to follow instructions with human feedback.
    *Advances in Neural Information Processing Systems* 35 (2022), 27730–27744.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask
    learners. *OpenAI blog* 1, 8 (2019), 9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Veličković et al. (2017) Petar Veličković, Guillem Cucurull, Arantxa Casanova,
    Adriana Romero, Pietro Lio, and Yoshua Bengio. 2017. Graph attention networks.
    *arXiv preprint arXiv:1710.10903* (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Webson and Pavlick (2021) Albert Webson and Ellie Pavlick. 2021. Do prompt-based
    models really understand the meaning of their prompts? *arXiv preprint arXiv:2109.01247*
    (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wen and Fang (2023) Zhihao Wen and Yuan Fang. 2023. Augmenting Low-Resource
    Text Classification with Graph-Grounded Pre-training and Prompting. *arXiv preprint
    arXiv:2305.03324* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2021) Junhan Yang, Zheng Liu, Shitao Xiao, Chaozhuo Li, Defu Lian,
    Sanjay Agrawal, Amit Singh, Guangzhong Sun, and Xing Xie. 2021. GraphFormers:
    GNN-nested transformers for representation learning on textual graph. *Advances
    in Neural Information Processing Systems* 34 (2021), 28798–28810.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. (2023) Jianxiang Yu, Yuxiang Ren, Chenghua Gong, Jiaqi Tan, Xiang
    Li, and Xuecang Zhang. 2023. Empower text-attributed graphs learning with large
    language models (llms). *arXiv preprint arXiv:2310.09872* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022a) Xikun Zhang, Antoine Bosselut, Michihiro Yasunaga, Hongyu
    Ren, Percy Liang, Christopher D Manning, and Jure Leskovec. 2022a. Greaselm: Graph
    reasoning enhanced language models for question answering. *arXiv preprint arXiv:2201.08860*
    (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2021) Xinyang Zhang, Chenwei Zhang, Xin Luna Dong, Jingbo Shang,
    and Jiawei Han. 2021. Minimally-supervised structure-rich text categorization
    via learning on text-rich networks. In *Proceedings of the Web Conference 2021*.
    3258–3268.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2022b) Yu Zhang, Zhihong Shen, Chieh-Han Wu, Boya Xie, Junheng
    Hao, Ye-Yi Wang, Kuansan Wang, and Jiawei Han. 2022b. Metadata-induced contrastive
    learning for zero-shot multi-label text classification. In *Proceedings of the
    ACM Web Conference 2022*. 3162–3173.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2022) Jianan Zhao, Meng Qu, Chaozhuo Li, Hao Yan, Qian Liu, Rui
    Li, Xing Xie, and Jian Tang. 2022. Learning on large-scale text-attributed graphs
    via variational inference. *arXiv preprint arXiv:2210.14709* (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
