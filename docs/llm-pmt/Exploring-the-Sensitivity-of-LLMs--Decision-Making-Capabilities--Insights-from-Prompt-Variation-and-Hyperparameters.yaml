- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:47:16'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Exploring the Sensitivity of LLMs’ Decision-Making Capabilities: Insights from
    Prompt Variation and Hyperparameters'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2312.17476](https://ar5iv.labs.arxiv.org/html/2312.17476)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Manikanta Loya^∗
  prefs: []
  type: TYPE_NORMAL
- en: manikanl@uci.edu
  prefs: []
  type: TYPE_NORMAL
- en: \AndDivya Anand Sinha^∗
  prefs: []
  type: TYPE_NORMAL
- en: dasinha@uci.edu
  prefs: []
  type: TYPE_NORMAL
- en: University of California, Irvine
  prefs: []
  type: TYPE_NORMAL
- en: \AndRichard Futrell
  prefs: []
  type: TYPE_NORMAL
- en: rfutrell@uci.edu
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The advancement of Large Language Models (LLMs) has led to their widespread
    use across a broad spectrum of tasks, including decision-making. Prior studies
    have compared the decision-making abilities of LLMs with those of humans from
    a psychological perspective. However, these studies have not always properly accounted
    for the sensitivity of LLMs’ behavior to hyperparameters and variations in the
    prompt. In this study, we examine LLMs’ performance on the Horizon decision-making
    task studied by Binz and Schulz ([2023](#bib.bib2)), analyzing how LLMs respond
    to variations in prompts and hyperparameters. By experimenting on three OpenAI
    language models possessing different capabilities, we observe that the decision-making
    abilities fluctuate based on the input prompts and temperature settings. Contrary
    to previous findings, language models display a human-like exploration–exploitation
    tradeoff after simple adjustments to the prompt. ¹¹1Code is available at the following
    github [link](https://github.com/manikanta-72/Sensitivity-of-LLM-s-Decision-Making-Capabilities).
  prefs: []
  type: TYPE_NORMAL
- en: '^*^*footnotetext: Equal Contribution'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The recent success of large language models (LLMs) at a variety of tasks has
    led to curiosity about their cognitive abilities and characteristics. As LLMs
    are increasingly integrated in daily life both as conversation partners and economic
    decision-makers (Munir et al., [2023](#bib.bib15); Chaturvedi et al., [2023](#bib.bib8);
    Yang et al., [2023](#bib.bib24)), such studies are necessary for understanding
    the limits and characteristics of such agents. An understanding of LLMs at a psychological
    level may also provide strategies for improved prompting and training. To this
    end, a number of researchers have recently adopted methods from cognitive psychology
    and behavioral economics to evaluate language models in the same way that humans
    have been evaluated (e.g. Linzen et al., [2016](#bib.bib13); Miotto et al., [2022](#bib.bib14);
    Phelps and Russell, [2023](#bib.bib17), among many others).
  prefs: []
  type: TYPE_NORMAL
- en: However, such work has not always paid due attention to the fact that LLM responses
    can be highly variable and sensitive to the details of the prompt used and to
    hyperparameters such as temperature. Limited interactions with LLMs—such as interactions
    using only one prompt—can be misleading (Bowman, [2023](#bib.bib4)). In this work,
    we follow up on the behavioral experiments conducted by Binz and Schulz ([2023](#bib.bib2)),
    who studied LLMs’ decision making using analogues of a number of well-known human
    experimental paradigms, finding strong divergences from human behavior. However,
    the experiments in the previous work used only one prompt per task, and did not
    study the effects of hyperparameters. We adopt the same task as the previous work,
    but systematically vary prompts and temperature.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our aims are both substantive—we seek to find whether, with basic changes to
    the prompt, models show human-like behavior in these decision making tasks—and
    methodological: we wish to emphasize that psychological LLM research must consider
    variability as a function of prompt and hyperparameters.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Background and Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As mechanistic understanding and control of LLMs remains complex, researchers
    have increasingly adopted methods from human behavioral sciences for characterizing
    LLMs’ behavior: in the same way that the human brain is largely a black box that
    must be probed using experimental methods and constructs, LLMs may be studied
    in the same way.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/87ab6da6742afad4a88ce4391c0628e6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Original Horizon 6 task prompt (Binz and Schulz, [2023](#bib.bib2)).'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to studies that have used the methods of cognitive psychology to
    understand LLMs’ reasoning and grammatical abilities (e.g., Linzen et al., [2016](#bib.bib13);
    Futrell et al., [2019](#bib.bib10); Cai et al., [2023](#bib.bib6)), researchers
    have increasingly adapted methods from psychometrics (Miotto et al., [2022](#bib.bib14);
    Bodroza et al., [2023](#bib.bib3); Abramski et al., [2023](#bib.bib1)), which
    seek to characterize LLMs in terms of personality variables such as agreeableness
    and conscientiousness, and methods from behavioral economics (Cartwright, [2018](#bib.bib7);
    Phelps and Russell, [2023](#bib.bib17); Horton, [2023](#bib.bib11)), which characterize
    LLMs’ decision-making in terms of preferences for risk and reward.
  prefs: []
  type: TYPE_NORMAL
- en: Prior research on prompting techniques (Wei et al., [2022](#bib.bib22); Wang
    et al., [2023](#bib.bib21)) has shown that subtle modifications in input prompts
    can lead to varied outcomes in reasoning tasks Cobbe et al. ([2021](#bib.bib9)).
    Srivastava et al. ([2022](#bib.bib19)) revealed that Large Language Models (LLMs)
    are notably susceptible to the precise wording of natural language questions,
    especially when presented in a multiple-choice setting. In a recent study, Ouyang
    et al. ([2023](#bib.bib16)) emphasized the influence of temperature adjustments
    on LLM’s performance in code generation tasks. Unlike previous studies, our research
    delves into the sensitivity of LLMs concerning economic decision-making abilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our work is a focused followup on Binz and Schulz ([2023](#bib.bib2)), investigating
    the sensitivity of one of their results to changes in prompt and hyperparameters.
    Binz and Schulz ([2023](#bib.bib2)) evaluated on decision-making, information
    search, deliberation, and causal reasoning in text-davinci-002 (Brown et al.,
    [2020](#bib.bib5)) by presenting it with prompts such as the one shown in Figure [1](#S2.F1
    "Figure 1 ‣ 2 Background and Related Work ‣ Exploring the Sensitivity of LLMs’
    Decision-Making Capabilities: Insights from Prompt Variation and Hyperparameters").
    We follow up on the tasks from the information search area, instantiated in the
    Horizon task, described in the following section. In this task, humans show a
    characteristic trade-off of exploration and exploitation (Wilson et al., [2014](#bib.bib23)),
    favoring exploration in early trials and exploitation later, whereas Binz and
    Schulz ([2023](#bib.bib2)) find that LLMs do not.'
  prefs: []
  type: TYPE_NORMAL
- en: The results of Binz and Schulz ([2023](#bib.bib2)), however, are based on single
    prompt and setting, limiting the generality of their results. Furthermore, observing
    the Horizon task prompt (and the others used throughout the paper), it does not
    follow what are now regarded as best practices for such tasks, for example the
    use of Chain-of-Thought (CoT) prompting Wei et al. ([2022](#bib.bib22))—the original
    prompt forces the LLM to choose a machine in the next token generated, without
    deliberation. Below, we investigate the behavior of LLMs on this task under systematic
    variations of temperature and prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Horizon Task Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f0011cc03eeaaf1e76505edd17fcc564.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) text-davinci-002
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/69a781c6870275268f48991340dd430b.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) text-davinci-003
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f5d05f727420fb4330d171aa77bdef00.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) gpt-3.5-turbo
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Mean regret obtained in the Horizon (multi-trial multi-armed bandit)
    task by humans and LLMs with varying temperature, using the prompt from . The
    solid black line indicates human performance; others are LLMs. Error bars show
    the standard error of the mean.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e0ca1d73d3cd011dc5e832c905497344.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Modifications in prompt for the Horizon task. Horizon 1 prompt is
    shown. In case of CoT, CoT-Exploit & CoT-Explore we explicit ask the model to
    summarize its choice at the end by appending the entire prompt with "Answer the
    following question and summarize your choice at the end as ‘Machine:[machine_name]’."
    at the beginning.'
  prefs: []
  type: TYPE_NORMAL
- en: The Horizon Task as shown in Binz and Schulz ([2023](#bib.bib2)) is a special
    case of the Multi-Armed bandit (MAB) setting. MAB problems (Sutton and Barto,
    [2018](#bib.bib20), Ch. 2) are one of the common problems in the area of Reinforcement
    Learning. This game involves an agent interacting with a slot machine possessing
    $k$ arms. Each arm the agent pulls has a reward associated with it defined by
    an underlying probability distribution. This game is played over multiple episodes
    with the goal of maximizing the accrued rewards.
  prefs: []
  type: TYPE_NORMAL
- en: One approach involves persistently selecting the arm that has delivered the
    maximum amount of rewards in the past. An alternative strategy involves thorough
    exploration of all arms to discern their respective underlying probability distributions,
    followed by the selection of the arm with the highest potential for reward. While
    this is feasible, every turn spent in discerning the underlying distribution,
    diverts from the primary goal of reward maximization. The former and latter strategies
    are known as exploitation and exploration respectively, and the exploration–exploitation
    dilemma is a fundamental concept in decision-making that arises in many domains.
  prefs: []
  type: TYPE_NORMAL
- en: 'To explore the extent to which humans use these strategies, Wilson et al. ([2014](#bib.bib23))
    reports experiments where participants were asked to play the Horizon Task. This
    task consists of a set of two-armed bandit problems, where participants are presented
    with two options, each associated with noisy rewards. The task comprises either
    five or ten trials, and in each trial, participants must select one option, receiving
    corresponding reward feedback. In the initial four trials of the task, participants
    have only one option and are provided with the corresponding reward feedback.
    These forced-choice trials create two distinct information conditions: “unequal
    information” and “equal information.” In the unequal information condition, one
    option is played three times, while the other option is played only once. In the
    equal information condition, both options are played twice. The five-trial setting
    is denoted Horizon 1, indicating that participants make decisions only once, while
    the ten-trial setting is referred to as Horizon 6, as participants make decisions
    over six rounds.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Binz and Schulz ([2023](#bib.bib2)) applied this experimental design to language
    models using the prompt in Figure [1](#S2.F1 "Figure 1 ‣ 2 Background and Related
    Work ‣ Exploring the Sensitivity of LLMs’ Decision-Making Capabilities: Insights
    from Prompt Variation and Hyperparameters"), and we follow their experimental
    setup exactly except for variations to the prompt and hyperparameters. The performance
    of LLMs is assessed by measuring the mean regret across multiple runs. The regret
    is defined as the difference between the optimal reward, which corresponds to
    the machine with the higher reward, and the actual reward obtained from the selection
    process. Human behavior favors exploitation in Horizon 1, but a gradual shift
    from exploration to exploitation in Horizon 6.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/88a42583fde0ac3803c59b58c3293316.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) text-davinci-002³³3Experiments with text-davinci-002 using CoT prompt failed
    due to its inability to summarize its choice at the end.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/53af4d3c303dfaf208936a045f10792f.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) text-davinci-003
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9b208a45f852b275faa0417b84b07b45.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) gpt-3.5-turbo
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Mean regret obtained by humans and LLMs on the Horizon task, varying
    prompt. ‘Quasi-CoT’ means a prompt of the form ‘Thinking step-by-step, I choose
    Machine …’ which does not enable true chain-of-thought reasoning. The temperatures
    for GPT-2, GPT-3, and GPT-3.5 are 1.0, 0.5, and 1.0 respectively. These temperatures
    show the greatest learning effect (negative slope) in the Horizon 6 task.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Varying Temperature
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The impact of various temperature settings (0.0,0.5,1.0) on all three OpenAI
    models²²2https://platform.openai.com/docs/models/overview tested is illustrated
    in Figure [2](#S3.F2 "Figure 2 ‣ 3 Horizon Task Experiments ‣ Exploring the Sensitivity
    of LLMs’ Decision-Making Capabilities: Insights from Prompt Variation and Hyperparameters").
    It is clear that the behavior of each model, as indicated by the mean regret line,
    differs according to the temperature. For Horizon 1, the lowest regret is obtained
    for temperature zero across all three models. Further, unlike text-davinci-002
    and as shown in Binz and Schulz ([2023](#bib.bib2)), the mean regret is lower
    than humans for both text-davinci-003 and gpt-3.5-turbo. In the case of Horizon
    6, there is a notable rise in the inital mean regret, suggesting that higher temperatures
    result in suboptimal decision-making. However, increasing temperature demonstrates
    a more pronounced learning effect, as evidenced by a greater negative slope.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Varying Prompt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To encourage deliberation during decision-making, we incorporate variations
    in the input prompt. Specifically, we explore two different variants of the Chain
    of Thought (CoT) prompting technique (Wei et al., [2022](#bib.bib22))—CoT and
    Quasi-CoT. In Figure [3](#S3.F3 "Figure 3 ‣ 3 Horizon Task Experiments ‣ Exploring
    the Sensitivity of LLMs’ Decision-Making Capabilities: Insights from Prompt Variation
    and Hyperparameters"), we illustrate the modifications made to the original prompt.
    The variant referred to as Quasi-CoT utilizes the prompt “Thinking step by step
    I choose Machine”, which forces the machine to make a decision before fully processing
    its reasoning. On the other hand, the CoT variant makes a decision only after
    fully processing its reasoning. The Quasi-CoT condition allows us to disentangle
    the effects of true step-by-step reasoning in CoT from the effects of prompting
    the LLM to think carefully.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The alteration in the behavior of LLMs due to changes in the input prompt is
    depicted in Figure [4](#S3.F4 "Figure 4 ‣ 3 Horizon Task Experiments ‣ Exploring
    the Sensitivity of LLMs’ Decision-Making Capabilities: Insights from Prompt Variation
    and Hyperparameters"). Across all models, CoT demonstrates lower-regret compared
    to both Quasi-CoT and the original prompt, whereas Quasi-CoT performs worse than
    original prompt. Furthermore, even in text-davinci-002, we find that altered prompts
    yield the human-like negative slope, indicating an exploration–exploitation trade-off.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 CoT Prompting with Hints
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To overcome the identified limitations in LLMs, such as their inaccuracies
    in computing averages (Razeghi et al., [2022](#bib.bib18); Imani et al., [2023](#bib.bib12))
    and sub-optimal exploration capabilities, we introduce additional hints within
    the input prompt to guide the decision-making process. Specifically, we designed
    two prompts, namely CoT-Exploit and CoT-Explore, which aim to facilitate explicit
    exploitation and exploration. The hints associated with these prompts are shown
    in Figure [3](#S3.F3 "Figure 3 ‣ 3 Horizon Task Experiments ‣ Exploring the Sensitivity
    of LLMs’ Decision-Making Capabilities: Insights from Prompt Variation and Hyperparameters").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1c7f797bc9ab70ccf91e4354d86b99c3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: gpt-3.5-turbo’s behavior under different variants of CoT prompts
    at temperature 1.0.'
  prefs: []
  type: TYPE_NORMAL
- en: In the CoT-Exploit prompt, we instruct the model to base its decisions on the
    average of observed experiences and equip it with the required mathematical calculations
    to make a decision. Likewise, in the CoT-Explore approach, we explicitly direct
    the model to select a machine with lower frequency among the observed experiences.
  prefs: []
  type: TYPE_NORMAL
- en: 'The performance of gpt-3.5-turbo, using various CoT prompting variants, is
    compared in Figure [5](#S3.F5 "Figure 5 ‣ 3.3 CoT Prompting with Hints ‣ 3 Horizon
    Task Experiments ‣ Exploring the Sensitivity of LLMs’ Decision-Making Capabilities:
    Insights from Prompt Variation and Hyperparameters"). As anticipated, CoT-Exploit
    outperforms CoT-Explore, displaying a consistent decrease in slope. However, CoT-Explore
    performs significantly worse than random decision-making. CoT-Explore primary
    concentrates on getting more information about each machine rather than overall
    rewards.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Discussion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Through our experiments, we have discovered that the decision-making capabilities
    of LLMs are influenced by both the prompts used and the temperature settings,
    more so by the choice of prompt rather than the temperature. This highlights the
    importance of varying prompts to elicit the desired behavior from LLMs during
    decision-making tasks, and that studies which have used only one kind of prompt
    are potentially misleading.
  prefs: []
  type: TYPE_NORMAL
- en: 'Intriguingly, we observed that the model gpt-3.5-turbo with the Quasi-CoT prompt
    (Figure [5](#S3.F5 "Figure 5 ‣ 3.3 CoT Prompting with Hints ‣ 3 Horizon Task Experiments
    ‣ Exploring the Sensitivity of LLMs’ Decision-Making Capabilities: Insights from
    Prompt Variation and Hyperparameters")) exhibits the closest resemblance to human
    behavior. This prompt alerts the model to the need for reasoning, but does not
    give it the space to actually perform any reasoning. The similarity of the Quasi-CoT
    result to humans suggests that humans may also struggle to fully process the associated
    information and reasoning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, by providing hints to guide the decision-making process, we have
    observed that superhuman performance can be achieved, as demonstrated by the CoT-Exploit
    variant (Figure [5](#S3.F5 "Figure 5 ‣ 3.3 CoT Prompting with Hints ‣ 3 Horizon
    Task Experiments ‣ Exploring the Sensitivity of LLMs’ Decision-Making Capabilities:
    Insights from Prompt Variation and Hyperparameters")). This result suggests that
    language model behavior in these tasks is potentially highly controllable.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have demonstrated that the psychological behavior of LLMs, as previously
    explored by Binz and Schulz ([2023](#bib.bib2)), is highly sensitive to the way
    these LLMs are queried. The non-human-like behavior observed by Binz and Schulz
    ([2023](#bib.bib2)) vanishes under simple variations of prompt, and super-human
    performance in terms of minimizing regret is easily achievable. Going forward,
    we urge careful consideration in the LLM psychology literature of the fact that
    model behavior can diverge under different settings.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have presented a focused extension of one of the studies from Binz and Schulz
    ([2023](#bib.bib2)), demonstrating sensitivity to prompt and hyperparameters which
    was overlooked in the previous work. However, our work is limited in that (1)
    we have only examined one of the tasks from Binz and Schulz ([2023](#bib.bib2)),
    (2) we have only presented a few variations of temperature and prompt, and (3)
    we have only experimented with some of the models available to us as of June 2023,
    selecting high-profile closed-source models over open-source models. Nevertheless,
    we believe that our overarching point that LLM psychology needs to take into account
    hyperparameters, prompts, and variability remains valid.
  prefs: []
  type: TYPE_NORMAL
- en: Ethics Statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work involves psychological studies of LLMs in economic decision making
    contexts. If LLMs are really deployed as economic decision makers, then ethical
    issues could result from biases and limitations of the models. We urge caution
    in such applications.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Abramski et al. (2023) Katherine Abramski, Salvatore Citraro, Luigi Lombardi,
    Giulio Rossetti, and Massimo Stella. 2023. Cognitive network science reveals bias
    in gpt-3, chatgpt, and gpt-4 mirroring math anxiety in high-school students. *arXiv
    preprint arXiv:2305.18320*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Binz and Schulz (2023) Marcel Binz and Eric Schulz. 2023. Using cognitive psychology
    to understand gpt-3. *Proceedings of the National Academy of Sciences*, 120(6):e2218523120.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bodroza et al. (2023) Bojana Bodroza, Bojana M Dinic, and Ljubisa Bojic. 2023.
    Personality testing of gpt-3: Limited temporal reliability, but highlighted social
    desirability of gpt-3’s personality instruments results. *arXiv preprint arXiv:2306.04308*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bowman (2023) Samuel R Bowman. 2023. Eight things to know about large language
    models. *arXiv preprint arXiv:2304.00612*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in Neural
    Information Processing Systems*, 33:1877–1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cai et al. (2023) Zhenguang G Cai, David A Haslett, Xufeng Duan, Shuqi Wang,
    and Martin J Pickering. 2023. Does ChatGPT resemble humans in language use? *arXiv
    preprint arXiv:2303.08014*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cartwright (2018) Edward Cartwright. 2018. *Behavioral Economics*. Routledge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chaturvedi et al. (2023) Rijul Chaturvedi, Sanjeev Verma, Ronnie Das, and Yogesh K
    Dwivedi. 2023. Social companionship with artificial intelligence: Recent trends
    and future avenues. *Technological Forecasting and Social Change*, 193:122634.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, Christopher Hesse, and John Schulman. 2021. [Training verifiers to solve
    math word problems](http://arxiv.org/abs/2110.14168).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Futrell et al. (2019) Richard Futrell, Ethan Wilcox, Takashi Morita, Peng Qian,
    Miguel Ballesteros, and Roger Levy. 2019. [Neural language models as psycholinguistic
    subjects: Representations of syntactic state](https://doi.org/10.18653/v1/N19-1004).
    In *Proceedings of the 2019 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
    Short Papers)*, pages 32–42, Minneapolis, Minnesota. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Horton (2023) John J Horton. 2023. Large language models as simulated economic
    agents: What can we learn from homo silicus? *arXiv preprint arXiv:2301.07543*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Imani et al. (2023) Shima Imani, Liang Du, and Harsh Shrivastava. 2023. [MathPrompter:
    Mathematical reasoning using large language models](https://doi.org/10.18653/v1/2023.acl-industry.4).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 5: Industry Track)*, pages 37–42, Toronto, Canada. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linzen et al. (2016) Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg. 2016. Assessing
    the ability of lstms to learn syntax-sensitive dependencies. *Transactions of
    the Association for Computational Linguistics*, 4:521–535.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Miotto et al. (2022) Marilù Miotto, Nicola Rossberg, and Bennett Kleinberg.
    2022. Who is gpt-3? an exploration of personality, values and demographics. *arXiv
    preprint arXiv:2209.14338*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Munir et al. (2023) Iqbal Munir et al. 2023. Artificial intelligence chatgpt
    in medicine. can it be the friend you are looking for? *Journal of Bangladesh
    Medical Association of North America (BMANA) BMANA Journal*, pages 01–04.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ouyang et al. (2023) Shuyin Ouyang, Jie M. Zhang, Mark Harman, and Meng Wang.
    2023. [Llm is like a box of chocolates: the non-determinism of chatgpt in code
    generation](http://arxiv.org/abs/2308.02828).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Phelps and Russell (2023) Steve Phelps and Yvan I Russell. 2023. Investigating
    emergent goal-like behaviour in large language models using experimental economics.
    *arXiv preprint arXiv:2305.07970*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Razeghi et al. (2022) Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and
    Sameer Singh. 2022. [Impact of pretraining term frequencies on few-shot numerical
    reasoning](https://doi.org/10.18653/v1/2022.findings-emnlp.59). In *Findings of
    the Association for Computational Linguistics: EMNLP 2022*, pages 840–854, Abu
    Dhabi, United Arab Emirates. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Srivastava et al. (2022) Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,
    Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya
    Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying
    and extrapolating the capabilities of language models. *arXiv preprint arXiv:2206.04615*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sutton and Barto (2018) Richard S Sutton and Andrew G Barto. 2018. *Reinforcement
    Learning: An Introduction*. MIT press.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. [Self-consistency improves
    chain of thought reasoning in language models](http://arxiv.org/abs/2203.11171).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi,
    Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in
    large language models. *arXiv preprint arXiv:2201.11903*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wilson et al. (2014) Robert C Wilson, Andra Geana, John M White, Elliot A Ludvig,
    and Jonathan D Cohen. 2014. Humans use directed and random exploration to solve
    the explore–exploit dilemma. *Journal of Experimental Psychology: General*, 143(6):2074.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2023) Hui Yang, Sifu Yue, and Yunzhong He. 2023. Auto-gpt for
    online decision making: Benchmarks and additional opinions. *arXiv preprint arXiv:2306.02224*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
