- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:42:53'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'A Better LLM Evaluator for Text Generation: The Impact of Prompt Output Sequencing
    and Optimization'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.09972](https://ar5iv.labs.arxiv.org/html/2406.09972)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: KuanChao Chu^*    Yi-Pei Chen^*    Hideki Nakayama
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This research investigates prompt designs of evaluating generated texts using
    large language models (LLMs). While LLMs are increasingly used for scoring various
    inputs, creating effective prompts for open-ended text evaluation remains challenging
    due to model sensitivity and subjectivity in evaluation of text generation. Our
    study experimented with different prompt structures, altering the sequence of
    output instructions and including explanatory reasons. We found that the order
    of presenting reasons and scores significantly influences LLMs’ scoring, with
    a different level of rule understanding in the prompt. An additional optimization
    may enhance scoring alignment if sufficient data is available. This insight is
    crucial for improving the accuracy and consistency of LLM-based evaluations.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '^†^†footnotetext: Equal contribution'
  prefs: []
  type: TYPE_NORMAL
- en: Using large language models (LLMs) [[1](#bib.bib1), [2](#bib.bib2)] as evaluators
    to assign scores to the given inputs have become prevalent. Leblond et al. [[3](#bib.bib3)]
    outputs a score between 0 and 1 to estimate the correctness of generated code,
    thereby automatically ranking its quality. Similarly, Park et al. [[4](#bib.bib4)]
    assign poignancy score to the generated text for the retrieval task. Other research
    explores using LLMs to assess generated texts, finding the LLM scores correlates
    higher with human evaluators than existing automatic metrics [[5](#bib.bib5),
    [6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8)].
  prefs: []
  type: TYPE_NORMAL
- en: However, designing evaluation prompt for LLMs is not a trivial task, especially
    for dialogue evaluation. Different models exhibit varied sensitivity to the nuances
    of input prompts. Even slight linguistic variations can lead to significant fluctuations
    in task performance [[9](#bib.bib9)]. Moreover, the inherent subjectivity in dialogue
    evaluation adds on the difficulty and versatility in LLMs’ evaluation results.
    While prompt optimization techniques [[10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12),
    [13](#bib.bib13)] have been developed to assist in designing more effective prompts,
    these methods require paired input-output samples for objective value calculation.
    Unfortunately, the lack of available dialogue-score pairing data hampers the application
    of prompt optimization in dialogue evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: In this study, we aim to investigate the influence of prompt design on dialogue
    evaluation, specifically focusing on how the output instructions affects the resulting
    scores. We have developed multiple prompt variations to assess the quality of
    a series of dialogues. These variations involve altering the sequence order of
    the outputs and examining whether including explanatory reasons along with the
    scores impacts the evaluation. Our analysis compares the influence of different
    prompts on the scoring outcomes across various versions of GPT models.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5fe537009bb9d0f631e0b541f7fef2a5.png)![Refer to caption](img/f4ff2eaa12602c928e006c0ce916daf4.png)![Refer
    to caption](img/1be18a936e792c4677c978185b6ff20d.png)![Refer to caption](img/362490a2bf0f801ae05c747d857088a7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Score distribution across 50 trials for each model and output instruction
    configuration for a dialogue set.'
  prefs: []
  type: TYPE_NORMAL
- en: We observed that the different order of output instructions can result in different
    scoring distributions by certain LLMs, even when the corresponding output reasons
    are similar. Considering the sequential generation nature of auto-regressive models,
    placing the score after the reasons allows it to reference both the reasons and
    the input prompt, a dynamic not possible when this order is reversed. The finding
    suggests that a “reason-first” output instruction might lead to a more comprehensive
    understanding and adherence to the specific requirements of the task.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we further investigate the effect of prompt optimization on the “reason-first”
    output instruction. This is to observe the most effective form of instruction
    in a hypothetical setting where paired data is available.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e4ec590fb6aa5c2526db9ae1a0740ea0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The form of prompt for LLM scorer in conversation evaluation. The
    special rules section is simplified for better readability.'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Output Instruction in Dialogue Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Config | Output Instruction in the Prompt |'
  prefs: []
  type: TYPE_TB
- en: '| ex (s) | Example JSON output: |'
  prefs: []
  type: TYPE_TB
- en: '|  | {"score": 5} |'
  prefs: []
  type: TYPE_TB
- en: '| ex (sr) | Example JSON output: |'
  prefs: []
  type: TYPE_TB
- en: '|  | {"score": 5, "reasons": "$<$"} |'
  prefs: []
  type: TYPE_TB
- en: '| ex (rs) | swap the order of “score” and “reasons” in ex (sr) |'
  prefs: []
  type: TYPE_TB
- en: '| json (s) | Output a json of the following format: |'
  prefs: []
  type: TYPE_TB
- en: '|  | {"score": "$<$"} |'
  prefs: []
  type: TYPE_TB
- en: '| json (sr) | Output a json of the following format: |'
  prefs: []
  type: TYPE_TB
- en: '|  | {"score": "$<$", "reasons": "point out the issues and your reasons for
    the rating"} |'
  prefs: []
  type: TYPE_TB
- en: '| json (rs) | swap the order of “score” and “reasons” in json (sr) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: The variations of output instruction.'
  prefs: []
  type: TYPE_NORMAL
- en: Approach
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this section, the task assigned to the LLM is to rate a given set of dialogues
    on a scale from 1 to 10, where 1 indicates no issues in the set of dialogues,
    and 10 signifies severe problems. Additionally, if specified in the prompt, the
    LLM is required to provide a rationale for the rating. The dialogues are presented
    in chronological order, and the output score is determined based on a comprehensive
    evaluation of the entire set, focusing on key aspects such as factual accuracy,
    repetitiveness, and coherence.
  prefs: []
  type: TYPE_NORMAL
- en: Along with the task description, we have integrated five customized rules into
    the prompt, derived from observations in previous experiments without these rules.
    The special rules include instructions for the LLM to prioritize the number of
    issues over their impact and to assign more weight to aspects exhibiting significant
    issues, rather than averaging out the score across all aspects.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final evaluation prompt is organized as follows: a set of dialogues, task
    description, special rules, and output instruction (see Table [1](#S2.T1 "Table
    1 ‣ 2\. Output Instruction in Dialogue Evaluation ‣ A Better LLM Evaluator for
    Text Generation: The Impact of Prompt Output Sequencing and Optimization")). For
    each set of dialogues, we conducted $N$ different models.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e8ca657be4ff8a10d5773ea9e3d95c45.png)![Refer to caption](img/6f92bba209bf283ea74c1dc641fc7e07.png)![Refer
    to caption](img/ea93325a8b17d6a490939aa975d4c563.png)![Refer to caption](img/6cc66ee3e178149ba8cd6488f7f0ed31.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Score distribution across 50 trials for each model and output instruction
    configuration for a dialogue set, with the ‘special rules’ omitted from the prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: Data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To assess the capability of LLMs in identifying issues within dialogues, we
    collected LLM-generated dialogues from Park et al. [[4](#bib.bib4)] and manually
    grouped them into 25 sets. Each set contains four to six dialogues and exhibits
    one or more problems, such as repetition or contradictions between dialogues.
  prefs: []
  type: TYPE_NORMAL
- en: Model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We selected four recent LLMs to serve as scorers: gpt-3.5-turbo-0613, gpt-3.5-turbo-1106,
    gpt-4-0613, and gpt-4-1106-preview (gpt-4-1106). Note that our aim is to analyze
    the evaluation scores across various models when altering output instructions,
    and not to compare them with human judgements for this task.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Result and Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Config | GPT-3.5-turbo | GPT-4 |'
  prefs: []
  type: TYPE_TB
- en: '| -0613 | -1106 | -0613 | -1106 |'
  prefs: []
  type: TYPE_TB
- en: '| ex (s) | 3.68 $\pm$1.05 |'
  prefs: []
  type: TYPE_TB
- en: '| ex (sr) | 4.20 $\pm$1.12 |'
  prefs: []
  type: TYPE_TB
- en: '| ex (rs) | 6.09 $\pm$0.90 |'
  prefs: []
  type: TYPE_TB
- en: '| json (s) | 4.03 $\pm$1.24 |'
  prefs: []
  type: TYPE_TB
- en: '| json (sr) | 4.66 $\pm$1.06 |'
  prefs: []
  type: TYPE_TB
- en: '| json (rs) | 5.78 $\pm$0.95 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Mean scores and std for 25 dialogue sets, evaluated across different
    models and output instruction configurations.'
  prefs: []
  type: TYPE_NORMAL
- en: The Importance of Output Instruction Order
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table [2](#S2.T2 "Table 2 ‣ 2.1 Result and Analysis ‣ 2\. Output Instruction
    in Dialogue Evaluation ‣ A Better LLM Evaluator for Text Generation: The Impact
    of Prompt Output Sequencing and Optimization") presents the mean scores and standard
    deviations (std) of 10 trials for all 25 dialogue sets across all configs and
    models. In both ex ($\cdot$) formats, the mean scores for the rs settings (output
    reasons before the score) are generally higher than their sr (output score before
    reasons) counterparts. ¹¹1The exception is observed with the gpt-4-1106 model.For
    instance, in the json (rs) config using gpt-4-0613, the mean score is 5.34, while
    it drops to 3.26 in json (sr), despite providing similar reasons. We conjecture
    that in the rs setting, the autoregressive nature of the model allows the score
    to be influenced by the previously outputted reasons.'
  prefs: []
  type: TYPE_NORMAL
- en: Different Levels of Rule Understanding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In a focused study on a single set with additional 40 trials, as depicted in
    Fig. [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ A Better LLM Evaluator for Text
    Generation: The Impact of Prompt Output Sequencing and Optimization"), we observed
    a trend consistent with the findings presented in Table [2](#S2.T2 "Table 2 ‣
    2.1 Result and Analysis ‣ 2\. Output Instruction in Dialogue Evaluation ‣ A Better
    LLM Evaluator for Text Generation: The Impact of Prompt Output Sequencing and
    Optimization"). However, as shown in Fig.[3](#S2.F3 "Figure 3 ‣ Approach ‣ 2\.
    Output Instruction in Dialogue Evaluation ‣ A Better LLM Evaluator for Text Generation:
    The Impact of Prompt Output Sequencing and Optimization"), when we removed the
    ‘special rules’ from the prompt, we found that most scores were lower and the
    distinctions between different settings became less pronounced. This highlights
    the models’ sensitivity to the changes of the prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Iteration | Output Instruction | score on $\mathcal{S}$) |'
  prefs: []
  type: TYPE_TB
- en: '| init(json(rs)) | 0 | Output a json of the following format: {”reasons”: ”point
    out your reasons for the rating on coherence”, ”score”: ”the rating”} | -1.271
    | 51.9 |'
  prefs: []
  type: TYPE_TB
- en: '| +GRIPS | 1 | Output a json of   following format: {”reasons”: ”point out
    your reasons for   rating on coherence”, ”score”: ”the rating”} | -1.267 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | 4 | Output json of following format: {”reasons”: ”point out your reasons
    for rating on coherence”, a ”score”: ”of rating”} | -1.234 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | 7 | Output json of following format ”{”reasons” ””point out your reasons
    for rating on coherence”, ”score” ””of rating”} | -1.100 |  |'
  prefs: []
  type: TYPE_TB
- en: '| +OPRO | 25 | Assess the provided summary’s coherence in relation to the source
    text, grading it on a scale of 1 to 5. Coherence refers to the seamless connection
    of ideas and the logical structuring of information from the source into a unified
    summary. The summary should not just be a cluster of related information, but
    a well-knit narrative reflecting the source’s key points. Ensure you provide concise
    reasoning for your rating, discussing the summary’s strengths or weaknesses in
    its coherence. Consistently refer back to these guidelines during your assessment.
    |  | 46.0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 43 | Evaluate the summary provided, grading its coherence with the original
    news article on a scale of 1-5. Consider how the summary reflects the logical
    progression, structure, and key details from the source text. A superior summary
    should interpret related details into a clear, comprehensive story, not merely
    gathering related facts. For an effective assessment, underpin your score with
    explicit examples from the text. Concentrate on the flow and organization of the
    summary throughout your evaluation. |  | 62.2 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Examples of output instructions with optimization methods GRIPS and
    OPRO. We pick the one with highest scores on $\mathcal{S}$ as the optimized instructions.'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Prompt optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Approach
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We assume there are sufficient paired input and output data samples for the
    scoring task. Hence, in this section, we examine how output instructions can be
    enhanced through additional prompt optimization. We apply two recent optimization
    methods to the existing prompt: GRIPS [[13](#bib.bib13)] and OPRO [[11](#bib.bib11)].
    For a given score set $\mathcal{S}$. Meanwhile, OPRO directly employs a LLM as
    an optimizer to generate new prompts, with previous score trajectories included
    as part of the reference material for the optimizer. Specifically, we utilize
    the json (rs) style for the output instruction in the initial prompt, optimizing
    only the output instruction section while keeping the other sections unchanged.'
  prefs: []
  type: TYPE_NORMAL
- en: Data and Implementation Details
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For this experiment, we utilize SummEval [[14](#bib.bib14)], which comprises
    1,600 text summarization samples derived from 100 unique document pieces and 16
    summarization models. Each sample includes human expert ratings ranging from 1
    to 5 across various aspects. We focus on the ”coherence” aspect for the expected
    output. To ensure rating diversity in $\mathcal{S}$, we uniformly sample 10% of
    the documents after ranking them by their average coherence score to create the
    score set for GRIPS, and use a halved subset for OPRO. The remaining 90% of the
    data is reserved for testing.
  prefs: []
  type: TYPE_NORMAL
- en: For GRIPS, we configure the editing operations at the word level and conduct
    10 search iterations, with 5 candidates in each iteration. For OPRO, we employ
    gpt-4-0613 as the optimizer, running 50 iterations with 2 generations each. We
    include two randomly selected data exemplars from $\mathcal{S}$ when generating
    the output instructions.
  prefs: []
  type: TYPE_NORMAL
- en: We calculate the mean absolute error (MAE) between the human ratings and LLM
    ratings to evaluate the overall score for $\mathcal{S}$, with the number of trials
    increased to 20.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Result and Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Changes in Output Instructions and Generalization on Test Data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the prompt examples shown in Table [3](#S2.T3 "Table 3 ‣ Different Levels
    of Rule Understanding ‣ 2.1 Result and Analysis ‣ 2\. Output Instruction in Dialogue
    Evaluation ‣ A Better LLM Evaluator for Text Generation: The Impact of Prompt
    Output Sequencing and Optimization"), OPRO demonstrates a preference for developing
    a diverse and detailed output instruction section. Some of the content either
    repeats or adds new details to the grading policy and may explicitly request the
    provision of reasons. The changes made using GRIPS, however, are comparatively
    limited due to the smaller search space. For example, the candidate with the best
    score on $\mathcal{S}$ underwent only five editing commands: “del-del-swap-swap-del.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [4](#S3.T4 "Table 4 ‣ The Changes in Output Instructions and Generalization
    on Test Data ‣ 3.1 Result and Analysis ‣ 3\. Prompt optimization ‣ A Better LLM
    Evaluator for Text Generation: The Impact of Prompt Output Sequencing and Optimization")
    lists the performance of the LLM scorer on the test set. Surprisingly, the output
    instruction modified with GRIPS yield improved scores, whereas OPRO shows the
    opposite. However, the number of iterations set for OPRO is fewer than what was
    used in the original paper, due to cost constraints. Therefore, its performance
    might be far from saturation. Following [[15](#bib.bib15)], we apply William’s
    test to Pearson’s r, and we found that the improvement with GRIPS over the initial
    instruction is statistically significant.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | MAE($\downarrow$) |'
  prefs: []
  type: TYPE_TB
- en: '| init(json(rs)) | 0.739 | 0.599 | 0.439 |'
  prefs: []
  type: TYPE_TB
- en: '| +GRIPS | 0.696 | 0.614 | 0.449 |'
  prefs: []
  type: TYPE_TB
- en: '| +OPRO | 0.780 | 0.466 | 0.328 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: The MAE and dataset-level correlation coefficients between LLM scorer
    and human experts ratings on the test set, using inital and post-optimization
    output instructions.'
  prefs: []
  type: TYPE_NORMAL
- en: The Score Difference for Swapped Orders Exists
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We also compare the json (rs) and json (sr) types of initial instructions. The
    averaged score on the test set is 3.656 for json (rs) and 3.270 for json (sr).
    This reflects a similar distribution difference to what we observed in the previous
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: First, although we intentionally divide a score set to study the effect of optimization,
    collecting related samples for a specific scoring task can be challenging. Second,
    these optimizations are applied globally to all data samples, while the optimized
    output instructions might vary for individual cases. Finally, the optimized instructions
    could break the structured output format and thus requires careful post-generation
    processing.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our study highlights the scoring sensitivity of LLMs to the order of output
    instructions, which can be amplified by task-specific rules. Users should verify
    that their intentions align with the outputs using crafted prompts. Additionally,
    prompt optimization has proven beneficial in aligning LLM outputs with ground-truth
    scores, emphasizing the need for sufficient data samples for arbitrary scoring
    tasks. These findings offer insights for designing prompts in subjective tasks
    like dialogue evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenAI [2023] OpenAI. Gpt-4 technical report. *ArXiv*, abs/2303.08774, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. [2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leblond et al. [2023] placeholder Leblond et al. Alphacode 2 technical report,
    2023. URL https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel Morris,
    Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra
    of human behavior. *UIST ’23*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. [2023] Mingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Shiping Yang,
    and Xiaojun Wan. Human-like summarization evaluation with chatgpt. *arXiv preprint
    arXiv:2304.02554*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et al. [2023] Chenhui Shen, Liying Cheng, Xuan-Phi Nguyen, Yang You, and
    Lidong Bing. Large language models are not yet human-level evaluators for abstractive
    summarization. In *Findings of the Association for Computational Linguistics:
    EMNLP 2023*, pages 4215–4233, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2023] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu,
    and Chenguang Zhu. G-eval: NLG evaluation using gpt-4 with better human alignment.
    In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language
    Processing*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo et al. [2023] Zheheng Luo, Qianqian Xie, and Sophia Ananiadou. Chatgpt as
    a factual inconsistency evaluator for abstractive text summarization. *arXiv preprint
    arXiv:2303.15621*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Leidinger et al. [2023] Alina Leidinger, Robert Van Rooij, and Ekaterina Shutova.
    The language of prompting: What linguistic properties make a prompt successful?
    In *The 2023 Conference on Empirical Methods in Natural Language Processing*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2023] Lichang Chen, Jiuhai Chen, Tom Goldstein, Heng Huang, and
    Tianyi Zhou. Instructzero: Efficient instruction optimization for black-box large
    language models. *arXiv preprint arXiv:2306.03082*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. [2023] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V
    Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. *arXiv preprint
    arXiv:2309.03409*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2023] Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans,
    and Joseph E. Gonzalez. TEMPERA: Test-time prompt editing via reinforcement learning.
    In *The Eleventh International Conference on Learning Representations*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prasad et al. [2023] Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal.
    Grips: Gradient-free, edit-based instruction search for prompting large language
    models. In *Proceedings of the 17th Conference of the European Chapter of the
    Association for Computational Linguistics*, pages 3827–3846, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fabbri et al. [2021] Alexander R Fabbri, Wojciech Kryściński, Bryan McCann,
    Caiming Xiong, Richard Socher, and Dragomir Radev. Summeval: Re-evaluating summarization
    evaluation. *Transactions of the Association for Computational Linguistics*, 9:391–409,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Cheng-Han Chiang and Hung-yi Lee. A closer look into using large language
    models for automatic evaluation. In *Findings of the Association for Computational
    Linguistics: EMNLP 2023*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Appendix A Full Special Rules
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table [5](#A1.T5 "Table 5 ‣ Appendix A Full Special Rules ‣ A Better LLM Evaluator
    for Text Generation: The Impact of Prompt Output Sequencing and Optimization")
    presents the full prompt of the special rules section in Fig. [2](#S1.F2 "Figure
    2 ‣ 1\. Introduction ‣ A Better LLM Evaluator for Text Generation: The Impact
    of Prompt Output Sequencing and Optimization"), which was trimmed for better readability.'
  prefs: []
  type: TYPE_NORMAL
- en: '| ”Unfinished conversation” need not be viewed as an issue. |'
  prefs: []
  type: TYPE_TB
- en: '| Do note that the scoring should not be an average of the performance in each
    of the three aspects. |'
  prefs: []
  type: TYPE_TB
- en: '| Even if the conversations are generally coherent but contain a issue in one
    aspect, the score should lean towards the higher end of the scale. |'
  prefs: []
  type: TYPE_TB
- en: '| Do not give a small score just because the issue is not very impactful. Consider
    the number of issues rather than its impact. |'
  prefs: []
  type: TYPE_TB
- en: '| Your overall score should reflect the utmost concern observed in any of the
    aspects. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Special rules for conversation evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Example Dialogue Set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table LABEL:tab:dialoguesample lists an example dialogue set we have studied
    in Section [2.](#S2 "2\. Output Instruction in Dialogue Evaluation ‣ A Better
    LLM Evaluator for Text Generation: The Impact of Prompt Output Sequencing and
    Optimization") The results shown in Fig. [1](#S1.F1 "Figure 1 ‣ 1\. Introduction
    ‣ A Better LLM Evaluator for Text Generation: The Impact of Prompt Output Sequencing
    and Optimization") and Fig. [3](#S2.F3 "Figure 3 ‣ Approach ‣ 2\. Output Instruction
    in Dialogue Evaluation ‣ A Better LLM Evaluator for Text Generation: The Impact
    of Prompt Output Sequencing and Optimization") are produced from this set. The
    original data can be found in [[4](#bib.bib4)]. ²²2https://reverie.herokuapp.com/arXiv_Demo/'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Example dialogue set for conversation evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Time: 07:08 |'
  prefs: []
  type: TYPE_TB
- en: '| Mei Lin: “Good morning, John. How did you sleep?” |'
  prefs: []
  type: TYPE_TB
- en: '| John Lin: “Not bad, although I had a lot on my mind. I’m curious about who
    will be running for the local mayor election next month.” |'
  prefs: []
  type: TYPE_TB
- en: '| Mei Lin: “Oh, that’s interesting. Have you asked anyone about it?” |'
  prefs: []
  type: TYPE_TB
- en: '| John Lin: “Yes, I’ve been asking everyone I meet. I’m also taking online
    classes to stay up to date on new medications and treatments.” |'
  prefs: []
  type: TYPE_TB
- en: '| Mei Lin: “That’s great. By the way, have you noticed anything strange with
    Eddy lately? He seems a bit rebellious.” |'
  prefs: []
  type: TYPE_TB
- en: '| John Lin: “Yes, I have noticed that. I think we need to keep an eye on him
    and make sure he’s okay.” |'
  prefs: []
  type: TYPE_TB
- en: '| Mei Lin: “Agreed. And speaking of neighbors, do you know Tamara Taylor who
    lives on the next block?” |'
  prefs: []
  type: TYPE_TB
- en: '| John Lin: “I’ve heard of her, but I haven’t had a conversation with her yet.”
    |'
  prefs: []
  type: TYPE_TB
- en: '| Mei Lin: “Okay. And what about our elderly neighbors, the Moores? You’ve
    been acquainted with them for a few years, right?” |'
  prefs: []
  type: TYPE_TB
- en: '| John Lin: “Yes, I really like Jennifer Moore’s art. She’s very talented.”
    |'
  prefs: []
  type: TYPE_TB
- en: '| Mei Lin: “That’s nice. Alright then, I’ll see you later. Have a good day
    at work.” |'
  prefs: []
  type: TYPE_TB
- en: '| John Lin: “Thanks. You too, Mei Lin.” |'
  prefs: []
  type: TYPE_TB
- en: '| Time: 07:53 |'
  prefs: []
  type: TYPE_TB
- en: '| John Lin: “Hey Tom, how’s it going?” |'
  prefs: []
  type: TYPE_TB
- en: '| Tom Moreno: “Good morning John, just checking the news before starting my
    day.” |'
  prefs: []
  type: TYPE_TB
- en: '| John Lin: “Yeah, I hear you. Have you heard anything about who might be running
    for the local mayor election next month?” |'
  prefs: []
  type: TYPE_TB
- en: '| Tom Moreno: “Actually, I was just reading an article about that. There are
    a few candidates being considered but I haven’t made up my mind yet.” |'
  prefs: []
  type: TYPE_TB
- en: '| John Lin: “Well, if you hear anything else, let me know. By the way, did
    you see the new medications and treatments I’ve been learning about in my online
    classes? There’s some really exciting stuff out there.” |'
  prefs: []
  type: TYPE_TB
- en: '| Tom Moreno: “No, I haven’t had a chance to look into that yet. It sounds
    interesting though. By the way, have you noticed anything strange with your neighbor’s
    rebellious son lately?” |'
  prefs: []
  type: TYPE_TB
- en: '| John Lin: “Actually, I have. I’m planning on keeping an eye on him and making
    sure everything is okay. It’s good to know that we are both keeping an eye out
    for the well being of our community. Oh, and thanks for the extra stock you gave
    me the other day. It really helped out.” |'
  prefs: []
  type: TYPE_TB
- en: '| Tom Moreno: “No problem, John. I always try to help out where I can. By the
    way, have you seen any of Jennifer Moore’s new artwork? She’s really been stepping
    up her game lately.” |'
  prefs: []
  type: TYPE_TB
- en: '| John Lin: “Yeah, I actually saw her new piece at the gallery the other day.
    It was really impressive.” |'
  prefs: []
  type: TYPE_TB
- en: '| Time: 08:07 |'
  prefs: []
  type: TYPE_TB
- en: '| John Lin: “Hi there, Jennifer! I couldn’t help but notice your beautiful
    artwork. I’m really looking forward to seeing your upcoming exhibition. Have you
    finalized the date yet?” |'
  prefs: []
  type: TYPE_TB
- en: '| Jennifer Moore: “Thank you, John! I appreciate your kind words. And yes,
    the exhibition is scheduled for next month. I’ve been quite busy preparing for
    it.” |'
  prefs: []
  type: TYPE_TB
- en: '| John Lin: “That’s great to hear. I remember Tom Moreno mentioning how much
    he enjoyed your last exhibition. By the way, I heard you’ve been mentoring young
    artists. That’s really inspiring. Can you tell me more about it?” |'
  prefs: []
  type: TYPE_TB
- en: '| Jennifer Moore: “Yes, I love mentoring younger artists. It’s such a joy to
    see them find their own creative voice. I have a few mentees right now who are
    working on some really interesting projects.” |'
  prefs: []
  type: TYPE_TB
- en: '| John Lin: “That’s wonderful. I’m sure they’re lucky to have you as their
    mentor. On another note, have you been following the local politics?” |'
  prefs: []
  type: TYPE_TB
- en: '| Jennifer Moore: “Actually, I’m not really interested in politics. I prefer
    to focus on my art and my relationships with the people I care about.” |'
  prefs: []
  type: TYPE_TB
- en: '| John Lin: “I see. That makes sense. Well, it was really nice chatting with
    you, Jennifer. I’ll be sure to mark the date of your exhibition on my calendar.”
    |'
  prefs: []
  type: TYPE_TB
- en: '| Jennifer Moore: “Likewise, John. Thank you for stopping by. And please do
    come to my exhibition. I’d love to see you there.” |'
  prefs: []
  type: TYPE_TB
- en: '| Time: 09:16 |'
  prefs: []
  type: TYPE_TB
- en: '| Giorgio Rossi: “Excuse me, are you John Lin?” |'
  prefs: []
  type: TYPE_TB
- en: '| John Lin: “Yes, that’s me. How may I help you?” |'
  prefs: []
  type: TYPE_TB
- en: '| Giorgio Rossi: “I noticed that you’re very knowledgeable about medications
    and treatments. I’ve been curious about some of the latest developments in that
    field. Would you mind sharing some of your insights with me?” |'
  prefs: []
  type: TYPE_TB
- en: '| John Lin: “Of course, I’m happy to share what I know. What specifically are
    you interested in?” |'
  prefs: []
  type: TYPE_TB
- en: '| Giorgio Rossi: “Well, I’m particularly intrigued by how some medications
    are being developed based on mathematical patterns found in nature. Do you have
    any knowledge about that?” |'
  prefs: []
  type: TYPE_TB
- en: '| John Lin: “Hmm, that’s not really my area of expertise. But I do know that
    Jennifer Moore, the artist who’s running for local mayor, is also a scientist
    who incorporates mathematical patterns into her work. Maybe she would have more
    information on that.” |'
  prefs: []
  type: TYPE_TB
