- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:47:05'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2401.10065](https://ar5iv.labs.arxiv.org/html/2401.10065)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Haritz Puerto¹, Martin Tutek¹, Somak Aditya², Xiaodan Zhu^(1,3), Iryna Gurevych¹
  prefs: []
  type: TYPE_NORMAL
- en: ¹Ubiquitous Knowledge Processing Lab (UKP Lab),
  prefs: []
  type: TYPE_NORMAL
- en: TU Darmstadt and Hessian Center for AI (hessian.AI)
  prefs: []
  type: TYPE_NORMAL
- en: ²IIT Kharagpur, ³Queen’s University
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.ukp.tu-darmstadt.de](https://www.ukp.tu-darmstadt.de)'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Reasoning is a fundamental component for achieving language understanding. Among
    the multiple types of reasoning, conditional reasoning, the ability to draw different
    conclusions depending on some condition, has been understudied in large language
    models (LLMs). Recent prompting methods, such as chain of thought, have significantly
    improved LLMs on reasoning tasks. Nevertheless, there is still little understanding
    of what triggers reasoning abilities in LLMs. We hypothesize that code prompts
    can trigger conditional reasoning in LLMs trained on text and code. We propose
    a chain of prompts that transforms a natural language problem into code and prompts
    the LLM with the generated code. Our experiments find that code prompts exhibit
    a performance boost between $2.6$ points on GPT 3.5 across multiple datasets requiring
    conditional reasoning. We then conduct experiments to discover how code prompts
    elicit conditional reasoning abilities and through which features. We observe
    that prompts need to contain natural language text accompanied by high-quality
    code that closely represents the semantics of the instance text. Furthermore,
    we show that code prompts are more efficient, requiring fewer demonstrations,
    and that they trigger superior state tracking of variables or key entities.¹¹1Code,
    prompt templates, prompts, and outputs are publicly available at [https://github.com/UKPLab/arxiv2024-conditional-reasoning-llms](https://github.com/UKPLab/arxiv2024-conditional-reasoning-llms).
  prefs: []
  type: TYPE_NORMAL
- en: Code Prompting Elicits Conditional Reasoning Abilities
  prefs: []
  type: TYPE_NORMAL
- en: in Text+Code LLMs
  prefs: []
  type: TYPE_NORMAL
- en: Haritz Puerto¹, Martin Tutek¹, Somak Aditya², Xiaodan Zhu^(1,3), Iryna Gurevych¹
    ¹Ubiquitous Knowledge Processing Lab (UKP Lab), TU Darmstadt and Hessian Center
    for AI (hessian.AI) ²IIT Kharagpur, ³Queen’s University [https://www.ukp.tu-darmstadt.de](https://www.ukp.tu-darmstadt.de)
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reasoning is a fundamental component of both human and artificial intelligence
    (AI). Some reasoning types have received much attention in the form of new benchmarks
    and models from the AI community in the last few years, such as mathematical (Patel
    et al., [2021](#bib.bib22); Chen et al., [2021](#bib.bib2); Cobbe et al., [2021](#bib.bib4)),
    logical (Liu et al., [2020](#bib.bib15), [2023a](#bib.bib12); Sinha et al., [2019](#bib.bib24)),
    and commonsense reasoning (Madaan et al., [2022](#bib.bib18); Liu et al., [2022a](#bib.bib13),
    [b](#bib.bib14); Wang et al., [2023](#bib.bib27)). However, other important reasoning
    types, such as conditional reasoning, remain understudied. Conditional reasoning
    draws alternative conclusions depending on the fulfillment of certain conditions.
    It is also a fundamental form of logical reasoning useful in many scenarios, such
    as in chatbots (e.g., ChatGPT), to answer real-world questions like eligibility
    for a visa or a loan. Despite the recent introduction of some benchmarks (Saeidi
    et al., [2018](#bib.bib23); Sun et al., [2022](#bib.bib25); Kazemi et al., [2023](#bib.bib8)),
    conditional reasoning abilities of LLMs remain unknown.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bd49ba7c37a99591e1cde3311e697099.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Code prompting converts a natural language problem into a code prompt
    and prompts a large language model with such code to generate a natural language
    answer.'
  prefs: []
  type: TYPE_NORMAL
- en: Recently, researchers have improved performance on reasoning tasks by combining
    LLMs with symbolic interpreters such as the Python runtime  (Gao et al., [2023](#bib.bib5);
    Chen et al., [2023](#bib.bib1); Lyu et al., [2023](#bib.bib17)) or SATisfiability
    solvers  (Ye et al., [2023](#bib.bib29)). Here, LLMs pretrained on code or a combination
    of text and code (henceforth text+code LLM) are used to convert the input task
    into a symbolic language (e.g., Python or SAT problems), which is then fed into
    an external interpreter to make use of its symbolic execution. However, offloading
    the final reasoning task to an external execution module does not help with, or
    sometimes confounds, our understanding of the reasoning abilities of the model
    because part of the performance gain may come from the correct execution of the
    external interpreter. In particular, the fundamental questions of what contributes
    to the reasoning abilities and how reasoning abilities are triggered are still
    open questions needing further understanding. Nevertheless, pretraining on code
    is considered an important component that contributes to and explains the improved
    reasoning ability of LLMs. State-of-the-art LLMs such as GPT 3.5 (Kojima et al.,
    [2022](#bib.bib10)), GPT 4 (OpenAI, [2023](#bib.bib20)), PaLM (Chowdhery et al.,
    [2023](#bib.bib3)), and Mistral 7B (Jiang et al., [2023](#bib.bib7)) have been
    pretrained on both text and code and have demonstrated considerable boosts in
    many reasoning benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this work, we investigate which aspects of code prompts, and in which way,
    elicit conditional reasoning abilities of GPT 3.5, a text+code LLM, across two
    datasets: (1) ConditionalQA (Sun et al., [2022](#bib.bib25)), a scenario-based
    question answering (QA) dataset and (2) BoardgameQA (Kazemi et al., [2023](#bib.bib8)),
    a boardgame-based QA dataset with conflicting rules. To understand the true benefit
    of code as an intermediate representation, we devise a chain of prompts, code
    prompting, that transform a natural language (NL) task into code and prompt the
    LLM with the generated code to elicit reasoning abilities. An illustration is
    provided in [Figure 1](#S1.F1 "In 1 Introduction ‣ Code Prompting Elicits Conditional
    Reasoning Abilities in Text+Code LLMs"). This setup has multiple benefits. Firstly,
    we fully utilize LLM’s reasoning ability without offloading any subtask to an
    external interpreter. In this way, we can make a fair comparison between text
    and code prompts without external variables such as interpreters. Secondly, enforcing
    compilability and executability of the generated code is not required, resulting
    in fewer interpreter-driven errors.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our contributions are summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We devise a chain of prompts that transforms a NL task into code to trigger
    conditional reasoning abilities in text+code LLMs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We conduct a comprehensive study to compare code prompts with text prompts,
    showing i) large performance gains between $2.6$ points while ii) being more efficient
    w.r.t. the number of demonstrations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We conduct an extensive analysis of why code prompts efficiently elicit conditional
    reasoning abilities in text+code LLMs, showing that prompting with code results
    in largely improved variable state tracking.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a2ecec96e4d34b113cb5e556691893d8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Code Prompts converts a natural language problem instance into code
    before solving it with a large language model.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most works generating code to solve natural language tasks use an external symbolic
    interpreter to run such code. Chen et al. ([2023](#bib.bib1)) and Gao et al. ([2023](#bib.bib5))
    showed consistent gains on mathematical problems, symbolic reasoning, and algorithmic
    problems by using LLMs aided by external code interpreters. Lyu et al. ([2023](#bib.bib17))
    show further gains in multi-hop QA, planning, and relational inference. Instead
    of leveraging a code interpreter, Ye et al. ([2023](#bib.bib29)) used an automated
    theorem prover with declarative code and showed consistent gains w.r.t. imperative
    code-interpreter-aided LLMs on arithmetic reasoning, logical reasoning, symbolic
    reasoning, and regex synthesis tasks. Lastly, Pan et al. ([2023](#bib.bib21))
    did not use any interpreter and instead created programs composed of multiple
    subroutines and used smaller specialized models to run them. In this way, they
    outperform text prompts on text LLMs for fact-checking tasks.
  prefs: []
  type: TYPE_NORMAL
- en: All these works employ an external solver system to run the code; therefore,
    the LLM is not conducting part of the reasoning. However, some works (Madaan et al.,
    [2022](#bib.bib18); Liu et al., [2023b](#bib.bib16)) suggest that LLMs of code
    may possess superior reasoning abilities than LLMs of text (i.e., pretrained only
    on natural language text). Madaan et al. ([2022](#bib.bib18)) observed improved
    commonsense reasoning, while Liu et al. ([2023b](#bib.bib16)) showed superior
    causal reasoning. This last work is based on the intuition that the large amount
    of if statements in the pretraining corpus enhances the causal reasoning abilities
    because they represent explicit causal relations. They conducted experiments on
    abductive and counterfactual reasoning tasks and showed that translating NL problems
    into code and then generating functions that return the answer to the problem
    with a code LLM outperforms prompting the same NL task in a text LLM. However,
    most popular LLMs are trained on text and code (e.g., GPT 3.5; Kojima et al. [2022](#bib.bib10),
    GPT 4; OpenAI [2023](#bib.bib20)) and it remains unknown whether the reported
    performance gains come from the prompt format, the abilities of the LLM, or a
    combination of both. Therefore, it remains unclear whether code prompts also outperform
    text prompts in text+code models and, if so, why code prompts elicit reasoning
    abilities. In our work, we aim to answer all these questions.
  prefs: []
  type: TYPE_NORMAL
- en: As far as we know, only the concurrent work of (Hussain et al., [2023](#bib.bib6))
    investigates the conditional reasoning abilities of LLMs. However, they only analyze
    the abilities of text-only LLMs after training them on ConditionalQA (Sun et al.,
    [2022](#bib.bib25)). Instead, we investigate how to use prompts to elicit conditional
    reasoning in text+code LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Code Prompting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Definition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We hypothesize that querying text+code LLMs with instances translated to (pseudo)-code
    will result in improved conditional reasoning capabilities. Our hypothesis is
    motivated by previous results showing that program-aided LMs exhibit superior
    results on reasoning tasks than regular text prompts (Gao et al., [2023](#bib.bib5);
    Chen et al., [2023](#bib.bib1); Lyu et al., [2023](#bib.bib17); Ye et al., [2023](#bib.bib29)).
    Thus, it comes naturally that prompting text+code models with instances translated
    to code could cause the underlying model to exhibit superior reasoning abilities
    when compared to text prompts. We formally define our hypothesis as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\sum_{p\in P}\sigma(LLM(T(p)),p)\leq\sum_{p\in P}\sigma(LLM(C(p)),p)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $p$ are functions that create a natural language (Text) prompt and Code
    prompt, respectively, for the same given problem.
  prefs: []
  type: TYPE_NORMAL
- en: We define code prompts as prompts that model a natural language (NL) problem
    with code. The code contains the logical structure needed to solve the problem,
    along with the original natural language text as code comments. To solve an NL
    task with code prompts, we define a chain of prompts that i) transforms the NL
    text into code and ii) prompts using this code to generate the answer in natural
    language. [Figure 1](#S1.F1 "In 1 Introduction ‣ Code Prompting Elicits Conditional
    Reasoning Abilities in Text+Code LLMs") illustrates this pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: The prompt that transforms the NL problem to code is comprised of code that
    closely follows the original NL text. In particular, it creates variables for
    the key entities in the question and documents and if blocks for the documents,
    representing their conditional statements. [Figure 2](#S1.F2 "In 1 Introduction
    ‣ Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs") illustrates
    this transformation.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Coding Features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To generate code as close as possible to the NL text, we use a programming language
    based on a simplification of Python. We only use boolean variables or variables
    that contain lists of strings. Variables follow the snake case naming convention.
    We also employ if statements to model conditional reasoning, but we do not use
    loops, functions, or classes. We create a code comment with the original NL text
    for each input sentence, and right after the code comment, we generate the code
    that represents the semantics of that sentence. However, we do not enforce that
    the generated code should be a runnable Python script.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experimental Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Throughout our experiments, we focus on two conditional question-answering
    QA datasets: ConditionalQA (CondQA; Sun et al., [2022](#bib.bib25)) and BoardgameQA
    (BGQA; Kazemi et al., [2023](#bib.bib8)). Solving these datasets requires advanced
    conditional and compositional reasoning capabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: ConditionalQA is a QA dataset where the answers are applicable under specific
    scenarios (i.e., conditional answers). Therefore, along with each question, the
    dataset provides a scenario that describes the background of the person posing
    such a question. Questions require multi-hop, compositional, and conditional logic
    over documents about public policies (e.g., the eligibility for a subsidy). Answers
    can be a span of the document, yes, and no. We use an oracle retriever to select
    the relevant passages to the question so that we can isolate the analysis of conditional
    reasoning abilities in LLMs from the retrieval component. The expected output
    is a chain of thought (CoT; Wei et al. [2022](#bib.bib28)) followed by the final
    answer. To create the CoT, we use the annotated evidence sentences.
  prefs: []
  type: TYPE_NORMAL
- en: BoardgameQA is a dataset that evaluates the ability to reason with contradictory
    information guided by preferences. For example, given a question about traveling
    abroad, information found online about regulations can be contradictory because
    rules may change over time. Answering questions in this dataset requires complex
    multi-hop reasoning with conditional, deductive, and compositional abilities.
    The domain of the problems is board games, which allows us to analyze the conditional
    reasoning abilities in a completely different domain from CondQA. BGQA is divided
    into multiple partitions focusing on different characteristics, such as the depth
    of the reasoning tree, the need for external information, etc. We focus on the
    main partition and its subpartitions (i.e., BGQA-1, BGQA-2, BGQA-3), where the
    number refers to the number of reasoning hops required to answer the question.
    This dataset also includes annotated chain-of-thoughts (CoT); therefore, we use
    their annotated input (“example”) as the input prompt and their annotated CoT
    (“proof”) as the expected output.
  prefs: []
  type: TYPE_NORMAL
- en: We include more details about the datasets in Appendix [A](#A1 "Appendix A Datasets
    ‣ Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs"),
    a formal definition of the prompts in [Appendix B](#A2 "Appendix B Prompt Formulation
    ‣ Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs"),
    and examples in [Appendix G](#A7 "Appendix G Prompt Examples ‣ Code Prompting
    Elicits Conditional Reasoning Abilities in Text+Code LLMs").
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 LLM Setup.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We perform our study using gpt-35-turbo-0613-16k through the Azure OpenAI service.
    All of our prompting methods are implemented using the Langchain library.²²2[https://github.com/langchain-ai/langchain](https://github.com/langchain-ai/langchain)
    We set the decoding temperature to zero and use greedy sampling to make the outputs
    deterministic. We execute our prompts with in-context learning. However, the amount
    of demonstrations is constrained by their long length and the context window of
    the model, especially in code prompts, which are longer than text prompts because
    they include the original natural language text and the corresponding code. Specifically,
    we used between three and nine demonstrations except for code prompts in BGQA-2
    and BGQA-3 where the maximum possible is 6. In other words, we provided from one
    to three demonstrations per class. For each experiment, we use a random sample
    from the training set as demonstrations. A detailed analysis of the impact of
    the number of demonstrations in the performance of the prompts is provided in
    [Section 5.4](#S5.SS4 "5.4 Code Prompts are More Sample-Efficient at Eliciting
    Reasoning Abilities ‣ 5 Experiments ‣ Code Prompting Elicits Conditional Reasoning
    Abilities in Text+Code LLMs"). We also report the costs of our prompts in [Appendix C](#A3
    "Appendix C Costs ‣ Code Prompting Elicits Conditional Reasoning Abilities in
    Text+Code LLMs").
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Evaluation.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We follow the evaluation metrics used in the original datasets. For CondQA,
    we report the F1 token overlap between the predicted answer and the label, while
    for BGQA, we report accuracy. We run the main experiments multiple times with
    different random seeds: two times on BGQA and three times on CondQA. We report
    the average and standard deviation performance across these runs. For the subsequent
    analyses of code prompts, we run each experiment once due to the inference costs.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt Method | CondQA | BGQA-1 | BGQA-2 | BGQA-3 |'
  prefs: []
  type: TYPE_TB
- en: '| Text Prompt | $56.32\pm 1.06$ |'
  prefs: []
  type: TYPE_TB
- en: '| Code Prompt | 59.17 $\pm$ 0.42 |'
  prefs: []
  type: TYPE_TB
- en: '| Atomic Statements | 55.94 | 57.80 | 47.80 | 35.60 |'
  prefs: []
  type: TYPE_TB
- en: '| Code $\rightarrow$ NL | 55.42 | 63.00 | 55.60 | 48.00 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Comparison of text prompt and code prompts on the validation set of
    each dataset. Last two rows analyse the impact of the input format on text prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We first compare the performance of the two prompting methods — text prompts
    and code prompts. We then conduct extensive ablation experiments to determine
    whether it is the code-translated instances what improve performance or if the
    improvement is caused by implicit text simplification ([Section 5.2](#S5.SS2 "5.2
    Code Format Elicits Reasoning Abilities ‣ 5 Experiments ‣ Code Prompting Elicits
    Conditional Reasoning Abilities in Text+Code LLMs")) or by models merely being
    exposed to code within prompts and not necessarily the instances translated to
    code ([Section 5.3](#S5.SS3 "5.3 Code Semantic is Important ‣ 5 Experiments ‣
    Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs")). Finally,
    we show that code prompting is more sample efficient ([Section 5.4](#S5.SS4 "5.4
    Code Prompts are More Sample-Efficient at Eliciting Reasoning Abilities ‣ 5 Experiments
    ‣ Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs"))
    when compared to text prompting and that models prompted with code exhibit superior
    state tracking capabilities ([Section 5.5](#S5.SS5 "5.5 Code Prompts Improve Variable
    Tracking in LLMs ‣ 5 Experiments ‣ Code Prompting Elicits Conditional Reasoning
    Abilities in Text+Code LLMs")).
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Code Prompting Improves over Text Prompting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Overall, as shown in [Table 1](#S4.T1 "In 4.3 Evaluation. ‣ 4 Experimental Setup
    ‣ Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs"),
    code prompts consistently outperform text prompts. In particular, we can observe
    a significant performance boost of $7.71$ points on CondQA. Furthermore, we can
    also observe how the gap between both prompting methods increases in BGQA as the
    difficulty of the task increases. This clearly indicates the potential benefits
    of using code to elicit reasoning abilities, especially on the multi-hop subsets.
  prefs: []
  type: TYPE_NORMAL
- en: We note that these consistent and substantial gains using code prompts are obtained
    despite the straightforward transformation of the text prompt, which does not
    incorporate new information as shown in [Figure 2](#S1.F2 "In 1 Introduction ‣
    Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs"). This
    suggests that the code format exhibits characteristics that trigger the conditional
    reasoning abilities of text+code LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: In the following experiments, we perform detailed ablation studies confirming
    that the performance improvements indeed originate from the code format and render
    additional insights into the potential reasons behind this phenomenon.
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt | CondQA | CondQA-YN | BGQA-1 | BGQA-2 | BGQA-3 |  |'
  prefs: []
  type: TYPE_TB
- en: '| Code Prompt | $\mathbf{60.04}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| Anonymous code | $58.42$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| Random code | $56.64$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| - Comments | N.A. | $64.92$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Ablation study on the relevance of code semantics within Code Prompts.
    CondQA-YN represents the partition of ConditionalQA with yes-no answers.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Code Format Elicits Reasoning Abilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We are now interested in whether the observed improvements in code prompting
    originate from the input instances being formatted as code or if they are caused
    by the implicit simplification of text that occurs when the instances are translated
    into code. To investigate this, we devise experiments with prompts that represent
    the intermediate states between natural language and code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inspired by Min et al. ([2023](#bib.bib19)), in our first study, we transform
    each NL sentence³³3We only transform the facts in BGQA since transforming the
    rules into atomic statements as well yields worse results. into a sequence of
    atomic statements, which we then append to the original sentence. In this way,
    the atomic statements can be seen as defining variables for each key entity in
    the text. Therefore, this new type of prompt would resemble code but without control
    flow and in natural language form. For instance, for the sentence Applying for
    the legal right to deal with someone’s property, money and possessions (their
    estate) when they die is called applying for probate, some atomic statements would
    be: i) Applying for the legal right is a process and ii) The process is called
    ‘applying for probate’. A complete example is provided in [Appendix D](#A4 "Appendix
    D Atomic Statements ‣ Code Prompting Elicits Conditional Reasoning Abilities in
    Text+Code LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: The prompt retains access to the original instance text (i.e., no loss of information)
    but is also augmented by simplified sentences in the form of atomic statements.
    This setup allows us to investigate whether the simplicity of the input prompt
    is the cause of improved reasoning abilities, regardless of the text and code
    formats.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our second experiment, we investigate whether the semantics of the code
    statements and not the form of the code are the reason behind the performance
    boost. For this purpose, we back-transform the code prompts into NL such that
    the reasoning statements (i.e., the if conditions) are clearly and concisely stated
    in natural language. Specifically, we map every variable into the format Key entity:
    variable without snake case. For instance, the variable husband_pass_away from
    [Figure 2](#S1.F2 "In 1 Introduction ‣ Code Prompting Elicits Conditional Reasoning
    Abilities in Text+Code LLMs") would be back-transformed as Key entity: husband
    pass away. To transform the if statements, we create a translation prompt by providing
    four demonstrations. These demonstrations simply translate the conditional statements
    within the  code-formatted instance back into natural language in the same manner
    as the variables. This makes the back-translated text as close as possible to
    the code text. We provide examples of this in [Table 6](#A6.T6 "In Prompt Probes.
    ‣ Appendix F Variable Tracking Setup ‣ Code Prompting Elicits Conditional Reasoning
    Abilities in Text+Code LLMs") from [Appendix E](#A5 "Appendix E Examples of Code
    Ablations ‣ Code Prompting Elicits Conditional Reasoning Abilities in Text+Code
    LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: From the results in [Table 1](#S4.T1 "In 4.3 Evaluation. ‣ 4 Experimental Setup
    ‣ Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs"),
    we see that (1) prompting with atomic statements does not yield performance improvements
    w.r.t. text prompts, and (2) mapping back from code to NL provokes performance
    drop w.r.t. code prompts, yielding similar results to the original text prompt.
    In the first case, we observe that the performance drop w.r.t. the original text
    prompt can be due to the sampling variance since the results are within the standard
    deviation of text prompts. However, we obtain a more acute drop for BGQA. This
    can be attributed to the format of this dataset. The atomic statements method
    we use (Min et al., [2023](#bib.bib19)) was devised for general texts such as
    Wikipedia pages. However, BGQA is a logic-based dataset, where the input facts
    are statements that are already minimal units of information and do not follow
    the style of general documents. Thus, creating atomic statements from these sentences
    can break the sentence structure needed to track the attributes of the subjects
    and objects of the sentences.
  prefs: []
  type: TYPE_NORMAL
- en: These results indicate that the enhanced complex reasoning abilities do not
    originate from input text simplification nor the conditional statements being
    extracted from the instance sentences, but rather the code format of the input
    instances.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Code Semantic is Important
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Previously, we have shown that the code format is necessary to elicit the reasoning
    abilities of text+code LLMs. Now, we aim to investigate which aspects of code
    are needed for this. In particular, we evaluate the impact of retaining the natural
    language text of the original instance within the code comments and the importance
    of the semantics of the code. To analyze the former, we have removed the code
    comments that include the original natural language text from the input and evaluated
    the performance of the new prompts. To analyze the latter, we perturbed the code
    to anonymize the variables and functions, as well as added random code whose semantics
    are completely irrelevant to the original natural language text. In the latter
    two cases, the code comments remain unmodified (examples illustrating them are
    provided in [Table 7](#A6.T7 "In Prompt Probes. ‣ Appendix F Variable Tracking
    Setup ‣ Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs")
    from [Appendix E](#A5 "Appendix E Examples of Code Ablations ‣ Code Prompting
    Elicits Conditional Reasoning Abilities in Text+Code LLMs")). Since CondQA includes
    span answers and removing the NL text would make it impossible for the model to
    generate the span, we only report performance on the yes-no answers partition
    (i.e., CondQA-YN).
  prefs: []
  type: TYPE_NORMAL
- en: Table [2](#S5.T2 "Table 2 ‣ 5.1 Code Prompting Improves over Text Prompting
    ‣ 5 Experiments ‣ Code Prompting Elicits Conditional Reasoning Abilities in Text+Code
    LLMs") shows that removing the NL text in the code comments yields a performance
    drop of $14.02$ on BGQA. This significant and consistent decrease in all datasets
    confirms that retaining NL text in comments is vital for the LLM to understand
    the input problem.
  prefs: []
  type: TYPE_NORMAL
- en: Code perturbations (anonymous code and random code) also confirm the importance
    of code semantics to elicit reasoning abilities. When we use anonymized code,
    we observe a performance reduction of almost $2$ on BGQA. This more pronounced
    drop is expected since the semantics and logic of the code mismatch the NL text,
    whereas anonymous code maintains the same logic on both NL and code.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, we also observe that the performance of random code prompts is
    similar to that of text prompts (Table [1](#S4.T1 "Table 1 ‣ 4.3 Evaluation. ‣
    4 Experimental Setup ‣ Code Prompting Elicits Conditional Reasoning Abilities
    in Text+Code LLMs")) in all datasets. This can be interpreted as the model being
    able to identify the irrelevance of the code to the text. Hence, the model disregards
    the code to solely focus on the code comments (i.e., the natural language text).
    This could be possible thanks to the provided demonstrations, which show answers
    that only refer to the natural language text.
  prefs: []
  type: TYPE_NORMAL
- en: These results confirm that code alone does not trigger reasoning abilities,
    and instead, the combination of code that represents the original natural language
    instance and the NL text exploits the potential of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Code Prompts are More Sample-Efficient at Eliciting Reasoning Abilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Given our observations that code prompts trigger conditional reasoning abilities
    better than text prompts, it is natural to ask the follow-up question: are code
    prompts also more sample-efficient at triggering conditional reasoning abilities
    when compared to text prompts? To answer this, we evaluate how the overall performance
    of the model changes with respect to the number of demonstrations.'
  prefs: []
  type: TYPE_NORMAL
- en: In the following experiments, we compare the performance gap of the two prompting
    methods under different numbers of demonstrations. [Figure 3](#S5.F3 "In 5.4 Code
    Prompts are More Sample-Efficient at Eliciting Reasoning Abilities ‣ 5 Experiments
    ‣ Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs") shows
    that when we only provide one demonstration per class (i.e., answer type in our
    datasets), the performance gap is the largest across all datasets. As expected,
    this gap decreases when we provide more demonstrations. These results indicate
    that code prompts trigger conditional reasoning more efficiently than text prompts,
    and this is one of the reasons for its superior performance. This phenomenon has
    further applicability in long-document and very low resource settings, where only
    one demonstration is available or fits within the input window, and therefore,
    it would be advisable to convert that demonstration into code.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/446f092ace79a6b0b9dd1da53762025b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Performance comparison between text prompts (blue) and code prompts
    (green) using 1, 2, and 3 demonstrations per class.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Code Prompts Improve Variable Tracking in LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '|  | Correct Instances | Incorrect Instances |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Dataset | Text Prompts | Code Prompts | Text Prompts | Code Prompts |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| CondQA | 71.08 | 4.39 | 60.79 | 11.39 |'
  prefs: []
  type: TYPE_TB
- en: '| BGQA-1 | 39.33 | 8.84 | 51.65 | 22.12 |'
  prefs: []
  type: TYPE_TB
- en: '| BGQA-2 | 44.79 | 15.04 | 52.54 | 24.75 |'
  prefs: []
  type: TYPE_TB
- en: '| BGQA-3 | 54.01 | 14.21 | 52.13 | 16.98 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Comparison of the percentage of memory errors made by GPT 3.5\. For
    each dataset, we separately compute memory errors for the instances where the
    model gives the correct and incorrect answers. Lower is better.'
  prefs: []
  type: TYPE_NORMAL
- en: We hypothesize that one of the reasons for the superior performance of code
    prompting is an improved ability to track the states of variables or key concepts.
    This hypothesis is based on the intuition that, for natural language in general,
    local context is the most important part of generating the next token Khandelwal
    et al. ([2018](#bib.bib9)); Sun et al. ([2021](#bib.bib26)). However, generating
    code is often more challenging because code frequently refers to previously defined
    functions, which can be dozens or even hundreds of lines apart. This resembles
    multi-hop reasoning, where the model may need to reference a key entity dozens
    of lines before. Therefore, an improved ability to look for distant co-references
    can be beneficial to multi-hop reasoning, which is also needed to solve our datasets.
  prefs: []
  type: TYPE_NORMAL
- en: To test our hypothesis, we devise the following experiment. Firstly, we define
    reasoning step as each output sentence split by “\n.” After generating each reasoning
    step, we stop the model generation and query about all key entities defined in
    the input prompt. In the case of text prompts, we query the model whether the
    given facts are true or not, and for code prompts, we query for the value of the
    (boolean) variables. In all cases, the model only has to generate True, False,
    a string, or unknown. Then, we compare the percentage of errors in text and code
    prompts. This number represents the memory errors committed by the model. The
    more memory errors, the more difficult is for the model to track and remember
    entities/variables. We provide further details on how we extracted the key entities
    to ask for, how we identified the reasoning steps in the chain of thought used
    to stop the model from conducting the probes, and examples of the prompt probes
    in [Appendix F](#A6 "Appendix F Variable Tracking Setup ‣ Code Prompting Elicits
    Conditional Reasoning Abilities in Text+Code LLMs") and its [Table 8](#A6.T8 "In
    Prompt Probes. ‣ Appendix F Variable Tracking Setup ‣ Code Prompting Elicits Conditional
    Reasoning Abilities in Text+Code LLMs").
  prefs: []
  type: TYPE_NORMAL
- en: We acknowledge that the output text may not be completely faithful to the internal
    beliefs of the model (Lyu et al., [2023](#bib.bib17)). Therefore, we first test
    whether this experiment can be a proxy metric of the internal belief of the model.
    To do this, we compare the memory error percentage of the prompting methods in
    instances where the model solves (i.e., correct instances) and does not solve
    (i.e., incorrect instances) the question. If incorrect instances yield a higher
    memory error, this would indicate that the model struggles more to remember the
    state of those variables, and this would make it fail to conduct the reasoning
    process. Therefore, our probes would be a proxy metric of the internal belief
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 3](#S5.T3 "In 5.5 Code Prompts Improve Variable Tracking in LLMs ‣ 5
    Experiments ‣ Code Prompting Elicits Conditional Reasoning Abilities in Text+Code
    LLMs") shows the results of this comparison. We observe that all prompting methods
    in all datasets consistently make more memory mistakes on incorrect instances
    than on correct instances, with the exception of text prompts on CondQA. However,
    the memory error in this case is significantly high, which may suggest that the
    model is not able to track entities correctly in any case. Therefore, we can use
    this experiment as a proxy measure of the memory of the model.'
  prefs: []
  type: TYPE_NORMAL
- en: From the same table, we can observe that Text Prompts make significantly more
    memory errors than code prompts on all datasets. Specifically, the gap is consistently
    more than 30% with peaks on CondQA (66.69%) and BGQA-3 (39.8%). Therefore, this
    experiment empirically confirms our hypothesis that code prompts can track the
    state of the variables/key entities better than text prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 5.6 Qualitative Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'All our previous experiments have focused on the accuracy of the answers. Obtaining
    a correct answer is correlated to a correct reasoning process. However, it is
    possible to obtain a correct answer despite an incorrect reasoning chain. Due
    to the difficulty of automatically evaluating the chain of thoughts, we perform
    a limited human evaluation. We sample ten instances from BGQA-3 (i.e., the dataset
    with the most complex reasoning chains) where both text and code prompts return
    the correct answer and manually inspect the quality of the reasoning chains. Based
    on this limited sample, we observe that text prompts conduct a 100% correct reasoning
    chain in only two cases, while code prompts do so in three cases. We identify
    two main types of errors: (1) wrong conditional reasoning and (2) commonsense
    errors. We provide examples of those in [Table 4](#S5.T4 "In 5.6 Qualitative Analysis
    ‣ 5 Experiments ‣ Code Prompting Elicits Conditional Reasoning Abilities in Text+Code
    LLMs"). This observation raises the question of whether the generated chain of
    thought is not faithful to the internal reasoning of the model, as suggested by
    (Lyu et al., [2023](#bib.bib17)) or whether the model generated the right answer
    from a greedy attempt to reach the closest plausible conclusion (code prompts
    shows a reasoning error in the last step of the CoT in three out of seven cases).
    We leave the analysis of faithfulness of chains of thoughts as future work.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Error Type | Example | Explanation |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Commonsense | We know the catfish has a harmonica, and according to Rule3
    "if the catfish has something to sit on, … | The model believes a harmonica is
    something to sit on. |'
  prefs: []
  type: TYPE_TB
- en: '| Conditional Reasoning | # We know the lion does not remove from the board
    one of the pieces of the dog, and according to Rule5 "if something becomes an
    enemy of the squid but does not remove from the board one of the pieces of the
    dog, then it steals five points from the mosquito." become_enemy(lion, squid)==True
    not remove_piece(lion, dog)==True steal_points(lion,mosquito,5)=rule5(lion) steal_points(lion,
    mosquito, 5)==True | We do not know  become_enemy(lion, squid) == True, but if
    we assume this, we reach the question variable, so we get an answer to the question.
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Examples of reasoning errors on correct instances by text and code
    prompts. Underline text is the cause of the error.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusions and Future Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work demonstrates that conditional reasoning abilities can be triggered
    by using code prompts in large language models (LLMs) of text and code. These
    code prompts contain the original natural language (NL) formulation as a code
    comment and code that formulates the logic of the text (e.g., documents and questions).
    To create these code prompts, we use in-context learning to teach an LLM how to
    automatically conduct such a transformation. Through multiple experiments, we
    show that code prompts trigger conditional reasoning abilities. They consistently
    and substantially outperform text prompts across two conditional reasoning datasets
    by between $2.6$ points. Our experiments show that even simple code can be beneficial
    as long as it closely follows the semantics of the NL text and is accompanied
    by the original NL text. In addition, code prompts require fewer demonstrations,
    making them more efficient than vanilla text prompts. Our experiments also suggest
    that code prompts achieve the performance boost due to their superior ability
    to track the state of variables or key entities.
  prefs: []
  type: TYPE_NORMAL
- en: In our future work, we plan to extend our experiments to other text+code LLMs.
    We also plan to conduct experiments investigating how pretraining on text, code,
    and text+code affects the triggering of reasoning abilities. Lastly, we would
    like to extend our experiments to a wider range of reasoning abilities to verify
    the generalization of our findings.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Transforming a natural language problem into code requires an intermediate step
    that raises the cost of the whole pipeline. However, this mapping is not a complicated
    task. Therefore, we believe it would be possible to train a small generative model
    to do it instead of using a large language model. In this way, we could minimize
    the cost of using code prompts without affecting its performance.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the costs of very large language models and the large size of BoardgameQA,
    we only ran the experiments two times with different random seeds. However, the
    variance is small enough to ensure the representativeness of the results.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work has been funded by the German Research Foundation (DFG) as part of
    the UKP-SQuARE project (grant GU 798/29-1), by the German Federal Ministry of
    Education and Research and the Hessian Ministry of Higher Education, Research,
    Science and the Arts within their joint support of the National Research Center
    for Applied Cybersecurity ATHENE, and by the European Union (ERC, InterText, 101054961).
    Views and opinions expressed are, however, those of the author(s) only and do
    not necessarily reflect those of the European Union or the European Research Council.
    Neither the European Union nor the granting authority can be held responsible
    for them.
  prefs: []
  type: TYPE_NORMAL
- en: We gratefully acknowledge the support of Microsoft with a grant for access to
    OpenAI GPT models via the Azure cloud (Accelerate Foundation Model Academic Research).
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we thank Max Glockner, Jonathan Tonglet, and Sheng Lu for their insightful
    comments and suggestions on a draft of this paper.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Chen et al. (2023) Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen.
    2023. [Program of thoughts prompting: Disentangling computation from reasoning
    for numerical reasoning tasks](https://openreview.net/forum?id=YfZ4ZPt8zd). *Transactions
    on Machine Learning Research*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2021) Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana
    Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan Routledge,
    and William Yang Wang. 2021. [FinQA: A dataset of numerical reasoning over financial
    data](https://doi.org/10.18653/v1/2021.emnlp-main.300). In *Proceedings of the
    2021 Conference on Empirical Methods in Natural Language Processing*, pages 3697–3711,
    Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chowdhery et al. (2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
    Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily
    Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael
    Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
    Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam
    Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander
    Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M.
    Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
    Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,
    Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern,
    Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2023. [Palm: Scaling language
    modeling with pathways](http://jmlr.org/papers/v24/22-1144.html). *Journal of
    Machine Learning Research*, 24(240):1–113.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, et al. 2021. [Training verifiers to solve math word problems](https://arxiv.org/abs/2110.14168).
    *arXiv preprint arXiv:2110.14168*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2023) Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu,
    Yiming Yang, Jamie Callan, and Graham Neubig. 2023. [Pal: program-aided language
    models](https://dl.acm.org/doi/10.5555/3618408.3618843). In *Proceedings of the
    40th International Conference on Machine Learning*, ICML’23\. JMLR.org.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hussain et al. (2023) Syed-Amad Hussain, Parag Pravin Dakle, SaiKrishna Rallabandi,
    and Preethi Raghavan. 2023. [Towards leveraging llms for conditional qa](https://arxiv.org/abs/2312.01143).
    *arXiv preprint arXiv:2312.01143*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,
    Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and
    William El Sayed. 2023. [Mistral 7b](https://arxiv.org/abs/2310.06825). *arXiv
    preprint arXiv:2310.06825*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kazemi et al. (2023) Mehran Kazemi, Quan Yuan, Deepti Bhatia, Najoung Kim,
    Xin Xu, Vaiva Imbrasaite, and Deepak Ramachandran. 2023. [BoardgameQA: A dataset
    for natural language reasoning with contradictory information](https://openreview.net/forum?id=BR1m3JIoKm).
    In *Thirty-seventh Conference on Neural Information Processing Systems Datasets
    and Benchmarks Track*, pages 1–23.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Khandelwal et al. (2018) Urvashi Khandelwal, He He, Peng Qi, and Dan Jurafsky.
    2018. [Sharp nearby, fuzzy far away: How neural language models use context](https://doi.org/10.18653/v1/P18-1027).
    In *Proceedings of the 56th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 284–294, Melbourne, Australia. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kojima et al. (2022) Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. 2022. [Large language models are zero-shot reasoners](https://proceedings.neurips.cc/paper_files/paper/2022/file/8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf).
    In *Advances in Neural Information Processing Systems*, volume 35, pages 22199–22213\.
    Curran Associates, Inc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lanham et al. (2023) Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner,
    Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson
    Kernion, et al. 2023. [Measuring faithfulness in chain-of-thought reasoning](https://arxiv.org/abs/2307.13702).
    *arXiv preprint arXiv:2307.13702*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2023a) Hanmeng Liu, Jian Liu, Leyang Cui, Zhiyang Teng, Nan Duan,
    Ming Zhou, and Yue Zhang. 2023a. [Logiqa 2.0—an improved dataset for logical reasoning
    in natural language understanding](https://doi.org/10.1109/TASLP.2023.3293046).
    *IEEE/ACM Transactions on Audio, Speech, and Language Processing*, 31:2947–2962.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2022a) Jiacheng Liu, Skyler Hallinan, Ximing Lu, Pengfei He, Sean
    Welleck, Hannaneh Hajishirzi, and Yejin Choi. 2022a. [Rainier: Reinforced knowledge
    introspector for commonsense question answering](https://doi.org/10.18653/v1/2022.emnlp-main.611).
    In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language
    Processing*, pages 8938–8958, Abu Dhabi, United Arab Emirates. Association for
    Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2022b) Jiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter
    West, Ronan Le Bras, Yejin Choi, and Hannaneh Hajishirzi. 2022b. [Generated knowledge
    prompting for commonsense reasoning](https://doi.org/10.18653/v1/2022.acl-long.225).
    In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 3154–3169, Dublin, Ireland. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2020) Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang,
    and Yue Zhang. 2020. [Logiqa: A challenge dataset for machine reading comprehension
    with logical reasoning](https://doi.org/10.24963/ijcai.2020/501). In *Proceedings
    of the Twenty-Ninth International Joint Conference on Artificial Intelligence,
    IJCAI-20*, pages 3622–3628\. International Joint Conferences on Artificial Intelligence
    Organization. Main track.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023b) Xiao Liu, Da Yin, Chen Zhang, Yansong Feng, and Dongyan
    Zhao. 2023b. [The magic of IF: Investigating causal reasoning abilities in large
    language models of code](https://doi.org/10.18653/v1/2023.findings-acl.574). In
    *Findings of the Association for Computational Linguistics: ACL 2023*, pages 9009–9022,
    Toronto, Canada. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lyu et al. (2023) Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao,
    Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. 2023. [Faithful chain-of-thought
    reasoning](https://arxiv.org/abs/2301.13379). *arXiv preprint arXiv:2301.13379*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Madaan et al. (2022) Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham
    Neubig. 2022. [Language models of code are few-shot commonsense learners](https://doi.org/10.18653/v1/2022.emnlp-main.90).
    In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language
    Processing*, pages 1384–1403, Abu Dhabi, United Arab Emirates. Association for
    Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Min et al. (2023) Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau
    Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. [FActScore:
    Fine-grained atomic evaluation of factual precision in long form text generation](https://doi.org/10.18653/v1/2023.emnlp-main.741).
    In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language
    Processing*, pages 12076–12100, Singapore. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. 2023. [Gpt-4 technical report](https://arxiv.org/pdf/2303.08774.pdf).
    *arXiv preprint arXiv:2303.08774*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pan et al. (2023) Liangming Pan, Xiaobao Wu, Xinyuan Lu, Anh Tuan Luu, William Yang
    Wang, Min-Yen Kan, and Preslav Nakov. 2023. [Fact-checking complex claims with
    program-guided reasoning](https://doi.org/10.18653/v1/2023.acl-long.386). In *Proceedings
    of the 61st Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*, pages 6981–7004, Toronto, Canada. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Patel et al. (2021) Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021.
    [Are NLP models really able to solve simple math word problems?](https://doi.org/10.18653/v1/2021.naacl-main.168)
    In *Proceedings of the 2021 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 2080–2094,
    Online. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saeidi et al. (2018) Marzieh Saeidi, Max Bartolo, Patrick Lewis, Sameer Singh,
    Tim Rocktäschel, Mike Sheldon, Guillaume Bouchard, and Sebastian Riedel. 2018.
    [Interpretation of natural language rules in conversational machine reading](https://doi.org/10.18653/v1/D18-1233).
    In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing*, pages 2087–2097, Brussels, Belgium. Association for Computational
    Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sinha et al. (2019) Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau,
    and William L. Hamilton. 2019. [CLUTRR: A diagnostic benchmark for inductive reasoning
    from text](https://doi.org/10.18653/v1/D19-1458). In *Proceedings of the 2019
    Conference on Empirical Methods in Natural Language Processing and the 9th International
    Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*, pages 4506–4515,
    Hong Kong, China. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2022) Haitian Sun, William Cohen, and Ruslan Salakhutdinov. 2022.
    [ConditionalQA: A complex reading comprehension dataset with conditional answers](https://doi.org/10.18653/v1/2022.acl-long.253).
    In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 3627–3637, Dublin, Ireland. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2021) Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and
    Mohit Iyyer. 2021. [Do long-range language models actually use long-range context?](https://doi.org/10.18653/v1/2021.emnlp-main.62)
    In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing*, pages 807–822, Online and Punta Cana, Dominican Republic. Association
    for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023) Wenya Wang, Vivek Srikumar, Hannaneh Hajishirzi, and Noah A.
    Smith. 2023. [Elaboration-generating commonsense question answering at scale](https://doi.org/10.18653/v1/2023.acl-long.90).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 1619–1635, Toronto, Canada. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian
    ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. 2022. [Chain-of-thought prompting
    elicits reasoning in large language models](https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf).
    In *Advances in Neural Information Processing Systems*, volume 35, pages 24824–24837\.
    Curran Associates, Inc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ye et al. (2023) Xi Ye, Qiaochu Chen, Isil Dillig, and Greg Durrett. 2023.
    [Satlm: Satisfiability-aided language models using declarative prompting](https://openreview.net/pdf?id=TqW5PL1Poi).
    In *Proceedings of NeurIPS*, pages 1–33.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In ConditionalQA, we use an oracle retriever to retrieve the relevant passages
    to the question. This retriever is based on the sentences annotated as evidence
    for the answer (i.e., rationales). We concatenate all sections that include one
    rationale and use the resulting passage as input document.
  prefs: []
  type: TYPE_NORMAL
- en: The sizes of all the datasets used in this work are provided in [Table 5](#A1.T5
    "In Appendix A Datasets ‣ Code Prompting Elicits Conditional Reasoning Abilities
    in Text+Code LLMs").
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Training Size | Dev Size |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| CondQA | 2338 | 285 |'
  prefs: []
  type: TYPE_TB
- en: '| BGQA-1 | 1000 | 500 |'
  prefs: []
  type: TYPE_TB
- en: '| BGQA-2 | 1000 | 500 |'
  prefs: []
  type: TYPE_TB
- en: '| BGQA-3 | 1000 | 500 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Sizes of the datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Prompt Formulation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CONDQA.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Firstly, we define the different components of a data point: scenario ($S$
    is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}tp=\text{&quot;Question:&quot;}+S+Q+\text{&quot;Document:&quot;}+D\\
    +\text{&quot;Let''s think step by step&quot;}\end{split}$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'where $+$ is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $to=R+\text{&quot;Answer:&quot;}+A$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'For code prompts, we first define a function ${C:\mathbb{NL}\rightarrow\mathbb{C}}$
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\begin{split}cp=\text{&quot;\#Question:&quot;}+C(S)+C(Q)+\\ \text{&quot;\#Document:&quot;}+C(D)\\'
  prefs: []
  type: TYPE_NORMAL
- en: +\text{&quot;\#Let's think step by step&quot;}\end{split}$$ |  | (3) |
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, we define the output format, $co$, as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $co=R+\text{&quot;\#Answer:&quot;}+A$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: BGQA.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Firstly, we define the components of a data point in this dataset: facts ($F$).
    Therefore, our text prompt is defined as follows⁴⁴4BGQA provides a field example
    with all the variables of the dataset concatenated with descriptions. We use this
    field as text prompt.:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}tp=F+R+Q\end{split}$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: This dataset also provides the CoT that leads to the answer. Therefore, we use
    that CoT as the expected output.
  prefs: []
  type: TYPE_NORMAL
- en: 'For code prompts, we follow the same approach as with the previous dataset.
    We define code prompts, $cp$, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $tp=C(F)+C(R)+C(Q)$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: 'with the output format ($co$) being:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $co=C(cot)$ |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: Appendix C Costs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Running a data instance from ConditionalQA with code prompts costs $0.04 while
    with text prompts $0.01\. On BoardgameQA-depth 3 (i.e., the partition with the
    most expensive prompts), the costs per question are $0.02 and $0.03 for text and
    code prompts, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Atomic Statements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Original sentence: <p>Applying for the legal right to deal with someone’s property,
    money and possessions (their estate) when they die is called applying for probate.</p>
    Atomic statements: Applying for the legal right is a process. The process is called
    ’applying for probate’. The legal right is to deal with someone’s property, money,
    and possessions. The someone is a person who has died. The property, money, and
    possessions are collectively called the ’estate’.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Examples of Code Ablations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An example of a back-translated code into natural language is provided in [Table 6](#A6.T6
    "In Prompt Probes. ‣ Appendix F Variable Tracking Setup ‣ Code Prompting Elicits
    Conditional Reasoning Abilities in Text+Code LLMs"). We can observe in both examples
    that the resulting natural language (NL) text is extremely similar to the original
    code. In addition, in the second example (BGQA), Rule2 is much simpler after the
    back-translation than its original description in NL.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 7](#A6.T7 "In Prompt Probes. ‣ Appendix F Variable Tracking Setup ‣
    Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs") shows
    examples of the multiple code ablations we conducted in [Section 5.3](#S5.SS3
    "5.3 Code Semantic is Important ‣ 5 Experiments ‣ Code Prompting Elicits Conditional
    Reasoning Abilities in Text+Code LLMs"). Random code replaces the code with a
    piece of code from another data point. In this way, the semantics of the text
    and code mismatch while we keep the code syntactically correct.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix F Variable Tracking Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Extracting key entities in BoardgameQA.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This dataset provides a list of “facts,” which are short and concise sentences
    describing the state of a key entity. Therefore, we use them without alterations
    as the key entities to ask for.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting key entities in ConditionalQA.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This dataset provides a scenario describing the background information of the
    person posing the answer. Since this scenario is a free-form text, we follow (Min
    et al., [2023](#bib.bib19)) to extract atomic statements and use them as the key
    entities to ask for.
  prefs: []
  type: TYPE_NORMAL
- en: Code Prompting variables
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: . To probe the variable tracking abilities of code prompts, we use the variables
    defined in the “facts” and “scenario” of BoardgameQA and ConditionalQA, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Probing memory at different steps in the Chain-of-Thought.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Inspired by Lanham et al. ([2023](#bib.bib11)), we truncate the Chain-of-Thought
    (CoT) at different completion states and probe the memory of the model. To break
    down the CoT, we split it by the character “\n”, which usually represents the
    end of a reasoning step. This is possible because our in-context learning demonstrations
    follow this format.
  prefs: []
  type: TYPE_NORMAL
- en: Number of probes.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For each dataset instance, we run $num\_facts\times num_{s}teps_{c}ot$ probes)
    because of the cost of the experiment. Due to the length of the demonstrations
    of ConditionalQA and its impact on the costs, we sample five facts and three partial
    CoTs for each instance, yielding an upper-bound of 15 probes per instance, and
    run the probes for 30 instances for each dataset partition (i.e., correct and
    incorrect instances).
  prefs: []
  type: TYPE_NORMAL
- en: Prompt Probes.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In all cases, we follow the following format: Sys. Prompt; ICL Demonstrations;
    Input Instance; Partial CoT; Probe.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The probe for text and code prompts in BoardgameQA is: “Now, I want to ask
    you about the value of some key entities you used. Your answers must be ‘yes‘,
    ‘no‘, or ‘unknown‘. It is very important that you only write one word. Is it true
    that {fact}?”'
  prefs: []
  type: TYPE_NORMAL
- en: 'The probe for text prompts in ConditionalQA is: “Now, I want to ask you about
    the value of some key entities you used. Your answers must be “True”, “False”,
    “unknown”, or a string. It is very important that you only write the exact value.
    From the speaker perspective, is it true that {fact}?”'
  prefs: []
  type: TYPE_NORMAL
- en: 'The probe for code prompts in ConditionalQA is: “Now, I want to ask you about
    the value of some key entities you used. Your answers must be “True”, “False”,
    “unknown”, or a string. It is very important that you only write the exact value.
    What is the value of the variable {var}?” A real example is provided in [Table 8](#A6.T8
    "In Prompt Probes. ‣ Appendix F Variable Tracking Setup ‣ Code Prompting Elicits
    Conditional Reasoning Abilities in Text+Code LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Text |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Code | # <p>You can apply to become the estate’s administrator if you are
    18 or over and you are the most ‘entitled’ inheritor of the deceased’s estate.
    This is usually the deceased’s closest living relative.</p> if applicant_age >=
    18 and entitled_inheritor and closest_relative: can_apply_estate_administrator
    = True |'
  prefs: []
  type: TYPE_TB
- en: '| Code $\rightarrow$ NL | <p>You can apply to become the estate’s administrator
    if you are 18 or over and you are the most ‘entitled’ inheritor of the deceased’s
    estate. This is usually the deceased’s closest living relative.</p> if you are
    18 or over and you are the most entitled inheritor of the deceased’s estate and
    you are the closest living relative, you can apply to become the estate’s administrator
    |'
  prefs: []
  type: TYPE_TB
- en: '| Code | # Rule2: Be careful when something removes from the board one of the
    pieces of the dog and also becomes an enemy of the catfish because in this case
    it will surely not burn the warehouse of the mosquito (this may or may not be
    problematic) rule2(something) = remove(something, piece_of(dog)) & enemy(something,
    catfish) => not burn(something, warehouse_of(mosquito)) |'
  prefs: []
  type: TYPE_TB
- en: '| Code $\rightarrow$ NL | Rule2: If something removes from the board one of
    the pieces of the dog and also becomes an enemy of the catfish, then it does not
    burn the warehouse of the mosquito |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Example of a back-translation ${\mathbb{NL}\rightarrow\mathbb{C}}$
    in ConditionalQA and BGQA-3\. Text in bold represents the main modification.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Text |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Original Code | # <p>To be eligible you must have left your country and be
    unable to go back because you fear persecution.</p> if left_country_and_fear_persecution:
    eligible_for_asylum = True |'
  prefs: []
  type: TYPE_TB
- en: '| Anonymous Code | # <p>To be eligible you must have left your country and
    be unable to go back because you fear persecution.</p> if var_1 var_2 = True |'
  prefs: []
  type: TYPE_TB
- en: '| Random Code | # <p>To be eligible you must have left your country and be
    unable to go back because you fear persecution.</p> if value_of_property_gone_down_by_more_than_50:
    eligible_to_claim = True getting_housing_benefit = True |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Examples code ablations.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Section | Role | Message |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Problem instance | Human | Question: My brother and his wife are in prison
    for carrying out a large fraud scheme. Their 7 and 8 year old children have been
    living with me for the last 4 years. I want to become their Special Guardian to
    look after them permanently. How long will it be before I hear back from the court?
    Document: <h1>What is a special guardian</h1> <p>You can apply to be a child’s
    special guardian when they cannot live with their birth parents and adoption is
    not right for them.</p> … Answers can be "yes" or "no". Let’s think step by step:
    |'
  prefs: []
  type: TYPE_TB
- en: '| Partial CoT | AI | <p>Within 10 days of receiving your application the court
    will send you a case number and a date for a meeting to set out:</p>\n |'
  prefs: []
  type: TYPE_TB
- en: '| Probe | Human | Now, I want to ask you about the value of some key entities
    you used. Your answers must be ‘True‘, ‘False‘, ‘unknown‘, or a string. It is
    very important that you only write the exact value. From the speaker perspective,
    is it true that the children have been living with me for the last 4 years? |'
  prefs: []
  type: TYPE_TB
- en: '| Probe | AI | True |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: Variable Tracking Example. Underlined text represents the variable
    to probe. Partial CoT is not the complete answer. The generation was stopped,
    and only the first step was used in this probe.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix G Prompt Examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| System: You are a helpful assistant that answers questions given a document.
    Answers must be a short span of the document. You have to extract the span from
    the document. Do not write anything else. I will give you some examples first.
    ICL Demonstrations… Human: Question: My brother and his wife are in prison for
    carrying out a large fraud scheme. Their 7 and 8 year old children have been living
    with me for the last 4 years. I want to become their Special Guardian to look
    after them permanently. How long will it be before I hear back from the court?
    Document: <h1>What is a special guardian</h1> <p>You can apply to be a child’s
    special guardian when they cannot live with their birth parents and adoption is
    not right for them.</p> <p>You’ll be responsible for looking after the child until
    they’re 18 (unless the court takes your responsibility away earlier).</p> <p>You’ll
    make all day to day decisions about the child, for example schooling and medical
    treatment. You do not have to discuss these decisions with the birth parents.</p>
    <p>You’ll need to get the consent of everyone who has parental responsibility
    for the child before you make some important decisions, for example:</p> <li>changing
    the child’s surname</li> <li>putting the child up for adoption</li> <li>taking
    the child abroad for more than 3 months</li> <li>the child having surgery for
    reasons other than improving health, such as circumcision, sterilisation or cosmetic
    surgery</li> <p>If you cannot get consent, you can ask the court to decide. Use
    the form ‘Make an application in existing court proceedings related to children’
    (form C2).</p> <h1>After you apply</h1> <p>Within 10 days of receiving your application
    the court will send you a case number and a date for a meeting to set out:</p>
    <li>a timetable for your case</li> <li>how it will be dealt with</li> <p>This
    meeting is called a ‘first directions hearing’.</p> <p>You must go to all hearings
    you’re told to unless the court excuses you. If you’re not able to go, contact
    the court office.</p> Answers must be a short span of the document. You have to
    extract the span from the document. Do not write anything else. Let’s think step
    by step: |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: Text prompt Example for ConditionalQA'
  prefs: []
  type: TYPE_NORMAL
- en: '| System: You are a helpful assistant. Your task is to process a pseudo-code
    that describes a question and a document. You need to reason using that document
    and the comments to return the answers. Answers must be a short span of the document.
    You have to extract the span from the code comments. Do not write anything else.
    I will give you some examples first. ICL Demonstrations… Human: # Question: My
    brother and his wife are in prison for carrying out a large fraud scheme. Their
    7 and 8 year old children have been living with me for the last 4 years. I want
    to become their Special Guardian to look after them permanently. How long will
    it be before I hear back from the court? maximum_redundancy_pay = 16320 housing_standards_and_procedures_in_Northern_Ireland
    = True ensure_vehicle_taxed_in_UK = True immigration_advisers_can_help_with_representation_at_tribunal
    = True supply_protective_clothing_and_equipment = True CBT_required_for_moped_and_motorcycle
    = True court_response_time = None # This is the variable that answers the question
    # <h1>What is a special guardian</h1> # <p>You can apply to be a child’s special
    guardian when they cannot live with their birth parents and adoption is not right
    for them.</p> if attorneys_appointed_jointly: all_attorneys_must_agree_to_make_decision
    = True disability_or_severe_disability_element_of_working_tax_credit = True mugging_without_physical_harm_emergency
    = True # <p>You’ll be responsible for looking after the child until they’re 18
    (unless the court takes your responsibility away earlier).</p> work_temporarily_for_hirer
    = True # <p>You’ll make all day to day decisions about the child, for example
    schooling and medical treatment. You do not have to discuss these decisions with
    the birth parents.</p> accounts_and_tax_returns_cover_financial_year = "1 June
    to 31 May" employer_operating_PAYE = True # <p>You’ll need to get the consent
    of everyone who has parental responsibility for the child before you make some
    important decisions, for example:</p> # <li>changing the child’s surname</li>
    # <li>putting the child up for adoption</li> # <li>taking the child abroad for
    more than 3 months</li> # <li>the child having surgery for reasons other than
    improving health, such as circumcision, sterilisation or cosmetic surgery</li>
    managed_by_fit_and_proper_persons = True check_court_order_for_authorization =
    True considering_fostering = True if not_connected_to_mains_sewer: septic_tank_used
    = True can_claim_tax_relief_if_taxed_twice = True extra_support_for_disability
    = True if operator_of_septic_tank_or_treatment_plant: follow_general_binding_rules
    = True # <p>If you cannot get consent, you can ask the court to decide. Use the
    form ‘Make an application in existing court proceedings related to children’ (form
    C2).</p> appeals_decision_time = "several months" if worker and informal_resolution_not_satisfactory:
    formal_grievance_complaint_possible = True time_limit_for_backdating_claims_services
    = 6 # <h1>After you apply</h1> # <p>Within 10 days of receiving your application
    the court will send you a case number and a date for a meeting to set out:</p>
    # <li>a timetable for your case</li> # <li>how it will be dealt with</li> # <p>This
    meeting is called a ‘first directions hearing’.</p> committee_recommendations_go_to_Prime_Minister
    = True check_adviser_registration = True meet_manning_levels = True recognised_as_charity_or_CASC
    = True apply_for_visa_for_other_reasons = True debt_paid_off = True if special_educational_needs_and_disabilities:
    affects_behaviour_or_socialisation = True # <p>You must go to all hearings you’re
    told to unless the court excuses you. If you’re not able to go, contact the court
    office.</p> payslip_can_include_tax_code = True VAT_zero_rate = 0 gas_equipment_installed_and_maintained_by_Gas_Safe_registered_engineer
    = True # Question: My brother and his wife are in prison for carrying out a large
    fraud scheme. Their 7 and 8 year old children have been living with me for the
    last 4 years. I want to become their Special Guardian to look after them permanently.
    How long will it be before I hear back from the court? # Answers must be a short
    span of the document. You have to extract the span from the code comments. Do
    not write anything else. # Let’s think step by step: |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10: Code Prompt Example for ConditionalQA'
  prefs: []
  type: TYPE_NORMAL
- en: '| System: You are a question-answering system that solves the problem of reasoning
    with contradictory information guided by preferences over sources of information.
    You must explain your answers step by step. ICL Demonstrations … Human: A few
    players are playing a boardgame The current state of the game is as follows The
    amberjack struggles to find food And the rules of the game are as follows Rule1:
    If the amberjack has difficulty to find food, then the amberjack removes from
    the board one of the pieces of the carp Based on the game state and the rules
    and preferences, does the amberjack remove from the board one of the pieces of
    the carp? AI: |'
  prefs: []
  type: TYPE_TB
- en: 'Table 11: Text prompt Example for BGQA-1'
  prefs: []
  type: TYPE_NORMAL
- en: '| System: You are a large language model of code that can interpret code. You
    are given a pseudo-code that resembles to first-order logic that models some scenario.
    You will be given a question and you have to answer it step by step. You can use
    a rule if and only if you know the antecedent of the rule. ICL Demonstrations
    Human: # A few players are playing a boardgame # The rules of the game are as
    follows # Rule1: If the amberjack has difficulty to find food, then the amberjack
    removes from the board one of the pieces of the carp. rule1() = difficulty_finding_food(amberjack)
    => remove_piece(amberjack, carp) # The current state of the game is as follows
    # The amberjack struggles to find food. difficulty_finding_food(amberjack) = True
    # Based on the game state and the rules and preferences, does the amberjack remove
    from the board one of the pieces of the carp? question = remove_piece(amberjack,
    carp) AI: |'
  prefs: []
  type: TYPE_TB
- en: 'Table 12: Code prompt Example for BGQA-1'
  prefs: []
  type: TYPE_NORMAL
