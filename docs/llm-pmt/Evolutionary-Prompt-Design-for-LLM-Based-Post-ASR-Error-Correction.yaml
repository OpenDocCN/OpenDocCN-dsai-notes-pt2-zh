- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:41:30'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Evolutionary Prompt Design for LLM-Based Post-ASR Error Correction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.16370](https://ar5iv.labs.arxiv.org/html/2407.16370)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Building upon the strength of modern large language models (LLMs), generative
    error correction (GEC) has emerged as a promising paradigm that can elevate the
    performance of modern automatic speech recognition (ASR) systems. One representative
    approach is to leverage in-context learning to prompt LLMs so that a better hypothesis
    can be generated by the LLMs based on a carefully-designed prompt and an $N$ GenSEC
    challenge show the effectiveness and potential of the proposed algorithms.¹¹1Our
    code is open sourced at [https://github.com/rithiksachdev/PostASR-Correction-SLT2024](https://github.com/rithiksachdev/PostASR-Correction-SLT2024).
  prefs: []
  type: TYPE_NORMAL
- en: Index Terms—  Evolutionary prompt optimization, post-ASR error correction, large
    language models, automatic speech recognition.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large language models (LLMs) [[1](#bib.bib1)], trained on massive textual data
    using neural network-based transformer architectures, have revolutionized many
    tasks in natural language processing. The auto-regressive learning nature of LLMs
    introduces and empowers a new “prompting” mechanism based on input instructions,
    where users can provide text prompts to guide LLMs to complete particular tasks
    [[2](#bib.bib2)] as a form of next token prediction.
  prefs: []
  type: TYPE_NORMAL
- en: One of these popular prompt-activated tasks is post-automatic speech recognition
    (ASR) text correction, where based on an $N$-hypotheses list.
  prefs: []
  type: TYPE_NORMAL
- en: When input prompts are critical for instructing LLMs on unseen ASR tasks, these
    task-activating prompts [[10](#bib.bib10)] are often empirically-designed and
    under-explored in the research community. For example, early works [[11](#bib.bib11),
    [12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14)] focus on iterative optimization
    at the waveform level to instruct acoustic models for new tasks, but there are
    fewer studies on optimizing LLM-prompts for ASR tasks.
  prefs: []
  type: TYPE_NORMAL
- en: To achieve better post-ASR error correction, this paper explores alternative
    prompts for this task, and investigates a conditional evolutionary strategies
    based prompt optimization algorithm, named EvoPrompt [[15](#bib.bib15)], to refine
    the alternative prompts. Evaluation results on the CHiME-4 subset of the HyPoradise
    dataset [[3](#bib.bib3)] show the effectiveness of the proposed algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: In the rest of this paper, we describe the proposed algorithm in Section [2](#S2
    "2 Proposed Algorithms ‣ Evolutionary Prompt Design for LLM-Based Post-ASR Error
    Correction"), experimental setup in Section [3](#S3 "3 Experimental Setup ‣ Evolutionary
    Prompt Design for LLM-Based Post-ASR Error Correction"), evaluation results in
    Section [4](#S4 "4 Evaluation Results ‣ Evolutionary Prompt Design for LLM-Based
    Post-ASR Error Correction"), and conclusions in Section [5](#S5 "5 Conclusions
    ‣ Evolutionary Prompt Design for LLM-Based Post-ASR Error Correction").
  prefs: []
  type: TYPE_NORMAL
- en: 2 Proposed Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fig. [1](#S2.F1 "Figure 1 ‣ 2 Proposed Algorithms ‣ Evolutionary Prompt Design
    for LLM-Based Post-ASR Error Correction") illustrates the proposed system, where
    an $N$-best list of hypotheses and a prompt are input to an LLM for error correction.
    This section first describes several empirically-designed alternative prompts,
    and then leverages an evolutionary algorithm named EvoPrompt [[15](#bib.bib15)]
    for prompt optimization.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/720f57ffe159298ce75fb80db54dc99d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 1: Approach overview, where an $N$-best list of hypotheses and a trainable
    prompt instruction are fed to a pre-trained LLM for error correction. Details
    of prompt design processes are shown in Fig. [3](#S3.F3 "Figure 3 ‣ 3 Experimental
    Setup ‣ Evolutionary Prompt Design for LLM-Based Post-ASR Error Correction").'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Alternative Prompt Design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Task $1$ of the GenSEC challenge, the default prompt provided by the challenge
    organizers is You need to do language model rescoring in ASR. Given the 5-best
    hypotheses, you need to report the true transcription from the 5-best hypotheses.
    We denote this prompt as Baseline Prompt. It has shown strong performance in post-ASR
    error correction tasks [[3](#bib.bib3)], but it is unknown whether there exist
    better prompts. To improve the performance, we propose the following alternative
    prompts:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prompt #1: This is a hard problem. You need to report the true transcription
    from the 5-best hypotheses..'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prompt #2: This is a hard problem. Carefully summarize in ONE detailed sentence
    the following captions by different people describing the same audio. Do not allude
    to the existence of the multiple captions. Focus on describing the content of
    the audio. The transcriptions have some intialism for corporations.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prompt #3: You have been given 5 different possible captions of the same audio,
    carefully summarize the given text in one sentence. Make sure to follow all the
    steps in identifying the right summary.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prompt #4: There is some financial data discussed in a meeting. You need to
    correctly give the true transcription from the 5-best hypotheses. Your task is
    to critically evaluate these option using english grammar. Mostly these transcriptions
    are in present continous tense.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prompt #5: There are five transcriptions hypotheses for a given audio and you
    need to report the true transcription by using english grammar rules.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'These prompts are designed based on our experiences with modern LLMs such as
    GPT [[16](#bib.bib16)], Claude [[17](#bib.bib17)] and LLaMA [[18](#bib.bib18)].
    In Prompt #1, we tell LLMs that the task is to report the true transcription based
    on a set of hypotheses. This could bias the LLMs towards re-scoring tasks. In
    both Prompt #1 and #2, we explicitly inform LLMs that the task is a hard problem.
    We empirically observe that this often leads to better performance in many tasks,
    not just limited to post-ASR error correction. In Promp #3, we ask the model to
    summarize the given hypotheses into one sentence, as summarization is a task closely-related
    to post-ASR error correction. In Prompt #4, we explicitly tell LLMs that the data
    we are dealing with are related to finance, which is true in the case of CHiME-4
    [[19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21)], as CHiME-4 is built upon
    the WSJ dataset [[22](#bib.bib22)]. In Prompt #5, we ask LLMs to provide gramatically-correct
    outputs. This could be helpful for post-ASR error correction, as we observe that
    some ASR transcriptions contain grammar errors.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Employing EvoPrompt for Prompt Optimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In many application scenarios, empirically-designed prompts are often not good
    enough. Prompt optimization, which aims at automatically finding a prompt that
    can lead to better performance for considered tasks, has been attracting wide
    research interests in LLM research.
  prefs: []
  type: TYPE_NORMAL
- en: A promising recent algorithm in this direction is EvoPrompt²²2[https://github.com/beeevita/EvoPrompt](https://github.com/beeevita/EvoPrompt)
    [[15](#bib.bib15)], which has shown strong performance and potential on natural
    language processing tasks such as text classification, simplification and summarization.
    EvoPrompt is an iterative prompt optimization algorithm starting from an initial
    set (or population) of $N$) for the next iteration. One of the key steps is in
    how to create each of the new prompts based on the best subset. In EvoPrompt,
    each new prompt is created based on two prompts randomly selected from the best
    subset of prompts, and LLMs are utilized to first cross-over the two prompts and
    then mutate the resulting prompt. See Fig. [2](#S2.F2 "Figure 2 ‣ 2.2 Employing
    EvoPrompt for Prompt Optimization ‣ 2 Proposed Algorithms ‣ Evolutionary Prompt
    Design for LLM-Based Post-ASR Error Correction") for an example. This evolutionary
    mechanism has been shown effective by EvoPrompt [[15](#bib.bib15)].
  prefs: []
  type: TYPE_NORMAL
- en: In this context, to find prompts that can result in better post-ASR error correction,
    we employ EvoPrompt for prompt optimization. We choose the Genetic Algorithm (GA)
    option in EvoPrompt (see Algorithm $2$ of [[15](#bib.bib15)]).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/68732df96c11f6e0f4cee46b92b13b88.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 2: Example of text prompt optimization processes through (i) cross-over
    and (ii) mutation performed by LLM-operators.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Experimental Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We validate the proposed algorithms on the task $1$ SLT Generative Speech Error
    Correction (GenSEC) challenge.³³3More details of the GenSEC challenge can be found
    at [https://sites.google.com/view/gensec-challenge/home](https://sites.google.com/view/gensec-challenge/home)
    The task is on learning a mapping from N-best list of hypotheses (produced by
    ASR models) to ground-truth speech transcription so that recognition errors can
    be corrected. The challenge is designed based on the HyPoradise dataset [[3](#bib.bib3)],
    which consists of pairs of the N-best list and correct transcription of the utterances
    derived based on multiple public ASR datasets such as LibriSpeech [[23](#bib.bib23)]
    and CHiME-4 [[19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21)] by using strong
    ASR models such as Whisper [[24](#bib.bib24)], and WavLM [[25](#bib.bib25)] based
    ASR models.
  prefs: []
  type: TYPE_NORMAL
- en: Different from the official challenge configurations, our experimental setup
    only uses the CHiME-4 subset of HyPoradise for training. This is mainly out of
    cost concerns, as the evolutionary algorithm needs to compute a score for every
    considered prompt in each iteration. In CHiME-4 [[19](#bib.bib19), [20](#bib.bib20),
    [21](#bib.bib21)], there are $8,738$ utterances in the training, validation and
    test set, respectively, and HyPoradise [[3](#bib.bib3)] provides five hypotheses
    for each utterance. The task is to predict the true transcription based on the
    five hypotheses. In this study, since our main goal is prompt optimization, our
    system does not leverage acoustic information in error correction and only exploits
    text-level information.
  prefs: []
  type: TYPE_NORMAL
- en: Besides evaluating optimized prompts on the test set of CHiME-4, we investigate
    the generalizability of the optimized prompts to unseen domains by using two other
    datasets (including Common Voice (CV) [[26](#bib.bib26)] and Wall-Street Jounral
    (WSJ) [[22](#bib.bib22)]) for evaluation. There are $836$ test utterances in CV,
    and HyPoradise provides five hypotheses for each of the test utterances.
  prefs: []
  type: TYPE_NORMAL
- en: The LLM we use for prompt optimization is Claude [[17](#bib.bib17)]⁴⁴4We use
    Claude 3.5 Sonnet. See details in [https://www.anthropic.com/news/claude-3-5-sonnet](https://www.anthropic.com/news/claude-3-5-sonnet).,
    which has shown competitive performance compared with many state-of-the-art LLMs.
    We provide one demonstration example [[27](#bib.bib27)] to the LLM before asking
    the model to perform error correction. We find that this demonstration strategy
    is very helpful for in-context learning. In Fig. [3](#S3.F3 "Figure 3 ‣ 3 Experimental
    Setup ‣ Evolutionary Prompt Design for LLM-Based Post-ASR Error Correction"),
    we provide an example of the input we use in our experiments to LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/66e128d0ea2b6e10920a56bdcf18b0cd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 3: Example input for in-context learning with one demonstration, where
    the first paragraph denotes the prompt, second and third denotes one demonstration
    example, and fourth and fifth requests LLMs to correct errors.'
  prefs: []
  type: TYPE_NORMAL
- en: We use word error rates (WER) as the evaluation metric, where we optimize our
    prompt on the training set of the Hyporadise and report WER performance on the
    unseen test set based on the prompted-LLM performance on the training set.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Evaluation Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Results of Alternative Prompts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In row $1$, indicating the effectiveness of the proposed alternative prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Results of post-ASR error correction on CHiME-4 test set'
  prefs: []
  type: TYPE_NORMAL
- en: '| Row | Prompts | #Demonstrations | #Iterations | WER (%)$\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| $0$ |'
  prefs: []
  type: TYPE_TB
- en: '| $1$ |'
  prefs: []
  type: TYPE_TB
- en: '| $1$ |'
  prefs: []
  type: TYPE_TB
- en: '| $1$ |'
  prefs: []
  type: TYPE_TB
- en: '| $1$ |'
  prefs: []
  type: TYPE_TB
- en: '| $1$ |'
  prefs: []
  type: TYPE_TB
- en: '| $2$ |'
  prefs: []
  type: TYPE_TB
- en: '| $2$ |'
  prefs: []
  type: TYPE_TB
- en: '| $2$ |'
  prefs: []
  type: TYPE_TB
- en: '| $2$ |'
  prefs: []
  type: TYPE_TB
- en: '| $2$ |'
  prefs: []
  type: TYPE_TB
- en: '| $3$ |'
  prefs: []
  type: TYPE_TB
- en: '| $3$ |'
  prefs: []
  type: TYPE_TB
- en: '| $3$ |'
  prefs: []
  type: TYPE_TB
- en: '| $3$ |'
  prefs: []
  type: TYPE_TB
- en: '| $3$ |'
  prefs: []
  type: TYPE_TB
- en: '| $4$ |'
  prefs: []
  type: TYPE_TB
- en: '| $4$ |'
  prefs: []
  type: TYPE_TB
- en: '| $4$ |'
  prefs: []
  type: TYPE_TB
- en: '| $4$ |'
  prefs: []
  type: TYPE_TB
- en: '| $4$ |'
  prefs: []
  type: TYPE_TB
- en: '| $5$ |'
  prefs: []
  type: TYPE_TB
- en: '| $5$ |'
  prefs: []
  type: TYPE_TB
- en: '| $5$ |'
  prefs: []
  type: TYPE_TB
- en: '| $5$ |'
  prefs: []
  type: TYPE_TB
- en: '| $5$ |'
  prefs: []
  type: TYPE_TB
- en: 4.2 Results of EvoPrompt for Prompt Optimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In $2$e, we observe that including one demonstration improves the performance.
  prefs: []
  type: TYPE_NORMAL
- en: In $2$%. This indicates the effectiveness and potential of EvoPrompt for prompt
    optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Analysis of Optimized Prompt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The optimized prompt obtained by applying EvoPrompt to Prompt-$1$ in the first
    iteration is: You are presented with 5 different transcription hypotheses for
    a single audio clip from a financial meeting. Your task is to critically evaluate
    these options, considering factors such as context, coherence, and English language
    conventions. Synthesize the most likely and accurate representation of the audio
    content into one concise, grammatically correct sentence that is mostly present
    continuous tense. Comparing it with Prompt #1 presented in Section [2.1](#S2.SS1
    "2.1 Alternative Prompt Design ‣ 2 Proposed Algorithms ‣ Evolutionary Prompt Design
    for LLM-Based Post-ASR Error Correction"), we observe that:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The mutated prompt provides clearer instructions, which facilitate the task
    for the LLM. Instead of a vague instruction like report the true transcription,
    an optimized instruction provides specific guidelines, such as Your task is to
    critically evaluate these options, taking into account factors like context, coherence,
    and English language conventions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The improved prompts are more appropriate to the context, ensuring that they
    are tailored to the dataset. For example, for the CHiME-4 dataset, which contains
    financial transcriptions, the mutated prompt is focused on financial terms and
    contexts.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Including one demonstration example in the layout of the prompts significantly
    helps the LLM understand the desired output style and how to correct errors. Demonstrations
    can provide a clear reference, reducing ambiguity, and guiding the LLM towards
    producing more accurate transcriptions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.4 Examples of Corrected Errors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Table [2](#S4.T2 "Table 2 ‣ 4.4 Examples of Corrected Errors ‣ 4 Evaluation
    Results ‣ Evolutionary Prompt Design for LLM-Based Post-ASR Error Correction")
    provides an example of the corrected errors. We observe that biasing LLMs towards
    the domain of finance helps correct errors such as “fidelity” and “gencorp”, which
    are financial terms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Example of corrected errors based on an utterance drawn from CHiME-4\.
    Best viewed in color.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Utterance | WER (%)$\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| 1^(st) Hypo. by AM | fatelli had contended that genecorp is not a qualified
    broadcaster because it failed to disclose allegedly improper political campaign
    contributions and foreign payments | $8.69$ |'
  prefs: []
  type: TYPE_TB
- en: '| 2^(nd) Hypo. by AM | fidelity had contended that jeane corp is not a qualified
    broadcaster because it failed to disclose allegedly improper political campaign
    contributions and foreign payments | $8.69$ |'
  prefs: []
  type: TYPE_TB
- en: '| Correction by LLM | fidelity had contended that gencorp is not a qualified
    broadcaster because it failed to disclose allegedly improper political campaign
    contributions and foreign payments | $0$ |'
  prefs: []
  type: TYPE_TB
- en: '| Ground-truth Transcription | fidelity had contended that gencorp is not a
    qualified broadcaster because it failed to disclose allegedly improper political
    campaign contributions and foreign payments | - |'
  prefs: []
  type: TYPE_TB
- en: 4.5 Generalizability of Optimized Prompts to Unseen Domains
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [3](#S4.T3 "Table 3 ‣ 4.5 Generalizability of Optimized Prompts to Unseen
    Domains ‣ 4 Evaluation Results ‣ Evolutionary Prompt Design for LLM-Based Post-ASR
    Error Correction") and [4](#S4.T4 "Table 4 ‣ 4.5 Generalizability of Optimized
    Prompts to Unseen Domains ‣ 4 Evaluation Results ‣ Evolutionary Prompt Design
    for LLM-Based Post-ASR Error Correction") report the results on the CV and WSJ
    test sets. They offer valuable understandings about the ability of the model to
    extend its performance beyond the data it is trained on. The best performing mutated
    prompt #$1$ from the CHiME experiement shows a deterioration in performance on
    both test sets, with a higher WER compared to the baseline. This implies that
    the mutations implemented in Prompt #1 are not beneficial for the process of generalization.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The mutated prompt #$2$%) on the CV test set. This indicates that the specific
    alterations in prompt may have contributed generalized traits that benefit both
    seen and unseen areas.'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the findings indicate that the ability of post-ASR error correcting
    models to make generalizations might vary greatly depending on the initial prompts
    employed (i.e., Prompt-#{$1,\dots,5$}). Further research could explore the specific
    words and characteristics of the prompt that contribute to its generalizability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Results of post-ASR error correction using evolutionary prompts on
    unseen Common Voice test set of Hyporadise'
  prefs: []
  type: TYPE_NORMAL
- en: '| Row | Prompts | #Demonstrations | WER (%)$\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | Baseline Prompt | 0 | $11.062\,860\,4$ |'
  prefs: []
  type: TYPE_TB
- en: '| $3$ |'
  prefs: []
  type: TYPE_TB
- en: '| $4$ |'
  prefs: []
  type: TYPE_TB
- en: '| $5$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Results of post-ASR error correction using evolutionary prompts on
    unseen Wall-Street Journal test set of Hyporadise'
  prefs: []
  type: TYPE_NORMAL
- en: '| Row | Prompts | #Demonstrations | WER (%)$\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | Baseline Prompt | 0 | $3.67$ |'
  prefs: []
  type: TYPE_TB
- en: '| $3$ |'
  prefs: []
  type: TYPE_TB
- en: '| $4$ |'
  prefs: []
  type: TYPE_TB
- en: '| $5$ |'
  prefs: []
  type: TYPE_TB
- en: 4.6 Cost of Proposed Algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The API for Claude 3.5 Sonnet is charged at $3 per million input tokens and
    $15 per million output tokens. It provides more input and output tokens at a reduced
    cost than the previous Claude 3 Opus model. The cost for a single experiment on
    the CHiME-4 dataset using Claude Sonnet 3.5 is around $$2.2$. Check out the pricing
    page of Claude⁵⁵5[https://www.anthropic.com/pricing#anthropic-api](https://www.anthropic.com/pricing#anthropic-api)
    for more details.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have proposed alternative prompts for post-ASR error corrections, and leveraged
    an evolutionary prompt optimization algorithm for prompt optimization. Evaluation
    results on the CHiME-4 subset of the GenSEC Challenge Task $1$ show the effectiveness
    of the proposed algorithms, suggesting that leveraging conventional evolutionary
    algorithms for prompt optimization is a promising research direction. Moving forward,
    we plan to investigate LLM fine-tuning to find better prompts for generative error
    correction.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgment
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The authors would like to thank Prof. Shinji Wanatabe for his valuable feedbacks
    and encouragements on experimental designs when initializing the project.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] S. Minaee, T. Mikolov, N. Nikzad, M. Chenaghlu, R. Socher, X. Amatriain,
    and J. Gao, “Large Language Models: A Survey,” *arXiv preprint arXiv:2402.06196*,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, “Pre-train,
    Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language
    Processing,” *ACM Computing Surveys*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] C. Chen, Y. Hu *et al.*, “HyPoradise: An Open Baseline for Generative Speech
    Recognition with Large Language Models,” in *NeurIPS - Datasets and Benchmarks
    Track*, vol. 36, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] S. Radhakrishnan *et al.*, “Whispering LLaMA: A Cross-Modal Generative
    Error Correction Framework for Speech Recognition,” in *Proceedings of Empirical
    Methods in Natural Language Processing*, 2023, pp. 10 007–10 016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Z. Gu, T. Likhomanenko, H. Bai, E. McDermott, R. Collobert, and N. Jaitly,
    “Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition,”
    in *arXiv preprint arXiv:2405.15216*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Y. Hu, C. Chen *et al.*, “Large Language Models are Efficient Learners
    of Noise-Robust Speech Recognition,” in *International Conference on Learning
    Representations*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] C.-H. H. Yang, L. Liu, A. Gandhe, Y. Gu, A. Raju, D. Filimonov, and I. Bulyko,
    “Multi-task language modeling for improving speech recognition of rare words,”
    in *IEEE Automatic Speech Recognition and Understanding Workshop*, 2021, pp. 1087–1093.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] B. Liu and I. Lane, “Attention-Based Recurrent Neural Network Models for
    Joint Intent Detection and Slot Filling,” *Interspeech*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] R. Ma, M. J. Gales, K. M. Knill, and M. Qian, “N-best T5: Robust ASR Error
    Correction using Multiple Input Hypotheses and Constrained Decoding Space,” *arXiv
    preprint arXiv:2303.00456*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] C.-H. H. Yang, Y. Gu, Y.-C. Liu, S. Ghosh, I. Bulyko, and A. Stolcke,
    “Generative Speech Recognition Error Correction with Large Language Models and
    Task-activating Prompting,” in *IEEE Automatic Speech Recognition and Understanding
    Workshop*, 2023, pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] S. Watanabe, T. Hori, and J. R. Hershey, “Language independent end-to-end
    architecture for joint language identification and speech recognition,” in *2017
    IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)*.   IEEE,
    2017, pp. 265–271.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] C.-H. H. Yang, Y.-Y. Tsai, and P.-Y. Chen, “Voice2Series: Reprogramming
    Acoustic Models for Time Series Classification,” in *International conference
    on machine learning*.   PMLR, 2021, pp. 11 808–11 819.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] H. Gao, J. Ni, K. Qian, Y. Zhang, S. Chang, and M. Hasegawa-Johnson, “WAVPROMPT:
    Towards Few-Shot Spoken Language Understanding with Frozen Language Models,” in
    *Interspeech*, 2022, pp. 2738–2742.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] K.-W. Chang, Y.-K. Wang, H. Shen, I.-t. Kang, W.-C. Tseng, S.-W. Li, and
    H.-y. Lee, “SpeechPrompt v2: Prompt Tuning for Speech Classification Tasks,” *arXiv
    preprint arXiv:2303.00733*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Q. Guo, R. Wang, J. Guo, B. Li *et al.*, “Connecting Large Language Models
    with Evolutionary Algorithms Yields Powerful Prompt Optimizers,” in *International
    Conference on Learning Representations*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] OpenAI, “GPT-4 Technical Report,” 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Anthropic, “Model Card and Evaluations for Claude Models,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] H. Touvron, T. Lavril *et al.*, “LLaMA: Open and Efficient Foundation
    Language Models,” in *arXiv preprint arXiv:2302.13971*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] J. Barker, R. Marxer, E. Vincent, and S. Watanabe, “The Third ’CHiME’
    Speech Separation and Recognition Challenge: Dataset, Task and Baselines,” in
    *IEEE Workshop on Automatic Speech Recognition and Understanding*, 2015, pp. 504–511.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] E. Vincent, S. Watanabe, A. A. Nugraha, J. Barker, and R. Marxer, “An
    Analysis of Environment, Microphone and Data Simulation Mismatches in Robust Speech
    Recognition,” *Computer Speech and Language*, vol. 46, pp. 535–557, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] J. Barker, R. Marxer, E. Vincent, and S. Watanabe, “The Third ‘CHiME’
    Speech Separation and Recognition Challenge: Analysis and Outcomes,” *Computer
    Speech and Language*, vol. 46, pp. 605–626, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] D. B. Paul and J. M. Baker, “The Design for the Wall Street Journal-based
    CSR Corpus,” *International Conference on Spoken Language Processing*, pp. 899–902,
    1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: An ASR
    Corpus Based on Public Domain Audio Books,” *IEEE International Conference on
    Acoustics, Speech and Signal Processing*, pp. 5206–5210, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever,
    “Robust Speech Recognition via Large-Scale Weak Supervision,” *Proceedings of
    Machine Learning Research*, vol. 202, pp. 28 492–28 518, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka,
    X. Xiao, J. Wu, L. Zhou, S. Ren, Y. Qian, Y. Qian, J. Wu, M. Zeng, X. Yu, and
    F. Wei, “WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech
    Processing,” in *IEEE Journal on Selected Topics in Signal Processing*, vol. 16,
    no. 6, 2022, pp. 1505–1518.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler, J. Meyer, R. Morais,
    L. Saunders, F. M. Tyers, and G. Weber, “Common Voice: A Massively-Multilingual
    Speech Corpus,” in *International Conference on Language Resources and Evaluation*,
    2020, pp. 4218–4222.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] S. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, and
    L. Zettlemoyer, “Rethinking the Role of Demonstrations: What Makes In-Context
    Learning Work?” in *Proceedings of Empirical Methods in Natural Language Processing*,
    2022, pp. 11 048–11 064.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
