# 智源大会 2024（三）



# 2024北京智源大会-多模态模型 - P4：多模态基础模型研究-代季峰 - 智源社区 - BV1sT421a7FU

哈喽哈喽哎能听见哈，好，那个非常高兴今天能够来做这样的一个分享啊。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_1.png)

我今天报告题目叫做多模态技术模型技术研究。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_3.png)

对然后首首先呢我们可以看见，就说当这个线已经是有共识了哈，就说大圆模模型呢，它它已经带来了非常大的一个改变啊，它带来这么大的一个改变，我觉得是来来自于两个方面，第一个方面呢它在具体的语言相关的任务上。

它的性能做的非常好了，通过skating low啊，但另外还有一点很重要的呢，是它的这个通用性就说它降低了呃，应对新的开放式任务的这个边际成本，这点其实是非常重要的，就像上一代的AI啊。

比如说以前这个对吧，上一代的AI创业公司做安防等等等等的啊，比如说你给一个城市啊，部署了一套安防的系统啊，我们要这个评全国卫生先进城市，所以接下来我们要严抓这个乱扔垃圾的行为。

所以呢我们需要你们给我们再部署一套，这个检测乱扔垃圾行为的这样一个系统啊，那你吭哧吭哧，你又得花十个研究员采集10万张图片，100张卡啊，然后再干个两个月啊，然后再给他交交付这么一套系统。

你应对任何一个新的任务，你的边际成本是很高的啊，但但是像以CHRGBT为代表的这种大圆模模，模大圆模型啊，它有很强的应对开放式的任务的能力啊，你比如说你在网上CHRTPT。

你响应这个上亿人的这个请求的时候，他并没有说open AI在给你，另外花研究员在帮帮你再发一痛，for for你的每一个任务，对吧啊，就是这样的一种呃，一个是呃应对具体的任务能力非常强。

另外一点呢就是开放式的这个任务的能力低，边际成本泛化的能力，带来巨大的一个生产力的一个变革，当然呢我们就想把这样的一种新更更更更新的，一种生产力的能力啊，带到更多的这种多模态的领域里面去。

因为这个世界并不是这个结构化的语言，世界就获取更多的信息，你需要通过视觉，然后以及你需要主动的去跟现实环境去打打交，打交道啊，就是呃这样的一种多模态的能力，会是如果我们也具有，就说是这样的一种啊。

每个任务能力很强，以及这种开放式的应对各种任务的能力，那会带来一个巨大的一个生产力的一个变革啊，所以呢啊我我们就就就就开，以这样的一个为出发点呢，我们就开启了我们的多模态的，基础模型的技术的一个研究。

对，然后呢我会具体的讲一讲啊，当然我这这页PPT是按照国内的这这这种高，高校里面的习惯做的这个PPT，然后在在在在场的，今天的333位，都都是比较这个西方学术这种风格，所以这个PPT风格会有点不太一样。

对我们先先先看一下这个呃，呃呃就是第一个主要的挑，主要的挑战就就是关关，关于就是预训练以及训练数据规模的问题对，然后呢，嗯哎这个确确确实这个软软件不太一样啊。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_5.png)

但没关系，并不影响这的理解对，就是说我们手术。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_7.png)

我们首我们首首首先关注的第一个问题呢，我们会会发现就是呃。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_9.png)

当然之前的各位讲讲讲者都有说到clip啊，就是说呃已有的这。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_11.png)

这这这这种就是图像编码器的这个预训练，通常都是用clip这样的。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_13.png)

在图文成对的数据上去进行训练的啊，但是呢就当我们真正要构造一个呃。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_15.png)

非常大规模的这个多模态的基础模型的时候。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_17.png)

我们会发现现在其实互联网上的图文成对数据，就算你都用光啊。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_19.png)

它的数量也是已经告急了啊，就是说这个他已经无法再支撑更大规模的这个。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_21.png)

多模态的大模型的这个预训练，然后以及呢它还有一个很重要的一个问题呢。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_23.png)

就说是图文成对数据里面，你你你可以你可以想象。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_25.png)

就比如在一整整个一个复杂的一个新闻，或者在一个文档里面啊。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_27.png)

然后其实图文成对的部分，那个其实是它里面中间非常少的一部分。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_29.png)

然后以及呢它的这种呃语言部分，它其实是非常薄非常薄弱的啊。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_31.png)

整个一个一个新闻，你的captain什么的，其实只有1。1。11。1点点的文字啊。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_33.png)

他其实很很多大段的逻辑对图的阐释什么的。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_35.png)

相关的东西其实都是在他正文里面，所以呢啊就就就就说。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_37.png)

如果你纯粹只在图文成对的上上，上面去训一个多模态模型的话，其实你的语言模型的部分是会训毁掉的啊。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_39.png)

对对，所以呢就说我我们就我单单就说呃，我们这边呢就说是我们在在在训这个，比如说上海人工智能实验室这边的，我们的这个多模态大模型的时候，我们就会想到原生的，就从这种图文教交错数据出发啊。

然后来进行这样的一个探索和一个训练啊，因为就是图文成成对的部分，只是比如说你那个，我不知道这个怎么样对就是图文成对的部分，只是在中间非常少的一部分，我们希望用到更加广阔的这个数据，还有这个知知识。

来训练我们的这个多模态的模型啊，啊为此呢，我们构构造了现现在开源的最最大的这样的一，个图文交错的这样的一个呃数据集啊，可以看到呢，这呢是跟现呃，是跟以前的这样的一个呃，开源的这种数据集的一个比较啊。

以前当然有有有有很多的是一个纯粹文字的，这样的一种数据集啊，你有很多的文字，但但是你并没有语言呃，呃不但但是你并并你并没有图像，然后我们有比如说像LIN2B啊，这样的图文成对的这样的这种这种数据集啊。

它里面有有一些图图图图像啊，还还算挺多的图像，两个bin啊，但是他的语言部分其实是非常薄弱的啊，然后呢呃我们构建了这个数据集呢，叫做OMONLYCOPUS啊，然后这个是清华还跟跟跟上海人工智能实验室。

大家一起打造的，对它以中英文为主的这样的一个语言，然后呢它有非常大量的这个图像啊，因为它可以更加兼容并蓄的这个收集，更多的互联网上的这样的一些素材，然后以及呢它有这个也是海量的这样的呃。

文本的这样的一个素材，它是现在国际规模最大的图文交错的数据集啊，这个呢是里面的一些例子啊，对可以看到就是它整个主要的这种数数据，就是这样的一种结构对，然后这个地方是这个项目的二维码。

嗯接下来呢我们就就就就就说怎么样，能够有效地利用这种互联网尺度的图文交交，交错数据进行一个预训练，对以前的这种，刚才各位嘉宾都有讲到的这个呃clip的方法哦，哇这个小小小小小绿点在上面压根就看不见。

对A图中间的这种嗯基于clip的方法方法呢，它设计来是利用这种图文成对的数据的啊，它能够在图文成对的数据上很好的工作，去非常好的这个有监督的啊，预训练，你的这个就就就是微信的这个encoder啊。

然后呢，嗯然后呢，以及托以及以前呢有有有C这样的一种方法啊，C这样的方法就是以前确实有有有工作啊，尝试去利用这种图文交错的数据啊，但是呢它其实它的出发点啊，并并不是说怎怎么样。

我能够很好的利用这些非常大规模，图文交错数据，我把我的微信的encoder也训好，然后把我我我拉我我language部分的decoder也也训好，他的出发点呢啊，其实或者说他他能够做到的事情呢。

是说是我拿一个已经预训练好的clip，比如说clip，尤其是clip已经预训练好的vision的encoder啊，我拿过来，然后我再拿过来一个已经预训练好的，一个language的model。

我把它们拼接起来，然后然后呢，我这个时候呢，我用一些图文交错的数据进行试训啊，最后达到一个呃呃部署起来效果还不错的，这样的一个图文多模态的这样的一个模型，它并并它并不是说是诶，我想从头把。

我怎么去更好的去利用图文交错的数据，去预训练我的这个vision的encoder啊，就他他他他并并不能够做到这样的一件事情，因为我们发会发现他会把这件事情给做毁掉，就说如果从头开始训的话。

然后我们这个方法呢B这个方这个方法呢啊，我们待会会会讲，我们提出来叫多模态信息压缩，学学学习这样的一种方式啊，我们首次支持了互联网尺度的图文交错数据的，端到端的啊预训练的算法啊，它可以直接利用。

就说图文交错的这种数据，互联网尺度的，然后from scratch，把你的微信encoder把把把它给训出来，把它给训好，然后后吃支持了我们现现现在最强的这个，视觉语言啊，技术模型。

INTERVL的这个V1。5版本的这个训练，然后项目的那个二维码是在这个地方啊，然后这个多模态信信息压压缩学习这个算法呢，啊我们是从语言这这这这边的这种压缩学。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_41.png)

学学习这样一种概念来的对，就就就说这个最最最近有一种观点。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_43.png)

非常受到大家的这个关注吧，啊就就就说他尝试去解释。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_45.png)

为什么像GPT这样的语言模型。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_47.png)

它能够具有类似于像AGI的这种能力啊，就所谓的压缩及制冷啊。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_49.png)

你把全世界的这种语料的这种知识，压缩到一个呃一个参数量有限的。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_51.png)

这种大模大模型里面啊，在这这样的一个信息压缩的一个过程中间。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_53.png)

自产产产生了制冷啊，然后呢我们也是非常bin这样的一种想法。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_55.png)

所以呢我们在做这个呃，图文交错的这个预训练的时候啊。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_57.png)

我们也是基于这样的一个多模态的信息压缩。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_59.png)

学学学习这样一个理念出发，来构造我们的这个图文交错数据的。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_61.png)

这样一种预训练的算法啊，只是说呢在这个里面呢，啊啊我我就不不不讲那些公式了哈。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_63.png)

我讲讲一讲背背背后的一些想法对，然后就只是在这个里面呢。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_65.png)

在图文教交错数据里面，它跟语言模模型这边有一点很很大的不一样的。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_67.png)

就是语言这边它是一个结构化的啊，这样的一个数据啊。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_69.png)

所所所所，所所以呢你只管压缩你所有的语料就好了。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_71.png)

它已经过滤掉了现实世界中间很多的噪噪音啊。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_73.png)

它已经是人类啊，就是说人类的知识的这个结结晶啊。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_75.png)

把把把这个繁杂的现实世界，通过语言把它给结构化，逻辑化，把它给屏蔽掉了。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_77.png)

把中间很很多的不相关的信息啊，但但是当你在全世界的这种图文交错数据上面。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_79.png)

去做这个预训练的时候啊。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_81.png)

你的图像在在在这边，其实它中间是有很多无关的这种信息的啊。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_83.png)

就是它有很多的信息，对于你这个训练这个图像的encoder。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_85.png)

或者一个多模态的大模型来来说，这些信息它它是LIS啊，它是不相关的啊。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_87.png)

以relevant啊，或或者说它它它就就就是它是一些无效的。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_89.png)

这个信息，所以呢我们的这个压缩学习compression learning啊。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_91.png)

它是在在图像层面，它是在latent的呃，encoder的这个这个feature上面去做的啊。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_93.png)

我们我们图像这边先先通过一个呃encoder啊。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_95.png)

先呃或获得一个隐变量的一个表示。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_97.png)

然后我们在这个隐变量上面去做这个，compression learning啊，这样呢可以就是通过学习过程中间自动的丢弃。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_99.png)

丢弃掉这个这些呃繁杂的世界的图像。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_101.png)

中间跟我们的学习目标不相关的，这样的一个部分。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_103.png)

这样获得一个更好的一个学习的效果，然后最后呢啊这个根根据这样的一种呃。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_105.png)

呃呃latent variable的compression learning啊。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_107.png)

这样这样的一种方式呢，我们会导出最后它的整个训练。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_109.png)

整个训练的这个target啊，包包包括了contrast。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_111.png)

CONTRASTIVE的lows，还有auto regressive的这个呃text generation laws啊。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_113.png)

这样的两个部分来组成细节呢，大家可以去查看论文，然后实现起来呢也是非常高效的。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_115.png)

因为我们要在全世界的这个图文，交作数据上面去做训练。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_117.png)

所所以说我们必须要确确保整个算算算法的，这个呃学学学学习的是一。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_119.png)

一个是呃，一个是accuracy啊，另外一个是以EFFENCY啊。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_121.png)

效率我们都得非非常关注，不然的话是做不好的。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_123.png)

Ok，那接下来呢，就就就就说我们用我们这样的这种数据，还有预训练的算法，我们怎么去预训练我们的这个呃视觉，还有图文的这样的一个基础的一个，feature的一个表征，然后呢嗯我们这呢就是我们做做出来的。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_125.png)

就是这个现在最好的开源的这个视觉语言，的基础模型叫做intern v l啊。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_127.png)

嗯我们这个思考这个东西的出发点呢，就就就就说也就是说呃。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_129.png)

我们现行就是你购构造一个图文的这样的一个。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_131.png)

多模态的一个基基础模型，它包括图和文的部部分啊，但现像现在刚刚才那个赛三林也说。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_133.png)

现在这种呃这种架构通常是这样的啊，然后我们会观察到呢。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_135.png)

就vision的encoder的部的部分，其实我们觉就觉得他有落后于这个时代啊。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_137.png)

最早的时候就是是你你为着这个I呃，这个iimage let上的的任务上面去去去围围。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_139.png)

去围着他来转啊。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_141.png)

这样的一种去训练的方式啊，然后呢，呃后来呢又迁移到ECLIP为代表的这样一种。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_143.png)

图文成对的这种CONTRASTIVE啊，预训练的方式啊。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_145.png)

你最后得到的这个VISHENCODER啊，它还还是跟像现在这样的一个多模态大模型。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_147.png)

后面接一个非常强的这个具有可以，你可以认为具有高高阶逻辑和认知。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_149.png)

逻认知能能能能力的大圆模大圆模型啊。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_151.png)

做作为你的这样的一个推理和高阶。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_153.png)

这个智能能力的中枢啊，视觉作为一个encoder，它并不是为这样的一种架构。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_155.png)

一开始它并不是为这样一种架构去设计的啊，所所以呢我我们在这做的这个intern v l。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_157.png)

这整个模型呢，我们其实就说是一开始。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_159.png)

我们就是为整个这个呃图文的这样的一个。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_161.png)

多模态的大模型，就一开始就是为这样的一个应用，或者说这样一个架构去进行设计的啊。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_163.png)

就是我们一开始就想就想想清楚，视觉作为一个很强的这个encoder啊。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_165.png)

然后语言作为一个呃，一个就是高高阶智能的中枢d decoder啊。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_167.png)

就是为这样的一个架构去进行设计的啊。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_169.png)

然后呢我们这里面呢采用了呃这个latent，concompression learning的方方方方法。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_171.png)

来进来训练我们的视觉的这个encoder啊，然后训练了一个非常大规模的线。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_173.png)

应该也是现在最强的，开源的这个视觉的encoder啊，然后以及在中间呢。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_175.png)

我们使用了一种就是叫做渐进式对齐的方案，来进行一个学一个一个学一个学习啊，因为你一开始，如果就如果就就如果，就就就就说接一个特别大的语言语言模型，然后来进进进进行这样一个视觉encoder。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_177.png)

从头从头开始训练的话，你的整个计算代价会是非常高的啊，所所以呢我我我们是在一开始在训练这个视觉，encoder的时候，我们是为啥我不能我哦，我们在一开始训练这个视觉encoder的时候。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_179.png)

我们是用一个相对比较小规模的语言模，语言模型。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_181.png)

在图文交交错数据上进行这个呃，视觉encoder与训练啊，然后视觉encoder训训的差不多之后，然后我们再换上一个呃，特别大的这样的一个非常强的，这样一个大的语言语言模型，进行进一步的这样一个续训啊。

通过模模型这种从小到大，数据从粗到精的这样一种渐进式的训练策略啊，大幅度的降低了大模型的这样一个训练的，这个呃成本啊，在有限的资源下，展现出来的卓越的这样一个能力啊，然后我们先先先先说这个模型拆出来。

中间的视觉编码器的部分啊，一个6B的一个视觉的encoder的一个模型啊，这个是呃现象是应该是现在最最好的，这个开源的这个视觉的encoder啊，我们的能力呢能够比肩这个谷歌的。

它闭源的VIT22B这样的一个呃性能，然后呢呃作为整体整个呃模整，整体整个模型就是视觉的encoder，加上语言的底decoder啊，这样啊整体整个模整个模型呢，它现在是世界上最强的。

开源的多模态的通用模型啊，性能媲美啊，基于gt4V啊，GEMINI啊，GROCK啊，等头部的这种商用的模型啊，然后呢这个模型2023年12月份发布啊，在哈根face上面呃。

增长趋势榜单上连续第一个月啊排行第一，然后在视觉语言技术模型总下载榜单上，排名前十啊，然后在他旁边的都是google啊，Meta，还有就是呃微软啊啊，更早时间发布的这种同种类型的一个模型，对，然后当啊。

北京志愿研研究院呢有发布这个评测体系啊，然后反正呃是是目前最好的开源模开源模型，然后在呃浦江实验室的到这个思兰品评测上面，它是一个榜单的榜单啊，然后呢他也也是呃就是它是十几个，这个就是多模态模型榜单。

然后他做一个加权求求求求求，和这样形成一个榜单的榜单啊，然后呢他又优于国国国内非常多的啊，像那个质朴，还有街月，还有那个阿里的这个这个闭源的模型，然后呢比我们比我们更更更好呢。

就是这个GGB4V的4月份的版本，以及最新的这个GP4O对啊，这个呢是我们可以跟大家展，展示了我们这些模这个模型的一些视觉内容，理解啊等等能力，然后这个呢是我们那个那个模型的一个。

online的一个demo啊，大家可可以先先存下这个二维码，主要是现现现现现场这么多人，如果同，因为我们那个服务器并并没有放放放很多啊，因为我们其实主主要还是就说是开源，这个weight啊。

这个并并不是拿拿来就说大规模的，这个给给全球这个上亿人服务的，所以我们放放demo的那个机器计算量比较小，如果你们同同时使用的话，我估计那个体验不会那么好，但拼平时你们正常使用的话，这个这个还是非常快。

然后体验也是非常好的对，然后这呢就是更多一些例子，比如说这个里面你问他宝马车牌号是啥，不拉不拉啊，他能给你很好的回答，这个哪个西瓜最熟对吧，我记得好像这个是质朴。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_183.png)

他他们他们的一个例子对，然后呢，Anyway，他也能给给你一个非常reasonable的一个答案。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_185.png)

对这个是我我自自自己加的一个例子，反正就是我们这个模这个模模型做，做出来之后的话，然后我们就就那个做这个模型的同学，大家就有一个微信群嘛，然后模型做出来之后，大家就拿自己这个手手机相册里面的各各种照。

照各种照片去试啊，然后就有一些非常有趣的或者FAURE的case，大家就会在在群里面聊，对，就这种感觉，感觉跟跟跟你去试用一个商用的模型，比如说你不知道它怎么训练的模模型相比，这个体验还是非常不一样的。

对，就说你说你用这个GPT，4V或者是什么的模模型对吧，你觉得它效效果挺好，OK行，那效果挺好，这个后后面是是魔法对吧，open AI的，open AI的magic对吧。

然后你做一个真正自己顺顺的模模型，然后性能当然跟跟它差差差不多，然后你发现一些效果特别好的例子的时候，你会觉得我靠这这些东西是是是怎，是怎么实现的，然后你你会要就比如说这个这个力，这个力例例子里面。

反正就说他能够在这种遮挡，很很严重的情情况下对吧，然后他他能够分分清楚左左右手的概念啊，然后他能够知能够准准确的告诉你答案，然后我自己看这个碟子，我觉得哦忘了对，就在在在在在在我的理解里面呢。

其实就是他就就像刚才刚才赛林，有有有有有讲的，他其实我的理解他这种在全世界种语料上，这种图文的这种语料上面训练之后，他其实是学会了这种呃单位的这种词源，跟这种视觉的单位元素之间，它的一个映射关系啊。

然后他学学学会了这样的一种映射关，映射关系之后呢，然后他又学他又并且它又具为具有一种组合，一种排列组合的能的能力啊，他能够理理解整个句子，把这些各种各样视觉元素排列组合起来之后。

它能够对应于一个什么样的一个，更复杂的一个概念啊，通过这些基础能力的组合呢，它能够形形成一个很强的一个泛化能力，就是我们在我们在在在测试我们模型的时候，会发现诶这个这个模型的能力。

我们没我们没有训练过啊，他是怎他究竟他究竟是怎怎怎么出来的啊，就是经经经常我们都会有这样的一种感慨，对啊，但这次这个是一种指令跟随能力，这是GPT4V，当时让我们觉得很惊艳的一个例子啊。

我们的模型也可以做的对。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_187.png)

然后这次这个是网网上，别别别别别别别人做的一个视频啊，这个不不是。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_189.png)

这个号称是最接近GPT4V的，开源可商用多模态大模型，一开始我也以为是在吹。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_191.png)

因为我们本本身也有图片识别和理解的，实际业务场景，就实测了一下，结果给我的感受就是。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_193.png)

将对传统的OCR形成降维式的颠覆式的打击，可以看一下公有云提供了类似服务。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_195.png)

比如证照识别，票据识别，各种资质的识别等等，价格其实还不便宜。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_197.png)

现在这个多模态的大模型就可以搞定了，我随便找了几张不同类型的图片看一下，效果真的是惊到我。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_199.png)

我上传了一张有点模糊的身份证，让他用JSON的格式返回，可以看到他返回的速度很快。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_201.png)

而且数据很准确，又用一张消费清单试一下，还是用JSON输出。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_203.png)

让我没想到的是，他竟然能够用JSON数组的格式，返回了所有的消费记录，再上传一张手绘的草图。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_205.png)

真没想到这么抽象的一张图。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_207.png)

他都能推理出来，而且给出了正确的原因，目前已经在GITHUB开源了。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_209.png)

模型也托管在哈根face上面，大家可以照着示例代码部署的体验一下。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_211.png)

好诶再往下对，然后这个是更多的一些一些例子啊。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_213.png)

对然后接下来嗯，下一个呢，我们就就就就说我们其实还是就说希望能够，刚刚刚就是刚才最开始的有说到，其实我们的出发点啊，就就就就就说是希望他能能能够，就说是第边际成本的实现开放任务的泛化啊。

虽然说这个呃语言这样这样的一种，一种交互的方式，它已经能够实现很多的开放的，这样的一种多模态的任务，但是还有非常多的任务啊，就是说其其实他现在在这个时间点并不是很好，能够用语言来进行刻画的。

但以及其实还有非常多的这种视觉任务，可能到和未来可能都是并不是，能够很好的适用于语言这样一种模态的对嗯，然后所然后，所以呢，就是我们这开发了一个通用的一个任务，解码器啊，然后它叫叫做呃。

他更更更早的一个版版版本叫做微信LOM啊，就是我我们这这我们这个系列，我们的想法呢，就是希望能够打造一个开放的这种多多多模态，以视觉为核心的多模态任务，的一个开放的这个decoder啊。

能够低边际成本的泛化到各种各样的，千奇百怪的这种以视觉为核心的，这样一种多模态任务上，这是我们这个系列工作的这个核心嗯，然后在这个版本里面呢，啊我们这儿创我箭头全全没了呵，OK然后在我们这个版本里面呢。

就就就就是我们嗯做了一个叫做向量链接的啊，这样的一个技术啊，然后英文的话对应的名名字应该要更贴切一些，大家看看论文就就就好了，对嗯对，因为我我在PPT平平就是做做做了。

就平时朋友可能在国内的一些对一些其他场合，我我我我也会用的，所以就基本上都尽量保尽量是用中文对，然后呢呃他基基本的想法呢，就就就就就就就说是呃，我们通过跟其他的东西的对比啊，然后我们来看哇。

不过这里面啥东西都看不见了，我也不知道该怎么讲啊，呵OK这就是我我我举举举举个例子吧，最左最左边这这这这个图呢就就是一种啊，以前经常用的就是用大圆模型为核心，去调不同工具的这样的一种做法呃。

以这个vision叫以visual chat gchat g p t为代表的啊，它就是以大圆模模模型作为一个agent，作为一个调用不同工具的一个核心，然后呢他会调，比如说已经已有的。

比如说上百个视觉的，比如说detection semitation，pose estimation啊，都还有，还有image generation等等不同的工具，它跟这些工具之间。

它这个链接其实最核心的是中间画的这个画的，那个那那那个线，但但是这个这个PPT的版本，版本就看不出来了，就是他跟他们之间的调用的接口，就只是一个是以语言和文字为接接接口，就是就是他会给你发一个指令。

说我调用你，然后给你这个模模型传入些什么参数，这个模这个模模型视觉的模模型做完事之后，他把这个检测到的框，或者说是这个估计到的pose或怎么着，就直接反，就把它接。

最终的结果返回给这个语言的这个指令中枢啊，它它们之间是这样一种比较松耦合的一种方式，并不能end to end的进行训练，也并不能够传feature啊，所以呢呃它的整个性能的上限是比较低的。

然后还有一种方式呢，就是中中间的这样的一种方式，它是就是它它它它它是一种紧紧耦合啊，就是视觉，就是说这个呃多模态的这个大模型，跟你某种扩展的能力。

比如说object detection或者是SEMITATION，或者是这个图像内容声生成，或者是视频声声生成啊，它是一个呃多多模态的大模型，跟一个专有的工具之间，它们形成一个特征的紧耦合的一个关系啊。

但但是呢它他们这类方法的问题呢，是他就是呃并不通用啊，就是我一个大一个大多模态模型，就绑定一个特定的工具，然后然后end tw的去训练啊，你你你就是说最后弄完之后啊。

这样的一套模型和工具大家就绑定在一起了，它并不具有开放式任务的泛化的这样一个能力，然后我们在这样这样的一一个方法呢，就是向量链接，它中间是有一个路由的一个机制啊，就是我们就是一个多模态大模型为中枢啊。

它可以向外去扩展，调用上百个或者几几百个也也行啊，各种各样的视觉或者多模态的工具，它们之间通过路由啊，它它这个多模态模模型，它它自己确定根据刚才的指令，我应该调用什么样的工具，然后呢。

并且呢它们之间的连接呢，是这个feature层面的这个连接啊，然后feature层面的连的连接呢，那你有可可就是这个大圆模型和工具之间，他们的这个传输的带宽会非常宽，然后以及在训练的时候。

你可以ENTTW的进行训练，所以呢就是具备呃，就上面两两种以前的方法的好处，啊这这这边这个图也看不了了，对，然后呢嗯说这这这这这最左边呢，其实我们是对各各种，就是现现现在大员模模就是单纯只用通过语言。

通过interface并不能够很好处理的，可能几十个上百个任务，我们把它跟根据这种不同的维度，把它给放在这个做坐坐标系上上上上上面啊，他们在不同的维度上，他们这些任务的难度是怎么样的啊。

然后呢所以我们这样以一个模型呢，就就就就就能够去覆盖掉呃，非常多的多种多样的这种多模态的任务啊，然后能够应用在不同的场景里面对啊，这呢是是一些例子，然后这是这个呢是这个算法的，这个项目的二维码啊。

已经开已经那个放出来了对，然后再然后这这个呢是是就是说呃，对在在在场的各位都很都很熟悉啊，大家都是以前都是wobject detection的对。

然后这个呢是调这个object detection的工具啊，然后来去去这个去去处理一些各种各样的，复杂的一些一些场景，对通过调这种工具呢，它能够对做很好的各种场景下的这种检测。

然后在这在这个是你问他一些，别别别的一些问题啊，就是说你让他说呃，比如比如说你问问他，你说你希望就是定位图片里面所有的人啊，然后呢你让让让他把那个QQ17个key point，这种标注。

把这里面所有的把都都给标出来啊，然后或或者你只只只只做一一个点位啊，要定位所有的人，然后呢你你把他这个right elbow啊，右右走的位置把他给点出来，或者是干些其其他的事情啊，都可以对。

好然后这这这这个商要让他去去去去那个调，这个就是呃image editing，或或或或或或者是图像生成啊，这样这样的一些工具来进行，进行图像的一些编辑，这样的一些操作啊。

最后呢啊是我们往这个呃就是与世界进行交，进行交互啊这个方向去走的一些尝试，那这个其实其实是相相对更早一些的，这个工作是我们去年5月份的时候哦，PUBLIPUBLIC出来的对。

然后我们我们其实在在在在呃去年的时候，去年年初的时候啊，那会就非常敏锐的就说意识到了，就就就说这个呃跟我就是这种嗯大模型，它的威力会拓展到跟这个虚拟环境，还有现实是现实环境中间的交互里面去。

就以前基于强化，纯粹基于强化学习的方法，有太多的这个缺点和问题啊，然后大圆模模型或者是大模型，它能够很好的弥补这样的缺点，所以我们在去年年初的时候啊，就开启了这样一个项目啊。

然后去尝试去去玩一个最开放的，然后卖的最好的这个游戏MINECRAFT啊，我的世界，然后呢在这个里面呢啊，然后我们就在MINECRAFT里面啊，最最最最早啊，就说跟英伟达的这个voyager2者同期啊。

这两工作最最早在这个我的世界里面证明了，这种大圆基于大圆模，大圆模型的智能体，相较于以前的强化学习的智能体啊，它具有一个非常强的一个泛化，还有这样的一个智能化的这样的一个能力。

对啊这个是我今天整个talk啊。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/cd36720746771989945984d85ff95f1d_215.png)

# 2024北京智源大会-多模态模型 - P5：A Real-World Approach to Intelligence-肖特特 - 智源社区 - BV1sT421a7FU

因为这也是视觉语言模型当你不能带一个两个男人，你知道的，有点，我想我也，当你的男性，你可以，找到了未来的替代方案，所以这是一个现实世界的方法，所以在我的博士生学习快结束的时候。

我从伯克利搬到了旧金山湾的另一边，去旧金山市，所以你们中的一部分人可能去过CD，所以当你看到旧金山这个词，你首先想到的是什么，嗯，有几件事是对的，也许金门大桥，大家以前都见过这座桥，也许是勇士。

你们中的一些人可能是篮球迷对吧，或者旧金山的山路，你知道这是一个多山的城市，D，四处走动不是那么容易的，或者如果你是爵士乐或传统流行音乐的忠实粉丝，你以前可能听过这音乐，我把我的心留在了旧金山。

法律要求旧金山的每个居民都能唱这首歌，嗯，我只是在开玩笑。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/5bdad4e6246c78b7b0c06444789d8201_1.png)

当然啦，你知道吗，这些头脑怎么可能产生这个概念，先回过神来，嗯，也许你在金门大桥是一座宏伟的大桥之前就开车上过它，它有几条狭窄的车道，或者你以前可能在桥上走过，风很大，但这美丽的景色。

也可能你自己就是个篮球运动员，你一直在看篮球比赛，在22届总决赛中，你知道战士们，它与凯尔特人竞争，勇士赢得了决赛，不幸的是，我是凯尔特人队的球迷，我们今年会回来，也许你和我一样，我以前住在这里。

这是一个相当大的冰雹，城市的一部分，我以前经常在这种路上走，大量运动，去附近我最喜欢的咖啡店，外面真的很美，或者就像我之前提到的，也许你有，你知道的，发送传奇托尼·班尼特之前的歌曲。

我把我的心留在了旧金山，但目前人工智能的学习模式是通过阅读维基百科，现在你知道了，他们从互联网上抓取数据，训练一个大型语言模型来记住这些事实，旧金山的权利，正式的旧金山市和县，这是惊人的，嗯。

它有人口啊，八十八万，437名居民，截至二十二，太神奇了，这座城市以前的历史是什么，在欧洲解决之前，那也很神奇，我称之为语言第一的方法，正如签署所回避的那样，然后呢，当然安当你记住所有这些事实。

听起来很神奇，对呀，你可以重复旧金山的整个历史，几乎每一本关于一篇文章的书，和一篇关于这座城市的新闻文章，当你不断重复，你就像没事一样，没有人能自己记住所有的信息，那一定是情报--嗯。

我认为我们从现实世界中学习第一方法，你去过金门大桥，你在看篮球赛之前就住在城里了，你也学会了唱城里的歌，所以这就是我所说的真正的，现实世界第一的方法，这就是我们作为人类如何了解这个概念，关于这个地方。

关于新知识，所以如果我们能接受这个想法，因为也许通过我今天的演讲，嗯，我想提供一种替代当前范式的方法，并介绍和讨论其中一些作品，我在加州大学伯克利分校读博士期间做过，我相信现实世界的学习有两个部分。

它们当然是交织在一起的，第一部分是观察，观察或感知自然是模型模型，当然跟视觉有关，包括声音，涉及很多其他的事情，第二部分是真正的行动，现在呢，能够获得，在现实世界中学习，你呀，你需要有化身的权利。

可能是机器人的物理化身，例如，你可以在现实世界中部署一堆摄像头，你必须能够与世界互动，然后观察之后有什么变化，从这些互动中学习，所以我想从这些作品中的第一个开始。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/5bdad4e6246c78b7b0c06444789d8201_3.png)

是片段，我在Meta AI研究所读博士期间写的任何论文，这个项目是由，所以在片段之前，现状是，我们预先定义了列克星敦的列表。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/5bdad4e6246c78b7b0c06444789d8201_5.png)

例如，那是字母表，例如，当我参与820K数据集时，由子弹乔带领，我们定义了一个类别列表，例如，你可以看到有一辆车，有椅子，有窗户，有桌子，在它下面，这些物体有很多部分。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/5bdad4e6246c78b7b0c06444789d8201_7.png)

我们训练模特的日子，例如开放网络，在这些数据集上预测像素，明智的分类或图像分割，你可以看到这个结果很不错，但你就像，好啦，所以沙发就是沙发，但如果我真的想看沙发上的枕头呢，我想更多地了解沙发。

不同的人关心不同的事情，作为人类，你能认出沙发上的很多东西，甚至你的猫，如果你以前养过猫，他们也能识别很多东西，他们通常在沙发上有他们最喜欢的地方，对呀，通常是一个垫子。

他们有时还喜欢抓挠他们最喜欢的沙发，把一切都毁了，嗯，好吧，不管怎样，所以我们想出了这个叫做快速分割的新任务，动机就像，让我们摆脱这种预定义词汇表的想法。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/5bdad4e6246c78b7b0c06444789d8201_9.png)

从一开始，所以它导致了你，这是一个系统，你可以用很多东西来提示，首先，它是交互式的点和框，例如，你可以在青蛙上画一个点，上面有一只蜗牛，系统就像，好的，好的，你想把管子的接头分开吗，他们就像不，实际上。

我只要青蛙不要蜗牛，然后在上面放一个负点，在这里有个红点，系统能够根据你的输入做出反应。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/5bdad4e6246c78b7b0c06444789d8201_11.png)

第二个真的很喜欢应用。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/5bdad4e6246c78b7b0c06444789d8201_13.png)

这在当时是非常有趣的，是当你把点，有自然的模糊性对吧。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/5bdad4e6246c78b7b0c06444789d8201_15.png)

就像从信息论的角度来看，如果没有进一步的投入，这是不可能解决的。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/5bdad4e6246c78b7b0c06444789d8201_17.png)

例如，如果我在一个人身上放一个点，在这种情况下，你真的不知道我是说那件夹克还是那个人。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/5bdad4e6246c78b7b0c06444789d8201_19.png)

或者只是上半身，这个点有很多潜在的组合。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/5bdad4e6246c78b7b0c06444789d8201_21.png)

因此，该系统实际上能够生产几个面具，这些面具可能与。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/5bdad4e6246c78b7b0c06444789d8201_23.png)

这种特殊的输入，第三个有趣的应用程序是像现在这样，你实际上有一个系统可以自动分割图像中的所有东西，因为你知道没有什么前科，所以如果你随便把。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/5bdad4e6246c78b7b0c06444789d8201_25.png)

或者统一地把这些想法放在图像上，而现在，您实际上可以从单个图像中获得许多不同的分割掩码，这真的很有趣，和原力，模型的方面是它自然地接受模型模型的输入，除了点和盒子之外，我们也可以处理可能的面具和短信。

而模型本身就是一个通用的分割模型，其实很简单，所以你有一个图像作为它的输入，你把它通过一个相当重的图像编码器，现在它正在加速，现在你有了一个特征地图或图像嵌入，现在你有提示了。

在你很好地提取了图像输入的嵌入之后，这种设计的好处是针对不同的快速输入，你只有一个非常轻量级的解码器，你需要重新计算，所以对于给定的图像，你可以预先提取所有的特征，并根据您喜欢如何提示它，那个啊。

计算速度真的很快，所以这有实际的好处，当然，我们真的没有数据来训练模型，所以不像，你知道的，大型语言模型，你可以在整个互联网上爬行，并训练模型预测下一个令牌，没有预科，你知道这一点，图像分割。

大规模图像分割，可以训练通用分段模型的数据集，所以我们必须建立这个数据引擎，循环范式中的模型，也很简单，所以你首先用你能得到的任何东西来预先训练一个模型，然后你收集数据，收集更多的面具，然后你训练模型。

然后你再去，结果呢，这是我们通过这个范例构建的最大的分割数据集，与之前最大的数据集相比，打开图像V 5，我们在一个数据集中有10亿个口罩，它大约是开放图像数据集的400倍。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/5bdad4e6246c78b7b0c06444789d8201_27.png)

那是关于你知道的，观察与感知，现在一切都好了，现在如果我们，我们如何开发视觉系统的视觉表示，这样我们就可以支持探员，或者是人体人工智能或者现实世界中的机器人来完成这些任务，在现实世界中。

这是我们在22年做的一篇论文。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/5bdad4e6246c78b7b0c06444789d8201_29.png)

伯克利和当时的现状，机器人应用视觉和机器人应用是每一个任务，你必须有一个视觉编码器，那么这意味着什么呢，所以这意味着，作为一个人想想，现在你的任务是到达那个红色的块，在桌子上，你得有个视觉编码器。

专门训练来完成这项任务，现在第二个任务，第二个任务是拿起桌子上的黄色块，现在你得训练另一个视觉编码器，然后我想打开冰箱门，然后你必须有这与我们的工作方式完全不同对吧，听起来不管我们去哪。

我们必须有一个不同的大脑部分，那是专门用来做那种事的，这可能会很不愉快，我想说作为一个聪明的人，所以我们改变了这种模式，让一个单一的pre。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/5bdad4e6246c78b7b0c06444789d8201_31.png)

训练有素的视觉编码器，适用于所有这些任务，我们是怎么做到的，那是我们在野外收集的数据，我们有五个不同的数据源，例如图像分类，或者人与物体的互动，和互联网视频数据，YouTube视频，例如，比如烹饪数据。

最重要的是，我们预先训练了一个掩码自动编码器，作为这些数据的可视化嵌入，动机是如果你知道我们能够很好地重建，那是信仰的巨大飞跃，当然，如果我们能重建人们在其中的数据，人们在做饭和画画，做一些随机的事情。

我们能够获得一个视觉表示，这对涉及的下游任务有好处，例如，现实世界的交互也是如此，然后使用这个预先训练的视觉表示，我们转移，我们很好，调整了几个机器人任务，比如，你知道的，捡起积木，嗯，关上玩具冰箱门。

从辅音槽里拿东西。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/5bdad4e6246c78b7b0c06444789d8201_33.png)

我们看到的是这些视觉嵌入，优于当时所有标准视觉模型，例如，剪辑模型或监督模型的图像或从头训练，当时负责监督imagenet模型和从零开始的训练，或者是那种范式，或用于机器人学习的标准管道。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/5bdad4e6246c78b7b0c06444789d8201_35.png)

我们当时也发现了有趣的事情，当我们试图缩放模型时，从，例如，这里的一个小模型显示为一个大模型的vi t s，基于Vt的小数据集模型，大约七点，七十五万帧，业绩实际上下降了啊相当大的差距，我们都很好。

缩放不起作用，但在这种情况下当然不是这样，因为当我们在论文中的更大的数据集上训练模型时，大约有四百五十万帧，我们实际上看到了我们想要的性能提升，所以真的很有趣，这真的很重要，你知道的。

Coscale您的模型大小和训练数据，当然啦，当然啦，该范式还与更大的视觉模型进行缩放，所以当时我们训练的最大的视觉模型是一个乡村模型，这大约是雷内特50号的十五到十四倍，在论文发表的时候。

Resnet 50模型实际上只是机器人学习的标准，你知道的，我认为这种预训练模式很有希望，实际上是为了在机器人学习中使用更强大的模型，所以一个自然的问题，当然啦，在我们完成论文之后。

我们能不能提前训练一下，你知道的，视觉表示，我们能多训练点吗，包括，例如，动作，范式的交互部分。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/5bdad4e6246c78b7b0c06444789d8201_37.png)

因此，由我的老朋友和合作者伊拉领导的团队，还是野蛮的乌伯克利，如果没有我的参与，这项工作是关于用感觉运动预训练进行机器人学习的吗。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/5bdad4e6246c78b7b0c06444789d8201_39.png)

所以不仅仅是视觉预训练。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/5bdad4e6246c78b7b0c06444789d8201_41.png)

当然我们预先训练视觉模型，以及整个轨迹，不同的摄像机和预感知，所有的东西都进入一个轨道，把它掩蔽起来，通过变压器发送出去，所以现在你可能想起来了，好啦，它看起来非常类似于鸟类模型的语言范式。

这正是作者的意图。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/5bdad4e6246c78b7b0c06444789d8201_43.png)

这听起来像是一个非常简单的范例，但我强调了网络上的两个发现，第一个是它能够跨实验室传输，那很有趣，所以誓言，机器人学习的神话，你在实验室开发的算法，在隔壁的实验室里行不通，为什么那口井，灯光变了。

人们想出了所有的借口，对呀，灯光不一样，这次不一样，我的相机位置不一样，桌子颜色不一样，你知道的，如果你只能关上自己的冰箱门，世界上没有其他冰箱听起来令人沮丧，也不真正智能，所以这是一个非常有趣的发现。

论文中最有趣的发现，通过预先训练这些表征，您可以从系统中看到很好的通用性或可转移性。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/5bdad4e6246c78b7b0c06444789d8201_45.png)

第二个是它不仅在实验室之间传输，它也是从机器人，我们有在X臂上收集的数据，然后我们用数据对模型进行预训练，并将其部署在机械臂上，而且效果很好，但比在法国模式下从头开始训练要好得多。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/5bdad4e6246c78b7b0c06444789d8201_47.png)

所以这真的很有趣，我演讲的第三部分是，你知道的，我们也想发展什么样的新的化身，现在您可能已经观看了GDCS，你可能一直在关注新闻，或者真的人形，我认为人形机器人真正有希望的是。

我们的世界是专门为我们建造的，你知道我们有这么多楼梯，我们有一个有趣的手柄，我们可以操纵，比如用我们的手，我们有点想，我们不想看书，设计和重新开发我们的世界，只是为了让我们，你知道我们可以。

我们可以在那里部署机器人，那是不公平的，作为一个人我会对我说，所以我们希望机器人能够生活在这样的世界里。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/5bdad4e6246c78b7b0c06444789d8201_49.png)

那是人形的，所以第一件事，当然啦，一旦你有了机器人就好了，你得让它走路，真的吗，如果它不走，这不是，和机械臂没什么区别，你骑在桌子上，就在，你知道的，那你就没学对多少。

你能学到的最好的东西是桌子上发生的一切，虽然那已经是，你知道很难，这是一个真正的桌面，脉动是一个非常具有挑战性的任务，你知道学习，但还是，你知道的，步行运动，所以我们从那里开始。

这篇论文最近发表在《科学》杂志上，加州大学伯克利分校团队领导的机器人技术。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/5bdad4e6246c78b7b0c06444789d8201_51.png)

美国机器人制造的digi机器人，机器人公司Lility Robotics，嗯，你可以看到它是一个全尺寸的人类机器人，我想它大概有四十磅重，也许五十公斤，它有一种有趣的向后弯曲的腿，看起来像鸵鸟。

公司设计了机器人，这样你就知道他们什么时候，你知道膝盖不会阻碍它，当它弯下腰的时候，这才是真正的动力，然后嗯，它是全尺寸的机器人，它有激光雷达，我们在报纸上并没有真正使用这两只胳膊，但不幸的是，没有手。

在机器人的特定版本中，我们使用因果转换器，轨迹只是机器人的中央马达输入，那主要是根据感知，例如，你们的联合立场，对他们来说，关节速度和其他一些东西，从机器人的陀螺仪输入，变压器所做的只是预测。

在关节空间，然后我们把它映射回机器人控制系统，那么我们如何训练这个机器人呢，我们不能真的部署，你知道现实世界中有一万个机器人，让它从那里学习，那会很混乱，你看到整个机器人舰队总是倒下，有点。

所以我们求助于模拟器，这是英伟达艾萨克C健身房模拟器嗯，我们跨越了某种，有点，你认识很多人，很多很多这个机器人的复制品，并让学习者从模拟中，所以在模拟中，我们随机改变了那个世界的物理性质，例如。

对于不同的机器人，地面的摩擦力是不同的，另一个机器人也是，你知道不同的尺寸，只是为了增强，有效载荷不同，你可以想象一些机器人没有携带任何东西，一些机器人真的带着，在那里露营的那种，也喜欢。

你知道不同类型的场地，例如那里有崎岖的地形，也像你知道的平坦的地形，也喜欢不同种类的斜坡，有点慢，好啦，所以嗯，即使在模拟井中，我们如何训练它，这是通过强化学习，当然第一步很好，如果你研究过强化学习。

你知道训练是很难的，我们称之为样本复杂度，需要一段时间，所以这就是为什么我们用这种两步走的方法，这在实验室里是首创的，在苏黎世的实验室，所以在第一步中，我们使用一种我们称之为特权信息，例如，环境参数是。

例如，摩擦系数系数到底是多少，你知道，在现实世界中，你无法真正衡量这一点，但是在模拟中你可以读到，所以你首先训练一个模型使用这个，然后在第二步中，你不仅要强化学习，也是模仿学习。

意思是向能够获得特权信息的老师学习的模型，你的学生模型没有特权信息，但它至少可以像模仿，老师做什么，感觉就像你在模仿超人，所以这会缩短你的训练轨迹。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/5bdad4e6246c78b7b0c06444789d8201_53.png)

我们在地平线上训练的东西，在那之后，我们只需从资产宝石模拟器中部署它，就身体与敏捷的相互作用而言，这并不精确，一个模拟器是由公司提供的，有更好的物理是极其缓慢的，当然，你不能在模拟器里做基于学习的算法。

然后你知道，把它转移到一个真正的机器人上。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/5bdad4e6246c78b7b0c06444789d8201_55.png)

所以它得到的是，它导致的是你可以在现实世界中直接部署的模型，嗯，零发，意义，我们这里不做微调，它已经可以做得很好了，例如，就像，你知道的，嗯，左上角是，它走在伯克利校园里，起作用了，它正走出南门。

穿过校园和其他地方可爱的小溪，所以这些都是，当他们被枪杀时，你还可以做的是，我们称之为全方位行走，所以不仅向前，也向后向后，这里是伯克利体育场，你可以看到就像，我觉得帧率有点低，所以就向后走而言。

感觉是相当深思熟虑的，而且一切也都可以转动，当然啦，同样有趣的是，我们看到这个突然出现的手臂在摆动，很像人类，虽然它不是完全对称的，但至少有点接近了，所以这种行为我们可能不会接受，你知道吗，说真的。

因为当我们走路的时候，我们总是摆动手臂，这很自然，但是你知道，过去的科学研究发现，这种行走行为导致了一种，一个更多，你可以称之为更节能的，一种行走模式或行为意味着。

就像你知道你不必吃那么多卡路里来走路一样，例如，一公里通常是我们走的路，但你可以，你知道我们也可以把你绑起来，所以你不能真的摆动你的手臂，或者你知道你只是抱着你的胳膊，或者你真的用相反的方式做。

你可以看到正常的，正常的，步行模式是最节能的模式，当然，反正常模式是最糟糕的模式，所以这就是为什么你看不到，你知道的，足球运动员往那边跑，嗯。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/5bdad4e6246c78b7b0c06444789d8201_57.png)

这个系统还有什么有趣的地方，它是在环境适应中这样做的吗，所以你看到的是机器人正走下斜坡，大家可以看到，嗯，它先，你知道这是，这是正常的正常行走模式，当它让我看看我是否能再玩一次，是的，在这里，是呀。

所以当它走下坡的时候，它是这样做的，你知道一小步，一旦它稳定下来，它又恢复了行走的模式，所以感觉它在适应上下文，取决于你知道，例如，机器人所在的地形，所以我们在这里做了一个小问题来可视化，例如，各州。

几种不同的隐藏状态，你也知道，映射隐藏状态，或者不同的维度，利用这个降维，呃技术，例如，偷袭技术，嗯，平坦的区域在x轴上用蓝色着色，斜坡是橙色的，你可以看到随着时间的推移，当它进入斜坡地形时。

两种不同的隐藏状态，他们的反应变化很大，一旦它回到平坦的地形上，海登州恢复了平坦地形的行为，在特征空间中，这些也是非常不同的，在这方面，你知道的，走在不同的地形上，还有就是，当然啦，外部干扰。

有我们应用不同种类的干扰，但是你知道，只是为了好玩，我在展示一种，你知道的，它被一个尤加球击中了，在这个过程中没有机器人的伤害，你知道的，另一个自然的问题，当然啦。

你可以问这个问题我们能更接近面具建模吗，例如，现在我们用变压器，那太好了，它是因果转换器，但是他们能通过数学建模学到更多吗，例如，不仅通过加固，学习答案实际上是你可以，当然团队又进行了一次。

伊利亚没有我的参与，我们可以分类，例如，作为下一个标记预测的人形运动。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/5bdad4e6246c78b7b0c06444789d8201_59.png)

好吧，现在每个人都遇到了同样的问题，现在终于发生在我身上，我真的不知道这个词到底是什么，但不管怎样，我会谈谈的，意义，就像在模拟中一样，我们已经有一群控制者了，例如，你在报纸上看到了，我们收集轨迹。

嗯你知道，就像你知道的传统机器人机器人控制器，这是第二个数字，第三个是你知道数据的MOCAP，所以这就像，你知道你在一个空间里有这个单词摄像机，我们捕捉，例如，人类如何行走，然后第四个是你知道的。

用于计算机视觉算法的三维重建，例如，人类是如何从视频，所以你可以看到不同的，非常不同的数据来源，你知道我们也尝试用不同的方法来注释数据。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/5bdad4e6246c78b7b0c06444789d8201_61.png)

所以你可以训练，当然使用完整的数据，所以它的作用是，你刚刚加入了弹道，你可以预测轨迹中的状态，但也很自然地，现在你可以用口罩数据进行训练，所以再一次，那看起来像一只鸟，所以我这里没有幻灯片。

你将能够看到你将能够看到的，其实从报纸上看，门模式比基于Moto强化学习的控制器更好，它也跟随你的命令更好的意思，就像如果你想走路，例如每秒一米，嗯，它能跟得更近，听你指挥，说到这里，前面是什么。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/5bdad4e6246c78b7b0c06444789d8201_63.png)

在…方面的权利，例如，申请，嗯，我首先想到的是自然和环境，例如，这是你知道的，你可以用SAM，例如，追踪不同的野生动物，并能够研究它们的模式，或者只是对嗯的统计分析。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/5bdad4e6246c78b7b0c06444789d8201_65.png)

动物甚至树木，例如，也是科学发现，嗯，生物学中的一些主要问题实际上只是人类的劳动，我们要训练，你知道的，这么多生物博士，他们有点，你知道的，小黄人做着主人布置的大量实验，在这种情况下，他们的博士后。

这些实验，如果我们有，例如，计算机视觉技术，在未来的机器人技术中，我们可以你知道。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/5bdad4e6246c78b7b0c06444789d8201_67.png)

简化此过程，第三个是增强现实，因为你知道当它涉及到现实，你知道你必须摆脱仅仅是互联网数据，这些基于互联网的模型，因为你真的需要进入杂草，好啦，你知道我怎么做，你知道的，去拿点东西。

或者我可以用这个做什么，我在哪一步，嗯，你也知道给予，例如，对用户的实时反馈，所以这真的涉及到对现实世界的理解。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/5bdad4e6246c78b7b0c06444789d8201_69.png)

第四个是自动家庭，嗯，所以这有点，你知道的，在过去的许多年里处于技术创新的中心，我们都梦想着一个地方，在那里我们可以做得更少，让机器，以及他们是否，你知道的，摄像机和机器人处理我们所有的任务，例如。

你知道当你出现的时候，它向你问好，它知道你的模式，你喜欢做什么，你在家里不喜欢为他们做什么，你的特质和行为，甚至在未来部署机器人可以为你做饭，可以帮你打扫你的家，我知道我们离那个现实还很远。

但部分还是可行的，已经可行了，例如，只是这种，你知道的，家里可以的摄像系统，真正理解空间的人，我敢肯定在讨论区，我们会讨论我们最喜欢的模特。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/5bdad4e6246c78b7b0c06444789d8201_71.png)

或者像我们希望在未来看到的现实世界的应用程序，所以最后，当然啦，放置欢乐，不错的机器人视频，当它走过旧金山所有这些美好的地方。



# 2024北京智源大会-多模态模型 - P6：圆桌讨论- Multimodal Perception and Generation - 智源社区 - BV1sT421a7FU

好大家好呃，对今天很很高兴有这个机会呃，就是精心准备了一个圆桌的环节，就是我们刚刚也听到了很多我们动态，包括视觉语言最新的一些进展，然后呃整个领域的进展也非常快，我们呃也想借这个机会。

然后有呃几位非常资深活跃的这个学者，然后我们讨论一些呃，现在领域内比较核心的一些问题，然后当然也包括技术的，或者包括一些这个high level的问题，然后呃刚刚在座的三位老师，大家刚刚都已经这个呃。

听过他们的精彩的报告了，然后线上我们还有两位呃两位嘉宾，然后呃我不确定已经，还没有呢啊对对对，我先介介介绍一下，对这个是呃家辉，然后家辉呃啊，之前做过很多很多非常非常突破性的工作啊。

包括这个呃这个party啊等等，然后他现在是open i的perception team的负责人，然后今天也是很荣幸有佳辉加入，我们来一起讨论啊一些问题来对，然后呃。

这个待会待会也很期待听听家辉的一些见解，佳慧能听到吗，哎感谢邀请，能听到啊，好的好的，然后另一位还有呃沈春华老师烧水，大家好，大家好，我我我很不好意思啊，我这个笔记本倒了半杯水，这个摄像头坏掉了。

沈沈老师，大家我相信也比较了解，他是国内我就比较资深的学者，也是我自己的这个导师对，然后，好那我们今天就直接开始，然后我呃准备了一些问题，然后呃不一定我能问完，最后希望呃就是给大家一些提问的机会。

然后对我们就直接开始吧，然后呃第一个第一个问题呃，我觉得也是呃开场一个比较好的一个问题是，大家觉得就我们看到的视觉跟语言的这个发展，先是in internet的，这个啊带领了我们很多年的发展到语言。

那呃在现在这个时间点，大家觉得视觉是AGI的核心部分嘛，然后有可能以什么形式贡献，并且能把我们往AJI的路上啊带到哪，然后呃，看哪位老师先只有一个话筒吧，都有都有都有，要要不谢老师呃，我能听到吗。

啊啊呃呃他应该能看出来我我我的回答肯定是，我当时觉得vision会是通往AGI路上的一个呃，一个这个critical path中的，中的中的很重要的一部分啊，至于为什么是这样。

我就我想说可能想问在座的各位，你愿不愿意丢弃你的为人对吧，我觉得呃我我我觉得可能应该大大，大家可能呃并不愿意做到这件事情，我我觉得我觉得是这样，就是呃另外一件事情是HI，我觉得跟很多它它不仅仅是智能。

它对human experience，emotion等等这些非常nuanced的，很难去通过语言capture的东西，也是紧密相连的，然后我觉得vision作为一个我们很重要的一个。

sensory的medium，我觉得是必不可少的啊，但可能之后可以继续讨论一下跟TECHNI个问题，但是至少我这边的观点是，我觉得vision是必不可少。

然后我也觉得大家应该work on vision more，好谢谢老师啊，我呃我的观点也是微信是必不可少的，但可能因为今天在座的几位，都都都都都是做微信perception的。

所以所以我估计可能大家观点都差不多，对我我觉得嗯从一个角角度来说吧，就是那个呃其其实就说分析人的这个认知的话，嗯就就就说有有有一类叫叫叫叫，叫做是explicit这个memory啊，就是显示的记忆啊。

那部分就说你你的那些寄到了一些东西，学到东西是可以用语言来进行描述的啊，这个是explicit memory，然后其实还有很大的一块叫做implicit，这个memory啊。

里面其中有一部分叫做procedure，这个MEMEMORY啊，就是过程记忆，或者就是影视记忆下面的过程记忆，比如说你学会怎么游，怎么游泳，怎么骑自行车，怎么投投投篮，比如说用one motion啊。

一段是投三分啊，我最近在练玩模型，我练了快两年了，最近总总算是练好了，然后结，结果刚打打了一两场学校的教职工篮球赛，结果就受伤了，然后打不了了，就呵对，然后就是这些东西的话。

就是这种on procedure memory为代表的这些东西，其实它不是用语言可以很精确地进行描述的，它是你的一个experience，它是适于要需要，就像刚才谢谢在领导说的这个sensory。

grounding啊，就是它是通过这种方式去进行学习的，所以我anyway，我觉得视觉是非常重要的，我说完了哎，我觉得啊，前面几位老师讲的都已经啊基本都讲到了，然后包括我自己的呃，talk里面我也提到。

就是我我相信呃real world experience是需要vision的，如果我我退一步讲，就是之前之前唯一没有提到的，是一个可以较为哲学的观点是什么呢，就是到我们到底为什么要vision对吧。

生物体为什么要要vision，那么你可以想象就是比如说曾经的哲学家，比如说嗯这个亚里士多德的说啊，我们要要要知道往哪看啊，那这个特别的有点形而上学的感觉，但是你要如果想从进化的角度来说。

那就是纯从达尔文的角度来看，那我们为什么要vision呢，哇你要繁衍生息对吧，就是make babies that，Can make babies。

That can make babies forever，嗯但是这个好像也不是很就是特别的呃，令人满足，这个这个答案，我觉得很多时候是一个一个不只是一个这种，这种竞争的关系，或者生物体之间的竞争关系。

还有一个就是一种从真实世界中，学习和和交互的一个过程，然后我觉得从这个角度来看，a vision应该是在智能形成的过程中必不可少的，家辉有什么补充吗，呃我很同意大家的观点啊，就是说我觉得啊。

东莫泰是这个AGI的必不可少的部分，然后当然就说理由也跟那个上面说的很定，有点像了，就说我们怎么来看HI对吧，定义是什么，那有一种定义，就是说AGI是希望能够就说完成大量，人类能完成的。

具有经济价值的合法的一些任务啊，那那我们可以想象这个事情，就是说假设明天一起来，我们就是不看任何东西啊，你可能本来一天能完成20个任务，你看看就说我们还能完成多少个任务对吗，就是呃所以这个是呃。

就是说非常显然的一个答案，当然就说就说大家大家都说这个呃，这个是需要的，就是说我觉得有一种情况下有可能是不需要的，但是这个也跟那个AJI的定义有关，就说我们今天讲AGI呀。

我们还是希望它能够benefit human，就是啊并不是简单的一个智能体，在那里就说自己跟自己，我觉得有一种情况下可以想象，就说如果世界上都是这个硅基的智能，就说没有没有人类的话啊。

有有可能他是不需要一些感知，或者说视觉，或者说某种这个啊传感器，但那个就是说跟我们今天所定义的，AGIS不一样，所以回过来就说AGI是要服务人类的，那他是需要这个啊感知啊，需要这个多模态啊，这些好。

谢佳慧稍稍时能听到吗，呃能听懂能懂，对那个我非常同意刚才家辉家辉的观点啊，就是这这个额视觉肯定是AI的一个必不可少的，一个component，但但是不是A加A，那这个就完全取决于你怎么定义A加I了对吧。

那我想我们在做都是做视觉的，那这个它的重要性这个就不用再debate了，是吧啊，对对对，是的对，我们先建立了这个共识，然后这个对，而下一个，我觉得也是现在大家相信都非常感兴趣的，一个问题。

就是我们看动态的任务，包括我们这个panel的topic叫mari model，perception and generation呃，我们可以把任务呃。

简单的分成generation和perception，那在现在这个时间点，就是大家觉得呃生成跟感知的关系是什么，然后动态的生成跟感知应该统一嘛，然后这个对，我就相信这这大家可能会有不同的一个看法。

然后佳慧要不你先来来得谈谈你的观点，因为你可能是这个最最接近的这个嗯，对我是研究过程中，是特别想把两个事情在一起做啊，然后我相信对我们在场所有的研究员来说，几乎就说这个答案是肯定。

就是说我们希望能把这个事情更尽可能简化，尽可能去unify啊，所以我觉得就是那有可能对工程来说，不一定是啊一个同样的这种这种这种希望啊，就对研究研究员来说是大家都喜闻乐见的事情。

那那这个问题我觉得关键还是在于说，我们有这个初衷，我们喜闻乐见这个感呃，就说合在一起，但是能能不能做到对吧，今天的有没有足够多的啊，证据说这条路应该是the way to go啊。

这个我我觉得今天就说一个领域共识定共识，可能还是还没有到那一步，就说还没有到，说我们只往这个UNIFI的方向上走，其他的跟ENLIF不相关的啊，直接就丢弃啊，这个我觉得今天来讲还也还是不对的啊。

所以所以如果今天来看，这个是不是应该unify，我觉得应该unify，但是就是还是evidence driven，就是说不能说我们要一条路摸到黑，就是对整个研究领域来说不能一条路摸到黑啊。

因为今天他就说以不盈利，fire的方法也能带带来很多任务上的这种突破啊，给大家带来很多帮助吧，好谢谢家伙，听起来就是还是有很多的这个研究的问题，我们待会也可以讨论到，然后特特对我。

我其实跟家辉的观点呃是完全一致的，就是现在我们作为这个研究的本身，肯定是希望他们他们能够成为一个主体，但但是呢现在主要是涉及到，我觉得工程上的问题，嗯暂时来言是是没有看到这个这个点的啊。

但有可能比如说是因为我们的这个嗯怎么说呢，learning objective不对，或者是我们我们什么地方还没有做做清楚嗯，这个还不好说呃，但是退一步讲，如果我我觉得，所以这是一个非常值得研究的问题。

但是如果退一步讲，比如说如果嗯我们是是否是因为现在的呃，比如说现在的呃paradigm有局限性，或者说它没有成为一个，比如说真正所谓的general intelligence的这样的一种能力。

使得它两种方式不能融合啊，这个也不好说，只能说这是一个非常非常重要的问题，我觉得作为作为研究本身需要解决它，好谢谢特特戴老师啊，我没有什么更新的观点，我完全同意两位的观点，所以我就不说了，好的谢老师呃。

那我给个更这个definitively answer吧，我觉得呃一定会统一，并且我觉得应该需要统一呃，我个人想法是这样的，我觉得当然大家研究这个general classify，已经有很长时间了。

然后并且从BAS的角度来说，你其实这个p s given y p p y given x，这两件事情其实是可以互相转化，所以ITIDTA也聊到对吧。

就是你your diffusion model is a secretly呃，As you shall classified，就这件事情我们已经能够看到一些signal，那从我的角度来说。

或者从representation learning的角度来说，我似乎觉得china mol是一个很不错的，去学习reputation的一个办法，因为呃简单来说就是说。

如果你现在只model like p y given ax，你的public density，其实只是在这个wide space上去compete的，不管这个Y是呃。

One thousand labels，或者是这个这个这个这个fix5CAPRA这个language，它还是一个比较离散的，比较有局限的一个空间。

但是如果你现在要model这个这个p x given y啊，那你的density是要在整个的这个large的这个，VODOM面上去compete的啊，这件事情必须带来的结果是。

我们要对这个image或者video或者整个的view content，会有更deeper的understanding，我们需要知道呃，比如说一只四条腿的猫。

要比一只这个三条腿的猫更likely to happen，然后这件事情我觉得是一个很N吹弱的问题，并且我觉得呃至少我个人而言，我觉得这是一个对understanding来说。

一个一个非常critical的事情，对啊，当然具体的时间上我完全同意前面嘉宾说的，就是我觉得我们不知道该怎么做，我其实对对谢老师，谢老师有个问题，就是我觉得这个在理论上是是很有意思的。

那你觉得这里面比如说呃在probability上面的modeling，它到底是应该是一个什么样子，laden space去model这样的probability。

比如说在general model的角度来说，它很多时候是pixel space，那这个的space它可能不是一个最好的，比如说laden space去去学习这样，比如说四条腿猫。

三条腿的猫这个概念对吧，right对呃，这就回到一样的观点对吧，他说we we should not work on general models。

Because we don't care about this，Kind like very deep construction。

we care about some real representation in some some form and latent space啊，但我觉得这些都是open question。

但是again maybe嗯，嗯这个final summary就是我觉得呃看到so2之后，我觉得其实这条路线是有可能能走得通的啊，然后具体的representation到底应该怎么做。

这些我觉得我们需要更多的research，沈老师在有呃，对这个问题有什么看法吗，喂沈老师，沈老师应该电脑又坏了，呵呵哎好，我们先待会待会沈老师再接进来了，然后我补充一点啊。

就是我们刚才讲到这个生成和这个感知，在视觉这个堵门啊，我觉得有可能我们需要再跳出来看，就是说有可能视觉和，比如说文字和其他的一些signal，就说它们放在一起的时候，也可能帮你做生成，它就能带来感知呃。

然后如果纯视觉，它也能带来一部分感知，但并不是就当我们讲，比如说理解的时候，今天还是再把一些visual的concept map到一个，比如说texture的concept。

比如说我们看到一条狗说这个是golden retriever，但它其实还是一个文字的表达，就是说以文字给它来约定，所以当我们在讲，就说是不是生成和理解要统一起来看的时候，就是啊。

有可能把这个视野再拓宽到其他的一些，signal上啊，有可能这个答案会更肯定一些，就是更更赞同那个赛宁老师的这个观点嗯，嗯嗯是的，对，我们后面还还还诶，沈老师，大家能听到我吗，可以可以哦。

我刚才这个麦克风也出问题了，这个很奇怪啊，哎anyway，呃，对我我我我接我，我补充一点，就是其实这里面有一个呃，就是从某些能力的角度来讲，就只因为你你我我们我们在谈感知嘛，CEPTION嘛。

那以前在视觉里面，几乎所有的CEPTION的任务都是用呃，DISCRETIMODEL在做嘛对吧，从基本上从八九十年代一直到呃，一直到最近这这这这35年前对吧，那么就就所以从我们新人利的角度来讲。

其实这这就是generative，Mogenerative model，Versus，这个discminting model，那以前为什么generative model做不好，这。

这里面当然最最主要的原因就是就是这个，computing power和和这个数据的问题嘛，对吧，这个高性max model就是一个general model对吧，这个这个在在在之前小数据的时候。

这个几乎是这个用处非常非常小，所以呢我我我的我我的一个理解，就是说以前不是说大家没有意识到，general to model的重要性，而是说啊受限于很多这个当时的这个呃限制。

他他没有办法去去把这个去训练，非常大的这个jn general timodel，没有办法把这个jn t model的这个呃，优势给发挥出来，所以不得已而为之去去去用这个discreen model用啊。

早期的support vector mach啊，boosting啊，后面这个用呃，这个CNN去训一个classify等等等等对吧，那么今天就是说从呃从GPT的呃。

这个success到到到现在这这最近这两三年时间，大家看到啊，这个general timodel，很显然是能够更好地利用大数据对吧，他这刚才赛林老师也提到这这这这这很明显嘛。

他他可以把这个dist性给学出来，那肯定直直接比你去去用啊，Discretion of discretion model，直接学一个mapping，他能他能学到更多的信息嘛，所以理论上来说。

你如果能把这个genental model做好的话，那perception的很多问题，可能就自然而然的就就做好了，就不需要呃，用以前的那一套这个discreen model的这种做法。

去去去去解决这个perception的问题，我就我就说这一点嗯，新龙我能我能在泡可以补个很短的简短两句，就是我就非常非常同意沈老师的观点，而且我我我我想在座的很多学生，其实是没有历史包袱的。

就是他是活在这个java i的时代对吧，但回过头去其实可以关注一下，比如说甚至30年前，40年前呃，比如说alan you，然后包括这个我PCD老师的作文。

他们那时候有这个analysis by thesis的学派对吧，就是大家认为对图像的理解，对图像analysis其实是需要对图像这个钉子的，就是生成来做到的。

其实有很多很非常有意思的discussion，并且阿兰佑也呃，最近也给了一些talk，在YOUTUBEYOUTUBE上都能看到，我觉得对我来说是很有启发的，虽然怎么样。

bridge他们这些old school的这些concepts to something呃，Can be can be deployed in in in in the current error。

I think it's another question，但是我觉得呃呃again，故个人而言，我觉得非常有启发，谢谢老师，然后对我我插入一个问题，本来没有准备，就刚刚佳慧也提到的呃。

可能就是大家可以考虑不同的模态，然后就我们想现在想统一的，包括研究上的大，除了生成和感知这两个任务，我们其实对于不同模态到底哪些模态应该统一，我们应该一起考虑，然后哪些模态可能是分开考虑。

大家现在好像没有一个呃比较成熟的共识，比如说我们是图片和文字视频，还是说音频，这个甚至更多的包括robotics的信号，这个大家呃怎么看待，这个只在呃更统一的多模态的语境下，不同的应该用哪些模态。

然后哪些模态可能带来更强的呃，这种这个SCALABLE的能力，然后他应该是有机融合的，这个对大可以分享一点，看如就是自由选择，可以有什么可以所有模态，所有模态，所有模态，比如说比如说视频，音频手势呃。

触觉呃，听觉呃，包括呃所谓的呃情感啊，这样所有的东西应该都在一起，因为很难去想象，就是呃我至少可能是因为我我们的自身的限制，就是我们学习的过程，永远都是所有的模态在一块的，而且很有意思的是。

其实东莫泰这个点主要在过去呃几十年里，很多时候是media study，是学communication的人在在用的一个词，其实不是在AI里面，是是这最近才才开始出来的，对吧，所以这就是回到了一个。

就是一个学习和沟通的一个过程，比如说嗯我用一个非常平和的语气跟你说话，和我用一个非常呃aggressive的语气跟你说话，这个我所表达的信息是是不一样的，然后这个信息其实对于呃文化而言。

呃是也是不一样的，比如说在在呃这里的文化也，同样的手势，可以在另一个文化里面，代表一个完全不同的意义，嗯这就说明呃我我猜测就是它不是一个嗯，他是一个怎么说呢，是跟学习是一个互相交互的过程。

你不能说我先学好了以后，我再去做多模态，而是我觉得你要从头就开始，从所有的模态一起开始学习，因为我觉得这是一个是一个非常，comprehensive的过程，好谢谢特特佳慧，有什么补充吗。

刚刚我也我也补充一点好了，就是说呃我觉得动作太本，本质呢它就是在某一个时间点有不同的信号，它在同步发生啊，那你为了就是说决定下一个时间点做什么事情，有什么事情会发生。

那就是说你必须得融合当前这个时间点的，所有的信号，所以这个在我看来就是说更加确定一点，就说比这个是不是应该生成，和这个理解应该统一，我会更加确信，就是说啊多个模态之间应该是要统一起来。

然后统一起来会发生，就是会能做更多的事情，好谢谢佳辉，沈老师或者对我说两句，就是说从research的角度来讲，从从我们在academia做做做research的角度来讲，那呃理论上来说。

你肯定你可以把所有的模态，只要你有数据对吧，可以把所有模态的数据都都都用起来，去训一个这个嗯transformer的模型，这这里面其实呃很重要的一点。

就是说因为因为现在现在就是放在transformer，或或者以前这个应该说是sequence to sequence，learning这个框架里对吧，这个这个现在几乎所有的模态。

你经过一个token ize之后，把它把把把这个input做成一个sequence，那你就很自然的就fit到这个transform，这个框架里，所以现在现在这个框架来处理啊。

不管什么样的模态都都非常方便，那在以前是做不到的，对不对，在在在在在LT在sequence to sequence learning，这个出来之前，在transform出来之前。

你不像是两维的视频是三维的，然后这个audio是一维的等等，你都要去啊，用用非常复杂的这个feature extract or去去做，做一个呃可以认为是呃pre processing，所以非常复杂。

OK那那现在非常方便，那那理论从从这个算法的角度来讲，你不管什么样的模态的数据，你你算法几乎不需要做改变嘛，所以所以很方便，就从从从从这个嗯从这个research的角度来讲。

那从application的角度来讲，那这个这个就取决于你到底想想，build什么样一个系统啊，对吧呃呃你如果做robotics的话，是不是需要把刚才特特说了，所有的这个这个有呃。

就是能采集到的数据都加进来，也许吧，但是anyway，这个这个可能这就从工程的角度来讲，可能会牵涉到一些什么data imbalance啊，等等这方面的一些问题了，好谢谢尚老师，我我稍微补补一句吧。

对我我就说，反正就是说这个肯肯定又刚才大家说了，就是多模态这个事情，从科研的追求来来来说，或者从长远的目标来说，这肯定是一个非常有吸引力的事情，当然说我我自私自己，我们肯定也是在努力在追求。

但是呢反正就实际做的时候，你真的要scanner up啊，然后做一个性能非常强的，一个多模态的大模型的系统，比如吃进去全互联网的数据，这种级别的来说，它中间还是会有非常多的现阶段的一些挑战的。

嗯来来自于不同的方面，对就说首首先我们观察到的，我们的感觉是觉得模态往上加的时候，它的有时候训练的复杂度带来的是几何层数的，在网上面在增长，如果你真的要训一个原生的多模态的话。

当然就说这就是训练方法上的问题，然后以及所谓的原生多模态呃，如果真真的要做做，做到说是在同一时刻，然后几件事情同时发生的话，对你的数据的采集，然后呃什么的，就因为现在这一代的算算法。

它其实对数据的利用效率并不是那么高的啊，就是成本这些会带来非常多的现实的挑战，所以我个人的观点，我会觉得原生多模态啊，就说从头开始训所有模态加进来，同一时刻什么的，这种训呃。

我觉得会是一个非常美好的长远的追求，但在现在的时刻，我觉得有很多的现实的问题需要去解解决，然后以及有些事其实会促使我们去反思，这一带的这个AI的这个训练算法等等，中间存在一些本质的问题啊。

比如说有灾难性遗忘啊等等等，带来各种各样的问题是吧，好谢谢戴老师，我可以说两句吗，可以可以可以呃，对我我我完全同意呃，Like all modalities，嗯matter。

But they are not equal，我只想说一件事，就是也是回到之前的talk，就我觉得language这个东西，可能要考虑单独拿出来看一看，就是呃就是就第一是它太强了啊。

会导致会影响其他MODITY的学习，第二是这个东西真的是人类特有的东西，然后呃至少对我个人而言，我还是比较希望能够看到某种意义上的animal。

intelligence或者cat intelligence，before a g i的啊，当然我觉得like this is a long term research，刚开始出来一点。

就是当我说到就所有模态的时候，可能还是比较就是赞同呃，塞宁的观点，就是所有的国泰应该在一块，但是我们不想要看到的是一个模态，去dominate其他的所有的模态对，尤其是当这个language。

比如说赛前在top里面也提到了，就是language上，你非常容易就像有一种cheating的感觉，因为它是一个所谓的人类特有的，至少我们认为是人类特有的东西，当它有的时候。

你可能很快的把它跟跟intelligence连到一块去，那那这时就会发现，就是我们在evaluate一个一个系统，它到底有没有INTELION的时候。

可能会有bias towards language啊，那这会是一个问题，但你说我们比如说做做一套系统，一个human computer interaction，他也必要有language。

他一定要有language，不然我没有办法跟他沟通，对，可以没有language吗，我觉得是是呃wow，我觉得在一个理想的世界里面是可以没有language的，因为比如说假设我们人不存在。

那我们啊就假设另一个智能体吧，那把它们丢到地球上，那它不见得一定要有language，他可能只要有一些简单的这种沟通方式，甚至甚至假设如果他们都有什么，我们从科幻的角度来说。

他们有这个可以用无线电信号去互相沟通，那他把他的这个hidden state发给你就好了，其实没有必要，没有必要用用language去跟你沟通的对吧，language是一个人特有的东西。

它是一个非常compact的东西，是因为我在这说话的速度，我不可能就跟你用这个呃，呃wireless signal这个速度去跟你跟你沟通，所以这个是我觉得是一个现实逼出来的，一个一个东西。

而不是他说是一是不是一定要一个intelligence，必要有的东西，非常希尔特非常棒的这个观点，我们播放到下一个下一个问题，然后呃呃就是我们刚也讨论到了，有不同模态的这个呃任务。

然后包括也有生成感知啊，理解的这些任务，呃，但是呢我们目前好像还没看到一个类似CHEGP，这样大家所有人都能感知到的这种呃突破，当然大家可能有不同的这个看法，就是我想问一下在座的各位呃。

老师就是大家认为呃对于多模态的，不管视觉语言或者是其他模态的深沉和感知呃，的理解的这些任务，什么时候或者达到什么程度，算实现了对应的CHANGP的moment，然后或者说我们现在已经有了吗。

还是说呃还需要一呃更长的时间来让大，让大家都能感触到的这种啊多么high的，真正的类似CHEERP的这种moment，我要不我先来回答一下啊对啊，其实我我这这今天的PPT里面有有提到。

就我我我我的个人观点是那样的，就从生产力发展的角角度上来说，就是文本这个模态模态的chat g p m moon的，我认为它具有特点是亮的，首先第一点它在一些重要的任务上，它的性能足够好。

然后第二点的话，它是能够低边际成本的，泛化到这个开放的任务上啊，就就说你一个chi g b t挂在网上了，你可以呃服务上亿人的，千奇百怪的各种各样的请求，而你服务每一个请求，它的后后面只有计算的成本。

你并不需要研究员，很贵的研究员对吧，比如说在在线的很贵的家辉对吧，这样的研究员为你每一个模型去进行一个，每一个绘画，进进行一个特调，对我觉得就是呃作为一个产品，它到了这样的一个时刻。

我觉得它才具有这种爆发，或者说就是让大大大大大改，就是切实的改变这个现实，这种生产方式的能力对，所以同理，我觉得多多多模态其距离moment也是一样的，就说反正在一些重要任务上，具有非常强的这种性能。

同时呢它应该要具有就低成本，低边际成本的泛化到开放的这样一些，请求和任务的能力，这我的我我的观点，谢谢戴老师教会线上非常贵的研究员，有没有什么补充，对对对，这个问题来本身来讲。

我觉得有一点我是不太同意的，就是说呃听上去好像这个语言的发展非常快，然后视觉啊多模态啊，就是说发展比较靠后啊，我觉得其实不是这样，就说你也可以讲，就是当年比如说CN出来的时候对吧。

比如说minister在recognize的时候，我觉得这个也是chergifty moment，因为大家这个寄邮件啊，什么你不需要人去摘抄这个邮编对吧，这个其实也是服务上千万用户啊。

然后包括你可以说啊就是RESONNET出来的时候啊，最早也是在一个视觉上啊，所以我觉得这个下集bt moment可能更多的是，就是说每个领域的这个my stone大家大都有多少。

然后是不是在持续的推进这个max on啊，就是说对于这个问题本身呢，我觉得没有特别赞同这个问题本身吗，哈哈好的，谢谢焦辉，对哦，我我稍微说一句话，就是我觉得这里面还有个BASSN的问题。

就是CHAGBT之所以好用，是因为他能写出来这些东西我是真写不了啊，他能讲的这些英文我是真讲不了对，但是一个多模态的模型，很多时候这些回答尤其它比较vision centric，或者说啊像戴老师说出来。

比如VILM里面我们能support，比如说process mation detection setation，一个三岁四岁的小孩就能做到，所以我们对他的这个expectation会更高。

呃我觉得我觉得这件事情可能是一个怎么讲呢，我觉得我觉得既是一个挑战，也是一个机会吧，就是我们一定会对这些多模态的，尤其是vision centric的task会有更高的expectation。

会对他的错误更不能容忍啊，Which means，我这些系统得要有更好的reliability呃和robustness，我觉得他才能够真正能够能够，真正被我们日常生活利用起来吧，好谢谢老师，没有补充。

有补充吗，没什么补充，OK那我们来到下一个问题，就是呃对就是对于现在的很多视觉原任务，包括我们最近两年看到的突破，可以看到是有两条技术线在推进的啊，一条是这个，比如说以GBT为代表的这种。

AUTOGRASSIVE的这种model，然后我们通过预测下一个token呃，去去在语言上面大规模预训练，而另一种像呃diffusion model这条线，包括最近的SARA。

然后呃相当于dominant的两条线的这个发展，然后呃那这个问题就是说这两条路线呃，就是分别的优势是什么，然后哪一条路路径潜力更大，我相信这也是大家很多人呃关注的一个问题。

然后看在座的几位对有什么各自的看法，教会好像你也是最接近的对，对于这个问题来来说，对我我觉得就说首先呢两两种方法，本质上都是把一个比较复杂的问题给拆开来啊，拆成多步。

然后用大量的计算在每一步上面大量的计算，然后你再多步合合起来就是auto gressive mo，有可能比如我们以前有个很有意思的事情，是当我们做这个图像生成的时候。

能发现拆成1000步左右是比较好的一个渠道服，然后我们去看DEFASION的mod，发现他也是1000步在训练的时候啊，所以本质上是啊我觉得很像的做法，但是有一些有一些区别是这样子。

就是当我们比如说应该让更多的精力在，每一个这个方法的时候啊，首先呢就是肯定我还是持有，这个就是说要open money的一些，不能说一条路走到死啊，然后其次呢，就是当我们在真实在评估这两个方法的时候。

我觉得如果说diffusion这样这一类的方法啊，想要成为比如说一个一个universal的一个solution的时候，呃，可能更多的是要去看，比如说他对文字的文字的这个处理，能不能达到像这个啊。

auto regressive model的这种效果啊，那那我们反过来再看，就是说to auto regressive model啊，我们今天有人做，比如说图像生成，有人做这个视频生成啊，相相相对来说。

我觉得离这个diffusion的结果更近一些啊，所以比如说两个方法要啊，就说要防止它压住的时候啊，我觉得这个auto gressive，在我心里会分量会更重一些啊。

然后diffusion就是说不容小觑对吧，不要就是不要这个封封死封死对，好谢谢佳辉沈老师，线上有先生有什么补充吗，呃我我我补充一点，就是说因为你刚才提到免费信做个生成嘛，那我我我我我提供一个。

就是我我从另外一个，从从这个图像的representation learning的角度来来比较，这这这几个方法对吧，其实到现在是没有一个定论的，就是说你呃你如果是把它做成AAR模型。

那那就这个呃戴老师最近的in intervl，你今年那篇CPI的PAPERI，已经做了很多downstream的这种task，就说这个呃，呃放在多模态里面训练出来的这样一个V额。

这个image encoder，他的能力非常这个这个能力非常强对吧，可以去做，可以去做很多detection啊，segmentation等等都能做的不错，那也就是说类似于这这个这个A2。

这样的一个训练方式，可以训练到一个非常好的一个image encoder，Image representation，那那从另外一个角度来讲，因为刚才刚才是森林老师也提到clip是吧，就你clip的话。

那其实他就是在训一个分类器嘛对吧，然后他用的是他他他用的信息其实都是一样的，都是配好的image加上text description对吧，跟李迅呃这个动模态是一样的。

然后defence model其实也是一样的，对不对，defence model你你你训的时候也是用的，用到的监督信号都是配好的，image加text，那么现在呃我们我们我们最近做了。

做了一个简单的实验，就是把这个defi model，就stable AI那个讯预训好的defi model unit拿出来，就作为一个相当于额initialization。

然后去做很多下游的这种segmentation啊，等等这些任务，这个效果也非常好，那么我我想说的是，你看啊这这这几种不同的训练的方式，如果我从呃representation learning。

Image，representation learning的角度来看的话，其实效果都不错，但是呢到目前为止也没有一个，我没有看到一个公平的一个对比，说到底哪一种方式啊，训出来的这样一个encoder。

这样一个representation呃，更更powerful，其实我没有看到嗯，Anyway，回到这个，你原来的问题就到底是哪一种方式更好，我我觉得这取决于你到底想干什么对吧。

如果说如果说是从从考虑这个呃，representation learning的角度来讲的话，呃我我我现在没有一个答案，我没有看到一个答案，到底哪一种方式更好，也许最后结论是差不多的是吧。

在在这个相同的呃呃训练数据，相同的训练信号的这个条件下面，那我我就说这一点好，谢谢沈老师，然后我们下一个问唉，还有补充吗，还说我们呃都行，有时间吗，有时间还有半小时哦，诶哈哈啊，对我弄我。

我可能简单补充一下，我觉我觉得可能这个问题，最后也it doesn't matter，就是我就还是相信the beer lesson，可能最后真正刀妹的东西。

还是说你的computer在那之后可能大家都行，那这里面会有一个inductive bias的问题，就是我个人觉得auto regress model呃，对language来说还是比较自然的。

或者disctized tokens啊，diffusion model对view data也是比较自然，比如它会有cost f，会有这种这种这种啊，这种moi skill的概念在里面啊。

或者说但这两件事情反过来，比如说我非要去assume一个Rest can order，然后去从左到右，从上到下到下的去扫我的一张图，做order aggressive，对我来说似乎不是一个很自然的。

这个这个not advice呃，但again我觉得可能computer上去呃，那不一定不一定会有太大区别啊，但是也许在某些场景下面，这些inductive bias可能会变变得比较关键。

所以我会我会倾向于从这方面考虑考虑吧，谢谢，啊看到我对我其实我很赞同这样的观点，因为呃之前的话，其实大家呃像我之前做的一些工作，比如说在在看这个CN和VIT之间的区别。

到后来其实就zoom out的一点点发现啊，到最后可能呃还是呃就是compute，或者说是data，就是在实际的意义上来看呃更重要，那么在这些呃model architecture啊。

到最后就像是提供了一些啊，下游task in搭载bias，然后呢还有就是在实际意义上的选择，比如说嗯VIT它确实或者说是transformers，它更确实非常容易去model各种各样的data啊。

而不是说就是它，而且它是SCALABLE的architecture，那么这两点加起来，可能是更多人选择选择它的原因呃，我觉得可能整个领域在在过去有一些变化，就是大家zoom out以后。

可能看到的更多的是OK我在同样的数据，同样的computer下面，达到的效果是不是差不多的啊，现在看起来好像是差不多的，但是还是没有完全的定论，可能还是the blesson是吧。

好我们啊下一个问题可能是一个具体一点的，就是我们讨论到呃刚刚视觉语言呃，主要是视觉或者视频这个更多模态的时候，一个呃很难避开的就是这个encoder的问题，就是或者叫COGNIZER也好。

encoder也好，我们怎么把这个呃这个这个模态的数据给它，编码成呃某一种表征送送到后面的模型，然后呃这里包括我刚刚呃报告里面，其实也分享了一点观点，就是现在好像有一个不可能三角。

就我们对图像或视频的token izer，很难兼顾这个compact lossless和离散，就是紧凑无损和离散的这个啊三个方面，可能现在最多有呃有两个方面，然后而这个问题。

可能对于我们未来想要做更统一的动态，可能也是一个核心的呃呃研究的问题，然后也想看呃，在座的包括线上的呃几位老师有什么看法，家辉好像这个也跟你最相关，啊对不可能三角就是说确确实我觉得是不可能。

因为文章说呃好像解决了这个不可能三角，比如说它既是这个离散的，然后呢他又能表达的很好，然后他跑一个，比如说像这种ms coco caption task，那其实当你看这个caption task的时候。

大量的这个当你看这个excel的时候，它其实还是这个文字的style，决定了你这个ELELE的高低，并不是说因为它里面可能就有一只狗对吧，他说一只狗在跑好，那我现在换一个问题问我说这张图片。

比如说这个从左到右，第五个pixel，从上到下第十个pixel，那基本上是没法做的，所以我觉得这个不可能三角是不可能，那更有可能的反而是说针对某一个点，比如说针对lost ace。

我们在什么样的task上，希望它是lost st啊，然后或者说针对于这个DISGRATIZATION啊，我们是不是真的需要DISGRATIZATION，DISCRATIZATION有什么好处啊。

就是我觉得可能从每一个点去就说，或者找一个点去突破啊，反而能打破这个不可能三角啊，比如说我举个例子，能看到一些领域里面的一些工作，比如说这个就不通过这个disk ties，然后来做一些生成。

然后同时他也能把理解做，能做成为他这个带宽还是比较宽嘛，就是那个传过来，或者说这个loss is这一块啊，有可能就是我就放弃这个，我要seek这个pixel value的这个task。

我就是比如说做一些robotics的task啊，有可能能出来一个很好的token anizer，它又是discrete啊，从这个pixel的意义上，它也是loss seed。

但是他能啊lost list完成这个robotics的share task啊，对这我谈我的观点吧，好谢谢教会我完全同意贾辉的观点，我觉得从就是从你冯培教育的角度嗯，你呃如果要你不知道在下游。

task是什么的情况下，呃，这个你是不可能去很难去做业，几乎不可能做压缩，因为我完全可以去构造一个一个task，使得你你被压，被压到的地方是是我要问的一切，是我要问的问题，那那这个你是做不出来的啊。

嗯我觉得到最后而言，就像佳慧说的是找到一个balance在什么task上去，需要一个什么样子的指标，我觉得可能呃lost less是一个如果是我的话，我会是为我一个先会放弃的地方。

是因为没有任何的就是生物体是真的，Lost eless，因为如果劳斯莱斯的话，说明我们要存exactly，这个光子打到我们的视网膜上的每一个刺激，That's impossible，对吧。

嗯而且呃所以这些对呃，执行大多数的task是是没有意义的，哦我就说一句，我觉得这个token izer，the toizer是一个fancy name。

但我觉得我们讨论这些东西其实是originally，我啊south surprise learning，我stand up for，就是一开始大家要做SSL，目的就是为了去学习一个更compact。

更maybe a better alignment，Aligned with the semantics，Um uh，不不用说lost eless，但是比较adaptive，或者说比较flexible。

能够去soft1些task的一些东西，所以我觉得其实至少在我看来，我觉得这一部分之前的SUBSURPRISE，Surprise，sin approach肯定是没有达到我们的预期的，但我觉得之后呃。

这也是一个很好的发展的方向吧，好谢谢老师，大老师或者陈老师有补充吗，说我没有补充好的好的，那我们对跳到下一个下一个问题，就是呃呃对这个这个问题，可能大家自己可以选择回答。

就是我们现在大家经常会讨论这个啊，语言里面skin law，然后那对于呃视觉呃，视频就是包括图像视频或者多么high的里面，他的我们现在已经看到了明显的skin law嘛，然后他如果没有的话。

他可能是呃什么样的，怼大可以要不肖老师，其实我我自己也不知道，我我只能我只能说可能在VN里面，我觉得我个人觉得我们还没有看到skin all。

至少没有看到这种language的g p t moon这种counterpart，呃，然后我觉得呃这里面还有一个重要的问题是，这个benchmark到底是什么的问题，然后我觉得对别人来说呃。

想要看到scaling law，或者我们叫做scaling observation吧，也需要依托于一个比较成熟，比较reliable的一个benchmark。

或者说至少是一个evaluation的protocol，我觉得这一部分我们也得要想一想该怎么办，线上的两位老师有有什么补充吗，嗯我觉得如果拆开来看的话，对于这个生成来说。

spring law还是相对来说比较好做啊，对于理解来说，因为这个理解的任务，很多时候它是跟这个文字连接在一起，导致就是说当你看这个SCENO的时候，就在看这个文字的SCRN动呢。

还是在看这个比较的scrning NO对吧，就是那这个会没有那么清楚啊，当然有一种做法，就是说诶我通过这个刚刚才讲到的对吧，就是说通过生成来做理解，那我觉得这里面可能会出来一些SCT0动。

并且这个SCL0动呢可能跟下游的一些，你所care的任务有一些相关联，这个会是很有意思的研究课题，好谢家辉好，我再补充一点啊，还是再补充一点，就说森乐当还见到过一个我觉得是比较PHILLIAS。

就是说呃当当一些vision的这个研究员，看到这个这个文字的这个模型越来越大对吧，从这个几百兆到一个币，然后淘到100币啊，到一个圈能啊，然后就是也有点心痒痒对吧。

然后也去这个still这个vision encoder啊，上来，比如说一个币，然后五个币，20个币，100个币对吧啊这个我觉得是盲目skill呃，不是一个好的，不是一个好的风气吧，就是嗯好谢谢佳辉对。

我其实想就再补充简单补充一下，就是指盲目scale，因为这个让我想到了，就是因为我自己是aviation爱好者啊，所以如果我我看到，比如说过去这个航空发展的历史的话啊，有一个阶段是从啊啊，比如说更快。

就是从从莱特兄弟以后对吧，这个有现代的呃航空航空学，然后再到一个阶段，就是大家永远都要造大飞机，为了造更大的飞机而造更大的飞机，而造更大的飞机，所以比如说这个747应该是没记错的话。

是是60年代的呃产品产物呃，70年代可能已经开始开始使用了，但是你看现在的话，比如说呃747也好，或者或者呃380也好，其实已经不太产了对吧，反而与此这个取代的是可能没有那么大的飞机。

可能是嗯350或者七呃，呃787这样的更更加呃呃优化的，优化的呃产品，所以我觉得就像盲目scale，到最后他要有一个就是我为什么要去scale，可以做出更更省油，更经济呃，更更先进的飞机。

那么我为什么一定要要用这个就要造的更大，更重更耗油呢，这是这是一个很好很有意思的问题，非常形象的一个对比，我很想说一句，就是有时候也不一定真的是盲目skill。

比如现在你你你们做这个Eva clip的这些model对吧，有时候你要问我，说这个model到比如说180个变量或者怎么样，到底普通的应该怎么，我其实不知道，所以而且我自己也做不了。

所以我看到这些结果之后，虽然可能说没有想呃，像我们想象的那种skin law，对，对我来说还是一个很有价值的一个一个data points，所以或者说只有有一些人做了这些事情。

我们才能够不去盲目skill，我觉得里面还是有些research in inside的好，然后我们下一个问题就刚呃可能也提到的，就是呃数据可能是现在大家都知道最关键的，然后最重要的一个部分。

然后特别是呃，因为语言数据可能相对来说形式简单一点，那当我们想要讨论到呃更多模态的时候，呃，它就是我们现有的数据的数据量以及数据形式，数据质量呃，是不是满足我们未来期待的动态的能力的。

就该包括刚畅想的动能力的这个要求，然后呃未来的形势呃可能是什么样的，然后它可能是来自哪里的，就这里我们可以看到，之前包括呃，图文对这种数据在clip各种呃的成功，那未来呃我们应该期待什么样更新的形式。

呃这里也想看各位老师有什么自己的这个看法，对我如果self serving一点的话，我会继续说，就是real real or data，因为就像我的talk里面提过的，有有几点。

一个是internet data，永远总会有一天会用完啊，这是一个，第二个是internet data和real data，还是有有懂main gap啊，这是这是第二点。

所以呃我觉得all over evidence pleads to，就是大家要更关心的去在rewards，里面如何去去采集数据，或者是利用这样的环境去去学习，嗯对我我说说一下我的观点。

对首首首先第一点呢我完全赞同特特说的，就是我会觉得就是说动模态模型，未来它的最重要的应用场景，还是说是能够主动式的去跟现实世界打交道啊，就说那个呃机器人啊什么，这样就跟现实世界进行主动的环境的交互。

就是说他不只是观察，被动的等着人来喂喂，给他网上数据，说他主动的在环境中间去看，去交互去听，我觉得这个会是创造最大生产力的地方，然后第二点呢，就是我，我觉我会觉得其实这个会引申出来一个问题。

就是说其实就说我觉得现在的学学习的算法啊，就是说他的那个效率什么，可能还需要进一步的提升，尤其你跟现实世界去打交道，这些数据其实都会更加贵，然后更加稀缺那样的啊，然后就说以及你看像小小猫。

小狗或者人类的孩子，其实他并不需要那么多的语言的监督，他就能够学得非常好啊，小猫小狗跟根根本就没有语言的监督，但别人照样会很好的跟环境去打去打交道，对就就就说我觉得呃其实这会引申出一个问题。

就说反正我觉得现在这代学学习算法，其实还有需要非常多的改进的地方，对，呃对呃对我，我可能稍微有点悲观，是我觉得这个数据可能如果尤其是SKAL，数据只能被discover，但没办法真正去CLUCT。

就像我们做power，我完当然完全统一，这个real world的distribution是非常非常重要的，但我觉得有时候一些数据，可能要依托于下一代的某种硬件平台等等。

我们才能够去discover这些数据，我现在看起来一个比较promising的数据，或者说在中间的状态可能还是internet skill的video吧，啊但他主要的问题向导师也说。

这个不是一个interactive environment，但也许我们可以通过像呃用用上general model，用用用现在的各种three d呃，reputation learning的技术。

可以让他somehow能够bridge the gap between like real，and synthetic enrolments啊，然后再去develop新的算法吧，对这是我的一个Hope。

好谢谢谢老师，线上家辉或者沈浩时有有补充吗，我我我我我说一点，就是除了N6P以外，其实现在就是NLP cf super learning非常成功嘛，但是现在在别的模态里面点3D点云也好，图像也好。

视频也好，我我们现在根本就没有看到，就是一个能够啊工作的非常好的一个，self server learning的一个方法对吧，就是现在你东步态还是在就是就用用用用text呃。

就配好的text和图像在在做监督嘛，那如就是如果还是以这种方式的话，没有在self修法learning这一块没有突破的话，那我们去去去收集这个数据，你连收应该收集什么样的数据，你都不知道对吧。

你只是去collect那些在这recorded video也好，image也好，那你这个标注成本会非常高啊，如果还是还是以目前这种学习的方式的话，所以anyway我我想说的是，这里面可能还是有啊。

非常非常多的这个工作需要去做，嗯是的，谢谢沈老师，佳慧呢啊我也是一样的观点啊，就是说啊可能重复一下，就是说一个是呃，我觉得数据量不是问题啊，可能关键在于怎么学，然后学习的算法是什么啊。

比如说能不能有self superfice的企业这些事情啊，本质上这个世界时间只要再往前挪动的话，他这个动态的，动态的数据就会往前往前去增加嗯，但是能不能把这些数据用起来。

然后能不能把它用到自己感兴趣的一，些任务上啊，这个可能是需要更多研究的地方，好的谢谢，当然还还有一点，就是还有一点，就是说就是因为问题里面，还有说这个数据有可能从从哪里来对吧。

今天那个莉莉娅在赶飞机没来，就说我同事就是呃图像生成，我觉得也是一个很好的数据，是有可能可以用起来啊，然后有可能跟新的学新的那个学习的方法啊，如果能有一些共同的突破的话。

会有些会有一些很有意思的东西出来，好谢谢家辉，然后呃我们留几个问题给现场的这个观众，然后看大家呃有想提问的可以举手，然后我们工作人员可以递麦过去，那那那那里，哦我想问一下。

那个森林老师讲的第一篇工作里面呃，就是我记得当时是把呃clip换成了加，加上了一个dino v two，然后当时第一部分在找那个返利的时候，说的是找了两张图片，然后他们的呃cliff的距离非常近。

但是dio v to距离非常远，然后这样来说明说这个cliff可能嗯，visual grounding不够好，那比如说我把那个cliff直接换成dino v two，然后我找一个DINO距离很近。

但是cliff距离很远的，那这样的话会不会他也可以找到一些返利呃，对需要澄清一点，是我们没有在做某种意义上，adversal training或whatever，其实这些这些呃。

embedding space上，discussion都只是帮助human annotator去去帮忙label，所以human alt还是在中间一环，需要去控制这个流程的啊。

所以所以我后来也受了这个results，其实啊我我们可能没有是你说的这件事情，但是至少比如说这件事可以去把dal way to，换成其他的soft spice learning model，呃。

这个conclusion也是成立的对哦，那你有没有试过就是再加入一些其他的feature，就比如说把那个self supervised learning model也多加几个进去，这样会变得更好吗。

呃欢迎关注我们之后可能会release的一个陪伴，我们会讨论一些相关的问题呃，呃short answer是可能没有那么直接，不是越多越好对哦，所以就是这个self supervise跟cliff。

之间还是是有区别的，是的是的是的，谢谢谢谢，好我们可以看下一个问题，这边，嗯老师你好，我想问一下，就是这种视觉模型需要增强它的可解释性吗，就是可解释性对这个视觉重要吗，问问哪一位老师都可以。

看是形容你应该回答一下，你总对吧，你这问题问在座的回答吧，对您回答一下，12时12时有什么，我我觉得不重要，能work就行，不不是不是一定要有科技就行，有有当然好，没有也不妨碍我们去用嘛，是的是的是的。

但是如果是用于呃医疗的话，他可能是需要有很精确，他他是怎么得出这个结论来的，如果把这种影像和和这种文本用在医疗上的话，他是不是需要可解释性，然后怎么样去发展这样的一个模型，我觉得这个也有点绝对。

其实很多就是就是很很早很早以前不是google，google写了一篇paper，然后呃忘了是做哪一次，是skin cancer吗，还是什么，我忘了哦，不是skin cancer，是REINAREINA。

就是那个眼睛的这个这叫什么，Anyway，就就就就好发啊，发了一篇nature science嘛对吧，去去用用这个用这个RINAGAIMAGE，去去训练个风来器去啊，Detect diabetes。

他的他的这个训练集，这个数据集它是每张图片找了七个端，七个这个specialist，七个医生去标的这个啊做的这个标注对吧，完了之后就是嗯majority voting嘛对吧。

那那这个七个里面如果有四个同意，那那就那就用用用这个观点对吧，那我我想说明什么呢，你看这个医生去标七个医生去标这张图片，大部分情况下都标出不同的结果，来这个这个训这个这个训练，训训练好的这个医生。

他他能解释吗，他也不能解释啊，那那你到医院去，不就是在这样的医生在帮你看病吗，你你不也接受这样的结果吗，所以这个还是kiss by kiss吧，我觉得对不对，对对是的，我我我那个行啊，对焦辉嗯。

我斗胆那个提一个呃跟陈老师不一样的观点，我觉得很重要，就是一方面呢是解释性能带来一些新的insights，然后啊其实就keep learning呃，他能到今天我觉得还是非常magical。

就是好像突然一个时间点是为给你这个技术，然后这结束能到今天的这个结果呃，然后在在可解释性上的研究，有可能能带来一些deep learning，对deep learning的理解啊。

然后其他下一下就说下一个突破啊，尤其是就是说，如果你今天是研究这个课题的事情呃，我是非常鼓励你继续研究下去啊，不要说这个改topic啊，我都听，因为有可能就是说继续研究下去。

有可能是下一个这个比如说skin skin law，这个Impact level的研究吧，嗯相信相信自己就是后续可解释性，对我我简单就是echo一下佳慧刚才说的我。

我我理解的INTERPRETABILITY有有好几种，一种是工程上和使用上的INTERPRETABILITY，比如说OK我这个系统在医疗上用了以后，我怎么知道它是通过什么样子的逻辑来来做了，这个预测。

那这是一个实用性上的呃，可解释性，但是另一个可解释性就像佳慧说到的，可解释性是OK我，我能够指导我们以后的研究的方向，能够值得指导我们给我们提供这个deep learning。

这个过程中的insights，我觉得在这样的可解释性的基础上，它会OK那我们假设吧以后知道往哪去scale，用什么样子的数据，那是不是我们就啊，能够更好的把这个领域往前往前推下去啊。

更快的去去把model啊带到下一个level，而不是说我们所有的东西都要试一遍，那所以我觉得在这个角度上的话，我觉得是是必要的，但是如果是在实用性的角度上来说的话，就像大家已经讨论过的。

这个是在有些时候是需要的，有些时候是不需要的，要不我们啊对那边有个男生一直举着，直接说吧，呃我想请问一下各位老师，那个呃随着我们在这种视觉语言模型当中，加入了更多的视觉的训练数据之后。

在纯语言的性呃性能上面，比如说reasoning相关的能力上面，呃我们有看到一些呃比较大的提升吗，呃以及呃在可见的未来当中，大家怎么看这件事情啊，谢谢嗯，这也是一个对很好的问题。

我只我看到g p t four v的tech report的时候，看起来没有特别大的提升，所以我们就要去问一下家辉了，家辉哈哈，研究中不方便透露，看来，我觉得这个问题本质是这样的啊，就是说当一一。

而且跟这之前问的问题有点像，就说我们几个模台是不是要合在一起做啊，那这里面本质上问题是在于，就是说互相之间有没有这种positive transfer，就是你做的这个问题。

能不能transfer到另外一个问题，然后嗯，然后甚至有没有可能就说有negative transfer，Negative transfer，我觉得还是一个可解的问题。

因为positive transfer，这个就说只能说god god bless deep learning吧，就是啊希望是能有一些这个POITI的TRANSER啊，那有些情况下。

我觉得还是很容易能看到一些positive transp，尤其是从比如说语言语言对这个多模态，就说语言上，比如说啊很简单的例子，就是说你的这个语言的输出更加的标准化啊。

会导致在比如说这个当你图文问答的时候，也会更加标准化，就是更加符合你的预期，更加让你读起来通顺，那就是说图片这个事情能不能对，因为你的问题是，图片能不能对这个语言，reasoning带来一定的提高嘛。

呃我觉得这个是有可能的，然后这个是研究中的，好的好的，还得我们可能都得还得研究一下，我能补充一句吧，呃就我觉得还有一个EVOLUE什么benchmark的问题，我一定能找到一些task。

没有vision你的language什么的，Completely fail，我也可以找到更多的task，没有vision呃，压根不许就是这个PERLANGUAGE，Performance。

没有任何的影响，就是所以还是取决于我们，我们到底应该怎么去define这个problem，但我相信比如说如果回到刚才所说的，如果我们care real world这些这些task的时候。

我相信有很多很多的差词，是没有为人是不行的哦，好戴老师或者生老师有补充吗，嗯对就回回到你刚才那个问题的话，就是现在的情况是嗯，至少我们这边观察到的情况是就说是呃，你的问题。

刚才是说是那个对于语言任务来讲对吧，然后图文的这个多模态的训练，是否对纯语言任务有帮助对，然后现在观察下，我们观察现象是没有帮助，甚至可能会有一些坏处，但就说确确实这件事情如刚才各位所说说的。

反正是呃是有各种各样的空间，以及现实中间很多任务，就是说他其实是就说反正越往现实世界走啊，那就视觉这些东西的重要性会越来越多，对所以所以这个是我们的呃，到benchmark的问题。

还是说这一代算法有局限性，很好的追问，不是你不能criticize说本体Mark有问题，那存远任务的本期Mark，别别人就定义的挺挺挺挺好的，对我我觉得还还是就说是现现在的算呃。

现在的算法的问题或也也不一定，你说这一代算法的问题，我觉得可能是也许是一些工工程设，设计上面的问题什么的，对，好然后那我们还最后还有一分钟，要不最后一个问题呃，呃那位那位那位同学举得最高举的嗯，你好。

我有我实际上有两个问题啊，一个一个一个鞋码，一个我问一个问题是这样的，就是呃我有一个疑问是说，因为现在token的话，实际上是呃就是一张图在token呃，就是encoder的时候会token成。

比如说呃一个就是比如说有101024个，那个什么呢，就是token嘛，就是图片token以后，那么的话如果每一张图，如果它的分辨率都是1024，那么的话它token化以后，它是这些token的一个组合。

那么这个组合它代表的意义是什么呢，它是一个呃它是一个排列组合吗，还是到底它有更深层的含义，对就是关于对这些token的理解，对就是它呃虽然都是102，比如说都是1024剖面率，可是它最后的我们就是。

比如那个token的那个密码本，实际上都是1024的，那么它实际上都是这些的组合，我不知道它的那个内部的含义到底是什么，就是说就像一个文章一样，我们的话token完了以后是一些，我不知道他是最后。

我我是把它类比类类比成一个，就是说像是一种token写的一篇文章一样，我不知道是不是这样能理解，这个问题我可以稍微说一下，就说我几年前看过这个hot book里面的这个code，就其它做法也很简单。

就是说你把整个这个英拉斯的token，都要翻译成同一个code，然后你把decode一下，你看看你出来的是什么东西啊，然后我们观看到，基本上还是一些很基础的颜色啊，然后有一些这种啊边边角角啊。

一些pattern啊，所以我可能更倾向于说，在这些排列组合的情况下，加上你的这个decoder，它才能把整个这个视觉的信息给decode出来啊，那回过台就是跟刚才那个问题很像，比如说要不要可解释性。

我的我的我的回答是需要的，就是说我们可能需要有一些可解释性的东西，来看一下这里面到底在干什么啊，是不是人能理解，然后是不是能理解之后能把这个事情做得更好，谢谢好，那，OK没有补充吗。

OK那我们今天正好时间到了，然后今天也很感谢这个大家在这边，这个一起参与我们这个panel，然后也感谢各位线上线下的这个嘉宾，让我们有更多问题可以这个下面再讨论，然后也感谢大家参与今天的这个动态论坛。

然后下午应该这大会还有一些其他论坛，也欢迎大家关注。

# 2024北京智源大会-大模型产业技术 - P1：论坛及嘉宾介绍：王仲远 - 智源社区 - BV1HM4m1U7bM

下午好，感谢大家来参加我们的大模型产业技术论坛啊，就像今天开幕式上所说的啊，大模型在过去的这一年，已经从实验室的研究的成果啊，开始进入到产业界，那我们也在过去一年看到产业呃，各家产业界各家公司啊。

在陆续的发布各种的模型啊，包括语言的模型，也包括像纹身图，纹身视频的模型，那么因此呢在今年的资源大会上，我们也增设了这样的一个产业技术论坛啊，那希望能够请产业界的朋友能够来介绍。

在大模型训练啊以及推理中啊，所解决的一些实际的一些技术的一个问题，那今天其实啊我们也确实有幸邀请到了，基本上代表了国内最先进水平的，这样各家公司的一个大模型号，他们有爱思科技创始人兼CEO王长虎啊。

对01万物的联合创始人王文浩好，算法负责人康战辉，对快手视觉生成与互动中心负责人万鹏飞，对然后还有一些呃专家朋友呢，待会会陆陆续续过来，那我们首先呢会有请爱思科技的创始人兼。

NCEO王长虎来做一个主旨的演讲，王昌乌博士呢深耕计算机视觉，人工智能领域20余年啊，曾担任字节跳动视觉技术负责人，从0~1，支撑了抖音，KTOK等国民呃视频产品的建设和发展。

承认微软亚洲研究院的主管研究员，发表了近百篇国际顶级会议和期刊的文章，拥有数百项的啊专利，他今天的这个演讲的题目呢是AI视频生成的，过去现在和未来，作为通用人工智能的重要一环，视觉生成大模型快速的发展。

推动了通用人工智能的发展，那么本报告呢将回顾视频生成的历史发展，进而呈现当下视频生成领域，最新技术的进展和应用，以及未来发展的趋势和将要面临的挑战。



# 2024北京智源大会-大模型产业技术 - P2：Al视频生成的过去，现在和未来：王长虎 - 智源社区 - BV1HM4m1U7bM

啊特别感谢嗯仲远院长的邀请啊，非常荣幸啊参加这个论坛嗯，我是王长虎，那我仅代表一个创业者啊，跟大家分享一下过去这一年，食品生成这个行业从应者寥寥，到现在妇孺皆知的这一年中，我个人的所见和所想。

嗯其实我在这个行业里边，AI方向已经啊深耕了好多年啊，过去也在大厂工作啊，有朋友问我为什么想不开出来创业了啊，我想主要原因还是跟在座的很多朋友一样，我们看到了一个嗯浪潮的到来，那有人把它叫做AGI啊。

对比过去的呃专专有人工智能啊，所以在通用人工智能时代的到来，那我更嗯想用从用户啊，从内容侧来把它叫做AIGC啊，对应过去的TGC时代，UGC时代，那一个浪潮的到来一定不是一蹴而就的啊。

阴阳经过很多先驱者要不断的探索，尝试AIGC也是如此，他最早可以追溯在到啊上世纪50年代，在人工智能出现之后，我们可以看到在早期的萌芽阶段啊，11957年啊，我们就用计算机啊。

科学家们用计算机创作出来了第一个啊曲子啊，那个时候技术有限啊，所以所有的尝试都局限在啊实验室实验阶段，那从啊上世纪90年代啊，到本世纪10年代，这个阶段是AI技师的技术沉淀累积的阶段。

也诞生了很多著名的工作，包括我们第一个第一部人工智能创作的小说，包括这种全自动的同声传译系统系统，但因为算法的限制，使得啊这些工作还是很难去真正面向普通用户，那一零代之后。

随着嗯干啊深成式对抗网络的出现，一系列的啊生死式的工作还真正的啊面向用户，从嗯图像生成啊到啊大眼模型啊，到纹身图，人声视频，从干到transformer，嗯到diffusion啊。

从我们的耳熟能详的g p t chat，GPGGPT到我们的journey之后，到SARA这上开启了一个新的篇章，那AI级其实其实是包括多种不同的内容，包括语言啊，视觉音频等等啊。

在chat GDP出现之后，大悦模型啊成了大家关注的焦点，在骚扰出来之前，其实视频生成赛道还是没有这么火啊，虽然出现之后呢也赢得了大家的关注，那有人问我，那视觉大模型和大语言模型，主要区别在什么地方。

那在我看来，二者其实是两种不同的内容，语言是在人类出现之后出现的，是人类文明的产物，嗯是对呃世界的信息的抽象，而视觉在人类出现之前就在那里，山在那里，水在那里是原生的，所以当chat GDP出现之后。

人们对代言模型的期望是啊，模拟人脑啊，构建归一生命在对，那么人们对视觉大模型，视频大模型的期望啊，其实更外在的是模拟世界啊，构建世界，那视频生成，它本质上是通过对世界的理解来做影像呈现，那影线呈现。

实际上经历了从记录到生成的演变的过程，那这里列了列举了过去呃，三个图像呈现的三个重要的节点啊，我们可以看到我们在3万多年前，人类就已经嗯在岩壁上面去绘制啊，创作自己看到的这个世界。

我们也看到栩栩如生的狮群，那第二张呃图像是在1826年法国的啊，摄影先驱创作的第一个啊，可以永久固定的成像的这样的图片，那第三个是人类历史上第一个视频啊，用了24台啊，照相机啊，拍摄出来的啊。

马在奔跑的这样的视频嗯，这也缘起于嗯关于马奔跑的时候，试题是否腾空啊这样一个辩论，那影像的呈现，其实嗯影像的生成其实看起来是离我们很远，但事实上就在我们的啊，离我们很近啊，我们可以看到小时候哦。

过节的时候啊，我们家长也会，然后帮我们去制作各种的灯笼，其中一个就是走马灯，你看它的原理是什么，我们点燃蜡烛啊，蜡烛的热气上升啊，带动这个呃轮轴的转动啊，轮轴上面我们有一些剪剪纸，那光影啊。

把剪纸影子投射在屏上，我们先看到这样一个现象，那后者是一个连环画，我们小时候也做过啊，也是很是非常非常有趣的年代啊，我们可以看到通过这样的快速快速嗯翻页，就可以呈现出这样一个动态的一个画面。

那随着随着抖音啊去talk啊，这种短视频的普及和发展，那视频生成啊真正走到了每一个人的手上，那早期的视频生成其实是是基于检索完成的啊，视频里面每一个素材，这都是呃原有都是在数据库里面找到的啊。

通过自动方式找到合适的素材啊，通过拼接形成这样的视频，那现在呢依然能够我依然能够刷到类似的视频，那第二个是部分生成啊，我们这这这是一个一个抖音的视频特效，我们可以看到用户可以用手掌啊来控制水珠。

那这部分的生成嗯往往是需要输入一个视频，然后呢，通过AI技术做局部的部分的这样的生成，那这种技术广泛的应用在各种嗯，部分生产需求的特效之中，包括美颜特效啊啊包括动漫嗯风的生成啊，啊包括各种局部的生成。

包括比如说我们加个猫耳朵啊，加个狗头等等，第三部分其实是我们现在常说的啊，这种通用的生成，输入一个prompt嗯，输入一张图片啊，生成凭空的生成视频啊，过去这一年啊，这方面进展飞速，从嗯2014年开始。

嗯视频生成技术已经发展了有十十个年头了，嗯随着干的出现，这种生成技术才真正的啊使用起来，早期的氮的技术，数技术，将早期应用在很多的啊，这种前面提到的这种部分生成特效生成上，但是呢对于通用的视频生成。

效果依然是差强人意的，嗯直到diffusion啊，20200年diffusion模型的出现啊，击败了gun啊，成为了啊图片生成视频生成的主流，那过去这一年23年开始，我们可以看到啊。

很多的视频生成的技术和产品啊逐渐出现啊，包括我们嗯知道的像NVIDIA的嗯VRDM啊，包括google的point嗯，包括一些嗯真正面向用户的产品啊，run为P卡。

包括我们公司开发的TIKSOURCE等等，那当然今年春节SORA横空出世，开启了一个新的纪元，首先介绍一下这个嗯生成对抗网络gun，gun是二啊，2014年由good fellow提出的嗯。

他缘起于这样的博弈论中的零和博弈的思想，就是通过两个网络啊，生成网络和判别网络的不断博弈啊，进而学习到数据分布来生成高质量的内容，那他的，嗯他的优缺点也是比较明显的啊，优点是呃是能够生成啊可控的嗯。

效果还是在某在呃有特定目标指时候，能够生成较好的这样内容，缺点啊也是比较明显，就是训练不好，就训练难度比较大，我们要同时优化两个两个网络，同时呢多样性是有限的，很难做通用的生成，嗯干提出之后。

那很多这种变种就纷纷被开发出来，包括cycle gun啊，DC gun啊啊，包括info gun啊等等，广泛的应用在很多的场景上，那也有的研人，研究人员，希望能够把感染的技术应用在视频生成之中。

比如说这里面提到了demand，在2019年啊，提出了一种方法嗯，因为要做视频生成，所以引入不仅有red net结构，还引入了RN来进行建模，同时在判别的时候采取了双判别器，空间判别器。

时间判别器来做视频生成，那这样的方法嗯，事实上呃生成视频质量依然有限，那diffusion是diffusion，是一个呃非常重要的一种呃方法，干和地球真像支撑了过去10年嗯，AIGC啊。

图像视频生成的啊发展，那DEUTION本身是啊在15年提出的，在20年正式击败干之后啊，才真正流行起来，他的主要思想其实是通过不断的给图片加噪声，来破坏数据分布，然后呢再逆向地去不断地去造。

对数据进行还原，这过程中嗯不断的去逼近啊数据的分布，然后生成啊高质量的内容，对因为那啊DEFUSION模型嗯，早期啊一直是啊，呃包括一个unit的架构来用它来进行去造啊，也是过去几年最主流的一种模式。

transform呃，D diffusion，transform d i t的出现啊，验证了这种新的一种结构，可以更好地进行SCUP，从而生成呃更高质量的内容，因此啊随着骚扰的出现啊，DITT这种体系。

SION加transformer架构逐渐的啊成为了主流，diffusion加unit这种架构啊的一个很典型的啊，很经典的视频生成的方法，其实是哎VIDIA提出的这种VRDM，在这之前啊。

大家做视频生成尝试，其实这个工作流其实是多非常多变的，那这个方法啊第一次更嗯，好的提出了一个比较有效的这种变，最后来成为一个主流的工作流，这是显生成关键帧，然后进行插针啊，最后进行超分啊。

这样一个通用的工作流嗯，他的方法主要是用的嗯嗯TEACUSION加three d unit架构，并且这个模型能够支撑啊多个任务，包括关键帧认真的生成嗯，包括插帧和超分等等，视频生成能力啊。

和原有的视频生成技术相比也有很大的提升啊，嗯这个呃呃除了干体确认之后之外啊，也有很多人与人员期望能够用大语言模型技术，来解决视频生成问题，因为诊双M在大圆模型里面的巨大的成功呃。

有一些研究人员嗯也非常坚信啊，safer啊是视频生成的最终答案，这是其中一个代表性工作，是google在啊去年年底啊提出的video point，它采用了一个嗯decoder only的啊。

自回归transformer来端到端的去生成，用大眼模型的技术来生成呃视频，它允许多种模态输入，然后用嗯专有的TOONIZER来把这些内容token化，然后输入到这个video的模型之中。

嗯这个工作其实是嗯呈现的效果是非常棒的啊，但是和嗯V人工作一样啊，虽然是一个公开发表论文，但是并没有产品化，也没有开源啊，所以我们也无法去真正的让用户来测试，这种方法的好坏，那今年啊年初春节期间啊。

骚扰横空出世，进一步推动了这个行业的发展啊，sorry用的架构就是刚才提到的啊，DEFUSION加tr双门架构啊，用啊transformer模块来替代DEFASION架构里边的，嗯去造的模块unit。

同时呢啊，他在在也利用了大圆模型来帮助做prom的增强，以及嗯训练是训练数据的呃，精细化达标，同时在encoder和decoder这两个方面呃，两个地方也做了一些创新。

那关于嗯SORA的解读其实是嗯非常丰富的，那我就不展开介绍了，那在我看来，SORA它的最重要的一个贡献就是啊，验证了DIT啊，define transformer在视频生成中的啊。

skin law规模定律，就是说模型越大啊，我们的嗯嗯space spatial temporal patch越小，我们的可用的优质量数据越多啊，会产生更我们的生能效果更好啊，这是一个例子啊。

啊SORA曾经的例子，当我们基础计算量嗯嗯一定的时候，你可以看到生成的视频还是有很多变形的，但是呢随着我们计算量增加，四倍到32倍的时候，生成的质量是非常非常好，已经非常非常好了啊，这里有一些例子啊。

也是来源于SARA啊的网页，可以看到当我们的镜头生成视频里面，镜头在平移啊旋转的时候，物体场景在三维空间里面，依然能够保证保持更好的一致性啊，说明这个模型已经具备了一些所谓的建模能力。

同时呢我们可以看到他发表的，发布的很多这种长视频，20秒的，60秒的长视频中也具备一些一致性的能力啊，里面的人物啊，物体出镜头又进来的过程中啊，人物的穿衣着依然可以保持一致啊。

同时呢可以看到嗯他的一些例子里边啊，可以呈现一种嗯物体间互动的建模能力，比如说左边啊这个咖啡里边的小船啊，它在行驶会带来咖啡的的波浪的运动，右边的一个人在吃汉堡的时候咬过，咬下之后。

我们汉堡上面也留下了一些呃印记啊，对但是因为嗯SORA其实并非公开可测啊，所以看到了很多例子是他放出来的，所以他依然啊披着一个神秘的面纱，尽管如此，它已经极大地推动这个行业的发展。

嗯技术也使得众多视频生成技术熟练到啊，啊GIT，而视频生存能力也进一步的提升，嗯SA出来之后，我们看到很多这种非常啊优秀的模型啊，纷纷出现啊，大家嗯也非常希望能够成为中国的SORA啊。

全球的SORA啊也获得了很多的报道啊，这里边有一些开源的呃呃模型啊，包括open sora啊，也有一些并没有开源啊，也没有产品化的一些模型，也有虽然没有开源，但是可以公测，可以已经产品化的一些模型啊。

那么最近几天其实发展很快，我们可以看到快手发布的可灵的视频，闻声识别能力，包括鲁马的嗯，图声识别能力也是非常惊艳的啊，啊这里就不过多介绍了，那过去这一年啊，我们可以看到啊，无论从嗯数据量。

计算量还是参数量上面都有了非常大的提升，这里这里边列了三个目嗯，三个工作，一个是23年7月份啊，上海AI lab的EMDF，然后还有23年底啊，google的video point。

以及24年2月份的嗯，OpenAI的SORA啊，我们可以看到啊，数据量计算量参数量都有几十倍的增加，那5月份我们自我们的嗯志愿研究院啊，携手中国传媒大学啊，啊对全球的上百个大模型啊。

做了一个专业的评测啊，其中包括一些视频模型嗯，比如啊在海外有一些一定用户基础的啊，run way呀，P卡呀，包括我们的PIXVERS，包括一些腾讯阿里的呃视频生成模型，也包括一些开源的模型。

包括open sara等等，那么评测结果我们也很开心看到啊，嗯pic vers啊，我们的产品啊在排名在前三名，因为SORA是本身是不是无法公开测试的，所以他只是个参考。

所以前三名是ruby pixros和P卡，那大家可以看到技术侧，大家逐渐趋同，但是视频生成能力最终会需要面向用户，那这些产品是如何呃呃设计功能服务用户啊，我们会看。

我们简单的和大家一起去看一下这三个模型，认为啊对所有的皮卡，他们的一些共同点和差异点，那run way其实是嗯视频生成产品化的一个先行者，在它出现之前，其实更多的嗯生成能力啊，还是在体现在论文里边嗯。

嗯runway这个公司已经成立了6年时间了，它是182018年成立的，早期的时候他是做啊继学习模型平台，后来开发了很多的呃AI视频编辑能力，在嗯去年3月份的时候，他发布了战兔啊，纹身视频能力。

也那是第一个产品化的，本身视频能力也吸引到了非常多的用户，我们可以看到它的UI里边，其实也体现了非常多的这种AI编辑的工具，有超过20个啊，针对不同的内容，不同应用场景啊，来满足用户的需求。

同时呢它也是最早的推出了叫做motion brush，运动笔刷的这个功能，哎我们可以看一下啊，用户可以通过笔刷的方式精准控制呃，视频里面的局部的内容的变化和运动，也得到了用户的好评。

嗯皮卡其实也是大家也非常熟悉的，这样一个视频生成能力啊，他早早他是成立这个公司，成立于去年年初，他是先从社区做起的，那个时候他承接了一些啊免责你使命的热力，之后又有把图变成视频需求的这样一些用户。

所以在社区里边啊，用户成长是非常快的，他也在早期，他也是重点把精力重点放在了图生视频上，那它的特点是什么呢，它与呃他很重视声音啊，口型，他在今年年初的时候。

与eleven lapse合作推出的AI口型和配音的功能，I've been down in the ruins，So deep，对同时呢它也推出了AI音效生成的功能。

用户可以啊输入通过prom控制也可以啊，让嗯让AI根据生成的视频内容，来自动的去匹配音效，我们可以看一下，You don't care what you're doing to me。

I've been down in ping，那我介绍一下那个爱吃的产品PIXR嗯，P4是它的呃，评测结果实然是超过P卡的，那同时呢我们在用户车啊已经每天的访问，用户也已经跟皮卡比肩了。

那我们是在今年1月份正式上线的这个模型，我们可以提供很多基础功能啊，包括尾声视频啊，图生视频，但我们也有特定的自己的特色功能啊，包括固定角色声视频，Carroto tour to video。

那我们会和我们特别关注啊，用户的可控生成，因为用户在创作视频的时候是实际上是有需求，确保不同镜头里边的人物要保持一致，同时也希望啊更啊精确的控制啊，视频里面的啊局部内容和背景它的变化。

那为什么会有角色一致性的功能，我们在呃，因为我们看到现在视频生成的时长，还是比较短的，我们能做到单镜头的生成，那我们在用户在真正用这个能力的时候，往往生成更长的视频，不管是广告片啊还是剧情片。

都需要集成多个镜头，那需要镜头里面的主角保持一致，不能是前几前几个镜头是刘德华，然后几个镜头变成梁朝伟了啊，至于我们开发的这个功能，我们可以看一下，用户可以自己创作上传一张图片来创作啊，你的形象角色。

然后根据这个角色来连续生成啊，同一个角色的不同的视频，可以看到这一个角色是上传一张照片，就可以生成这个角色相关的不同的AI视频，同时我们也开发了magic brush运动笔刷的功能。

那它的那个易用性效果要超过RV的，我们可以看一下这个，用户可以用笔刷啊来涂抹区域，选择物体，并且啊勾画轨迹，那这个物体就可以按照轨迹来运动，One fire in，对那过去这一年啊。

其实我们啊也经历了diffusion嗯，加unite到底啊，嗯defend transformer这样的一个升级啊，在我们创业早期的时候啊，资源有限，那我们用嗯最短的时间能达到效果，达到全球第一梯队啊。

当前呢我们也很和很多同行一样，用DIT的方这种架构来，希望能够做出中国的骚扰啊，全球的SORA，那未来的话我们也会探索更多的可能性，那下面简单介绍一下，我们这几个这种可控生成技术啊。

背后的一些一些呃原理啊，首先我们一看这种可控生成啊，固定角色声视频这样的功能，Character to video，我们需要把角色固定住啊，并且把它融合到我们的视频之中。

那固定角色其实现在有不同的方法啊，这里列了两个典型的方法，一个是LAURA，一个是EPIC adaptor，那这种方法其实是嗯是有很大的差异，LAURA我们知道啊，就是每个id都需要重新去训练。

所以训练成本是比较大，但它的好处是天花板比较高啊，然后保真度啊啊美学性啊都非常好，那EPIC aptor的优点是什么，只训练一次就好了啊，他用呃呃很多的这种ID来训练一个呃嵌入啊，一个模块。

然后嗯插入到嵌入到我们的呃，进生成模块里边啊，用户用户输入新的id的时候，就必须要重新训练了，呃他的优点是就是成本很低啊，速度很快，但是呢问题是什么，问题是上限不够高啊，冰雪米学质量啊也是偏低的。

那我们针对这两种方法的问题，其实我们设计了一个网一个结构，我们基于这IPADAPTOR的这种架构，我们添了增添两个模块，因为解决保真度的问题，我们增加了一个嗯呃判别模块。

来确保啊生成的内容要啊符合用户的意图啊，第二的话是针对它美学度不高的这个问题啊，那我们也啊加增加了一个，reinforce learning的这样一个模块啊，希望能够提升它的美学度。

所以无论从主观的对比，还是说我们客观指标来看，我们都是要优于这两种方法，嗯这个是matter brush啊，memor brush怎么做的啊，这里也对比了一些嗯，学界的那些典型的守塔的一些工作。

左边的这个是drag n型型的工作啊，它的主要方法是是这样的，是嗯它通过用户涂抹对呃，用把用户涂抹区域啊进行标签化啊，得到它的语义信息，同时呢又把语音信息啊，包括区域信息。

包括语音信息转化成GOSHMAP，然后通过这个嗯，control net来注入到我们的生成模型里边啊，这是这样一个过程，那过程嗯也比较复杂，并且嗯会导致它对局部的控制啊不够精准。

以及呢智能背景会呃呃呃会不稳定，我们可以看到右边的一些比对比啊，上上面这一行是我们啊通过笔刷刷了两只手，然后希望左手和右手都要逆时针的转动，但是你看中间这个左边这个这个，他手持上没有暗语系的运动的。

并且整个背景非常不稳定，所以可以看到我们的方法，它真的是按我们的意图在运动，如果想让这个运动更大，只是只要把我们箭头拉长就可以了啊，啊这是另外一个工作，就是魔法video嗯，这工作的思路是什么样的呢。

啊他先把用户的输入啊转化成一个稠密的光流，然后用s to d来做这件事情啊，结把最后把这个结果通过，也通过CTRNET注入到SVD里边，那它实际上增加了一个新的模块啊，嗯导致训练难度会更大啊。

并且模型也比较臃肿，嗯它对它的问题是对嗯物体的精准控制不够，我们可以看到右边是一个粒子啊，其实我们画绿圈的那个部分其实是没有让它动，它依然也动了，那那我们就针对这几个问题啊。

我们也开发了我们这样一个啊新的算法嗯，一方面呢我们让在在交互层面，我们做了一些呃创新啊，让用户可以更方便来控制嗯物体的运动，同时呢呃模型层面为大大简化了呃，模型架构啊。

不需要基于Ctrl net来呃注入到SVD里边啊，我们的用户的输入可以通过一些预处理之后，那通过一个encoder嗯，再经过一个我们预先训练好的ADAPTOR就可以啊，注入到我们的呃生成呃模型里边啊。

这样的话模型整个的框架大大简化了啊，呃所以效果好也高效呃这是两个例子啊。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/348a6f7fc49eeb4837bf5e528e6c6457_1.png)

我们可以再仔细看一下，对左边的话是我们希望这这个人马斯克啊，他的嗯把这呃让他人脸去动，怎么动呢，我们画了一个很奇怪的轨迹啊，你可以看到最后怎么动的，对右下边是是另外一个例子，对事实上我们可以看到啊。

嗯现在嗯，我们的虽然我们有非常多的优秀产品出现，但是生成的视频的时长还是非常有限，而且往往生成是单一的镜头，嗯所以视频生成呀还远没到菜的GP时刻，但是呢即使这样啊。

在现阶段已经有很多的创作者已经可以用啊，这些AI视频产品来创作有价值的啊，嗯能够嗯好玩的，甚至能够为他们带来商业化收入的内容，我们可以一起看一下啊，这个啊视频是我们的一个海外的创作者啊，一个动漫粉丝啊。

啊根据181988年，几十年前的一个日本动画片啊，aka来他的一个呃预告片来重新啊啊，生成了一个AI版的预告片啊，显示了二者的对比是啊，下边是原片啊，上面是用peace生成的，好我们就不看完了啊。

好我们再看这个例子啊，这是也是我们一个国内的创作者，他受受嗯央视电影频道来邀请，然后以嗯那个荆楚大地的一些传统文化呃，呃代表凤啊作为主题来创作的，一个关于春秋时代的叫楚庄王啊，一鸣惊人。

问鼎中原的这样一个故事，我们可以看一下，喝喝呀，干杯干杯，我们依然能看到一些AI的痕迹哈，荆斩棘，楚国方有今日，你不理朝政政策，天后有何颜面去见楚国21先君，啊哈里面的镜头都是pier是完成的，不记得。

但是嗯现在AI还代替不了导演啊。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/348a6f7fc49eeb4837bf5e528e6c6457_3.png)

所以这些元素都是由专业的这种那个AI导演，来拼来拼接成一个完整的片子，峡谷水货深林，直到那天，那我小心，四面楚歌，危机四伏，他究竟是谁，何以舍命救我就不玩了啊，我们啊现在用这种AI视频啊啊作为镜头。

可以去叙事啊，可以讲故事啊，做这种宣传片啊，也可以去做广告嗯，这个视频是我们一个海外的一个一个创作者啊，他创作出来的广告视频，他在海外因为嗯各种原因啊，导致无法现场去嗯去工作，他是一个导演啊。

我发现场拍摄的时候，他就资金就就断了啊，所以他也寻求他的呃其他的方式来创作呃，内容啊，正好看到pix vers之后啊，还用pixel来创作出来的一个AI广告片，这是在今年1月份啊。

在海外嗯发布的嗯嗯引起了很多人的啊follow啊，也带动了这种AI广告片的这样的，生成的一个潮流，我们可以看一下，这是他创造第一个啊AI广告片，好第一个广告片其实没赚到钱，但帮他赚到了流量之后呢。

就有很多人来找他啊，为了让他给他付费，然后然后让他创作啊，广广商业化的广告片，这是一个啤酒，啤酒的广告，我就不继续展示了，然后面还有其他的一些广告耶，对那现在聊一聊嗯，未来视频生成需要突破的一些技术。

我们知道这种DIT架构的出现啊，骚扰的出现，极大的那提升了食品生成的啊稳定性啊，食品生成质量，但是依然还有很多不足，那接下来往哪个方向去努力，第一个我觉得是需要嗯对运动更好的建模，对世界更好的建模。

这是open n i展示的它的一个bad case，可以看到这个杯子诶，突然就违反物理规律的跳起来了，而且液体不知道什么时候就已经啊，已经洒在桌面上了，对这第二个例子也可以看到，那狗的数量是几只呀。

我感觉它在时刻都在变化啊，对所以这种嗯视频生成，我们会往往会看到它生成内容有一些违和，不符合物理规律啊，甚至不符合自然规律，那这会导致什么呢，导致我们在用AI视频的时候，生成AI视频的时候。

需要频繁的抽卡，我们试个两次出来一次，或是一可用的，或是或试十次才能出来一次可用的，所以抽卡概率还是比较低，那未来我们希望通过更好的对运动的建模，和对世界的建模，能够提升啊我们的呃生成视频的成功率。

另外的话虽然我们现在很多的媒体报道啊，发现呃声称啊，很多模型都声称可以做很长的时的视频的生成，但真正的产品化的大，大家可用可用的往往只是几秒钟的生成，唉这是一个有很大的gap啊。

我们看到现在能用的产品也就3~5秒，这是什么原因呢，是因为你要生成更长视频，意味着误差会累积嗯，会导致你的抽卡概率变得更低啊，所以如何生成更长视频啊，也是我们接下来要做的事情啊，第三个。

我们要需要重点攻坚的是多镜头场景的生成，我们都知道现在已有的视频生成能力，往往生成的内容只是单镜头的啊，那如何在我们在真真正使用视频的时候，往往是多个镜头组合在一起的啊，我们可以看左边这个例子。

这也是欧盟S2的一个视频，对气球人啊很有名啊，但是后来这个创作者呃，说到其实里他用到了很多的后处理和抽卡，才实现了这样一个片子，对对，那我们如何能够嗯去表达镜头语言。

并且把它合理的嗯嗯放到我们的模型里边，从而生成电影级别的多镜头的内容，也是未来AI视频生成要解决的问题，那我们特别希望能做到实时生成，大家在玩抖音快手特效的时候，你发现诶真的是能实实时生成，对不对。

但是现在我们生成一个三五秒钟的视频，可能需要耗时几十秒甚至几分钟啊，市场就比较呃比较比较长，他有两个问题，一个问题是嗯如果等待时间比要比较长，往往只有只有专业用户才能用起来，普通用户很难去玩起来。

第二的话是推理时间长，意味着推理成本高，所以如何做到实质生成，嗯是一个非常重要的一个方向啊，既能够解决用户的体验问题，又能够嗯嗯降极大的降低推理成本，另外的话就是当我们的实质生存做到极致。

就是呃把模型部署在手机端里啊，它能够提供更好的隐私保护和交互体验，那当然了，我们也呃做视频生成大模型跟大圆模型一样，那必然的需要面临就隐私啊，伦理的这样的挑战啊，比如这种DEFAKE啊。

深度伪造数据如果做得非常非常行逼真的话，如何能够确保它不作恶，如何去阻止恶意用户作恶啊，当然我们的呃技术发展特非常非常快，我们监管层面也需要和技术一起打磨啊，持续的去升级嗯。

事实上嗯市民生成并没有达到chat GDP时刻，它依然在快速发展过程中，但我们已经欣喜的看到他正在逐渐去啊，重塑视频创作的工作流，那现在AI视频生成技术，已经在逐渐的去替代啊，演员替代啊，背景啊。

替代摄像摄像头，那未来的话，我们可以预见它一定能够影响千行百业，包括呃游戏呀，影视呀，动漫呀，教育呀，广告啊等等等等，但是我们不仅希望能够服务好那些专业创作者，我们也希望能够进一步的。

更大的去降低使用门槛，希望能够做到技术普惠，能让每天玩抖音啊，玩快手玩TIKTOK，这些普通用户也能够去用起来，玩起来啊，能够言出法随的去生成啊，高美观度啊，高创意的视频啊。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/348a6f7fc49eeb4837bf5e528e6c6457_5.png)

这是我的目标，那最后呢我用一个视频来结束我的分享，啊啊题目是为什么要用AI创作视频，这也是里面的所有素材，都是嗯pik source的创作者来做出来的。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/348a6f7fc49eeb4837bf5e528e6c6457_7.png)

我们从未停止想象。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/348a6f7fc49eeb4837bf5e528e6c6457_9.png)

所以文明从未停下脚步，雨来自云中的龙，闪电是神明敲响的鼓声，想象经过的地方总有故事诞生，在一粒沙中展开末日的冒险，去一朵花里建造粉色的宫殿，每一处平凡的角落都能被想象的火种点燃，想象是一种语言。

让我们不再孤单，一个生命开始理解另一个生命，一个人的梦想也能被一群人看见，这里每个镜头都是日常，又是AI，这是AI生手，现在轮到你了，用像素构建星辰大海。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/348a6f7fc49eeb4837bf5e528e6c6457_11.png)

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/348a6f7fc49eeb4837bf5e528e6c6457_12.png)

好谢谢大家啊，是不是超时了啊，谢谢长虎啊，那个呃整个pex words生成，确实令人非常印象深刻哈，然后看看现场的朋友有没有什么问题，想要问长虎的，大家可以举手示意啊，我们还有大概23分钟的时间。

也许可以回答一到两个问题啊，好啊，给那位穿蓝色衣服的男士吧，可以你可以站起来啊，我喂喂诶我问一下啊，就是现在咱们这个视频比较短，就几秒钟，但是这个几秒钟生成的时候，这个成本大概是多少呢。

是不是我们能够承受的这种啊，那我先回答你这个第一个问题啊，成本其实很很简单，你可以算一下，比如说生成几几秒钟的视频啊，如果能用看它耗时嘛啊，耗时如果是几十秒的话啊，如果是4090啊，或者是A100。

你可以算一下4090大概啊用时四嗯，40秒啊，或者60秒，它的成本是多少啊，大概是一个一个一个卡一嗯，100大概一一小时大概一两美金左右啊，所以成本还还是呃不高啊，这是第一个。

第二的话是说嗯我们是否能承担啊，以后的更好的模型出现，一方面可能会极大的去提升成本，同时技术会飞，技术也在飞速的进步啊，前面说的如果能做到更快的生成啊，有可能会极大的降低我们的成本。

但是呢不管成本高还是低，看谁来用啊，普通用户可能未必愿意为此付费，但是我们的很多专业的创作者，我们动画的创作者，广告的创作者一定会为此付费，为什么呢，因为我们用普通的拍摄的方法去创作广告啊。

12分钟广告是我们成本可能很高，可能几万块钱甚至更多，但是用AI创作成本就极大的降低了，是所以如果能用几美金赚到几万美金，这个生意就划算对吧嗯，很划算啊，好呀，现场还有没有朋友想提问的啊。

这位啊啊啊就你嗯哎老师您好，就是我刚刚看到那个我们给那个马斯克，给他画一个很曲折的一个轨迹，它就能够非常好的去遵从这个效果，我觉得很惊艳，但是我想问的就是，我们再去训练他这样一种精细化的指令。

遵从的时候，我们用到的监督信号是什么，或者说或者说如果不是用的那种，有监督学习的话，就是说这个是怎么训练，来达到如此精细的控制效果的，谢谢啊，这个事实是这样的，就前面已经已经介绍了。

就是说嗯它嗯它是一个可控生成啊，我们的模型本身用到了呃几个信息，一个就是我们需要用户去选择物体，并且把它涂抹上，那我们知道他关注的那个嗯，关注的是哪一部分内容啊，同时的话我们还知道它的。

因为用用户还会去画它的轨迹方向啊，所以这也是一个信号，两个这两个信号是我们输入啊，我们肯定有一些啊神秘的预处理技术，然后把它转化成我们模型能够识别的，这样的内容，然后去通过一种非常轻量化的机制。

绕开了这种control net，这种非常轻量化机制，来直直接注入到我们的底底层生成模型之中啊，大概是这样一个架构啊，啊，那那如果我们的预处理，就能够把这个精细的东西给它转化成一个。

我们训练的时候能够用的形式，那我们这个训练的意义更多在哪，我觉得这个预处理，是不是本身它就已经蕴含了一些能力了，嗯是的，就是说嗯所谓的预处理，就是把我们嗯人我们的一些raw data嗯。

嗯转化成嗯那个进行进行信息压缩啊，信息编码转化成更有效的一个表达啊，这个过程是意义非常大的啊，不管是换之前的介绍的很多工作，这种encoder其实在整个模型生成里边。

视频生存里面是扮演了非常重要的一个角色，谢谢您诶，好好好谢谢谢谢常护，时间关系。

# 2024北京智源大会-大模型产业技术 - P3：百川大模型技术与应用实践：谢剑 - 智源社区 - BV1HM4m1U7bM

大家下午好啊，非常荣幸这个重演邀请，然后给大家做一个这个百川的这个大模型的，技术和应用的这个实践的分享啊，对第一part我可能跟大家先啊，分享一下，百川在这个通用大模型上的一些这个，技术的实践。

然后第二拍也给大家分享一下，在这个大模型基础上，我们的一些原生应用的一些探索，对其实百川是一个还蛮年轻的公司，我们大概在去年4月份的时候，就是小川发公开信，然后就成立这个公司。

但是我们也同时是一个奔跑速度非常快的，一个公司啊，就是大概我们在4月份成立之后啊，很快在6月份就发布了第一个这个7B的，这个开源的模型，当然当时其实整个市场上应该开源的模型。

还没有一个这个真正在中文上可用的，并且可商用的这个模型，我们当时是这个第一个发布开源并且可商用，同时紧接的很快，基本上是属于一个月发一个模型的这个节奏，然后在这个7月份发13B，然后紧接着很快就到啊。

后面基本上到53B，以及在我们在近期，也就在5月22号的时候，我们发布了我们的旗舰的，就是百川四的这个模型，对一定意义上我觉得还比较自豪的是说，百川一定意义上推动了整个国内中文大模型的，这个开源生态。

可能如果大家遥想一下，去年大概在45月份，56月份的时候，尤其67月份的时候，其实整个国内中文大模型，真正的开源的这个很少，所以当时我们其实在开发了7B13B，以及连续开了第二代之后。

现在基本上越来越多的很，就是也很开心，看到很多的这个这个大模型的这个公司，包括大厂都在开，并且也开得越来越大，然后呃到了我们后面，因为我们觉得这个越来越大的这个模型，其实开源出来。

其实很多人也无法真的很能能用起来，所以到后面的这个闭源的模型当中，我们其实也在持续的这个优化，然后百川山是我们大迁移的这个模型，这个大概在今年年初的时候发布的，然后在这个也很荣幸在这个资源的这个评测。

包括这个super crew的评测当中表现的还比较优秀，那接下来我会重点讲一下，我们在百川市上的一些这个技术的这个探索，那个百川市相对于百川商而言，能力又有了一个比较全面的这个提升。

当然包括很多指令跟随呀，信息理解等等各个方面的这个能力，提能力的提升，然后在这个啊国内的这个super crew的评测当中，也就排行比较靠前，当然我们其实实在的来看的话，其实因为它有个分的吧。

在数学能力上其实还是比open AI要差，那当然在文科上确实是有有一定的这个优势，百川四同时也有一个多模态的这个模型，在benchmark上，我们基本上很快要接近到这个g b t four v。

当然在这个里面，我们其实啊这个模型演进了这么多代，就是在百川事上，我们也做了很多这个技术的这个探索，也跟大家一块分享一下，第一块呢其实是在这个预训练上啊，我们都知道，其实预训练里面数据是至关重要的。

那么怎么去筛选这个数据，或者说去获得这个高质量的这个数据啊，我相信大家都有非常多的这个实践，但整个行业的这个发展，这个趋势逐步的从人类标准的这种方式去筛选，就是我们觉得诶什么是好的数据，什么是粘的数据。

但其实人类标准这个数据筛选，往往往往通常情况下有遇到的一个问题，是说坏的数据是很容易能够去分辨的，但什么是真正好的，这个数据其实并没有那么容易，所以我们其实在这个阶段，逐步的也引入到了这种模型的筛选。

也就说我们用百川山，这个其实在meta后面再公布到拉玛山的时候，也看到他们也有类似的这个工作，其实我们也在做类似的，就是百川三的这个模型，用相关的一些指标来这做这个数据的筛选，当然除了筛选之外。

你会发现现在其实大模型本身自己生成，就能够生成很多高质量的，尤其是知识密度比较高的这种数据，所以我们在数据合成上，也做了很多的这个探索和工作，包括这个模型的这个改写。

也包括这个高质量的这个数据的生成跟合成，那第二趴其实是啊，做了这么这这么长时间的这个训练之外，这个我们公司的预训练的团队，同学们也开始在探索说诶，怎么样从经验真正的做到这个科学的这个部分，那我们都知道。

这个在做这个长窗口的这个训练当中，如果大家还是在用这个LOPE的这个这个，position的这个embedding的话，通常会有一个经验性的感觉，是我的base就是我的窗口要大的话。

我的这个基底的这个贝斯也要大，但大到多少，以及到底怎么样大，其实大家很多时候都比较empirical的这种方式，然后我们的同学也做了很多，这个实验就是真正的做了一个这个实验，是说哎。

那如果你要把窗口长度扩到一定的程度的话，那么你的这个base实际上是有一个lower bound的，但这个我们也也公开了我们的这个论文，大家如果感兴趣的话，也可以去看这个the base of。

LOPE的这个这篇论文，所以这样的话，实际上我们对于这个模型的窗口长度的，这个能力的这个扩充过程中，也能够做到更加科学化的去设定我们的这个base，当然除了这个预训练之外。

另外一个非常重要的部分就是对齐，在对齐这块我们做了几个啊，比还比较有意思的工作，第一个呢就是我们试图去理解说，在对齐当中预训练的这个知识，和所谓的这个在对齐，你通过这个对齐的这个训练。

把它illicit出来，是这个这两个之间的关系是什么样子，我们做了一件事情，就是把这个transformer的，就整个网络的这个在过最后的logic那一层，之前的这个embedding拿出来。

把这个embedding拿出来之后呢，我们做了一个聚类，把这个聚类来分说好和坏的，这个这个这个这一部分的这个区分，然后当然我们把这个把它定义成叫做。

Cognitive for the capability，也就说你不再在做最后一层的这个动作之前，实际上这个模型本身在这个影城的这个表示上，其实就具备了这个它一定的这个知识的感知，所以这个的话你会发现。

说我们在做这个实验的时候，你会发现说这一层的这个知识和能力，实际上在随着这个预训练的，就右侧的这个图上，你会发现随着这个预训练的这个token数的增加，他的整个能力是不断的在提升的。

但是呢另外一个往往下看一个的，就说我们现在其实是最终是让模型输出，这个结果到底是好和坏，当我们把在这篇论文中，把它定义成叫做expressive的这个capability。

也就是说让模型已经自己能够表达出通文字，表达出它的这个判断好和坏的这个能力了，那在这个这个当中你会发现，其实如果你去看右侧的这个实验图，你会发现其实在做对齐，或者说在LHF等等。

就SFT和IOSF的这些对齐，其实它是在逐步的这个在提升，你会看到它的准确率在提升，但是它其实没有超过这个刚才说的，cognitive capability的这部这部分的这个知识。

所以一定程度上其实是在嗯嗯在共识，一个比较重要的这个业界的这个一个一个概念，就是说在预训练当中，我们其实LEAR了很多知识，那alignment alignment当中其实更多的程程度上。

我们是当然抛开这个指令遵循本身啊，就在knowledge这一项，我们更多其实是能够去illicit，然后这篇paper当然我们也在今年的SML上发表，大家如果感兴趣也可以去深入的去看一下。

那除了说去探究其中alignment里面的，对其当中的一些关键的这些呃，概念和这个这个原理之外，当然在对齐的一些方法上，我们也做了一些不同程度的这个探索和创新，其中在这个SFT上啊，SFT的团队做了。

我们做了很多不同的这个创新之外，同时做了一个模型融合的工作，这工作当然可能大家在上一代人，就是在传统的机器学习里面，应该有很多类似这种ensemble的这种方法，但是怎么把这个模型本身。

通过参数本身自己的融合，又没有去增加这个复杂的这个计算量，同时能够平衡不同的模型的这个效果，其实也是新的大模型时候的一些呃，值得持续去探索的一个工作，然后在强化学习这块，我们做了啊两个大方面的这个工作。

第一方面呢就是分了不同的这个阶段，就是第一个阶段呢其实是做了一些这种啊，应该说是这种改进版的这种DPO的一些方法，就是在其中。

我们叫sequential的这个preference的optimization，这个也是最近发的一篇paper，就说核心其实是说我们有不同的这个preference，就是啊对齐的这个价值观的对齐过程中。

可能你会有不同维度的这个要求，那其实如果把这些维度都合在一起，其实你会发现它其实很有时候会比较难平衡，那这个过程中，我们其实把这些维度做了一个区分，同时做了一步一步的这种微调，这是第一个。

第二个呢就是在强化的这部分，就RDRDXF这块我们做了一个工作，比较重要的工作，其实是把L的HF，就大家可能RL的就reinforcement learner。

Name from human feedback，这个可能大家非常熟悉了，也就是open AI在推，这个在写印刷CGBT的paper里面，明确提到，他们用这种方法，更大的提升了这个模型的这种指令遵循。

但同时我们会发现，其实当模型发展到今天的这个状态的时候，很多时候模型自己能够对于一些能力上，它其实是能够做AI的feedback，所以怎么把这两个做一个充分的融合，这是一个比较重要的点。

第二个点就是啊大家都知道，其实在对齐的过程中，模型的能力就有点像是在啊，小朋友在学习的过程中，他其实也是一步一步的这个爬坡的，所以如果能够做到持续的叫iterative的这种L的，这种XF。

那其实它是可以让整个模型的能力，一步一步的往上走，所以这个过程当中，其实我们也做了这部分的这个工作，对，当然除了这个关注模型的这个算法的效果之外，大家也都知道，今天其实啊整个成本上。

或者说在推理的这个效率上，其实也大家都非常关注，那其实我们在这个过程中，也做了很多的这个探索，其中比较重要的一个就是大家都知道，可能这个啊有一些，降低这种推理成本和提高效率的这种工作。

其实是做这种投机的采样，也就说你在做next token的prediction的时候，我同时有一个并行的这个去预测，后面好几个这个token，那我这个过程中就有点像是。

我提前去预测后面的好几个token，当然这个过程中现在就是传统的一些方案上，其实它的命中率会相对会低一些，所以我们也跟北大联合做了一个研究的，和这种创新的工作，是把一些序列的这个知识。

跟这个并行的这种解码去结合起来，使得这个投机采样的这个命中率，进一步的这个提升，当然还有一个就除了百川是自己的模型之外，其实百川也一直在做一个这个探索，就是我们希望是说未来的模型。

能够有机会去执行更多复杂的任务，就是我我自己一直是这么看的，就现在的模型，很多时候，其实我们现在很很多场景下，还在做这个人类中的这种快思考，就所谓的system one的这种思考。

但是真正要进入到人类社会中去，真正去帮助人类解决很多复杂的任务，一定要需要慢思考，所以百川在agent上面也做了一些技术探索，但这个工作其实也是我刚刚临时在下面加的，一个也是比较比较巧的。

也是比较这个算是在公共场合上，第跟大家第一个去分享的，就是我们大概在前上一最最近的几天之内，我们拿了这个这个复杂任务版，就盖亚的这个全球第一，就我大概介绍一下盖亚的这个task。

的这个benchmark是一个什么样子的一个task，其实他是那个meta的这个首席科学家，young lak和这个hagin face的这个啊，还有还有几个在做agent相关的。

提出了一个新的这种AJI的这种评测方案，它其实比较重要的，其实是评测一些需要比较复杂操作和规划的，这种任务上的整个模型的能力，那它的特点是什么呢，就是题目其实对于人类来说还挺简单的。

但对于很多先进的这个系统来说，却其实有比较大的挑战性，如果你去看的话，其实人类可能能做到90多分，但其实如果你看g b d four加plugin，可能最多能得15分。

然后题目有很多也很很多很更现实的问题，题目在这个过程中需要去调用很多的这种工具，并且往往都不是一步，基本上都不是一步能做到的，它都需要做比较多步的这个过程，而且人其实大部分的这个任务。

都已经超过了6分钟，就超过了5分钟，所以比如说类似这种的问题，就是你看这个，可能是美国国立研究院网站上列出的，2018年1月到5月，在寻常痔疮患者中进行的，幽门螺杆菌实验的实际能数目是多少，像这种。

其实你往往都需要把它拆解成，很多不同的这种查询，然后再去做，对这个也是我们近期刚拿到的一个结果，就是我们在做的一些这种啊，一个我们内部的一个系统，然后拿到了这个第一，当然其他的这个参赛者。

就是其他的这个下面的这个benchmark的有好几个，就之前的第一是微软的一个工作，当然我们拿到这个之后，其实也就在前几天，很快这个拉玛的这个拉玛二和拉玛三的，他们这个核心的工程师有很快就来找找到我们。

当我们现在其实也在整理，因为我们其实发的时候也也还没有，那么confidence说能拿到第一啊，就是现在也在整理这个代码和技术报告，对，然后我大概brief的介绍一下，我们这套系统。

其实在这过程中做了一些比较重要的，我们希望从season one到season two的一个探索上，一个比较大的改变，其实有几个，第一呢就是说当baby a g i，这个是一个比较比较common的。

就是说我们做啊这个thinking thought reaction，然后再做refraction，就这一套的这个编排的机制，其中还有两个比较重要的是说啊，要做这么长序列的这个规划当中。

会需要一个global的memory，就你怎么去manage这样的一个global的memor memory，就像人类打草稿或者说做很复杂的题目一样。

那你怎么去manager这样子一个global memory，进而去做到几个模块之间互相协同，再一个就是multi agent的这种方式呃，当然这个背后叫做social of mind。

就是我们用好几个agent之间相互去对话，和相互修正的这种方式去提升这个，然后再一个点就是从调用工具上来看，我们是把这个IAG进化到了，这个所谓的这个web agent。

就是我们都知道现在的搜索很多的搜索增强，其实是你把这个网页拿过来，然后我再丢到大模型里面做一个summary，但实际上人类在真正在做这些复杂任务的时候，它其实是把这些所有的这些网页啊等等。

这些把这个搜索，这些都当成很多工具，你可能是搜索搜索完之后再去点击，然后page down page up就做很多这种的工作，当然我们很快会去开放，就是发出我们的technical report。

和这个公开我们的代码，那么感兴趣的同学到时候也可以关注我。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/16c2c6ce38720924679a1b81b3c36f5b_1.png)

对讲完讲完这个，我也很快跟大家分享一下，百川大模型在应用上的这个原生，应用上的一些探索，我们认为说呃，现在在大模型的这个技术的基础上，我们觉得说如果真的要在助手这个维度上。

能给大家在日常生活中提供真正有用，有帮助的助手，我们觉得应该具备两个比较重要的，第一是要懂搜索，就是大模型固然知识非常大，但它更适合，其实作为一个reasoning的这个brain。

他的知识一定是有限的，不管从时效性啊等等各个方面都是有限的，第二个呢就是真正要成为一个像人一样的，这个助手就我做我做了很多年的这个助手，想要真的要成为这个助手的话。

其实他应该像人一样能够会互动的这种交互，而不是像其实chi g b t，这个时候还更多的有点像是在一问一答的，还是一个这种问答器对，所以基于这两个的这个思考，其实我们做了这个AI的这个原生的这个应用。

白小印，这个白小印上其实呃第一个我们说懂搜索，比如说从这个引擎盖打不开怎么办，当然这些大家都其实是在直接的做这种回答，那其实啊我们的一个思考是说好诶，这个好像，sorry不太能播放，我大概讲一下吧。

就是呃第一个就是懂搜索，比较重要的，其实是说我们能够知道说去哪里去搜索，比如说可能如果你要找论文，你应该是去IKF啊，去一些非常专业的这种网站去搜索，但这些都是大模型，帮你自己自己解决了就懂搜索。

能够去定向的网站去搜索，拿到最好的这个质量，第二个呢就是多轮的这种搜索，就是你可能不只是一部搜索能做到的，那你可能比如说对比中美两国在大模型，行业上的这种差距，那你实实在在可能需要去先了解中国的大模型。

对吧，再了解美国的大模型，然后再去做这个大模型，两个行业的这种差距，所以我们其实可能会做多轮的这种搜索，把这个问题拆解成不同的步骤，最后再给大家一个更结构化的这种解析，再一个就是把这个搜索的这个结果。

通过刚才说的就是多轮的这种搜索，和更复杂的这个分析之后，其实是会把这个结果嵌入到整个这个搜索的，这个最后的这个结果上，不好意思啊，今天这个嗯播放有点问题，再一个其实就是结构化，就大家如果能去看的话。

比如说对比过去4年，绍兴和宁波的GGBTGDP，就是我们一方面是找到好的搜索源，定向搜索能做得好，然后你能把复杂的任务像人一样，我可能收第一步，搜第二步，然后再做，最后我们还希望说能够更结构化的。

能够去把这个信息真正的组织成知识，比如说这种GDP的这种对比，你其实可能很快用一个表格，你很很容易就能看出来他们之间的这种对比，再一个就是会提问啊，其实你会发现今天这个大模型有很多prom。

其实也挺挺有意思，以及他的回答，就说你你说让一个大模型说帮我写一个这个，甚至有很多说帮我写一篇作文，然后啪啪啪他就给你写出来了，这我感觉怎么说呢，其实每个人带的这个需求。

他肯定不是说我你给我随便给我来一篇作文，我就能用的，所以在这种其实是一个典型的这种，其实需求都并不明确的这种情况，那更好的这种应用形态是，你应该是跟它能够去做互动和交互，能够更清晰的让用户把自己的需求。

表达清楚之后，我再给你一个好的这种结果，Hey sorry，所以我们的这个名字啊，就是我们的整个这个应用的名字叫做白小印，对这个白起名字的这个来源，其实也就是也比较简单易懂啊，就是我们希望是一呼百应。

然后不管是响应上能够快，然后反应上能够快，应答上能够好，然后有做到这个有求必应，当然这个形象啊，就是算是百川入海跟这个结合的这么一个形象，当然我们更期待的其实是能够做一个有温度的，这种人工智能。

这个助手的这种产品，能够真正做到从工具到伙伴，当然更更期待的是，未来是我们能造很多不一样的这种人，对回到这个百川的这个从大的这个价值观上，我们一直期待的是创造健康和快乐。

就是在大模型能够辅助人类创人们创造，同时当我们还会在医疗的健康上有很多的探索，以及在快乐上，也希望能够给大家带来更多不一样的这种应用，对讲完这个我们的技术和应用的这个一些，探索的实践后。

也给大家分享一下啊，我们对一些未来有一些是代表我个人的啊。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/16c2c6ce38720924679a1b81b3c36f5b_3.png)

就是我对未来的一些技术的这个展望呃，我大概做了几个这个技术趋势，发展趋势的一个一个总结吧，啊前前三个叫大多会就是大师说，当然就是我们这个模型会持续的这个scale up啊。

我看前几天有一些linking的information，是说g p t five是50多万亿，这个这个50多万亿的这个参数，当然他可能做还有做完MOE之后，他t FPS大概是乘十倍吧。

就他们每每每一代都期望是十倍的这个概念，这是第一个，当然参数的规模，数据规模一定会持续的这个scale，第二个呢就是多模态，这个多模态应该叫any to any吧，就是从长期来看。

它一定是any to any的，但同时还有一个比较重要的，其实是实时的这种自然交互，就for欧其实给大家呃一个很明确的这种回答，是说human like的这种interaction。

显然也是未来大模型一个非常重要，或者说AI走向更多的这种应用场景，更智能的，更内能的这种体验很重要的一个方向，第三其实是普惠吧，但其实一定意义上，我觉得其实是模型能力。

或者说模型在这个推理成本一定的情况下的，模型能力的这种进这种提升，现在很可能比摩尔定律的这个速度更快，就是三个月到四个月之间，可能这个cost就能够下降到50%以下，甚至以上。

所以capability在能力一定的情况下，推理成本会不断的在往下走，不管是从算法本身的优化，还是从这个硬件本身的这个优化上，还有两个其实是我觉得比较重要的，未来的一个就是如果大模型的技术。

或者说通用的这个在走向AGI当中，非常重要的两个点，第一个呢就是han long horizon的这种task，也就说现在其实大模型到了今天的这个状态。

其实我觉得更多的还在system one的这个状态，就是你快思考的这种状态，那么其实能往往能帮助解决人类的任务，是在啊人类的大概5分钟之内能解决的，这个task，往往是。

那怎么样真正能走向所谓的再往前的AGI，一定是能够帮助人类解决，超过人类5分钟能做的这个，所以这种残序列的复杂的这个任务，怎么样能够真像人一样，用system two就慢思考的这种方式去做规划。

去做工具的使用，去做长城的这种planning，最后能够把这个任务完成，这个显然是接下来非常重要的，当然可能也可以推荐大家去听一个john shuman，最近的一个这个他的talk。

他其实也在当中反复的提到这个了，当然第五个其实是自学习和进化，但我觉得这个其实就更高level的点了，就说alpha go到alpha zero走了一个，另外就走了一个很重要的一个变化。

是说ALPHAGO，我们前面还要用很多人类的棋谱，去做supervise的这种learning，再去做一个起点，但到go就zero之后，就说OK，那我就一开始就开始做自我演绎，和这种对抗和学习。

那如果说啊AI再往一套走，如果能突破所谓的self boost或self play anyway，就是不同的，能借助，不就不再借助人类本身的这个数据的输入，或者说supervise的这种输入的话。

还能够持续的进展，那这个可能会是对于后面非常关键的突破，也是非常大的一个一个挑战吧，OK好，那我的分享就今天就到这，谢谢大家，好谢谢谢建博士，然后我们还有3分钟时间，看看现场有没有朋友想要提问的。

举个手吧，呃那边后后面那位举手的，对想问谢博士一个问题啊，就是咱们在A镇的，相应在大模型之外，去构建这个复杂的这个推理网络了，但比如他跟比如说我在模型内部，比如加一些这种吹search这种方式上。

这可能是一探索的方向上，会有什么样的不一样呢，这两者对我觉得是这样子看的，就是实际上这个问题，其实呃我们一直一般来说来解决这种复杂任务，最后肯定是这两个东西双管齐下。

我们或者说你其实刚才mention的，可能是另外第三个，但其实我在我看来，第二第三可能概念一样，我大概讲一下，就说你首先我在推理的时候，是不是我在推理的时候，我不管是用啊tree的这种方式。

结合MCTS这种搜索的方式，Anyway，就把推理的过程的多路径都打都做出来，并且做用plant的方法去做，这肯定要做，但他肯定一定程度上也会提升这个cost的。

那这个动作其实跟所谓的这个agent里面去做，刚才所谓的这种planning和就思考反思，调用工具等等，这个本质是1111个事，然后再一个是是说那我怎么样去真正提升模型，自己本身能够去做这件事情。

那这个是一定是这个这个过程中积累的，这些数据和方法，要能够重新训回到这个模型里面去，所以在我看来，这一这两个是一定是双管齐下一起去做的明白，所以您觉得会比如先通过这个就是外部的，或加一些推测的方式。

但得出来的结果可能再返回来本身，提高模型本身的比对，我觉得这是一个这就两个路，一定要一起走的明白。