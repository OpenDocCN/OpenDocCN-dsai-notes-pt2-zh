# P5：p5 建造 makemore 第四部分：成为一个反向传播忍者 - 加加 zero - BV11yHXeuE9d

大家好。今天我们再次继续实现 makemore。到目前为止，我们已经到了这里的多层感知器，我们的神经网络看起来是这样的。

![](img/86f8499f4a173e8882bce59ebef07fb4_1.png)

![](img/86f8499f4a173e8882bce59ebef07fb4_2.png)

我们在过去的几节课中实现了这个。现在我相信每个人都非常兴奋要深入研究循环神经网络及其所有变种，以及它们是如何工作的，图表看起来也很酷，这非常令人兴奋和有趣，我们将获得更好的结果，但不幸的是，我认为我们还需要在这里待一节课，原因是我们已经。

![](img/86f8499f4a173e8882bce59ebef07fb4_4.png)

训练了这个多层感知器，我们得到了相当不错的损失，我认为我们有。

![](img/86f8499f4a173e8882bce59ebef07fb4_6.png)

对架构及其工作原理有了相当不错的理解，但这里的代码行。

![](img/86f8499f4a173e8882bce59ebef07fb4_8.png)

这里的一个问题是丢失了向后传播，也就是我们正在使用一个 PyTorch 的自动微分。

![](img/86f8499f4a173e8882bce59ebef07fb4_10.png)

为了计算我们所有的梯度，我希望去掉使用向后传播，并希望我们手动在张量级别上编写我们的反向传递，我认为。

![](img/86f8499f4a173e8882bce59ebef07fb4_12.png)

这是一个非常有用的练习，原因如下。我实际上有一整篇关于这个的博客文章。

![](img/86f8499f4a173e8882bce59ebef07fb4_14.png)

关于这个主题，我喜欢称反向传播为一个漏水的抽象，我的意思是，反向传播并不仅仅使你的神经网络神奇地工作，并不是说你可以随便堆叠任意的乐高块和可微函数，只需交叉你的手指并进行反向传播，一切就会很好，事情不会自动运行。

这是一个漏水的抽象，因为如果你不理解其内部结构，你可能会自损，神奇的是，它可能不会工作或者不能最优工作，因此你需要理解它在内部是如何工作的，如果你希望调试它，如果你希望在你的神经网络中解决它，那么这里的这篇博客文章深入探讨了一些内容。

一些例子，比如我们已经覆盖了一些例子，比如平面。

![](img/86f8499f4a173e8882bce59ebef07fb4_16.png)

这些函数的细节，以及你不想过度饱和它们，因为你的梯度。

![](img/86f8499f4a173e8882bce59ebef07fb4_18.png)

将会死去的情况是死亡神经元，我已经覆盖过，还有爆炸的情况。

![](img/86f8499f4a173e8882bce59ebef07fb4_20.png)

在我们即将讨论的前馈神经网络中，消失梯度的情况。

![](img/86f8499f4a173e8882bce59ebef07fb4_22.png)

你也会经常在网上遇到一些实例，这是我在互联网上随机代码库中发现的一段代码，实际上它在实现中有一个非常微妙但相当严重的错误，这个错误表明作者并不真正了解。 

![](img/86f8499f4a173e8882bce59ebef07fb4_24.png)

理解反向传播，他们试图在某个最大值处剪切损失，但实际上他们想剪切梯度，使其有最大值，而不是试图在最大值处剪切损失，间接导致一些异常值被忽略。

剪切一个异常值的损失，你将其梯度设置为零，看看这个。

![](img/86f8499f4a173e8882bce59ebef07fb4_26.png)

你可以通读它，但如果你知道自己在做什么，会避免一系列微妙的问题，因此我认为，正因为 PyTorch 或其他框架提供自动微分，我们就可以忽略其工作原理，这显然不是这样。

![](img/86f8499f4a173e8882bce59ebef07fb4_28.png)

我们讨论了自动微分，并编写了微型自动微分引擎，但微型引擎仅限于单个标量，因此原子是单个数字。我认为这还不够。我们应该考虑在张量层面进行反向传播，总体来说，我认为这是一个很好的练习，非常有价值，你会变得更优秀。

在调试神经网络时更好，确保你理解自己的操作，这将使一切变得非常明确，因此你不会对隐藏的内容感到紧张。总体而言，我们将变得更强大，所以让我们开始吧。顺便提一下，如今手动编写反向传递并不推荐。

现在除了作为练习之外，没有人这样做，但大约 10 年前，在深度学习中，这非常标准，实际上是普遍的，因此当时每个人都习惯于手动编写反向传播。

![](img/86f8499f4a173e8882bce59ebef07fb4_30.png)

手动进行反向传递，包括我自己，这就是你会做的事情。我们曾经手动编写反向传递，现在大家只需调用丢失的反向传递，我们失去了某些东西。我想给你几个例子，这里有一篇 2006 年 Jeff Hinton 和 Russell Select Enough 的论文。

![](img/86f8499f4a173e8882bce59ebef07fb4_32.png)

这在当时具有影响力，训练一些被称为限制玻尔兹曼机的架构，基本上这是一个训练的自编码器，来自大约 2010 年，我有一个用于训练限制玻尔兹曼机的库，那个时候是用 MATLAB 编写的，所以用 Python。

![](img/86f8499f4a173e8882bce59ebef07fb4_34.png)

深度学习并没有被广泛使用，所有的都是 MATLAB，MATLAB 是一个科学计算包，大家都会使用，所以我们会写 MATLAB，这几乎不是一种编程语言，但它有一个非常方便的张量类，这是一个计算环境，你在这里运行，当然所有的都在 CPU 上运行，但你会得到非常好的图表。

使用它和一个内置调试器，这真是不错。现在，这个包中的代码是在 2010 年我。

![](img/86f8499f4a173e8882bce59ebef07fb4_36.png)

我为适应受限布尔机所写的代码在很大程度上是可识别的，但我想展示你如何……我在创建 xy 批数据时初始化神经网络。

![](img/86f8499f4a173e8882bce59ebef07fb4_38.png)

![](img/86f8499f4a173e8882bce59ebef07fb4_39.png)

所以它有权重和偏置，就像我们习惯的那样，然后这是训练循环，我们实际上执行前向传播，然后在这里，此时甚至不一定使用反向传播。

![](img/86f8499f4a173e8882bce59ebef07fb4_41.png)

传播用于训练神经网络，因此这特别实现了对比散度。

![](img/86f8499f4a173e8882bce59ebef07fb4_43.png)

该函数估计一个梯度，然后在这里我们获取这个梯度并用于参数更新。

![](img/86f8499f4a173e8882bce59ebef07fb4_45.png)

沿着我们习惯的方向，是的，你可以看到基本上人们在干扰。

![](img/86f8499f4a173e8882bce59ebef07fb4_47.png)

直接使用这些梯度，并且在行内，这并不常见于使用自动求导引擎。

![](img/86f8499f4a173e8882bce59ebef07fb4_49.png)

这是我 2014 年发表的一篇论文中的另一个例子，标题是“片段嵌入”。

![](img/86f8499f4a173e8882bce59ebef07fb4_51.png)

![](img/86f8499f4a173e8882bce59ebef07fb4_52.png)

我所做的是对齐图像和文本，因此这有点像“Clip”，如果你熟悉的话，但它并不是在整个图像和整个句子的层面上工作，而是在单个对象和句子的片段层面上工作，我在嵌入它们，然后计算一种类似于“Clip”的损失，并且我拿到了 2014 年的代码。

我实现了这个，已经在 numpy 和 python 中，这里我在实现成本函数。

![](img/86f8499f4a173e8882bce59ebef07fb4_54.png)

实现不仅仅是成本，而且还手动实现反向传播是标准做法，所以在这里，我在计算图像嵌入和句子嵌入，最后的函数我计算得分，这就是。

![](img/86f8499f4a173e8882bce59ebef07fb4_56.png)

损失函数，一旦我有了损失函数，我就在这里执行反向传播。

![](img/86f8499f4a173e8882bce59ebef07fb4_58.png)

通过损失函数和神经网络进行反向传播，我添加了正则化。一切都是手动完成的，你只需直接进行反向传播，然后使用梯度检查器确保你对梯度的数值估计与在反向传播中计算的梯度一致。这在很长一段时间内都是非常标准的做法，但今天当然。

使用自动求导引擎是标准做法，但这确实很有用，我认为人们在直观上理解这些神经网络是如何工作的，所以我认为这是一个很好的练习，再次强调，这是我们想要达到的地方。

![](img/86f8499f4a173e8882bce59ebef07fb4_60.png)

这是我们当时实现的 Jupyter Notebook，我们将保持一切不变，因此我们仍将有一个具有批量归一化层的两层多层感知器，因此正向传播基本上与本次讲座相同，但这里我们将去掉损失的反向传播，而是将手动编写反向传播。

![](img/86f8499f4a173e8882bce59ebef07fb4_62.png)

本次讲座的启动代码，我们在这个 Notebook 中成为反向传播的忍者，这里的前几个单元与我们熟悉的完全相同，因此我们正在进行一些导入，加载数据集。

![](img/86f8499f4a173e8882bce59ebef07fb4_64.png)

处理数据集的过程没有改变，现在我引入一个工具函数。

![](img/86f8499f4a173e8882bce59ebef07fb4_66.png)

稍后将用于比较梯度，具体来说，我们将有我们手动估计的梯度，以及 PyTorch 计算的梯度，我们将检查其正确性，假设当然 PyTorch 是正确的。

![](img/86f8499f4a173e8882bce59ebef07fb4_68.png)

然后这里是我们相当熟悉的初始化，因此我们有角色的嵌入表、第一层、第二层以及中间的批量归一化，并在这里创建所有参数。你会注意到我稍微改变了初始化，以使其成为小数字，因此通常你会将偏差初始化为零，而我将其设置为。

要生成小的随机数，我这样做是因为如果你的变量初始化为零，有时会掩盖梯度实现中的错误，因为当一切都为零时，它会简化并给出一个更简单的梯度表达式，而不是你本来会得到的。因此，通过生成小数字，我试图揭示。

在这些计算中你也注意到我在第一层使用了 b1。我尽管在之后使用了批量归一化，但仍然使用了偏置，所以这通常不是你所做的事情，因为我们谈到过你不需要偏置，但我在这里这样做只是。

![](img/86f8499f4a173e8882bce59ebef07fb4_70.png)

为了好玩，因为我们将有一个与之相关的梯度，并且我们可以检查我们是否。

![](img/86f8499f4a173e8882bce59ebef07fb4_72.png)

即使这个偏置是多余的，仍然正确计算，因此在这里我计算一个单一的批次，然后我在这里进行前向传播。现在你会注意到，前向传播的长度显著扩展，与我们所习惯的相比，这里的前向传播仅在这里。前向传播之所以更长有两个原因，首先这里我们只是进行了一个 fdath 交叉熵，但在这里我。

![](img/86f8499f4a173e8882bce59ebef07fb4_74.png)

带回一个明确的损失函数实现，同时我将实现分解成可管理的部分，所以我们在前向传播中有很多更多的中间张量。这是因为我们即将向后计算梯度，从底部到顶部，因此我们将向上移动，就像我们。

例如，前向传播中的锁定属性张量，在反向传播中我们将有一个 d。锁定属性，它将存储损失对锁定属性张量的导数。因此，我们将在这次反向传播中伪装每一个这些张量，并沿途计算它。例如，我们有 a、b 和 raw，在这里我们将计算一个 d b。

和 raw，所以在这里我告诉 pytorch 我们想保留所有这些中间值的梯度。因为在练习一中我们将计算反向传播，所以我们将计算所有这些 d 变量，并使用我上面介绍的 cmp 函数来检查我们相对于 pytorch 所告诉我们的正确性，这将是练习一。

反向传播整个图形，现在快速预览一下第二个练习将要发生的事情，下面我们完全分解了损失，并手动反向传播通过所有构成它的小原子片段，但在这里我们将损失合并成一个交叉熵调用，而是将通过分析推导出。

使用数学和纸笔，我们将损失对 logits 的梯度进行计算，而不是逐个小块进行反向传播，我们将分析性地推导出这个梯度，并且我们将实现它，这样效率更高，稍后我们会看到。然后我们将对批量归一化做同样的事情，因此不再拆分。

批量处理为所有的小组件，我们将使用笔和纸以及数学和微积分来推导通过批量处理层的梯度，因此我们将以更有效的方式计算反向传播，而不是独立地反向传播每个小部分，所以这将是练习三。

在练习四中，我们将把所有内容结合在一起，这是训练这个两层 MLP 的完整代码，我们将基本上插入我们的手动反向传播，我们将获取损失的反向传播，你将基本上看到你可以完全使用自己的代码获得所有相同的结果，唯一从 PyTorch 中使用的是 torch。

张量使计算更高效，但除此之外，你将完全理解在你的警报中需要前向和反向传播的内容，并进行训练。

![](img/86f8499f4a173e8882bce59ebef07fb4_76.png)

我认为这将是非常棒的，所以让我们开始吧。 好的，我已经运行了这个笔记本的所有单元。

![](img/86f8499f4a173e8882bce59ebef07fb4_78.png)

到这里，我将擦除这个内容，并开始实现反向传播。 从 delockprobs 开始，我们想要了解这里应该放置什么，以计算损失相对于所有 lockprobs 张量元素的梯度。 现在我要给出答案，但我想在这里做个简短的说明，我认为这在教学上会非常有用。

![](img/86f8499f4a173e8882bce59ebef07fb4_80.png)

你实际上是要进入这个视频的描述中找到这个无聊笔记本的链接。 你可以在 github 上找到它，也可以在 google colab 上找到，因此你不必安装任何东西，你只需去 google colab 的网站，就可以尝试自己实现这些导数或梯度，如果你不能做到，欢迎来我的视频看看我怎么做。

所以请先自己尝试，然后再看看我如何给出答案，我认为这对你来说会是最有价值的，我推荐你这样进行这次讲座。 所以我们来。

![](img/86f8499f4a173e8882bce59ebef07fb4_82.png)

从 delockprobs 开始，delockprobs 将保存损失相对于所有 lockprobs 元素的导数。 lockprobs 中包含什么？

形状是 32 乘 27，因此 delockprobs 也应该是大小为 32 乘 27 的数组，这不会让你感到惊讶，因为我们想要损失相对于所有元素的导数，所以这些大小总是会相等。 那么 lockprobs 如何影响损失呢？ 好吧，损失是负的 lockprobs，索引范围为 n。

yb 的平均值。 现在提醒一下，yb 基本上是一个包含所有正确索引的数组。 所以我们这里做的是取一个大小为 32 乘 27 的 lockprops 数组。

![](img/86f8499f4a173e8882bce59ebef07fb4_84.png)

对，然后我们在每一行中进行操作，在每一行中我们插入，插入出。索引 8，然后 14 和 15，依此类推。所以我们沿着行向下，这是迭代器的范围 n。然后我们总是将指定列的索引从这个张量 yb 中插出。所以在零行我们取第八列，在第一行我们取第 14 列。

等等。所以 lockprobs 会提取出所有正确下一个的锁定概率。

![](img/86f8499f4a173e8882bce59ebef07fb4_86.png)

序列中的字符。所以这就是它的作用，当然，它的形状或大小是。

![](img/86f8499f4a173e8882bce59ebef07fb4_88.png)

32，因为我们的批量大小是 32。所以这些元素被提取出来，然后它们的平均值和。

![](img/86f8499f4a173e8882bce59ebef07fb4_90.png)

负值变成损失。所以我总是喜欢用简单的例子来理解。

![](img/86f8499f4a173e8882bce59ebef07fb4_92.png)

导数的数值形式。这里发生的事情是，一旦我们提取出这些例子。

![](img/86f8499f4a173e8882bce59ebef07fb4_94.png)

我们取平均值，然后取负值。所以损失基本上可以这样写。是负的 a 加 b 加 c 的总和，而这三个数字的平均值可以说是负的。除以三。这就是我们如何实现三个数字 a，b，c 的平均，尽管我们这里实际上有 32 个数字。那么，损失对 dA 的影响是什么呢？对吧？那么。

如果我们从数学上简化这个表达式，这就是负三分之一的 a 加上负三分之一的 b 加上负三分之一的 c。那么，损失对 dA 的影响是什么？

这只是负的三分之一。所以你可以看到，如果我们不仅有 a，b 和 c，而是有 32 个数字，那么损失对 d 的影响，你知道，每一个数字将是一个除以 n。更一般地说，因为 n 是批量的大小，在这种情况下是 32。所以损失对 lockprobs 的影响。

![](img/86f8499f4a173e8882bce59ebef07fb4_96.png)

在所有这些地方是负的三分之一。现在，lockprobs 内部的其他元素呢？

因为 lockprobs 是一个大型数组。你会看到 lockprobs 的检查是 32 乘 27，但只有 32 个参与损失计算。那么其他没有被提取出的元素的导数是什么？好吧，它们的损失直观上是零。所以它们的梯度直观上也是零。这是因为它们没有参与损失。

所以这个张量内部的大多数数字并未影响损失。因此，如果我们改变这些数字，损失不会改变。这相当于我们说，损失对它们的变化率为零。它们没有影响。所以这是一个实现的方法。

![](img/86f8499f4a173e8882bce59ebef07fb4_98.png)

这个导数。然后我们从形状为 32 乘 27 的零开始，或者我们可以简单地说。

![](img/86f8499f4a173e8882bce59ebef07fb4_100.png)

由于我们不想硬编码数字，因此不这样做，让我们做 torched 的 zeros，像 lockprobs。因此，基本上这将创建一个完全形状与 lockprobs 一样的零数组。然后我们需要在确切这些位置设置导数为负的 1/n。所以我们可以这样做。以相同方式索引的 lockprobs 将被设置为负值。

1 除以 0，除以 n，对吧，就像我们在这里推导的那样。那么现在让我擦掉这些推理。

![](img/86f8499f4a173e8882bce59ebef07fb4_102.png)

然后这是 D lockprobs 的候选导数。让我们取消注释第一行。

![](img/86f8499f4a173e8882bce59ebef07fb4_104.png)

检查这是正确的。好的，所以 CMP 运行了，让我们回到 CMP。你看到什么。

![](img/86f8499f4a173e8882bce59ebef07fb4_106.png)

它所做的是计算我们计算的值 dt 是否恰好等于 pytorch 计算的 t.dot.grad。然后确保所有元素完全相等。然后将其转换为单个布尔值，因为我们不想要布尔张量。我们只想要布尔值。然后在这里，我们确保，如果它们不是。 

完全相等，可能因为某些浮点数问题它们大致相等，但它们非常，非常接近。所以在这里我们使用 torched 的 all close，这有一点点的波动，因为有时你可以得到非常，非常接近的结果。但是如果你使用略微不同的计算，因为浮点运算。

你可能会得到一个略有不同的结果。所以这是在检查你是否得到一个大致接近的结果。然后这里我们检查的是最大值，基本上是具有最高差异的值。这个差异以及这两者之间的绝对值差异是什么。因此，我们在打印是否有确切的相等或近似相等。

以及最大的差异是什么。所以在这里，我们看到我们实际上。

![](img/86f8499f4a173e8882bce59ebef07fb4_108.png)

![](img/86f8499f4a173e8882bce59ebef07fb4_109.png)

具有确切的相等。因此，当然，我们也有近似相等。最大差异恰好为零。所以基本上，我们的 delog props 和 pytorch 是完全相等的。

![](img/86f8499f4a173e8882bce59ebef07fb4_111.png)

被计算为 log props.dot.grad 在其反向传播中。因此到目前为止，我们做得很好。

![](img/86f8499f4a173e8882bce59ebef07fb4_113.png)

![](img/86f8499f4a173e8882bce59ebef07fb4_114.png)

好的，所以让我们继续我们的反向传播。我们有 log props 通过一个 log 依赖于 props。因此 props 的所有元素都逐元素应用了 log。现在，如果我们想要 deep props，记得你的微图训练，我们有一个 log 节点。它接收 props 并创建 log props。而 deep props 将是那个单独操作 log 的局部导数乘以。

关于其输出的导数损失，在这种情况下是 D log props。那么这个操作的局部导数是什么呢？嗯，我们是逐元素取 log，接下来我们可以在这里做。

![](img/86f8499f4a173e8882bce59ebef07fb4_116.png)

看，嗯，五个中的任何一个是你的朋友，log x 的导数 d by dx 简单地就是 1/x。

![](img/86f8499f4a173e8882bce59ebef07fb4_118.png)

所以在这种情况下，x 是问题。因此我们有 d by dx 是 1/x，也就是 1/问题。然后这是局部导数。然后我们希望训练它。因此这是。

![](img/86f8499f4a173e8882bce59ebef07fb4_120.png)

链式法则，乘以 do log props。接下来让我取消注释并在当前位置运行单元格。

![](img/86f8499f4a173e8882bce59ebef07fb4_122.png)

我们看到我们在这里计算的 props 的导数是完全正确的。

![](img/86f8499f4a173e8882bce59ebef07fb4_124.png)

所以注意这里是如何工作的。props 将被反转，然后在这里逐元素相乘。所以如果你的 props 非常接近 1，这意味着你的网络当前正确地预测了字符，那么这将变成 1/1，V log props 就直接传递。但如果你的概率分配不正确，比如正确的。

这里的字符概率非常低，然后用 1.0 除以它会提升这个概率，然后乘以问题。所以基本上，这行的直观意义在于，它正在提升当前分配了非常低概率的样本的梯度。你可以这么看。接下来是计数一些 imp。因此我们想要这个的导数。

![](img/86f8499f4a173e8882bce59ebef07fb4_126.png)

![](img/86f8499f4a173e8882bce59ebef07fb4_127.png)

现在让我在这里暂停一下，介绍一下这里发生的一般情况，因为我知道这有点困惑。我们有来自神经网络的 logits。这里我正在做的是找出每一行的最大值，并为数值稳定性减去它。我们讨论过如果不这样做，你会遇到一些数值问题。

logits 取了两个大的值，因为我们最终对它们进行指数运算。因此这样做仅仅是为了数值安全。然后这里对所有的 logits 进行指数运算以创建我们的计数。接着我们希望对这些计数求和并进行归一化，使得所有的 probes 之和为 1。现在这里我没有使用 1/计数和，而是使用了负一的幂。

数学上它们是相同的。我只是发现 pytorch 在除法的反向传播实现上有问题，虽然它会给出一个真实结果，但在 star star，负一 的情况下并不会发生。所以我使用这个公式。但基本上，这里发生的一切。

![](img/86f8499f4a173e8882bce59ebef07fb4_129.png)

我们得到了 logits，我们想对它们进行指数运算，并对计数进行归一化，以创建我们的概率。只是这个过程发生在多行之间。所以现在，我们首先想要计算导数，我们想要回传到计数，过一会儿再回传到计数。那么，计数的总和应该是多少？现在我们实际上需要小心。

![](img/86f8499f4a173e8882bce59ebef07fb4_131.png)

在这里，因为我们必须仔细检查形状。因此，计数的形状，以及计数和的形状是不同的。特别是计数是 32 乘 27，但这个计数。

![](img/86f8499f4a173e8882bce59ebef07fb4_133.png)

![](img/86f8499f4a173e8882bce59ebef07fb4_134.png)

和的形状是 32 乘 1。因此在这个乘法中，我们也有隐式广播。

![](img/86f8499f4a173e8882bce59ebef07fb4_136.png)

pytorch 需要这样做，因为它需要将 32 个数字的列张量水平复制 27 次，以对齐这两个张量，以便进行元素乘法。因此，实际上。

![](img/86f8499f4a173e8882bce59ebef07fb4_138.png)

这看起来像是以下的玩具示例。我们真正拥有的是 props 是计数乘消耗。所以这是 a 等于 a 乘 b，但 a 是三乘三的，b 只是三乘一的列张量。因此，pytorch 在内部复制了 b 的元素，进行了这个操作。

![](img/86f8499f4a173e8882bce59ebef07fb4_140.png)

在所有列之间。例如，b 中的第一个元素 b1 将被复制。

![](img/86f8499f4a173e8882bce59ebef07fb4_142.png)

在这个乘法中，在所有列之间。现在我们试图进行回传。

![](img/86f8499f4a173e8882bce59ebef07fb4_144.png)

这个操作用于计数。那么，当我们计算这个导数时，这一点很重要。

![](img/86f8499f4a173e8882bce59ebef07fb4_146.png)

认识到这两个看起来像是一个单一操作，但实际上是两个操作，顺序执行。pytorch 进行的第一个操作是复制了这个列张量，基本上在所有列上复制了 27 次。这是第一个操作，复制。然后第二个操作是乘法。所以让我们先。

回溯乘法。如果这两个数组大小相同，而我们只有 a 和 b，它们都是三乘三的，那么我们该如何回传乘法？

如果你只有标量而不是张量，那么如果 c 等于 a 乘 b，那么 c 关于 b 的阶数是什么？嗯，它只是 a。因此，这就是局部导数。在我们的案例中，我在进行乘法并回传通过乘法本身。

![](img/86f8499f4a173e8882bce59ebef07fb4_148.png)

![](img/86f8499f4a173e8882bce59ebef07fb4_149.png)

这是逐元素的，将是局部导数，在这种情况下，仅仅是计数。因为计数就是 a。因此这是局部导数，然后乘以链式法则。deprops。因此这里是导数或梯度，但相对于复制的 b。但是我们没有复制的 b，只有一个单一的 b 列。那么我们现在如何进行反向传播呢？

通过复制？直观上，这个 b 一是同一个变量，只是被多次重用。因此，你可以将其视为在 micrograd 中遇到的情况。因此在这里，我只是拉出我们在 micrograd 中使用的随机图形。我们有一个示例，其中一个单一节点的输出输入到基本上是图的两个分支，直到最后一个函数。

我们讨论的是在反向传递中正确的做法是需要对到达任何一个节点的所有梯度进行求和。因此，在这些不同的分支中，梯度将进行求和。如果一个节点被多次使用，则在反向传播期间对其所有用法的梯度求和。

![](img/86f8499f4a173e8882bce59ebef07fb4_151.png)

所以在这里，b 一在所有这些列中被多次使用。因此，在这里正确的做法是沿着所有行进行横向求和。因此在维度一中求和。但我们想保留这个维度，以便计数它们和它的梯度将完全保持相同的形状。因此我们想确保保持它们的真实。因此我们不会丢失这个维度。

这将使计数的形状完全是 32 乘 1。因此揭示这个比较为。

![](img/86f8499f4a173e8882bce59ebef07fb4_153.png)

好的，运行这个，我们看到我们得到了完全匹配。因此这个导数是完全正确的。

![](img/86f8499f4a173e8882bce59ebef07fb4_155.png)

让我擦掉这个。现在让我们也反向传播到计数，这是这里的另一个变量。

![](img/86f8499f4a173e8882bce59ebef07fb4_157.png)

用于创建道具。因此，从道具到某些信息的计数，我们刚刚做了这个，让我们深入了解计数。所以计数将是计数是一个。所以 dc 通过 da 仅仅是 b。因此，所以它是某些信息的计数。然后乘以链式法则，d 道具。现在，计数，我在的是三，二乘一。D 道具是 32 乘 27。因此这些将广播良好，并将给我们 decounts。没有。

这里需要额外的求和。因为在这里进行相乘时，会发生广播，因为计数 am 需要再次复制以正确相乘。

![](img/86f8499f4a173e8882bce59ebef07fb4_159.png)

d 道具。但这将获得正确的结果。因此，就单一操作而言。我们已经从道具反向传播到计数，但我们实际上无法检查计数的导数。

![](img/86f8499f4a173e8882bce59ebef07fb4_161.png)

我稍后会处理这个。原因是因为计数的一部分依赖于计数。因此这里有第二个分支，我们必须完成，因为计数的一部分在反向传播。到计数的一部分，而计数的一部分将反向传播到计数。因此计数是一个被使用了两次的节点。

它在这里被用作道具，穿过这个其他分支经过计数的一些信息。所以即使我们已经计算了它的第一个贡献，我们仍然必须计算。

![](img/86f8499f4a173e8882bce59ebef07fb4_163.png)

之后它的第二个贡献。好的，我们继续这个分支。我们有导数。

![](img/86f8499f4a173e8882bce59ebef07fb4_165.png)

对于计数的一部分，现在我们想要计数的一部分的导数。因此计数的一部分的导数等于，这个操作的局部导数是什么？所以这基本上是元素两次的 1 除以计数的一部分。所以计数的一部分的负一次方等同于 1 除以计数的一部分。如果我们从墙那里去。

![](img/86f8499f4a173e8882bce59ebef07fb4_167.png)

alpha，我们看到 x 是负的，d 对 d x 的导数基本上是负 x 的负二次方，对吧？负一除以平方与负 x 的负二次方是相同的。

![](img/86f8499f4a173e8882bce59ebef07fb4_169.png)

因此，计数的一部分的局部导数将是负计数的一部分的负二次方。这是局部导数乘以链式法则，即计数的一些信息。因此就是计数的一部分。

![](img/86f8499f4a173e8882bce59ebef07fb4_171.png)

让我们取消注释并检查我是否正确。好的，所以我们有完美的相等。

![](img/86f8499f4a173e8882bce59ebef07fb4_173.png)

这里没有任何形状的草图，因为这些是相同的形状。

![](img/86f8499f4a173e8882bce59ebef07fb4_175.png)

好的，接下来我们想要通过这条线进行反向传播。我们有的计数是计数的一部分。

![](img/86f8499f4a173e8882bce59ebef07fb4_177.png)

沿着行来进行一些操作。因此我在这里写了一些帮助信息。我们必须记住计数的维度，当然是 32 乘 27，而计数的一部分是 32 乘 1。因此在这个反向传播中，我们需要将这一列的根转换为机器学习中的根数组。

![](img/86f8499f4a173e8882bce59ebef07fb4_179.png)

那么这个操作在做什么？我们正在输入某种输入，比如一个 3 乘 2 的矩阵 A，并将行相加到列张量 B 中。B1 B2 B3 基本上就是这样。

![](img/86f8499f4a173e8882bce59ebef07fb4_181.png)

现在我们有损失对 B 的导数，B 的所有元素。

![](img/86f8499f4a173e8882bce59ebef07fb4_183.png)

现在我们想要交付损失，相对于这些小的 a。那么 b 是如何依赖于 a 的，这基本上就是我们要追求的，即这个操作的局部导数是什么。

![](img/86f8499f4a173e8882bce59ebef07fb4_185.png)

好吧，我们可以看到这里 B1 仅依赖于这些元素。B1 对这里所有元素的导数是零。但对于这些元素，比如 a one one、a one two 等，局部导数是一个。因此 db one 对 d a one one 的导数就是一个。所以是一个、一个和一个。因此当我们计算损失对 B1 的导数时。

B1 对这些输入的局部导数在这里是零，但在这些元素上是一个。因此，在链式法则中，我们有局部导数乘以 B1 的某种导数。由于局部导数在这三个元素上是一个，乘以 B1 的导数的局部导数将只是 B1 的导数。

所以你可以把它看作一个路由器。基本上，添加是梯度的路由器。无论来自上方的梯度是什么，它都会均等地路由到所有参与该加法的元素。因此在这种情况下，B1 的导数将均等地流向 a one one、a one two 和 a one three 的导数。

因此，如果我们对 B 的所有元素和这个列张量的导数，即我们刚刚计算的 d counts sum。

![](img/86f8499f4a173e8882bce59ebef07fb4_187.png)

![](img/86f8499f4a173e8882bce59ebef07fb4_188.png)

我们基本上看到这意味着这些现在流向 A 的所有元素，并且它们是水平流动的。因此，我们想要的是取大小为 32 乘 1 的计数总和，并希望将其水平复制 27 次以创建 32。

![](img/86f8499f4a173e8882bce59ebef07fb4_190.png)

通过 27 数组。因此，有很多方法来实现这个操作。当然，你可以简单地复制。

![](img/86f8499f4a173e8882bce59ebef07fb4_192.png)

张量。但我认为一个简单的方式是计数仅仅是 torch dot once，就像是一个形状为计数的二维数组。因此 32 乘 27 的计数总和。这样，我们就让广播在这里基本上实现了。

![](img/86f8499f4a173e8882bce59ebef07fb4_194.png)

复制。你可以这样看。但我们也必须小心，因为。

![](img/86f8499f4a173e8882bce59ebef07fb4_196.png)

计数已经全部计算。我们在这里早些时候计算过。这只是第一分支，而我们现在完成第二分支。因此，我们需要确保这些梯度相加。

![](img/86f8499f4a173e8882bce59ebef07fb4_198.png)

所以加等于。然后在这里，让我们注释掉比较，并确保万事如意。

![](img/86f8499f4a173e8882bce59ebef07fb4_200.png)

我们得到了正确的结果。因此，pytorch 也同意我们的这个梯度。好的，希望如此。

![](img/86f8499f4a173e8882bce59ebef07fb4_202.png)

我们现在对此有了一些了解。Counts 是 norm logits 的一个元素。因此现在我们想要 D norm logits。因为这是一个元素操作，一切都很简单。e 的 X 的局部导数是什么？它著名地就是 e 的 X。因此这是局部导数。这是局部导数。我们已经计算过了，它在 counts 里面。所以我们。

也可以潜在地重用 counts。那是局部导数乘以，呃，decounters。虽然看起来有点搞笑，但常量 decounters 是对 norm logits 的迭代。现在让我们。

![](img/86f8499f4a173e8882bce59ebef07fb4_204.png)

![](img/86f8499f4a173e8882bce59ebef07fb4_205.png)

抹去这个，让我们验证一下，看起来很好。所以这是正常的 digits。好的，我们在这里。

![](img/86f8499f4a173e8882bce59ebef07fb4_207.png)

在这一行，现在是正常的 digits。我们有这个，我们正在尝试计算 logits 和 logit maxes。因此在通过这一行进行反向传播时。我们必须在这里小心，因为形状再次不相同。因此这里发生了一个隐式广播。因此正常。

![](img/86f8499f4a173e8882bce59ebef07fb4_209.png)

digits 的形状是 32 乘 27。Logits 也是如此。但 logit maxes 仅为 32 乘 1。因此这里在减法上有一个广播。现在我尝试再次写出一个例子。我们基本上有 C 等于 A 减去 B。我们看到由于形状，这些是三乘三，但这一只是一个列向量。因此，例如，C 的每个元素。

我们必须看看它是如何形成的。C 的每个元素只是对应的 A 的元素，基本上减去那个相关的 B。所以现在很清楚，所有这些 C 对其输入的导数对于对应的 A 都是 1，而对应的 B 则是负 1。因此，C 上的导数会平等地流向对应的 A。

然后也到对应的 B。但除此之外，B 也被广播。因此我们将不得不做额外的求和，就像我们之前所做的那样。当然，B 的导数将经历 A 减去，因为这里的局部导数是负 1。所以 C 32 乘 D B 3 是负 1。因此让我们实现这一点。基本上。

D logits 将精确复制导数。

![](img/86f8499f4a173e8882bce59ebef07fb4_211.png)

正常的 digits。因此 D logits 等于 D 正常 logits，我会做一个 dot clone 以确保安全。所以我们只是在做一个复制。然后我们有 D logit Maxis 将是 D 正常 logits 的负值，带有负号。然后我们必须小心，因为 logit Maxis 是一个列向量。正如我们之前所看到的，因为我们不断复制相同的元素到所有列。

然后在反向传播中，因为我们不断重用这个。这些都是就像是对那个变量的单独分支。因此，我们必须沿着一个进行求和，这样它们就会保持相等为真。

![](img/86f8499f4a173e8882bce59ebef07fb4_213.png)

以便我们不破坏这个维度。然后 logit 最大值将保持相同的形状。

![](img/86f8499f4a173e8882bce59ebef07fb4_215.png)

现在我们必须小心，因为这个 D logits 不是最终的 D logits。这是因为。

![](img/86f8499f4a173e8882bce59ebef07fb4_217.png)

我们不仅能通过这里获得梯度信号到 logits，而且 logit 最大值是 logit 的函数，这就是第二个分支到 logits。因此这还不是我们对 logits 的最终导数。

![](img/86f8499f4a173e8882bce59ebef07fb4_219.png)

我们稍后会回到第二个分支。现在，D logit 最大值是最终的导数。

![](img/86f8499f4a173e8882bce59ebef07fb4_221.png)

所以让我在这里取消注释这个 CMP，然后我们来运行一下。看看 logit 最大值，如果 PyTorch 同意我们的看法。

![](img/86f8499f4a173e8882bce59ebef07fb4_223.png)

![](img/86f8499f4a173e8882bce59ebef07fb4_224.png)

这就是通过这一行的导数。在我们继续之前，我想在这里暂停一下。

![](img/86f8499f4a173e8882bce59ebef07fb4_226.png)

简而言之，我想看看这些 logit 最大值及其梯度。我们之前在上一讲中提到，我们这样做的唯一原因是为了我们在这里实现的 softmax 的数值稳定性。我们谈到如果你获取这些 logit 的话，对于这些例子中的任何一个，也就是这个 logit 张量的一行。如果你加或减任何。

如果所有元素的值相等，那么探针的值将保持不变。你没有改变 softmax。此操作唯一的作用是确保 X 不会溢出。我们使用最大值的原因是我们保证每一行的 logit 最高的数字是零。因此这将是安全的。这基本上带来了后果。

如果改变 logit 最大值不改变属性，因此不改变损失，那么 logit 最大值的梯度应该为零。因为这两件事是相同的。所以确实我们希望这个数值非常非常小。确实我们希望这是零。现在因为。

![](img/86f8499f4a173e8882bce59ebef07fb4_228.png)

浮点数的某种奇怪性，这并不完全等于零，只有在某些行中是这样。但是我们得到的极小值像是负九或十的指数级数值。所以这告诉我们。

![](img/86f8499f4a173e8882bce59ebef07fb4_230.png)

logit 最大值的值并没有影响损失，正如它们不应该那样。这感觉有点。

![](img/86f8499f4a173e8882bce59ebef07fb4_232.png)

诚实地说，通过这个分支反向传播有点奇怪。因为如果你有任何实现，比如。

![](img/86f8499f4a173e8882bce59ebef07fb4_234.png)

F dot 交叉熵和 pytorch，并且你将所有这些元素一起处理，而不是逐个进行反向传播，那么你可能会假设这里的导数是完全为零。因此，你会在某种程度上跳过这个分支，因为它只是为了数值稳定性。但有趣的是，即使你将所有内容拆分成完整的。

原子，你仍然可以按照你希望的方式进行计算，以确保数值稳定性，正确的事情会发生，你仍然会得到非常非常小的梯度。基本上，反映出这些值与最终损失无关。

![](img/86f8499f4a173e8882bce59ebef07fb4_236.png)

好的，现在让我们继续通过这一行进行反向传播。我们刚刚计算了 logit Maxis，现在我们想要通过这个第二个分支向 logits 进行反向传播。在这里，当然，我们取了 logits，并沿着所有行取了最大值，然后查看其值。现在，这在 pytorch 中的工作方式是，max 返回值和。

![](img/86f8499f4a173e8882bce59ebef07fb4_238.png)

它返回这些值的索引，也就是列出最大值的位置。现在在前向传播中，我们只使用了值，因为那是我们所需要的。但在反向传播中，知道这些最大值发生的位置是极其有用的，我们有发生的位置的索引。这当然会帮助我们进行反向传播。因为应该是什么。

![](img/86f8499f4a173e8882bce59ebef07fb4_240.png)

在这种情况下，反向传播应该在这里吗？我们有一个 32x27 的 logis 张量，在每一行中，我们找到最大值，然后将该值提取到 logit Maxis 中。因此，直观上来看，基本上通过这里流动的导数应该是 1 乘以局部导数，对于被提取的适当条目局部导数为 1，然后再乘以全局导数。

这是关于 logit Maxis 的内容。所以实际上我们在这里所做的，如果你仔细想想，就是我们需要取出 delogit Maxis，然后将其散布到这些 logits 的正确位置，从哪里获取最大值。因此我想出了这一行代码来实现这一点。让我简单说一下。

![](img/86f8499f4a173e8882bce59ebef07fb4_242.png)

这里有很多内容。因此，这一行代码，你可以做得与我们在这里做的非常相似。

![](img/86f8499f4a173e8882bce59ebef07fb4_244.png)

在这里我们创建一个零数组，然后填充正确的元素。因此我们在这里使用索引。

![](img/86f8499f4a173e8882bce59ebef07fb4_246.png)

我们会将它们设置为 1。但你也可以使用一个心跳。因此，在这个心跳下，我取了 logit Maxis 的第一个维度索引，并告诉 PyTorch，每一个这些张量的维度应该是 27。因此，这将会发生什么。

![](img/86f8499f4a173e8882bce59ebef07fb4_248.png)

嗯，好的，我道歉，这太疯狂了。Beau Thieau 的时间展示了这一点。它实际上只是一个数组。

![](img/86f8499f4a173e8882bce59ebef07fb4_250.png)

每行的 Maxis 来自于哪里，该元素为 1，其他所有元素均为零。所以每行都是一个 one-hot 向量，这些索引现在填充了一个单一的 1。

![](img/86f8499f4a173e8882bce59ebef07fb4_252.png)

在适当的位置。然后我在这里做的就是与 logit Maxis 相乘。请记住。

![](img/86f8499f4a173e8882bce59ebef07fb4_254.png)

请记住，这是一个 32 乘 1 的列。因此，当我将其与 logit Maxis 相乘时。

![](img/86f8499f4a173e8882bce59ebef07fb4_256.png)

logit Maxis 会广播，这一列将被复制，然后逐元素相乘将确保这些仅路由到其中一个被打开的位。因此，这是一种实现这种操作的另一种方式。这两种方式都可以。

![](img/86f8499f4a173e8882bce59ebef07fb4_258.png)

![](img/86f8499f4a173e8882bce59ebef07fb4_259.png)

使用。只是想展示一种等效的方法。我使用了加等于，因为我们已经在这里计算了 logits。这是现在的第二个分支。所以让我们。

![](img/86f8499f4a173e8882bce59ebef07fb4_261.png)

查看 logits 并确保这是正确的。我们看到我们得到了完全正确的答案。

![](img/86f8499f4a173e8882bce59ebef07fb4_263.png)

接下来，我们想要继续处理这里的 logits。这是矩阵乘法和线性层中的偏置偏移的结果。因此，我打印出了所有这些中间张量的形状。我们看到 logits 当然是 32 乘 27，正如我们刚才看到的。然后这里的 H 是 32 乘 64。这是 64 维的隐藏状态。然后这个 w 矩阵将这 64 维向量投影到。

27 维。然后有一个 27 维的偏移量，这是一个一维向量。现在我们应该注意，这里的加法实际上是广播，因为 H 乘以 w2 会给我们一个 32 乘 27 的结果。然后这个加 B2 在这里是一个 27 维的向量。现在在广播的规则中，这个偏置向量将会发生什么是，这个 27 维的一维向量。

将与左侧填充维度为一对齐。基本上，它将变成一个行向量。然后它将垂直复制 32 次，形成 32 乘 27 的矩阵。接着，有一个元素偏置乘法。现在，问题是我们如何从 logits 反向传播到隐藏状态，即权重矩阵 w2 和偏置 B2。你可能会认为我们需要去某个地方。

矩阵微积分。然后我们必须查找矩阵乘法的导数。但是，实际上你不需要做任何这些。你可以回到基本原理，在纸上自己推导出来。具体来说，我喜欢做的，发现对我有效的，是找一个具体的小例子，然后完全写出来。

然后在分析的过程中，了解这个小例子是如何工作的。你将理解更广泛的模式。你将能够概括并写出这些导数在像这样的表达式中的完整一般公式。那么我们试试这个。所以在这里的低预算制作中，但。

我在这里所做的就是在纸上写出来。实际上我们感兴趣的是，我们有 A 乘以 B 加上 C。这产生了 D。我们有损失相对于 D 的导数。我们想知道损失相对于 A、B 和 C 的导数。这里是两个维度的矩阵乘法的示例，二维。

乘以一个二维矩阵加上一个只有两个元素的向量 C one 和 C two，给我一个二维矩阵。现在注意到这里有一个叫 C 的偏置向量。偏置向量是 C one 和 C two。但是正如我在这里描述的，那个偏置向量会在广播中变成行向量，并会在垂直方向上复制。所以这里也发生了这种情况。C one C two 被复制。

垂直地。我们看到结果是有两行 C one C two。那么现在当我说写出来时，我的意思就是像这样，基本上将这个矩阵乘法分解成实际在背后发生的事情。因此，矩阵乘法的结果以及它是如何工作的。D one one 是 A 的第一行与 B 的第一列之间的点积的结果。

所以 a one one B one one 加上 a one two B two one 加上 C one，等等，其他所有 D 的元素依此类推。一旦你真的写出来，就会很明显。这只是一堆乘法和加法。我们知道从微分梯度如何区分乘法和加法。

![](img/86f8499f4a173e8882bce59ebef07fb4_265.png)

所以这不再可怕了。这不仅仅是矩阵乘法。这只是繁琐，不幸的是。但这是完全可行的。我们有 DL 对所有这些的导数。我们想要 DL 对所有这些小变量。那么我们如何实现这一点，实际上如何获得梯度？好吧，低预算制作继续在这里。所以让我们举例推导。

关于损失相对于 a one one 的导数。我们在这里看到 a one one 在我们的简单表达式中出现了两次。影响是 D one one 和 D one two。那么，DL 对 D a one one 的导数是什么呢？它是 DL 对 a one one 的导数乘以 D one one 的局部导数，而在这种情况下就是 B one one，因为这就是乘以 a one one 的内容。所以。

同样，D 一二 对于 A 一一 的局部导数就是 B 一二。因此，B 一二 在链式法则中将乘以 DL by D 一二。然后因为 A 一一 被用来生成 D 一一 和 D 一二，我们需要将这两条平行链的贡献相加。

这就是为什么我们通过将这两个贡献相加得到一个正值。这样我们就得到了对 A 的导数 DL by D a。一一。我们可以对 A 的所有其他元素进行完全相同的分析。当你简单地写出来时，这实际上是超级简单的，就像这样的表达式。

你会发现这个矩阵 DL by D a，正是我们想要的，如果我们将它们排列成与 A 相同的形状。因此 A 只是一个 2×2 矩阵。所以这里的 DL by D a 也将是一个形状相同的张量，现在包含导数。因此，DL by D a 一一，等等。我们实际上可以将这里写出的内容表示为一个。

矩阵乘法。因此，恰好是 DL by D 所有我们推导出的公式，通过取梯度实际上可以表示为矩阵乘法。特别地，我们看到这是这两个矩阵的矩阵乘法。所以这是 DL by D，然后矩阵乘以 B，但 B 实际上是转置的。因此，你会看到 B 二一 和 B 一二 有。

位置发生了变化。而之前我们当然有 B 一一、B 一二、B 二一、B 二二。所以你可以看到，这个其他矩阵 B 是转置的。因此，基本上，我们所拥有的故事很简单，通过在非常简单的例子中拆分表达式，得到 DL by D a 其实是 DL by D D 矩阵与 B 转置的乘积。

这就是我们到目前为止所得到的。现在我们也想要关于 B 和 C 的导数。

![](img/86f8499f4a173e8882bce59ebef07fb4_267.png)

现在对于 B，我并没有进行完整的推导，因为老实说，这并不复杂，只是让人烦恼，令人疲惫。你可以自己做这个分析。你会发现，如果你取这些表达式并对 B 进行微分，而不是 A，你会发现 DL by D B 也是一个矩阵乘法。在这种情况下，你需要对矩阵 A 进行转置。

然后将其与 DL by D D 进行矩阵乘法。这就是给你 DL by D B 的原因。然后在偏移量 C 一和 C 二这里，如果你再次对 C 一 进行微分，你会发现像这样的表达式，C 二则是像这样的表达式，基本上你会发现 DL by D C 是简单因为它们只是偏移这些表达式，你只需取。

D 的导数与 D 的 DL 矩阵相乘，你只需对列求和。这会给你 C 的导数。简单来说，矩阵乘法的反向传播就是矩阵乘法，而不是像在标量情况下那样 D 等于 A 乘 B 加 C。我们可以得到一个非常相似的结果，但现在是矩阵乘法。

这是标量乘法的导数。因此，D 对 A 的导数是 DL 与 DD 的矩阵乘 B 转置，这里是 A 转置乘以 DL 与 DD。但在这两种情况下都是矩阵乘法，带有导数和乘法中的另一个项。对于 C 来说，是 A 的求和。现在我会。

![](img/86f8499f4a173e8882bce59ebef07fb4_269.png)

我告诉你一个秘密，我永远记不住刚才得出的反向传播公式，尽管我可以很好地通过这些表达式进行反向传播。之所以能行是因为维度必须匹配。让我给你一个例子。假设我想创建 D H，那么 D H 应该是什么，第一，我必须知道 D H 的形状。

H 的形状必须与之相同，而 H 的形状是 30 到 64。另一个信息是，D H 必须是 D logits 与 W2 某种矩阵乘法的结果。D logits 是 32 到 27，W2 是 64 到 27。要使形状或计数匹配，这里只有一种方式，确实是正确的结果。特别是 H 需要是 32 到 64。

唯一的方法是将 D logits 与 W2 进行矩阵乘法。你看，我必须将 W2 转置，以使维度匹配。所以是 W2 转置。这是唯一的方法来使这两个部分相乘以使形状匹配。

![](img/86f8499f4a173e8882bce59ebef07fb4_271.png)

事实证明这是正确的公式。如果我们来到这里，我们想要 D H，就是 D A，我们看到 D A 是 DL 与 DD 的矩阵乘 B 转置。所以 D logits 乘以 B，即 W2 的转置，这正是我们所拥有的。因此没有必要记住这些公式。同样。

![](img/86f8499f4a173e8882bce59ebef07fb4_273.png)

现在如果我想要 D W2，我知道它必须是 D logits 与 H 的矩阵乘法。

![](img/86f8499f4a173e8882bce59ebef07fb4_275.png)

可能有几个转置，或者其中也有一个转置。我不知道是什么方向，所以我必须查看 W2，看到它的形状是 64 到 27。这必须来自这两个矩阵的乘法。因此，要得到 64 到 27，我需要取 H 并转置，然后进行矩阵乘法。

所以这将变成 64 乘以 32，然后，我需要将你的小调整为 32 乘以 27，这将给我 64 乘以 27。因此，我需要将你的小与这个 D logits 结合起来，形状正是这样。这是使维度匹配的唯一方法，使用矩阵乘法。如果我们来看这里，我们看到这正是这里的内容。因此 A 转置 A 对我们来说是 H。

将其与 D logits 相乘。所以这是 W2，然后是 D B2。

![](img/86f8499f4a173e8882bce59ebef07fb4_277.png)

只是垂直求和，实际上以同样的方式只有一种方法可以使形状匹配。我不必记得这是沿着零轴的垂直求和，因为这是。唯一合理的方法，因为 B2 的形状是 27。因此为了得到这里的 D logits，它是 32 乘以 27。因此，知道这只是对 D logits 在某个方向上的求和，该方向必须是。

零，因为我需要消除这个维度。所以是这个。因此这有点像是。 应急方案。让我复制粘贴并删除这个，让我过来这里。这是我们的反向传播。

![](img/86f8499f4a173e8882bce59ebef07fb4_279.png)

密码是线性层，希望如此。所以现在让我们取消注释这三个，我们来检查一下。

![](img/86f8499f4a173e8882bce59ebef07fb4_281.png)

所有三个导数都正确并运行，我们看到 H，W2 和 B2 都是完全正确的。

![](img/86f8499f4a173e8882bce59ebef07fb4_283.png)

![](img/86f8499f4a173e8882bce59ebef07fb4_284.png)

所以我们通过线性层进行反向传播。接下来我们已经有 H 的导数。

![](img/86f8499f4a173e8882bce59ebef07fb4_286.png)

我们需要通过 10H 反向传播到 H preact。所以我们想要导出 D H preact。在这里我们必须通过 10H 进行反向传播，我们在 micrograd 中已经做过这个。

![](img/86f8499f4a173e8882bce59ebef07fb4_288.png)

我们记得 10H 是一个非常简单的反向公式。不幸的是，如果我只是把 D 放入。

![](img/86f8499f4a173e8882bce59ebef07fb4_290.png)

dx 的 10H 进入α，这让我们感到困惑。它告诉我们这是一个双曲正割。

![](img/86f8499f4a173e8882bce59ebef07fb4_292.png)

x 的平方函数。它并不是特别有用，但幸运的是谷歌图片搜索没有让我们失望。

![](img/86f8499f4a173e8882bce59ebef07fb4_294.png)

它给我们提供了更简单的公式。特别是如果你有 A 等于 10H 的 z，那么 D A。通过 D z 反向传播 10H 就是 1 减去 A 平方。请注意，1 减去 A。平方 A 在这里是 10H 的输出，而不是 10H 的输入 z。因此 D A 通过 D z 在这里。以 10H 的输出为依据进行了公式化。在谷歌图片搜索中我们也有。

完整的推导如果你想要实际使用 10H 的定义并进行数学计算。以找出 1 减去 10H 平方 z。因此 1 减去 A 平方是局部导数。在我们的案例中。

![](img/86f8499f4a173e8882bce59ebef07fb4_296.png)

也就是 1 减去 10H 的平方输出，这里是 H。所以它是 H 的平方，那就是局部导数，然后乘以链式法则 D H。所以这将是我们的候选实现。

![](img/86f8499f4a173e8882bce59ebef07fb4_298.png)

所以如果我们来到这里然后取消注释，让我们期待最好的结果，并且我们得到了正确的答案。

![](img/86f8499f4a173e8882bce59ebef07fb4_300.png)

![](img/86f8499f4a173e8882bce59ebef07fb4_301.png)

好的，接下来我们有 D H P 激活，我们想要将其反向传播到 gain、B 和原始以及 B 和 bias。所以这里这是最佳项参数 B 和 gain 以及 bias 在最佳项内，采用的是 B 和原始，这是精确的单位高斯，并对其进行缩放和偏移。这些是最佳项的参数。现在这里有一个乘法，但值得注意的是。

注意，这个相乘与这里的矩阵相乘是非常非常不同的。矩阵相乘。我们是这些矩阵的行与列之间的点积。这是一个元素。

![](img/86f8499f4a173e8882bce59ebef07fb4_303.png)

两次相乘，所以事情变得简单了很多。现在我们确实需要对这行代码中的一些广播操作保持谨慎。所以你可以看到 B 和 gain 以及 B 和 bias 是。

![](img/86f8499f4a173e8882bce59ebef07fb4_305.png)

一行 64，但 H 的预激活和 B 以及原始是 32 行 64。因此我们必须对此保持谨慎，并确保所有的形状都能正常工作，并且广播能正确回传。

![](img/86f8499f4a173e8882bce59ebef07fb4_307.png)

所以特别地，让我们从 D B 和 gain 开始。因此 D B 和 gain 应该是，这里再次是元素两次相乘，无论何时我们有 A 乘以 B 等于 C，我们看到局部导数。这里如果这是 A，局部导数就是 B 另一个。所以局部导数就是。只是 B 和原始，然后乘以链式法则。因此 D H 的预激活。这是候选梯度。

![](img/86f8499f4a173e8882bce59ebef07fb4_309.png)

现在我们再次需要小心，因为 B 和 gain 的大小是一行 64，但这里会是 32 行 64。因此，在这种情况下正确的做法当然是，B 和 gain 这里是一个包含 64 个数字的规则向量，它在这个操作中被垂直复制。因此，应该做的更正是求和，因为它正在被复制，因此所有的。

每一行的梯度现在反向流动需要加起来等于同样的张量 B 和 gain。因此如果要在所有零和所有示例中求和，基本上就是这个方向，这样它就被复制了。现在我们还需要小心，因为 B 和 gain 的形状是一行 64。所以事实上，我需要将它们保持为真实值。否则我只会得到 64。现在我没有。

![](img/86f8499f4a173e8882bce59ebef07fb4_311.png)

事实上，我真的记不清为什么我将 B 和 gain 以及 B 和 bias 设置为一行 64。

![](img/86f8499f4a173e8882bce59ebef07fb4_313.png)

但偏置 B one 和 B two 我只是让它们成为一维向量，而不是二维。

![](img/86f8499f4a173e8882bce59ebef07fb4_315.png)

张量。因此我不能确切回忆起为什么我把 gain 和 bias 设为二维，但它。

![](img/86f8499f4a173e8882bce59ebef07fb4_317.png)

只要你保持一致并保持相同，就没关系。因此在这种情况下。

![](img/86f8499f4a173e8882bce59ebef07fb4_319.png)

我们将保持维度，以使张量形状工作。接下来我们有 B 和 raw。因此。

![](img/86f8499f4a173e8882bce59ebef07fb4_321.png)

D B 和 raw 将变为 um B 和 gain 乘以 D H preact。这就是我们的链式法则。现在怎么样呢？

![](img/86f8499f4a173e8882bce59ebef07fb4_323.png)

这个维度是多少？我们必须小心，因此 D H preact 是 32 x 64。B 和 gain 是。

![](img/86f8499f4a173e8882bce59ebef07fb4_325.png)

1 x 64。因此，它将被复制以创建这个乘法，这是正确的。

![](img/86f8499f4a173e8882bce59ebef07fb4_327.png)

因为在前向传递中，它也以同样的方式被复制。因此实际上我们不需要在这里加括号，我们完成了，形状已经正确。最后对于偏置，这个偏置与我们在线性层看到的偏置非常相似。我们看到 H preact 的梯度将简单地流入偏置并相加，因为这些只是。

这些只是偏移量。因此，我们基本上希望这成为 D H preact，但它需要沿着正确的维度求和。在这种情况下，类似于 gain，我们需要跨零维进行求和。这些示例是因为偏置复制得非常快。我们还希望保持它们的真实性。因此，这将基本上对其进行求和并给我们一个 1 x 64 的结果。

所以这是候选实现，它使所有形状都能工作。让我把它展示在这里。

![](img/86f8499f4a173e8882bce59ebef07fb4_329.png)

然后让我取消注释这三行，以检查我们是否得到了所有结果的正确结果。

![](img/86f8499f4a173e8882bce59ebef07fb4_331.png)

三个张量。事实上，我们看到所有这些都正确反向传播了。所以现在我们得到了。

![](img/86f8499f4a173e8882bce59ebef07fb4_333.png)

批量规范层。我们看到这里 B 和 gain 以及 B 和 bias 是参数，因此反向传播结束。但 B 和 raw 现在是标准化的输出。因此，这里我所做的当然是将批量规范分解为可管理的部分，以便我们可以逐行反向传播。但基本上发生的是 B 和 mean I 是求和。因此，这就是。

B 和 mean，抱歉变量命名。B 和 diff 是 x 减去 mu。B 和 diff 二是 x 减去 mu 的平方，这在方差内部。B 和 var 是方差。因此 sigma 平方就是 B 和 var。基本上是平方和。因此这是 x 减去 mu 的平方然后求和。现在你会注意到一个偏离。

这里将其标准化为一除以 M，即样本数量。这里我标准化为一除以 N 减一，而不是 N。这是有意为之，稍后我会再提到这一点。这被称为贝塞尔修正。但在我们的案例中我想这样。B 和 var 变成基本的 B 和 var 加上 epsilon。

Epsilon 是一个负五。然后一除以平方根与提升至负 0.5 次方是相同的。因为 0.5 是平方根，负数使其变为一除以平方根。因此 B 和 var M 是这里的分母一除以。而且我们可以看到，B 和 var，即这里的 x hat 等于 B 和 diff 分子乘以。

B 和 var 在。这一行创建 pre H pre act 是我们已经反向传播过的最后一块。因此现在我们要做的是，我们在这里，有 B 和 raw，首先需要反向传播到 B 和 diff 和 B 和 var 在。所以现在我们在这里，有 D B。

![](img/86f8499f4a173e8882bce59ebef07fb4_335.png)

和 raw，我们需要通过这一行进行反向传播。现在我已经写出了这里的形状，实际上 B 和 var 在是一个 1 乘以 64 的形状。因此，这里发生了广播，我们必须小心。但是这只是逐元素的简单乘法。到现在我们应该对获得 B 和 diff 感到相当舒适。我们知道这只是 B 和 var 在。

乘以 D，B 和 raw。相反，要获得 D，B 和 var 在，我们需要取 B 和 diff，并将其乘以 D，B 和 raw。因此这是候选项。但当然，我们需要确保。

![](img/86f8499f4a173e8882bce59ebef07fb4_337.png)

![](img/86f8499f4a173e8882bce59ebef07fb4_338.png)

广播是遵循的。因此，特别是 B 和 var 在与 D，B 和 raw 相乘时，将是可以的，并给我们 32 乘以 64 的结果，正如我们预期的那样。但 D，B 和 var 在将是 32 乘以 64，乘以 32 乘以 64。因此，这是 32 乘以 64。但是，当然，D，B，这个 B 和 var 在只是 1 乘以 64。因此，这里第二行需要在样本间求和。而因为有这个。

这里的维度，我们需要确保保持它们的历史。因此这是候选项。让我们擦掉这个，然后向下滑动并实现它。让我们注释掉 D，B 和 var。

![](img/86f8499f4a173e8882bce59ebef07fb4_340.png)

在和 D，B 和 diff。现在，我们实际上会注意到 D。B 和 diff，顺便说一下，将是错误的。

![](img/86f8499f4a173e8882bce59ebef07fb4_342.png)

所以当我运行这个时，B 和 var 在是正确的，B 和 diff 是不正确的。这实际上是。

![](img/86f8499f4a173e8882bce59ebef07fb4_344.png)

预期之内，因为我们还没有完成 B 和 diff。因此特别是，当我们滑动这里时，我们在这里看到。

![](img/86f8499f4a173e8882bce59ebef07fb4_346.png)

![](img/86f8499f4a173e8882bce59ebef07fb4_347.png)

B 和 raw 是 B 和 diff 的一个函数。但实际上 B 和 var 是 B 和 var 的一个函数。这个又是 B 和 diff 的一个函数，因此又是 B 和 diff 的一个函数。它在这里出现。所以 B、D 和 diff，这些变量名称真是奇怪。抱歉。它分支成两个分支，而我们只做了其中一个分支。我们必须继续我们的反向传播，最终回到这里。

回到差异中。然后我们就能做一个加等于，得到实际的当前梯度。

![](img/86f8499f4a173e8882bce59ebef07fb4_349.png)

现在，验证 CBMP 也能正常工作是好的。它不会只是对我们撒谎，告诉我们一切都是正确的。实际上，它可以检测到你的梯度不正确。因此这很好。

![](img/86f8499f4a173e8882bce59ebef07fb4_351.png)

也看看。好的，现在我们这里有导数。我们正在尝试通过这条线进行反向传播。因为我们提升到负 0.5 的幂，所以我提到了幂法则。我们看到基本上 B 和 var 现在会带下指数。

负 0.5 乘以 X，就是这个。现在提升到负 0.5 减 1 的幂，也就是负 1.5。现在，我们还需要在头脑中应用一个小的链式法则，因为我们需要进一步求 B 和 var 对这个括号内表达式的导数。但由于这是元素级操作，所有的。

这相当简单，仅此而已。因此这里没有什么可做的。这是局部导数，然后乘以全局导数来创建链式法则。这只是乘以 B 和 var。所以这是我们的候选者。让我把这个拉下来，并取消注释检查。

![](img/86f8499f4a173e8882bce59ebef07fb4_353.png)

我们看到我们得到了正确的结果。现在，在我们通过下一条线进行反向传播之前。

![](img/86f8499f4a173e8882bce59ebef07fb4_355.png)

我想简要谈谈这里的说明，我在使用最佳性修正时，分母为(n-1)而不是 n，当我在这里对平方和进行归一化时。

![](img/86f8499f4a173e8882bce59ebef07fb4_357.png)

现在，你会注意到这是与论文的偏离，论文中使用的是 1/n 而不是 1/(n-1)。这里的 m 是 rn。因此，实际上有两种方法来估计数组的方差。一种是有偏估计，即 1/n，另一种是无偏估计，即 1/(n-1)。现在，令人困惑的是，在论文中，这是。

描述得不是很清楚，而且我认为这是一个重要的细节。他们在训练时使用了有偏版本。但后来，当他们谈论推理时，他们提到在推理时使用无偏估计，也就是基本上在推理中使用 n 减一版本，以校准运行均值。

以及运行方差，基本上。因此，他们实际上引入了训练和测试的不匹配。在训练中，他们使用了有偏版本，而在测试时，他们使用无偏版本。我觉得这非常令人困惑。你可以阅读更多关于贝塞尔校正的信息，以及为什么将方差除以 n 减一能给出更好的估计。

在样本为一个种群的情况下，样本量非常小。而这确实是我们的情况，因为我们处理多个批次。这些小批次是更大种群的一个小样本，也就是整个训练集。因此，如果你仅仅使用 1/n 来估计，结果就会出现问题。

这实际上几乎总是低估方差。而且这是一个有偏估计器。建议使用无偏版本并除以 n 减一。你可以参考我喜欢的这篇文章，它实际上描述了完整的推理，我会在视频描述中链接它。现在，当你计算整体方差时，你会注意到。

他们会选择无偏标志，无论是否要除以 n 或 n 减一。令人困惑的是，他们没有提到无偏的默认值是什么，但我相信默认情况下无偏为真。我不确定为什么文档没有说明这一点。现在，在批归一化 1D 中，文档同样有点错误和混淆。它说标准差是通过。

有偏估计器。但这实际上并不完全正确。人们已经指出这点。

![](img/86f8499f4a173e8882bce59ebef07fb4_359.png)

在多个问题系统中，因为实际上兔子洞更深，他们确实严格遵循论文。而他们在训练时使用有偏版本。但在估计运行标准差时，我们使用的是无偏版本。因此，这又出现了训练和测试的不匹配。总之。

我不喜欢训练和测试之间的不一致。我基本上认为。

![](img/86f8499f4a173e8882bce59ebef07fb4_361.png)

我们在训练时使用有偏版本，而在测试时使用无偏版本的事实。我基本上认为这是一个 bug。我认为没有合理的理由。实际上，他们没有深入探讨这篇论文中背后的推理。因此，这就是为什么我基本上在自己的工作中更倾向于使用贝塞尔校正。不幸的是。

![](img/86f8499f4a173e8882bce59ebef07fb4_363.png)

批量规范不接受一个关键词参数，告诉你是否想在训练测试中使用无偏版本或有偏版本。因此，任何使用批量规范的人，基本上在我看来，代码中都有点错误。而这在你的批量小批量大小稍大的情况下，实际上是一个较小的问题。但仍然。

我可能有些不可理解。所以也许有人可以解释一下为什么这样可以。但现在。我更倾向于使用。

![](img/86f8499f4a173e8882bce59ebef07fb4_365.png)

无偏版本在训练和测试时始终一致。这就是我为什么使用一个。

![](img/86f8499f4a173e8882bce59ebef07fb4_367.png)

在 N 减一上。好的，那么现在我们实际回传通过这行。

![](img/86f8499f4a173e8882bce59ebef07fb4_369.png)

所以我总是喜欢做的第一件事就是仔细审查形状。因此，在。

![](img/86f8499f4a173e8882bce59ebef07fb4_371.png)

特别是在这里，查看所涉及的形状，我看到 B 和 var 的形状是 1x64。

![](img/86f8499f4a173e8882bce59ebef07fb4_373.png)

所以这是一个行向量，B 和 D 如果两个点的形状是 32x64。所以显然这里。我们在求和。

![](img/86f8499f4a173e8882bce59ebef07fb4_375.png)

零轴以通过求和压缩这里的形状的第一维度。所以立刻。

![](img/86f8499f4a173e8882bce59ebef07fb4_377.png)

实际上这暗示着在反向传播中会有某种复制或广播。也许你注意到了这里的模式，但基本上每当你在前向传播中有一个求和时，那会在同一维度的反向传播中变成复制或广播。反之，当我们在前向传播中有复制或广播时，这表明一个。

变量重用。因此在反向传播中，这变成了在完全相同的维度上的求和。希望你注意到这种对偶性，这两者有点像是相反的。

![](img/86f8499f4a173e8882bce59ebef07fb4_379.png)

在前向和反向传播中相互作用。现在，一旦我们理解了形状，接下来要。

![](img/86f8499f4a173e8882bce59ebef07fb4_381.png)

我总是喜欢做的事是，我喜欢在脑海中查看一个例子，以便大致了解变量依赖关系在数学公式中的表现。所以这里。我们有一个二维。

![](img/86f8499f4a173e8882bce59ebef07fb4_383.png)

在最后的数组中，有一个常数进行缩放。然后我们进行垂直求和。

![](img/86f8499f4a173e8882bce59ebef07fb4_385.png)

在列上。因此，如果我们有一个 2x2 矩阵 A，然后我们在列上求和并缩放。我们将得到一个行向量 B1 B2。B1 以这种方式依赖于 A，它是 A 的某些缩放和，而 B2 以这种方式依赖于第二列，求和和缩放。所以查看。

![](img/86f8499f4a173e8882bce59ebef07fb4_387.png)

基本上，我们现在想做的是，我们有 B1 和 B2 的导数，我们想要将它们反向传播到 A。显然，只需在脑海中进行微分，这里的局部导数是 1 除以 n 减去 1，针对每一个 A。基本上 B1 的导数必须通过 A 的列进行流动，缩放因子为 1 除以 n 减去 1。

大致上就是这样。所以直观上，导数流告诉我们 dBn，dF2 将是这个的局部导数。

![](img/86f8499f4a173e8882bce59ebef07fb4_389.png)

操作。顺便说一句，这有很多方法来做到这一点，但我喜欢这样做。将 Bn，dF2 的一个切片导出。因此，我将创建一个初始为 1 的大数组。然后我会缩放它。所以 1.0 除以 n 减去 1。这是一个 1 除以 n 减去 1 的数组。这就像是局部导数。现在用于链式法则。

我会简单地乘以 Bb 和 r。注意这里会发生什么。这是 32 乘以 64。这只是 1 乘以 64。所以我让广播进行复制，因为在 PyTorch 内部，基本上 dBn 是 1 乘以 64 的行向量，在这个乘法中，会被垂直复制，直到两者形状相同。

形状。然后将有一个元素供我们乘以。因此，广播基本上是进行复制。最终我将得到 dBn，dF2 的导数。所以这是这个。

![](img/86f8499f4a173e8882bce59ebef07fb4_391.png)

候选解决方案。让我们把它放在这里。让我们取消注释这一行，检查一下。然后让我们。

![](img/86f8499f4a173e8882bce59ebef07fb4_393.png)

希望一切顺利。确实我们看到这是正确的公式。接下来，让我们进行微分。

![](img/86f8499f4a173e8882bce59ebef07fb4_395.png)

在 Bn，dF 中。所以这里我们有 Bn，dF 是元素 y 的平方，以创建 Bn，dF2。所以这是一个相对简单的导数，因为这是一个简单的逐元素操作。所以这有点像标量情况。如果这是 x 的平方，那么它的导数是 2x，对吧？所以它就是 2 乘以 Bn，dF，这是局部导数。然后乘以链式法则。

这些的形状是相同的，它们的形状相同。所以乘以这个。这就是反向传播。

![](img/86f8499f4a173e8882bce59ebef07fb4_397.png)

把它带到这里。现在我们必须小心，因为我们已经。

![](img/86f8499f4a173e8882bce59ebef07fb4_399.png)

计算了 Bn，dF，对吧？所以这只是另一个分支回到 Bn，dF。因为 Bn，dF 会已经从 Bn 的原始值反向传播到这里。所以我们现在完成了。

![](img/86f8499f4a173e8882bce59ebef07fb4_401.png)

第二个分支。所以这就是我为什么要加等于。如果你记得，我们之前有一个不正确的。

![](img/86f8499f4a173e8882bce59ebef07fb4_403.png)

Bn 的导数，dF4。我希望一旦我们附加这最后缺失的部分。我们得到了准确的。

![](img/86f8499f4a173e8882bce59ebef07fb4_405.png)

正确性。因此，让我们运行。现在，Bn，dF2，Bn，dF 实际上显示了准确的导数。

![](img/86f8499f4a173e8882bce59ebef07fb4_407.png)

![](img/86f8499f4a173e8882bce59ebef07fb4_408.png)

所以这让人感到安心。好的，现在让我们通过这一行进行反向传播。我们首先要做的，当然是检查形状。我在这里写下来了。基本上。这个的形状是 32 乘以 64。HP Bn 也是相同的形状。但是 Bn，mean i 是一个行向量，1 乘以 64。所以这里的减法实际上会进行广播。因此我们必须小心。还有。

这给了我们一个提示，再一次，由于对偶性，前向传播中的广播意味着变量的重用。因此，在反向传播中将会有一个求和。所以让我们现在写出反向传播。反向传播到 HP Bn。因为这些形状相同。每个元素的局部导数只是对应元素的 1。

基本上，这意味着梯度只是简单地复制。这只是一个变量赋值，它是，质量。因此，我将克隆这个张量以安全起见，创建一个精确的。dBn。然后在这里进行反向传播，我倾向于做的是，dBn。意思是我基本上会得到什么是局部导数？嗯，它是负的 torch dot 1。

形状与 b 和 f 类似。然后乘以这里的导数 dBn。这是对复制的 b 和 mean i 的反向传播。因此，我仍然需要。通过复制和广播进行反向传播。我通过进行求和来做到这一点。所以我。将整个内容进行求和，我将在零维上进行求和。

复制。所以如果你仔细审视一下。你会注意到这个和那个是相同形状。因此，我在这里所做的实际上没有太多意义，因为这只是一个全是 1 的数组在乘以 dBn。因此，实际上，我可以这样做。这样是等效的。

![](img/86f8499f4a173e8882bce59ebef07fb4_410.png)

所以这是候选的反向传播。让我在这里复制它。然后让我注释掉这个。

![](img/86f8499f4a173e8882bce59ebef07fb4_412.png)

还有这个。输入。结果是错误的。该死。实际上，抱歉，这本该是错误的。

![](img/86f8499f4a173e8882bce59ebef07fb4_414.png)

![](img/86f8499f4a173e8882bce59ebef07fb4_415.png)

![](img/86f8499f4a173e8882bce59ebef07fb4_416.png)

而且应该是错误的，因为我们正在从 b 和 f 反向传播到 h preb 和。但是我们还没有完成，因为 b 和 mean i 依赖于 h preb，因此会有来自这个第二分支的第二部分导数。所以我们还没有完成，我们预期它是。错误的。好了，所以让我们不要从 b 和 mean i 反向传播到 h preb。

![](img/86f8499f4a173e8882bce59ebef07fb4_418.png)

![](img/86f8499f4a173e8882bce59ebef07fb4_419.png)

所以在这里，我们必须小心，因为沿着零维度有广播或求和。因此这将转变为反向传递中的广播。我要在这一行上加快速度，因为这与我们之前的行非常相似。并且像以前一样进行乘法。实际上，d h preb 将会被梯度缩放为 1/n。

然后基本上这个梯度这里，b 和 mean i 将会被缩放为 1/n。然后，它会流遍所有的列并沉积到 d h preb 中。所以我们想要的，就是这个东西被缩放为 1/n。我们会把常数放在前面。因此缩小梯度，现在我们需要在这里的所有行中复制它。

我喜欢通过 torch dot 一个基本上是 h preb 的滑动来做这件事。我将让广播来完成复制的工作。

![](img/86f8499f4a173e8882bce59ebef07fb4_421.png)

所以这是 h preb，希望我们可以使用加法赋值。这是广播，然后这是缩放。所以这应该是正确的。

![](img/86f8499f4a173e8882bce59ebef07fb4_423.png)

好的。这完成了浴室层的反向传播，我们现在在这里。

![](img/86f8499f4a173e8882bce59ebef07fb4_425.png)

让我们反向传播到线性层一。在这里，因为一切都变得有点复杂。

![](img/86f8499f4a173e8882bce59ebef07fb4_427.png)

这条线看起来很疯狂，我在这里复制粘贴了一行，让我们反向传播通过这一行。

![](img/86f8499f4a173e8882bce59ebef07fb4_429.png)

首先，当然我们检查形状，我们看到这是 32 乘 64。M cat 是 32 乘 30。W one 是 30 乘 30 乘 64，而 B one 则是 64。所以如我所提到的，反向传播通过线性层。

![](img/86f8499f4a173e8882bce59ebef07fb4_431.png)

层之间的匹配非常简单。所以我们来做这个。我们有 d M cat。应该是 d h preb 与 w one 的某种有趣的乘法，再加上一个转置。因此为了使 MCAT 变为 32 乘 30，我需要将 d h preb（32 乘 64）与 w one 的转置相乘。为了得到 d w one，我需要最终得到 30 乘 64。所以为了达到这个，我需要。

em cat 转置并将其与 d h preb 相乘，最后得到 d B one。这是一个加法，我们发现基本上我只需要在 d h preb 中对元素求和。并沿某个维度进行求和，为了使维度正确，我需要沿着零轴求和，以消除这个维度，我们不保留它，这样我们只想得到一个单一的结果。

维度向量为 64。这些是声称的导数。让我把它放在这里。

![](img/86f8499f4a173e8882bce59ebef07fb4_433.png)

取消注释三行代码，然后祈祷一切顺利。一切都很好。好的，我们现在继续。

![](img/86f8499f4a173e8882bce59ebef07fb4_435.png)

快到了。我们有`em cat`的导数，我们想要导数，我们想要反向传播。

![](img/86f8499f4a173e8882bce59ebef07fb4_437.png)

进入`em`。所以我又把这一行复制到了这里。这是前向传播，然后这是。

![](img/86f8499f4a173e8882bce59ebef07fb4_439.png)

因此请记住，这里的形状是 32 乘以 30，而`em`的原始形状是 32 乘以 3 乘以 10。所以在前向传播中的这一层，正如你回忆的那样，是这三个 10 维字符向量的连接。因此我们现在只想撤销那个。因此这实际上相对简单。

![](img/86f8499f4a173e8882bce59ebef07fb4_441.png)

这是一个简单的操作，因为反向传播的视图是什么？视图只是数组的重新表示。这只是你如何解释数组的逻辑形式。所以让我们重新解释一下，回到之前的状态。换句话说，`em`不是 32 乘以 30。它基本上是`em cat`。但是如果你把它视为原始形状，那么就是`em dot shape`。

你可以将元组传入视图。因此，这应该没问题。我们只是重新表示那个视图，然后我们。

![](img/86f8499f4a173e8882bce59ebef07fb4_443.png)

![](img/86f8499f4a173e8882bce59ebef07fb4_444.png)

在这里取消注释这一行，希望，好的，`em`的导数是正确的。

![](img/86f8499f4a173e8882bce59ebef07fb4_446.png)

![](img/86f8499f4a173e8882bce59ebef07fb4_447.png)

因此在这种情况下，我们只需将这些导数的形状重新表示为原始视图。所以现在我们到了最后一行。唯一剩下的就是通过反向传播的这个索引操作，`M`是`C`在`XB`处。因此，正如我之前所做的，我在这里复制粘贴了这一行。

![](img/86f8499f4a173e8882bce59ebef07fb4_449.png)

![](img/86f8499f4a173e8882bce59ebef07fb4_450.png)

让我们看看所有相关内容的形状，并提醒自己这是如何工作的。所以`em dot shape`是 32 乘以 3 乘以 10。这是 32 个示例。然后我们有三个字符。每个字符都有一个 10 维的嵌入。这是通过使用查找表 C 来实现的，C 有 27 个可能的字符，每个字符是 10 维的。我们查看了这些行。

在这个张量`XB`中指定的内容。因此，`XB`是 32 乘以 3。它基本上为每个示例提供了该示例中哪个字符的身份或索引。因此在这里，我展示了这个张量`XB`的前五行中的三个。所以我们可以看到，例如，这个批次中的第一个示例是第一个字符中的第一个字符。

第四个字符进入神经网络。然后我们想要预测序列中在字符 114 之后的下一个字符。因此，基本上发生的事情是`XB`中有整数。这些整数分别指定我们要提取的`C`的哪一行，对吗？然后我们将提取的行整理成三个、两个乘以三的形状。

10 维张量，我们只需将它们打包，我们只需将它们打包到这个张量中。现在发生的事情是我们有 D amp。所以对于每一个基本上挑出来的行，我们现在有它们的梯度，但它们排列在这个 32 乘 3 乘 10 的张量中。所以我们现在所要做的就是将这个梯度向后路由通过这个赋值。

所以我们需要找出这 10 维嵌入的每一个是来自 C 的哪一行。然后我们需要将它们存入 DC。所以我们只需要撤销索引。当然，如果 C 的任何行被多次使用，这几乎是肯定的。比如第一行被多次使用，那么我们必须记住到达那里梯度需要相加。

所以对于每个出现，我们必须进行相加。那么现在我们来写出来。我实际上不知道在 Python 中有没有比 for 循环更好的方法。所以也许有人可以想出一个。

![](img/86f8499f4a173e8882bce59ebef07fb4_452.png)

向量化的高效操作，但现在我们就用 for 循环。让我创建一个 torch 的 zeros，像 C 一样初始化一个全零的 27 乘 10 的张量。然后老实说，4k 在范围内。

![](img/86f8499f4a173e8882bce59ebef07fb4_454.png)

xb 的形状在零处。也许有人有更好的方法来做到这一点，但对于 J 在范围内。xb 的形状在一处。这将迭代 xb 的所有元素，所有这些整数。

![](img/86f8499f4a173e8882bce59ebef07fb4_456.png)

然后我们获取这个位置的索引。所以索引基本上是 xb 在 kj 的位置。这就像 11 或 14 等的例子。现在在前向传播中，我们取了 C 的一行在索引，并把它存入 m 在 k aj。这就是发生的事情。这就是它们打包的位置。所以现在我们需要向后操作，我们只需要路由。

在 kj 的位置 dm。我们现在为每个位置都有这些导数，并且是 10 维的。它只需要进入 C 的正确行。因此 dC 实际上在 ix 是这个。但是加等于，因为可能会有多次出现，比如同一行可能被使用过很多次。所以所有这些导数都将通过索引向后传递。

![](img/86f8499f4a173e8882bce59ebef07fb4_458.png)

它们将相加。这是我的候选解决方案。我们把它复制到这里。让我们取消注释这个。

![](img/86f8499f4a173e8882bce59ebef07fb4_460.png)

![](img/86f8499f4a173e8882bce59ebef07fb4_461.png)

交叉我们的手指。嘿，所以就这样。我们已经通过整个庞然大物进行了反向传播。

![](img/86f8499f4a173e8882bce59ebef07fb4_463.png)

![](img/86f8499f4a173e8882bce59ebef07fb4_464.png)

所以我们开始吧。这完全说得通。那么现在我们进入第二个练习。结果基本上是。

![](img/86f8499f4a173e8882bce59ebef07fb4_466.png)

在这个第一次练习中，我们做了太多的工作。我们反向传播得太多了。这都是好的练习等等，但这不是你在实际中会做的事情。原因是。

![](img/86f8499f4a173e8882bce59ebef07fb4_468.png)

例如，这里我将损失计算分开成多行，并将其分解为最小的原子部分，我们分别对所有这些进行了反向传播。

![](img/86f8499f4a173e8882bce59ebef07fb4_470.png)

但事实证明，如果你只看损失的数学表达式，实际上你可以用纸和笔进行微分，很多项会相互抵消并简化。而你最终得到的数学表达式可以显著更短，并且比通过你所做的所有小部分反向传播要容易实现得多。所以在此之前。

我们有这个复杂的前向传递，从 logits 到损失，但在 PyTorch 中，一切都可以简单地拼接成一个单一的交叉熵调用。你只需传入 logits 和标签，你就得到了与我在这里验证的完全相同的损失。因此，我们之前的损失和来自一系列操作的快速损失作为一个单一的数学表达式是相同的。

但这要快得多。

![](img/86f8499f4a173e8882bce59ebef07fb4_472.png)

向前传递。反向传递也要快得多。原因是如果你。

![](img/86f8499f4a173e8882bce59ebef07fb4_474.png)

只需查看这个数学形式并再次微分，你将最终得到一个非常小且简短的表达式。这就是我们想要在这里做的。我们希望在单个操作中，或一次性快速直接进入 D logits，并且我们需要将 D logits 实现为 logits 和 YBs 的函数。但这将比我们在这里所做的任何事情显著更短。

![](img/86f8499f4a173e8882bce59ebef07fb4_476.png)

为了得到 D logits，我们必须走到这里。因此所有这些工作都可以在一个。

![](img/86f8499f4a173e8882bce59ebef07fb4_478.png)

更简单的数学表达式，你可以在这里实现。所以你可以尝试一下，基本上看一下损失的数学表达式是什么，并对 logits 进行微分。所以让我给你一个提示。你当然可以完全自己尝试。

![](img/86f8499f4a173e8882bce59ebef07fb4_480.png)

但如果不是，我可以给你一些数学上的提示，帮助你开始。所以基本上这里发生的事情是我们有 logits，然后是 softmax，它接受 logits 并给你概率，然后我们使用正确下一个字符的身份来提取出一行概率，取它的负对数以得到我们的负值。

对数概率。然后我们将所有的对数概率或负对数概率平均起来，以获得我们的损失。基本上，对于单个示例，我们得到的损失等于负对数概率，这里的 p 可以看作是所有概率的向量。在 y 位置，y 是标签。我们得到了这里的 p。

当然，这是 softmax。所以这个概率向量的 p 的 i 分量就是 softmax 函数。将所有的 logits 基本上都提高到 e 的幂并进行归一化，所有结果加起来等于一。现在如果你在这里写出 p of y，你可以直接写出 softmax。我们基本上感兴趣的是损失的导数。

基本上这是这个表达式的 d by dli，其中我们有以特定标签 y 为索引的 l。在底部，我们有一个 j 的和 e 的 lj，以及所有这些的负对数。所以可能试试用笔和纸，看你是否能推导出损失对 dli 的表达式，然后我们将在这里实现它。

![](img/86f8499f4a173e8882bce59ebef07fb4_482.png)

好的，我将在这里透露结果。这是我推导出来的一些数学内容。

![](img/86f8499f4a173e8882bce59ebef07fb4_484.png)

从分析上来看梯度。因此我们在这里看到，我只是应用了你们本科第一年或第二年的微积分规则，如果你上过这门课。我们看到表达式实际上简化了很多。你需要在分析中分开考虑，当你感兴趣的 logits 的第 i 个索引要么等于标签，要么不等。

等于标签。然后表达式以稍微不同的方式简化和抵消。我们最终得到的东西非常简单。我们要么最终得到基本的 p at i，这里的 p 再次是经过 softmax 处理的概率向量，要么是 p at i 减去一。无论如何，我们只需要计算 softmax p，然后在。

正确的维度，我们需要减去一。那就是梯度的形式。从分析上来看。因此，让我们基本上实现这一点。我们必须记住，这仅仅是针对第 i 个 logit 完成的。

![](img/86f8499f4a173e8882bce59ebef07fb4_486.png)

对于单个示例。但这里我们处理的是示例的批次。因此我们必须小心。

![](img/86f8499f4a173e8882bce59ebef07fb4_488.png)

的结果中。然后一个批次的损失是所有示例的平均损失。换句话说，对于所有单个示例的损失，总和后再除以 n。我们还需要通过这一过程进行反向传播，并且要小心。

![](img/86f8499f4a173e8882bce59ebef07fb4_490.png)

因此 d logits 将是 f dot softmax。PyTorch 有一个你可以调用的 softmax 函数。我们希望在 logits 上应用 softmax，并且希望在维度 1 上进行。所以基本上我们希望在这些 logits 的行上执行 softmax。然后在正确的位置，我们需要减去 1。因此 d logits 在遍历所有行并索引时。

由正确标签提供的列在 yb 内。我们需要减去 1。最后，这是平均损失，它是损失。在平均值中，所有损失加起来有一个 1/n。因此我们还需要通过这个除法进行反向传播。因此，梯度也必须缩放为 n，因为均值。但除此之外，这应该是结果。所以现在。

如果我们验证这一点，我们会发现没有得到完全匹配。但同时，从 PyTorch 的 logits 和我们这里的 d logits 之间的最大差异约为 5e 负 9。所以这是一个微小的数字。因此，由于浮点的不精确性，我们没有得到确切的位级结果。但基本上，我们得到了正确的答案。大约。现在。

我想在这里稍作停顿。

![](img/86f8499f4a173e8882bce59ebef07fb4_492.png)

在我们进行下一个练习之前，因为我想让我们对 logits 有一个直观的理解。因为它有一个美丽而非常简单的解释，老实说。因此在这里。我正在获取 logits 并对其进行可视化。我们可以看到，我们有 32 个例子的批次，包含 27 个字符。那么直观上 logits 是什么，对吧？

Logits 是前向传播中概率矩阵的概率。但是这里，这些黑色方块是我们减去 1 的正确索引的位置。那么这在做什么，对吧？这些是 logits 上的导数。因此让我们看看这里的第一行。这就是我正在做的事情。

![](img/86f8499f4a173e8882bce59ebef07fb4_494.png)

在这里。我正在收集这些 logits 的概率，并且我只取第一行。这是概率行。然后第一行的 logits 乘以 n，仅仅是为了不在这里缩放 n，所有内容都更易于解释。我们看到了。

![](img/86f8499f4a173e8882bce59ebef07fb4_496.png)

当然它正好等于概率，但正确索引的位置减去 1。所以在那个位置减去 1。所以注意，如果你在 0 处获取 logits。

![](img/86f8499f4a173e8882bce59ebef07fb4_498.png)

如果你求和，它实际上会和为 0。因此你应该把这些梯度视为每个单元的一个力。我们基本上会拉低不正确字符的概率。并且我们将拉高正确索引的概率。这就是每行基本上发生的事情。推动和拉动的量是完全平衡的。

因为总和为零。因此，我们对概率的下拉和对正确字符概率的上推是相等的。所以这有点像排斥和吸引是平衡的。现在把神经网络视为一个巨大的滑轮系统，我们在 logits 的顶部拉升。

我们在下拉错误的概率，同时提升正确的概率。在这个复杂的滑轮系统中，因为一切都是数学上确定的。可以把它视作这种张力转化为复杂的滑轮机制。最终我们在权重和偏置上得到了拉动。在每次更新中，基本上都是这样。

我们就是在这些元素的方向上轻轻拉动。参数也会慢慢适应这种拉动。这就是高层次上训练和神经网络的样子。我认为这些梯度中的推拉力量实际上是非常直观的。

我们在正确答案和错误答案之间推拉。我们施加的力量实际上与前向传播中产生的概率成正比。例如，如果我们的概率完全正确，那么它们会在正确位置上为 1，其余位置为 0。

那么对于这个例子，logits 将会是全零的行。没有推拉的效果。因此，预测的不准确程度正好等于你在那个维度上将要获得的推或拉的量。如果你在这里有一个非常自信的错误预测，那么这个元素将会被拉下去。

将会被强烈拉动。正确答案将被提升到同样的程度，而其他字符不会受到太大影响。因此，你的错误预测程度与拉动的强度成正比。这是在所有维度中独立发生的。

![](img/86f8499f4a173e8882bce59ebef07fb4_500.png)

这是这个张量。想象一下，这种思考方式非常直观且常用。这基本上就是交叉熵损失的魔力，以及它在神经网络反向传播中的动态作用。

![](img/86f8499f4a173e8882bce59ebef07fb4_502.png)

现在我们来进行第三个练习，这是一个非常有趣的练习，具体取决于你对“有趣”的定义。我们将对批量归一化做的事情与第二个练习中的交叉熵损失相同。也就是说，我们将其视为一个粘合的单一数学表达式，并以非常高效的方式进行反向传播，因为我们将推导出一个更高效的结果。

更简单的批量归一化反向传播公式。我们将使用纸和笔来完成。因此之前我们将批量归一化拆分成所有小的中间部分和所有的原子操作。然后我们一个个地进行反向传播。现在我们只需进行一次批量处理的前向传播。一切都粘合在一起。

我们看到得到的结果与之前相同。现在对于批量反向传播。我们希望基本上实现一个单一的公式，通过整个操作进行反向传播。也就是批量归一化。因此在之前的前向传播中，我们取 H pre-bn。预归一化的隐藏状态并创建 H pre-act。

隐藏状态就是，在激活之前。在批量归一化的论文中。H pre-bn 是 X，H pre-act 是 Y。所以在反向传播中。我们现在想做的是，我们有 D H pre-act，想要生成 D H pre-bn。我们希望以一种非常高效的方式做到这一点。这就是目标。

给定 D H pre-act 计算 D H pre-bn。为了这个练习，我们将忽略 gamma 和 beta 及其导数，因为它们的形式非常简单，与我们之前的计算方式非常相似。那么让我们在这里计算这个。为了帮助您。

![](img/86f8499f4a173e8882bce59ebef07fb4_504.png)

有点像我之前做的，我开始在纸上进行实现。我用两张纸推导反向传播的数学公式。基本上要设置问题，只需写出 mu sigma 平方方差。X i hat 和 Y i 完全按照论文的方式，除了贝塞尔修正。

然后在反向传播中，我们有损失对所有元素 Y 的导数。请记住，Y 是一个向量。这里有多个数字。因此，我们有关于所有 Y 的导数。然后有一个 gamma 和 beta。这有点像计算图。gamma 和 beta，有 X hat。然后是。

mu 和 sigma 平方以及 X。因此我们有 D L 对 D Y i 的导数，我们不会 D L 对 D X i 的导数，针对这些向量中的所有 I。这是计算图，您必须小心，因为。我在这里注意到这些是向量。这里面有许多节点在 X，X hat 和 Y 中。但 mu 和 sigma，抱歉，sigma 平方只是单独的标量，单个数字。

因此，您必须对此小心。您必须想象这里有多个节点，否则您会搞错数学。例如，我建议您按照以下顺序进行。一，二，三，四，关于反向传播。因此反向传播到 X hat，然后到 sigma 平方，再到 mu，然后到 X。就像人类学排序一样。

micro grad，我们将从右到左进行。你所做的完全相同，只是用符号在纸上进行。所以对于第一项，我不会透露太多。如果你想要 X i hat 的 dl，那么我们只需对 y 取 dl 并乘以 gamma，因为这里的这个表达式，其中任何个体 Y i 只是 gamma 乘以 X i hat 加上 beta。

所以这对你帮助不大。但这基本上给你所有 X hats 的导数。因此，现在尝试通过这个计算图推导出 dl 对 sigma square 的导数是什么。然后 dl 对 mu 的导数是什么，最后 dl 对 x 的导数又是什么。试试看，我会逐步揭示答案。好的。

因此，要获得 dl 对 sigma square 的导数，我们必须再次记住，正如我提到的，这里有许多 Xs。X hats。并记住 sigma square 只是这里的一个单一数值。因此，当我们查看 dl 对 sigma square 的表达式时，我们必须考虑所有可能的路径，因为基本上有许多 X hats，它们都与之相关。

所有这些都依赖于 sigma square。因此，sigma square 的影响范围很大。有许多箭头从 sigma square 指向所有 X hats。然后每个 X hat 都有一个向 sigma square 反向传播的信号。这就是为什么我们实际上需要对 i 从 1 到 m 的 dl 对 X i hat 进行求和，这就是全局梯度乘以 X i hat 对 sigma 的导数。

square，这是此操作的局部梯度。然后在数学上。我在这里进行计算并简化，你将获得 dl 对 sigma square 的某种表达式。我们将在反向传播到 mu 然后最终到 X 时使用这个表达式。那么现在让我们继续对 mu 的反向传播。那么 dl 对 mu 的导数是什么？现在再次小心。

mu 影响 X hat，实际上 X hat 是许多值。因此，例如，如果我们的迷你批次大小是 32，就像我们正在处理的例子一样，那么这就是 32 个数字和 32 个指向 mu 的箭头。然后，mu 指向 sigma square 仅仅是一个箭头，因为 sigma square 是标量。因此，总共有 33 个箭头从 mu 发出。然后它们都有。

梯度进入 mu，它们都需要被相加。因此，这就是当我们查看 dl 对 mu 的表达式时，我在所有 dl 对 X i hat 的梯度和 dX i hat 对 dmu 的梯度之间进行求和的原因。所以这是这个箭头和这里的 32 个箭头。然后加上来自这里的一个箭头，即 dl 对 sigma square 的梯度乘以 d sigma square 对 d mu 的导数。

现在我们必须计算那个表达式。让我揭示剩下的部分。在这里简化并不复杂，第一个项。你只是得到一个表达式。对于第二个项，发生了一些非常有趣的事情。当我们查看 d sigma square 对 d mu 并简化时，在某一点，如果我们假设在。

是一个特殊情况，其中`mu`实际上是`Xi`的平均值，就像在这个案例中那样。如果我们把它代入，那么梯度实际上会消失并变为零。这使得整个第二项抵消。因此，如果你只得到这样的数学表达式，并查看`d sigma`平方对`d mu`的。

你将得到一个关于`mu`如何影响`sigma square`的数学公式。但如果这是一个特殊情况，即`mu`实际上等于平均值，就像在合理化的情况下那样，那么梯度将实际上消失并变为零。因此整个项抵消，我们在这里得到一个相对简单的表达式，即`dl`对`d mu`的导数。好的。

现在我们进入最疯狂的部分，就是推导`dl`对`d Xi`的导数，这最终是我们要追求的。现在我们来数一数。首先，`X`里面有多少个数字？正如我提到的，有 32 个数字。这里有 32 个小`Xi`。现在让我们数一数从每个`Xi`发出的箭头数量。这里有一个箭头。

指向`mu`的箭头，指向`sigma square`的箭头。然后还有指向`X hat`的箭头。但这个箭头，让我们仔细审视一下。每个`Xi hat`只是`Xi`和其他标量的一个函数。因此，`Xi hat`只依赖于`Xi`，而不依赖于其他的`X`。因此，在这个单一的箭头中，实际上有 32 个箭头。但这 32 个箭头正。

完全平行。它们没有干扰。它们只是沿着`X`和`X hat`平行。你可以这样看。那么，从每个`Xi`发出的箭头有多少？有三个箭头，`mu`、`sigma`、平方，以及相关的`X hat`。所以在反向传播中，我们现在需要应用链式法则。我们需要将这三项贡献相加。

如果我把它写出来，就像这样。这就是我们的过程。我们正在通过`mu`、`sigma square`和`X hat`进行变换。这三项就在这里。现在我们已经有了三个这样的项。我们有`dl`对`dx i hat`的导数。我们有`dl`对`d mu`的导数，这是我们在这里推导的。还有`dl`对`d sigma square`的导数，这是我们在这里推导的。但我们。

这里需要三个其他术语。这个，这个，还有这个。所以我邀请你尝试推导它们。并不复杂。你只需查看这些表达式，并对`Xi`进行求导。试试看。不过这是结果。或者至少是我得到的。是的。我只是在对所有这些表达式进行`Xi`的求导。老实说。

我认为这里没有什么太棘手的。这是基本的微积分。现在变得稍微复杂一点，因为我们现在要把所有内容整合在一起。所以所有这些项与所有这些项相乘，并根据这个公式相加。这将得到一个。

![](img/86f8499f4a173e8882bce59ebef07fb4_506.png)

有点复杂。所以最终发生的事情是你得到一个大的表达式。当然，在这里需要非常小心的是，我们正在处理特定的`dl by dx i`。但是当我们插入一些这些项，比如说这个项，`dl by d sigma squared`。你会看到`dl by d sigma squared`，我最终得到了一个表达式。我在对小的 i 进行迭代。

在这里。但是我不能用 i 作为变量来插入，因为这个 i 与这个 i 不同。这里的 i 只是这里循环中的一个局部变量。所以在这里，当我插入时，你会注意到我将 i 重命名为 j。因为我需要确保这个 j 不是这个 i。这个 j 就像一个局部迭代器，遍历 32 个项。所以你必须。

对此小心。当你将这里的表达式插入到这里时，你可能需要将 i 重命名为 j。你必须非常小心，什么实际上是与`dl by d xi`相关的 i。所以这些中有一些是 j，有一些是 i。然后我们简化这个表达式。我想这里需要注意的一个重要点是，一堆项会自然地出来。

前面的，你可以重构它们。这里有一个`sigma squared`加上`epsilon`，并且这个值的指数为负的三分之二。这个`sigma squared`加上`epsilon`实际上可以分解成三个项。每一个都是`sigma squared`加`epsilon`的负一分之二。所以这三者相乘等于这个。然后这三个项可以因为。

乘法。所以其中一个实际上会到达前面，并最终出现在这里。一个与这个项结合，另一个与这个其他项结合。当你简化表达式时，你会注意到一些出来的项只是`xi hats`。所以你可以通过重新写来简化。最后我们得到的是一个相当。

这里有一个简单的数学表达式，我无法进一步简化。但是基本上你会注意到，它只使用了我们拥有的东西，并推导出我们需要的东西。所以我们有`dl by dy`，对于所有的 i。这些在这里使用了很多次。而且在附加的部分，我们使用的是这些`xi hats`和`xj hats`。它们来自于前向传播。除此之外，这就是。

一个简单的表达式，它给我们`dl by d xi`对于所有的 i。这就是我们最终感兴趣的。所以这是批量归一化反向传播的解析结束。现在让我们实现这个最终结果。好吧，我将表达式实现成了一行代码。你可以看到最大差异很小。所以这是这个公式的正确实现。现在。

我基本上告诉你，从这个数学表达式得出这个公式并不是一件简单的事。这个公式包含了很多内容。而这一切都是单独练习的。因为你必须考虑到，这个公式仅仅是针对一个单一神经元和一批 32 个样本。但是我在这里所做的是，实际上我们。

有 64 个神经元。因此，这个表达式必须并行评估所有 64 个神经元的批归一化反向传播。这样必须在输入的每一列中进行。此外，你会看到这里有一堆求和，我们需要确保在进行求和时，它们能正确地广播到所有内容上。

这里的其他内容。因此，得到这个表达式是非常复杂的。我邀请你基本上仔细查看并逐步理解。这是一个完整的练习，以确保这一切都是正确的。但是一旦所有的形状一致，并且你说服自己这是正确的，你还可以验证 PyTorch 得到的答案也是完全相同的。所以这给了你很多。

有一种安心感，这个数学公式在这里被正确实现，并且在这个批次项层内的 64 个神经元中以正确的方式并行复制。

![](img/86f8499f4a173e8882bce59ebef07fb4_508.png)

好的，最后练习四要求你将所有内容整合在一起。这里我们重新定义了整个问题。所以你会看到我们从头开始重新初始化神经网络。

![](img/86f8499f4a173e8882bce59ebef07fb4_510.png)

一切都在这里。因此在调用反向损失时，我们希望使用手动方式。

![](img/86f8499f4a173e8882bce59ebef07fb4_512.png)

反向传播在这里如我们之前所推导的那样。因此，向上复制粘贴我们已经推导出的所有代码块，把它们放在这里，驱动你自己的梯度，然后优化这个神经网络。

![](img/86f8499f4a173e8882bce59ebef07fb4_514.png)

网络，基本上使用你自己的梯度一直到批归一化的校准和损失的评估。我能够获得一个相当不错的损失，基本上和之前达到的损失相同。这不应该令人惊讶，因为我们所做的只是。

![](img/86f8499f4a173e8882bce59ebef07fb4_516.png)

我们真的深入到了损失和反向传播中，把所有代码提取出来并插入到这里。但是那些梯度是相同的，所有内容都是相同的，结果也是相同的。

![](img/86f8499f4a173e8882bce59ebef07fb4_518.png)

只是我们对其背后的每个细节都有充分的了解。

![](img/86f8499f4a173e8882bce59ebef07fb4_520.png)

在这种特定情况下很多是反向传播。好的，这就是我们的所有代码。这是完整的。反向传播，基本上是交叉熵损失和批量归一化的简化反向传播。因此，反向传播通过交叉熵，第二层，10 h null。线性，批量归一化通过第一层和嵌入。因此你。

看到这大概只有 20 行代码之类的。这就是。给我们梯度的东西。现在我们可以潜在地删除损失和反向。因此我有代码的方式。

![](img/86f8499f4a173e8882bce59ebef07fb4_522.png)

设置是你应该能够在填入这些内容后运行整个单元。这将运行。

![](img/86f8499f4a173e8882bce59ebef07fb4_524.png)

只需 100 次迭代，然后中断。它中断是因为它给你一个机会来检查你的。

![](img/86f8499f4a173e8882bce59ebef07fb4_526.png)

对比 pytorch 的梯度。我们看到的梯度并不完全相等。它们大约是相等的。差异很小，大约在负九的数量级。我老实说不太清楚这些差异来自哪里。因此，一旦我们对梯度的基本正确性感到有信心。

![](img/86f8499f4a173e8882bce59ebef07fb4_528.png)

我们可以去掉梯度检查。我们可以禁用这个中断语句。然后我们可以。基本上禁用反向的损失。我们不需要它。这真是太好了。然后在这里，当我们进行更新时，我们不会使用 p.grad。这是 pytorch 的旧方式。我们不再有这个，因为我们不再进行反向传播。我们将使用。

这个更新，你看到我在迭代，我将梯度安排为与参数在同一顺序，并且我将梯度和参数压缩成 p 和 grad。然后在这里，我将用我们手动推导的梯度进行一步。因此最后一部分。是现在这些都不需要来自 pytorch 的梯度。所以在这里你可以做的一件事。

![](img/86f8499f4a173e8882bce59ebef07fb4_530.png)

你可以用 torch.no.grad 来处理整个代码块。实际上你是在说。

![](img/86f8499f4a173e8882bce59ebef07fb4_532.png)

你是在告诉 pytorch，嘿，我不会对任何这个调用 backward。而这个最后。

![](img/86f8499f4a173e8882bce59ebef07fb4_534.png)

让 pytorch 更加高效。然后我们应该能够直接运行这个。它正在运行。你会看到反向的损失已被注释掉，我们正在优化。

![](img/86f8499f4a173e8882bce59ebef07fb4_536.png)

所以我们将结束这个运行，希望能得到一个好的结果。好的，我允许神经网络。

![](img/86f8499f4a173e8882bce59ebef07fb4_538.png)

![](img/86f8499f4a173e8882bce59ebef07fb4_539.png)

最后完成优化。因此在这里，我根据参数校准了批处理，因为我没有保留。

![](img/86f8499f4a173e8882bce59ebef07fb4_541.png)

在他们的训练循环中跟踪运行均值和方差。然后在这里，我运行了损失。

![](img/86f8499f4a173e8882bce59ebef07fb4_543.png)

你会看到我们实际上获得了一个相当好的损失，和之前的结果非常相似。然后在这里，我正在从模型中进行采样，我们看到一些像胡言乱语的名字。这是我们有点习惯的。因此，基本上模型有效，并且样本结果相当不错，和我们习惯的相比。所以一切都是一样的。但当然，关键是我们。

![](img/86f8499f4a173e8882bce59ebef07fb4_545.png)

我们没有使用很多的反向传播。我们没有使用 PyTorch 的自动求导。我们手动估计了梯度。因此，希望你在看这个神经网络的反向传递时，会觉得其实并不复杂。每一层大约只有三行代码。而且大部分内容都是相当直接的。

可能除了批量归一化的反向传递之外，其他都是一样的。

![](img/86f8499f4a173e8882bce59ebef07fb4_547.png)

这相当不错。好的，这就是我想在本讲中涵盖的所有内容。希望你觉得这很有趣。老实说，我喜欢的是，它给了我们非常好的多样性层以进行反向传播。我认为它提供了一个相当不错且全面的理解，关于这些反向传递是如何实现的以及它们是如何工作的。你也能够推导出。

你可以自己来做。但当然，在实践中，你可能不想这样，而是希望使用 PyTorch 的自动求导。但希望你对梯度是如何在神经网络中反向流动的有一些直觉，从损失开始，如何流经所有变量和中间结果。如果你理解了大部分内容，并且对这些有一些感觉。

然后你可以把自己算作左边这些强壮的狗狗，而不是右边的狗狗。

![](img/86f8499f4a173e8882bce59ebef07fb4_549.png)

就在这里。现在，在下一讲中，我们将实际上讨论递归神经网络、LSTM 以及所有其他变体。我们将开始复杂化架构并实现更好的对数似然。因此，我真的很期待这个。

![](img/86f8499f4a173e8882bce59ebef07fb4_551.png)

那我们下次见。
