# P4：p4 Building makemore Part 3：激活值与梯度，BatchNorm - 加加 zero - BV11yHXeuE9d

大家好。今天我们继续实施 Makemore。在上一次讲座中，我们实现了基于 Benjio 等人 2003 年提出的字符级语言建模的多层感知器。所以我们遵循了这篇论文，取了一些。

![](img/7e929a646a3b3c13dd787fc42b498be7_1.png)

过去的一些字符，并使用多层感知器预测序列中的下一个字符。

![](img/7e929a646a3b3c13dd787fc42b498be7_3.png)

所以我们现在想要做的是，继续向更复杂和更大的神经网络发展，比如递归神经网络及其变体，如 GRU 和 LSTM 等。不过，在此之前，我们必须再停留在多层感知器的层面上稍微长一些。我想这样做，因为我希望我们能有一个非常好的直观理解。

在训练期间神经网络中激活值的表现，尤其是向后传播的梯度，以及它们的表现和外观。这对于理解这些架构的发展历史非常重要，因为我们会看到，递归神经网络尽管在表达能力上很强大，但仍是一个通用逼近器。

从原则上讲，它们可以实现所有算法。我们将看到它们不是很容易优化的，使用的第一阶基于梯度的技术也不是很有效，且我们一直在使用这些技术。理解它们为什么不易优化的关键在于理解。

激活值和梯度，以及它们在训练中的表现。我们会看到，由于递归神经网络尝试改善这种情况，因此有很多的方差。

![](img/7e929a646a3b3c13dd787fc42b498be7_5.png)

这是我们必须走的路径，让我们开始吧。这次讲座的起始代码主要是之前的代码，但我稍微清理了一下。所以你会看到我们导入了所有的 Torch 和 Matplotlib 工具。我们像之前一样读取单词。

![](img/7e929a646a3b3c13dd787fc42b498be7_7.png)

八个示例单词。总共有 32,000 个。这里是所有小写字母和特殊点标记的词汇表。我们正在读取数据集并处理它，创建三个分割：训练集、开发集和测试集。在多层感知器中，这是完全相同的。

![](img/7e929a646a3b3c13dd787fc42b498be7_9.png)

相同的多层感知器，不过我移除了这里的一堆魔法数字。取而代之的是，我们有字符嵌入空间的维度和隐藏层中隐藏单元的数量。所以我把它们提取出来，这样我们就不需要每次都去更改这些魔法数字。使用相同的神经网络，我们优化了 11,000 个参数。

现在经过超过 200,000 步，批次大小为 32。你会看到我在这里稍微重构了一下代码，但没有功能上的变化。我只是创建了一些额外的变量，增加了一些注释，并去掉了所有魔法数字，其余部分完全一样。然后当我们进行优化时，我们看到我们的损失看起来大致是这样的。我们看到训练和验证损失大约为 2.16。

诸如此类。在这里，我对任意分割的评估稍作了重构。所以你传入一个字符串，指定想要评估的分割。然后根据训练、验证或测试，我进行索引，得到正确的分割。接着这是网络的前向传播和损失评估及打印。这样做让它更简洁。你会注意到的一件事是。

注意，这里我使用了装饰器 `torch.no_grad`，你也可以查阅文档。基本上，这个装饰器在函数上会使得该函数中的所有操作都被 `torch` 视为不需要计算梯度。因此，它不会进行任何账务处理。

这样做可以跟踪所有梯度，以便将来进行反向传播。就好像这里创建的所有张量都有 `requires_grad` 设置为 `false`。这使得一切都更高效，因为你告诉 `torch` 我不会在这些计算上调用 `dot.backward`，因此你不需要在后台维护图。

所以这就是它的作用。你也可以使用上下文管理器与 `torch.no_grad` 结合使用。

![](img/7e929a646a3b3c13dd787fc42b498be7_11.png)

可以让它们消失。然后在这里，我们有一个模型的样本，就像之前一样。只是神经网络的一个简单前向传播，从中获取分布进行采样，调整上下文窗口并重复，直到得到特殊和标记。我们看到开始出现更好看的词。模型生成的结果仍然不算惊艳，且仍然没有完全命名，但已好很多。

比我们使用 Bagram 模型时要好。这就是我们的起点。现在第一件事。

![](img/7e929a646a3b3c13dd787fc42b498be7_13.png)

我想要仔细审查的是初始化。我可以看出我们的网络初始化得非常不当。

![](img/7e929a646a3b3c13dd787fc42b498be7_15.png)

在初始化时配置。有很多问题，但我们先从第一个开始。看看零联邦的第一次迭代。我们记录到的损失是 27，而这很快降到大约一或二。所以我可以告诉你，初始化是完全混乱的，因为这个值太高了。

在神经网络的训练中，几乎总是能对初始化时的损失有一个大致的预期。而这完全取决于损失函数和问题的设置。在这种情况下，我不期望得到 27。我期望一个更低的数字，我们可以一起计算。基本上，在初始化时我们希望是这样的。

对于任何一个训练示例，可能会出现 27 个字符。在初始化时，我们没有理由相信任何字符比其他字符更可能。因此，我们预期最初输出的概率分布是均匀分布，给所有 27 个字符分配大致相等的概率。所以基本上，我们喜欢的是任何字符的概率。

概率大约是 1/27。这是我们应该记录的概率，然后损失是负对数概率。所以我们把它包装在一个张量中，然后可以对其取对数，负对数概率就是我们期望的损失，即 3.29，远低于 27。因此，现在发生的事情是，在初始化时，神经网络正在生成概率。

有些字符非常自信，而有些字符则非常不自信。基本上，发生的事情是网络非常自信地出错，这就是造成记录非常高损失的原因。因此，这里有一个较小的四维示例。

![](img/7e929a646a3b3c13dd787fc42b498be7_17.png)

问题的例子。假设我们只有四个字符，神经网络输出的 logits 非常接近零。当我们对所有零进行 softmax 时，我们得到的是一个分散的概率分布。总和为 1，完全均匀。在这种情况下，如果标签是 2，其实并不重要标签是 2 还是。

三、一个或零，因为它是均匀分布，所以我们记录的损失是完全相同的，在这种情况下是 1.38。因此，这是我们对于四维示例预期的损失。我当然可以看到，随着我们开始操纵这些 logits，损失会发生变化。所以有可能我们偶然间锁定，这可能是一个很高的数字，比如你知道的。

5 或者类似的情况。那么在这种情况下，我们会记录非常低的损失，因为我们在初始化时偶然将正确的概率分配给了正确的标签。更可能的是，某个其他维度会有一个高 logit，然后发生的事情是我们开始记录更高的损失。而且发生的事情基本上是 logits 输出的分布都是混乱的。

像这样，你知道它们会取极端值，我们记录的损失非常高。例如，如果我们随机选择四个字符，这些字符是均匀分布的。它们通常是正态分布的数字。在这里，我们也可以打印出 logits、概率以及损失。因此，因为这些 logits 大部分接近零。

![](img/7e929a646a3b3c13dd787fc42b498be7_19.png)

损失的一部分是可以的。但是假设这像乘以 10 一样。你可以看到，由于这些是更极端的值，很不可能猜测正确的桶，然后你自信地错误并记录非常高的损失。如果你的 logits 更加极端，你可能会得到极端的损失，比如无穷大。

即使在初始化时也是如此。所以基本上这不好，我们希望网络初始化时 logits 大致为零。实际上，logits 不一定是零，只需要相等。例如，如果所有 logits 都是 1，那么由于 softmax 内部的归一化，这实际上是可以的。但从对称性来看，我们不希望它是任意的。

正数或负数我们希望它全部为零，并记录我们预期的损失。

![](img/7e929a646a3b3c13dd787fc42b498be7_21.png)

初始化。所以现在让我们具体看看在我们的例子中哪里出错。这里是初始化，让我重新初始化神经网络，并在第一次迭代后中断。这样我们只看到初始损失为 27。这太高了，直观上我们可以预期涉及的变量，并且我们看到这里的 logits 如果我们打印一些。

如果我们打印第一行，我们看到 logits 具有相当极端的值，这正是造成错误答案中虚假自信的原因，并使损失非常高。因此，这些 logits 应该更接近零。那么现在让我们思考一下，如何才能让这个神经网络输出的 logits 更接近零。

你可以看到，这里 logits 是通过隐藏状态乘以 w2 加上 b2 计算得出的。所以首先，我们将 b2 初始化为合适大小的随机值，但因为我们想要大致为零，我们实际上不希望添加随机数的偏差，所以我将在这里添加一个乘以零的项，以确保 b2 在初始化时基本为零，第二个是 h 乘以。

w2。所以如果我们希望 logits 非常非常小，那么我们就会乘以 w2 并使其更小。例如，如果我们将 w2 缩小到 0.1 的所有元素，然后如果我再进行第一次迭代，你会看到我们离预期的更近。所以我们大致想要的是 3.29，这是 4.2。我可以把这个值甚至更小变成 3.32，好的，所以我们越来越接近。

![](img/7e929a646a3b3c13dd787fc42b498be7_23.png)

更接近。现在你可能想知道我们是否可以将其设置为零，那么我们当然会得到正是我们所期望的。

![](img/7e929a646a3b3c13dd787fc42b498be7_25.png)

我们在初始化时所寻找的，我通常不这样做的原因是因为我非常紧张。

![](img/7e929a646a3b3c13dd787fc42b498be7_27.png)

![](img/7e929a646a3b3c13dd787fc42b498be7_28.png)

我会在一秒钟后向你展示为什么不想将神经网络的权重设置为零。你通常希望它是小数字，而不是完全为零。在这个特定情况下的输出层，我认为这样做是可以的，但我会在一秒钟后展示，如果这样做，事情会很快出错。所以我们就设定为 0.01。在这种情况下，我们的损失足够接近。

![](img/7e929a646a3b3c13dd787fc42b498be7_30.png)

但是有一些熵，它并不完全为零，存在一些小的熵，这用于对称破缺，正如我们稍后会看到的那样。Logits 现在接近零，一切看起来很好。因此，如果我删除这些并去掉中断语句，我们可以使用这个新的初始化进行优化，让我们看看我们记录的损失。

好吧，我让它运行，你会看到我们一开始很好，然后稍微下降了一些。

![](img/7e929a646a3b3c13dd787fc42b498be7_32.png)

现在损失的图形没有这种曲棍球形状的出现，因为基本上在曲棍球棒的最初几次迭代中，损失的发生是优化仅仅在压缩 logits，然后重新排列 logits。因此，我们基本上去掉了损失函数的这个简单部分，那里的权重只是。

只是被压缩了，因此我们一开始并没有获得这些轻松的收益。

![](img/7e929a646a3b3c13dd787fc42b498be7_34.png)

我们正在获得训练实际神经网络的一些艰难收益，因此没有曲棍球棒的出现。所以好事情发生了，初始化时的损失是。

![](img/7e929a646a3b3c13dd787fc42b498be7_36.png)

![](img/7e929a646a3b3c13dd787fc42b498be7_37.png)

我们所期望的，损失看起来不像一个曲棍球棒，这对任何神经网络来说都是如此。

![](img/7e929a646a3b3c13dd787fc42b498be7_39.png)

可能性训练和一些需要关注的内容。其次，输出的损失实际上是相当的。稍微有一些改善。不幸的是，我之前的内容被我删除了。我相信这是 2.12，而这个是 2.16，因此我们得到稍微改善的结果，原因在于我们花费了更多的时间。

![](img/7e929a646a3b3c13dd787fc42b498be7_41.png)

在优化神经网络时花费更多的时间，而不仅仅是在前几千次迭代中，可能只是压缩权重，因为它们在初始化的开始阶段过高。因此，需要关注的事项就是这一点。

![](img/7e929a646a3b3c13dd787fc42b498be7_43.png)

现在让我们看看第二个问题。让我重新初始化我们的神经网络，并让我重新引入断点语句。因此我们有一个合理的初始损失。尽管在损失的层面上看起来一切都很好，并且我们得到了一些预期的结果，但这个神经网络及其初始化内部仍然存在更深层的问题。因此，logits 现在没问题。现在的问题是。

H 的值是隐藏状态的激活值。现在如果我们只是可视化这个向量，抱歉这个张量 H，有点难以看清，但问题大致上是你会看到多少元素是 1 或负 1。现在回想一下 torch dot 10 H，10 H 函数是一个压缩函数。它接受任意数字并将其压缩到负一到一的范围内，并且。

一切都是如此平滑。因此，让我们查看 H 的直方图，以更好地了解这个张量内部值的分布。我们可以首先做到这一点。好吧，我们可以看到 H 有 32 个示例，每个示例有 200 个激活。我们可以将其视为负一，扩展成一个大的向量，然后调用 to list 将其转换为一个大型的 Python 浮点数列表。然后。

我们可以将这个传递到 PLT dot hissed 来生成直方图，并且我们说我们想要 50 个箱子，并用分号来抑制我们不想要的输出。因此我们看到这个直方图，并且可以看到大多数值明显是负一和一。因此，这个 10 H 是非常活跃的，我们还可以看看，基本上这是什么原因，我们可以查看输入到 10 H 的预激活值。

我们可以看到预激活值的分布非常广泛。这些值在负 15 到 15 之间，这就是为什么在 torch dot 10 H 中所有值都被压缩，并限制在负一到一的范围内，许多值在这里取非常极端的值。现在如果你对神经网络不太了解，可能不会将其视为问题。

但如果你对反向传播的黑暗艺术非常熟悉，并且对这些梯度如何在神经网络中流动有直观的理解，你会看到你在这里的 10 H 激活的分布，而感到紧张。那么让我给你展示一下原因。我们必须记住，在此期间。

![](img/7e929a646a3b3c13dd787fc42b498be7_45.png)

反向传播就像我们在微梯度中看到的一样，我们从损失开始进行反向传递，并向后流经网络。特别是我们将通过这个 torch dot 10 H 进行反向传播，这一层由每个示例的 200 个神经元组成，并实现了一个元素两次的 10 H。因此让我们看看在反向传递中 10 H 发生了什么。

我们实际上可以回到第一次讲座的微梯度代码，看看我们是如何实现 10 H 的。我们看到这里的输入是 X，然后我们计算 T，它是 X 的 10 H。所以 T 在负一和一之间，是 10 H 的输出，然后在反向传播中，我们如何通过 10 H 反向传播？我们取出那个梯度，然后乘以它。

是链式法则与局部梯度的形式，表现为 1 减去 T 的平方。那么，如果你的 10 H 输出非常接近负一或一，会发生什么呢？如果这里代入 T 等于一，你会得到零，乘以那个梯度，无论那个梯度是什么，我们都在消除梯度，有效地阻止了向后传播。

10 H 单元。同样，当 T 为负一时，这将再次变为零，那个梯度就停止了，从直观上看，这是有意义的，因为这是一个 10 H 神经元，如果它的输出非常接近一，那么我们就在这个 10 H 的尾部，所以改变输入不会对 10 H 的输出产生太大影响，因为它位于 10 H 的平坦区域。

因此对损失没有影响，确实这个 10 H 神经元的权重和偏置对损失没有影响，因为这个 10 H 单元的输出位于 10 H 的平坦区域，改变它们不会影响损失，这也是另一种证明方法。

实际上，梯度基本上会变为零，消失。确实当 T 等于零时，我们得到一乘以那个梯度，因此当 10 H 恰好取零值时，那个梯度就被传递过来。所以这基本上意味着，如果 T 等于零，那么这个 10 H 单元是处于不活跃状态，梯度就会传递，但你越处于平坦尾部。

梯度被压缩，实际上你会看到通过 10 H 的梯度只能减少，其减少的量与这个 10 H 的平坦尾部的位置成平方比例，这就是这里发生的事情。

![](img/7e929a646a3b3c13dd787fc42b498be7_47.png)

这里的担忧是，如果所有这些输出 H 都在负一和一的平坦区域，那么通过网络流动的梯度将在这一层被摧毁。现在这里有一些挽救的特质，我们可以实际感知到问题，我写了一些代码，我们想要查看的正是这些。

在 H 取绝对值并查看它在平坦区域中出现的频率，比如大于 0.99。

![](img/7e929a646a3b3c13dd787fc42b498be7_49.png)

你得到的是以下内容，这是一个布尔张量。在布尔张量中，如果这个为真则为白色，如果为假则为黑色。基本上，我们这里有 32 个示例和 200 个隐藏神经元，我们看到很多是白色的，这告诉我们所有这些 10 H 神经元都非常活跃，并且它们处于一个平坦的尾部，因此在所有这些。

在这种情况下，反向梯度将被破坏。如果对于任何一个。

![](img/7e929a646a3b3c13dd787fc42b498be7_51.png)

在这 200 个神经元中，如果整个列是白色，那么我们就有了所谓的死神经元。这可能是一个 10 H 神经元，其中权重和偏置的初始化可能使得没有单个示例能激活这个 10 H 在其活跃部分。如果所有示例都落在尾部，那么这个神经元将永远不会学习。

这是一个死神经元，因此仔细检查这个并寻找完全白色的列，我们看到并非如此。因此，我没有看到任何单个神经元是全白的，因此对于每一个这些 10 H 神经元，我们确实有一些示例可以激活它们。在 10 H 的活跃部分，因此一些梯度会流动，这个神经元将会学习。

神经元会改变，它会移动并执行某些操作，但在某些情况下，你可能会发现自己处于死神经元的情境中。对于 10 H 神经元来说，这意味着无论你从数据集中插入什么输入，这个 10 H 神经元总是完全地激活或完全负激活，然后它将不会学习，因为所有的梯度。

这不仅对 10 H 是正确的，对许多其他非线性函数也是如此。

![](img/7e929a646a3b3c13dd787fc42b498be7_53.png)

人们在神经网络中使用的函数，所以我们确实在使用 10 H，但 sigmoid 会有完全相同的问题，因为它是一个压缩神经元，因此 sigmoid 也将面临同样的情况。基本上，这同样适用于 sigmoid，同样也适用于 relu，因此 relu 在零以下有一个完全平坦的区域。如果你有一个 relu 神经元，那么它就是一个通道。

如果它是正的，而预激活是负的，它将被关闭。由于这里的区域完全平坦，因此在反向传播时这将是完全的。

![](img/7e929a646a3b3c13dd787fc42b498be7_55.png)

将梯度置零，所有梯度将被设定为零，而不仅仅是一个非常非常小的数字，这取决于 t 的正负，因此你可以得到例如一个。

![](img/7e929a646a3b3c13dd787fc42b498be7_57.png)

死的 ReLU 神经元基本上看起来就是，如果一个带有 ReLU 非线性的神经元从不激活，那么对于数据集中你插入的任何例子，它永远不会打开，总是在这个平坦的区域，那么这个 ReLU 神经元就是一个死神经元，它的权重和偏差将永远不会学习，它们永远得不到梯度，因为这个神经元从未激活。

这种情况有时会在初始化时发生，因为权重和偏差使得一些神经元不幸地永远处于死亡状态，但也可能在优化过程中发生。如果你有一个过高的学习率，举个例子，有时这些神经元会得到过多的梯度，从而被击出数据流形，结果是。

从那时起，没有任何例子能激活它的神经元，所以这个神经元将永远处于死状态，这有点像网络中一种永久性的大脑损伤。有时，如果你的学习率非常高，举个例子，你有一个带有 ReLU 神经元的神经网络，你训练这个神经网络，得到了一些损失，但实际上你做的是通过某些部分进行训练。

整个训练集你前馈你的例子，可以发现从未激活的神经元。它们是你网络中的死神经元，所以这些神经元将永远不会激活，通常在训练过程中，这些 ReLU 神经元会改变、移动等，然后因为某处的高梯度，它们被随机击退，然后没有任何东西激活它们。

从那时起，它们就死了，这就像是一些神经元可能发生的永久性大脑损伤。这些其他非线性函数如 Leaky ReLU 不会那么严重地遭受这个问题，因为你可以看到它没有平坦的尾部，几乎总是会得到梯度。而 ELU 也相对常用，它也可能遭受这个问题，因为它有平坦。

所以这只是需要注意的事情，也是需要担忧的事情，在这种情况下。

![](img/7e929a646a3b3c13dd787fc42b498be7_59.png)

我们有太多激活 h 取极端值，而且因为没有列。

![](img/7e929a646a3b3c13dd787fc42b498be7_61.png)

白色，我认为我们会没事，实际上网络优化得很好，给了我们一个相当不错的结果。

![](img/7e929a646a3b3c13dd787fc42b498be7_63.png)

虽然有损失，但这并不是最优的，这不是你想要的，特别是在初始化期间。

![](img/7e929a646a3b3c13dd787fc42b498be7_65.png)

所以基本上发生的事情是，这个流向 10h 的 h 预激活。

![](img/7e929a646a3b3c13dd787fc42b498be7_67.png)

这太极端了，太大了，导致分布在两侧过于饱和，这不是你想要的，因为这意味着这些神经元的训练减少，因为它们更新不那么频繁。那么我们该如何解决这个问题呢？

![](img/7e929a646a3b3c13dd787fc42b498be7_69.png)

![](img/7e929a646a3b3c13dd787fc42b498be7_70.png)

前激活是来自 c 的 mcat，这些是均匀的高斯分布，但然后它乘以 w1 加 b1，h 前激活离零太远，这造成了问题，所以我们希望这个前激活更接近于零，类似于我们在 logis 中所拥有的。所以这里我们实际上希望得到非常相似的东西，现在把偏置设置为非常小的数字是可以的，我们可以选择。

乘以 0.01 以获取一点熵，我有时喜欢这样做，这样在这 10 个神经元的初始状态中会有一点变化和多样性，我发现实际上这有助于优化，接下来是权重。

![](img/7e929a646a3b3c13dd787fc42b498be7_72.png)

也可以像这样压缩，所以我们乘以 0.1，重新运行第一批。

![](img/7e929a646a3b3c13dd787fc42b498be7_74.png)

现在让我们看看这一点，首先让我们看看这里，你现在看到，因为我们乘以。

![](img/7e929a646a3b3c13dd787fc42b498be7_76.png)

乘以 0.1 后，我们有了更好的直方图，这因为前激活现在。

![](img/7e929a646a3b3c13dd787fc42b498be7_78.png)

在负 1.5 和 1.5 之间，我们希望有更少的白色，好的，这里没有白色。

![](img/7e929a646a3b3c13dd787fc42b498be7_80.png)

![](img/7e929a646a3b3c13dd787fc42b498be7_81.png)

基本上是因为没有神经元在任何方向上饱和超过 0.99，所以。

![](img/7e929a646a3b3c13dd787fc42b498be7_83.png)

这实际上是一个相当不错的状态，也许我们可以稍微增加一点。抱歉，我这里在改变 w1，也许我们可以调整到 0.2，好的，也许像这样是不错的选择。

![](img/7e929a646a3b3c13dd787fc42b498be7_85.png)

![](img/7e929a646a3b3c13dd787fc42b498be7_86.png)

分布，所以也许这就是我们初始化的方式，现在让我擦掉这些，让我。

![](img/7e929a646a3b3c13dd787fc42b498be7_88.png)

从初始化开始，让我进行完整的优化而不打断，看看结果是什么。

![](img/7e929a646a3b3c13dd787fc42b498be7_90.png)

![](img/7e929a646a3b3c13dd787fc42b498be7_91.png)

好的，优化完成，我重新运行损失，这就是我们得到的结果。

![](img/7e929a646a3b3c13dd787fc42b498be7_93.png)

作为提醒，我把我们在这次讲座中看到的所有损失都列了下来。所以我们看到这里确实有改进，提醒一下，我们开始时的验证损失是 2.17，当我们通过修正 softmax 使其自信地错误后，下降到了 2.13，通过修正 10H 层过饱和，降到了 2。

10，这当然是因为我们的初始化更好，因此我们花更多的时间进行有效训练，而不是效率不高的训练，因为我们的梯度设置为零。

![](img/7e929a646a3b3c13dd787fc42b498be7_95.png)

我们必须学习一些非常简单的东西，比如最开始的 softmax 的过度自信。

![](img/7e929a646a3b3c13dd787fc42b498be7_97.png)

我们花费周期去压缩权重矩阵，这基本上是在说明初始化及其对性能的影响，通过了解这些神经网络及其激活和梯度的内部情况。现在我们处理的是一个非常小的网络，这只是一个多层感知器的单层，因此由于网络非常浅。

优化问题实际上相当简单，并且非常宽容，因此即使我们的初始化非常糟糕，网络仍然最终学习到了，只是结果稍微差一些。然而，一旦我们开始处理更深的网络，比如说有 50 层的网络，情况就会变得复杂，这些问题会层层叠加。

因此，你实际上可能会遇到网络基本上完全不训练的情况。如果你的初始化足够糟糕，并且你的网络越深、越复杂，对这些错误的宽容度就越低，因此这点一定要引起注意，需要仔细检查、绘图，并要小心处理。

![](img/7e929a646a3b3c13dd787fc42b498be7_99.png)

是的，好的，这对我们有效，但现在我们有的全是这些。

![](img/7e929a646a3b3c13dd787fc42b498be7_101.png)

像点二这样的魔法数字，我从哪里想到这些，应该如何设置这些。如果我有一个层数很多的大型神经网络，显然没有人会手动进行这种设置。实际上，有一些相对有原则的方法来设置这些尺度，我想在这里介绍一下。

![](img/7e929a646a3b3c13dd787fc42b498be7_103.png)

现在让我们来贴一些我准备的代码，以激励对这个话题的讨论。

![](img/7e929a646a3b3c13dd787fc42b498be7_105.png)

我在这里做的事情是，我们有一些随机输入 x，这些输入是从高斯分布中抽取的。这里有 1000 个样本，都是 10 维的，然后我们在这一层有一个权重。这个权重的初始化也是使用高斯分布，就像我们在这里所做的一样。这些神经元在隐藏层中查看 10 个输入，这里有 200 个神经元，然后我们这里就像之前那样。

在这种情况下，乘法 x 乘以 w 以获得这些神经元的预激活值。基本上，这里的分析是考虑假设这些是均匀的高斯分布以及这些权重。

![](img/7e929a646a3b3c13dd787fc42b498be7_107.png)

如果我做 x 乘以 w，并且现在暂时忽略偏置和非线性，那么这些 gosh 的均值和标准偏差是什么？所以在这里一开始，输入仅仅是一个均值为 0，标准偏差为 1 的正态 gosh in 分布。标准偏差再次只是 gosh in 扩展的度量，但一旦我们乘以。

在这里，我们查看 y 的直方图，看到均值当然保持不变，大约是零。因为这是一个对称操作，但我们看到标准偏差扩展到了三。因此，输入的标准偏差是 1，但现在增长到了 3，所以你在直方图中看到的是这个 gosh in 正在扩展，我们正在从输入中扩展这个 gosh in。

我们并不想这样，我们希望大多数神经网络具有相对相似的激活。因此，整个神经网络中的单位 gosh in 大致相同，问题是我们如何缩放这些 w，以保持这种分布仍然是 gosh in。因此，直观上，如果我将这些 w 的元素乘以一个更大的数字，比如说乘以 5，那么这个 gosh in 就会不断增长。

标准偏差现在是 15，因此基本上这些输出 y 中的数字变得越来越极端。但如果我们将其缩小，比如说 0.2，那么相反，这个 gosh in 正在变得越来越小，正在缩小，你可以看到标准偏差是 0.6。那么问题是，我在这里乘以什么才能准确保持标准偏差为 1。

结果表明，当你通过方差计算这个乘法时，正确的数学答案是你应该除以 fan in 的平方根。fan in 基本上是输入元素的数量，这里是 10，所以我们应该除以 10 的平方根。这是一种计算平方根的方法，即将其提高到 0.5 的幂，这是相同的。

进行平方根运算时，当你除以 10 的平方根时，我们看到输出。真是的，它的标准偏差正好是 y 的标准偏差，现在毫无疑问，有很多论文对此进行了研究。

![](img/7e929a646a3b3c13dd787fc42b498be7_109.png)

如何最佳初始化神经网络？在多层感知器的情况下，我们可以有相当深的网络，里面有这些非线性，我们希望确保激活表现良好，并且它们不会扩展到无穷大或缩小到 0。问题是我们如何初始化权重，以便这些激活在整个网络中采取合理的。

值。现在，有一篇论文对此进行了相当详细的研究。

![](img/7e929a646a3b3c13dd787fc42b498be7_111.png)

经常被提到的是 Kaiming Hetal 的论文，标题是《深入研究互动激活器》。在这篇论文中。

![](img/7e929a646a3b3c13dd787fc42b498be7_113.png)

他们实际上研究卷积神经网络，特别是 relu 非线性激活函数，以及 p relu 非线性激活函数，而不是 10H 非线性，但分析是非常相似的。

![](img/7e929a646a3b3c13dd787fc42b498be7_115.png)

基本上，他们关心的 relu 非线性激活函数就是在这里。

![](img/7e929a646a3b3c13dd787fc42b498be7_117.png)

这里是一个压缩函数，所有负数都简单地限制为 0，因此正数被通过，但所有负数都被设置为 0。因为你基本上丢弃了一半的分布，他们在分析中发现。

![](img/7e929a646a3b3c13dd787fc42b498be7_119.png)

在神经网络中，前向激活你必须用增益进行补偿。因此，在这里他们发现，基本上当他们初始化权重时，必须用标准差为 fanon 的平方根除以 2 来进行零均值初始化。我们在这里初始化的是具有 fanon 平方根的高斯分布，这里的 NL 就是 fanon，所以我们有。

是 fanon 的平方根，因为我们这里有除法，现在他们必须添加这个因子。

![](img/7e929a646a3b3c13dd787fc42b498be7_121.png)

因为 relu 基本上丢弃了分布的一半，并将其限制在 0，所以你会得到一个初始因子。此外，本文还研究了神经网络前向传播中激活的行为，以及反向传播，我们必须确保梯度也表现良好。

因为最终它们会更新我们的参数，而他们在这里找到的东西经过很多分析后。

![](img/7e929a646a3b3c13dd787fc42b498be7_123.png)

我正在观看的分析有点难以接近，他们发现基本上如果你正确初始化前向传播，反向传播也会近似初始化，直到与隐藏神经元数量的大小有关的一个常数因子，但他们经验性地发现这不是。

这个选择非常重要，现在这个时间初始化也在 PyTorch 中实现。

![](img/7e929a646a3b3c13dd787fc42b498be7_125.png)

所以如果你去 torch.nn.init 文档，你会发现类似于正态分布，在我看来，这可能是初始化神经网络的最常见方式，现在它接受几个关键字参数，所以第一，它想知道你想要归一化激活还是想要归一化梯度，使其始终以零均值和单位方差。

一个标准差，因为他们发现这篇论文说这并不太重要，大多数人只是把它保留为默认值，也就是 pen，然后第二次传递非线性，这取决于你使用的非线性，因为我们需要计算略微不同的增益。因此，如果你的非线性只是线性，那么这里的增益将是。

![](img/7e929a646a3b3c13dd787fc42b498be7_127.png)

一样的公式在这里，但如果非线性是。

![](img/7e929a646a3b3c13dd787fc42b498be7_129.png)

还有其他一些地方我们会得到略微不同的增益，所以如果我们到这里的顶部，我们会看到，例如在 relu 的情况下，这个增益是根号二，原因是这个。在这篇论文中你可以看到二在根号内部，所以在这个情况下增益是根号二，而在线性或恒等的情况下，我们只是得到增益为一。

tenh 是我们在这里使用的，建议的增益是五分之三，直观上我们为什么需要在初始化的基础上增加增益，因为 tenh 就像 relu 一样是收缩的。

![](img/7e929a646a3b3c13dd787fc42b498be7_131.png)

转换，这意味着你正在从这个矩阵中提取输出分布。

![](img/7e929a646a3b3c13dd787fc42b498be7_133.png)

乘法，然后你以某种方式压缩它，现在 relu 通过将所有低于零的值夹紧到零来压缩它，而 tenh 也压缩它，因为它是一个收缩操作，它会把尾部挤压进来，因此为了抵消这种压缩，我们需要稍微提升权重，以便将所有内容重新归一化回标准单位标准差。

![](img/7e929a646a3b3c13dd787fc42b498be7_135.png)

所以这就是为什么现在会有一点增益，我在跳过这个部分。

![](img/7e929a646a3b3c13dd787fc42b498be7_137.png)

快速进行这个部分，我实际上是故意这样做的，原因是大约七年前，当这篇论文写成时，你必须对激活函数、成分及其范围和直方图非常小心，你必须非常注意增益的精确设置，以及对使用的非线性的仔细审查。

所以一切都非常麻烦、脆弱，并且对神经网络的训练非常精细安排，特别是如果你的神经网络非常深，但现代有许多创新使得一切显著更加稳定，更加规范，并且初始化这些网络变得不那么重要，其中一些现代创新是。

示例是残差连接，我们将在未来讨论使用多种归一化层，比如批量归一化、层归一化、组归一化，我们将深入探讨这些内容，以及更好的优化器，不仅仅是这里使用的简单优化器，而是稍微复杂一些的优化器。

rms prop 和特别是 adam 等所有这些现代创新使得你不必精确校准神经网络的初始化，所有这些都说完后，实际上我们应该怎么做呢？在初始化这些神经网络时，我基本上只是通过 fan in 的平方根来归一化我的权重。所以，基本上我们在这里做的与我现在做的差不多。

为了准确无误，我们回到这种正常的实现方式。我们希望将标准差设置为增益除以 fan in 的平方根。

![](img/7e929a646a3b3c13dd787fc42b498be7_139.png)

为了设置我们权重的标准差，我们将按照以下步骤进行，基本上，当我们有一个 torch 的随机数，并假设我创建一千个数字，我们可以查看这些数字的标准差，当然，这是一个分布的量，让我们把这个数字稍微放大一点，以便更接近 1，所以这是均值为 0 和单位标准差的高斯分布的扩展。

现在，基本上当你将这些数值乘以比如说 0.2 时，实际上就是缩小了高斯分布，使其标准差为 0.2。因此，你在这里乘以的数字最终成为这个高斯分布的标准差。这里的标准差是 0.2 的高斯分布，当我们对 w1 进行采样时，但我们希望将标准差设置为增益。

在 fan in 的平方根上乘以增益，换句话说，我们想要乘以增益。

![](img/7e929a646a3b3c13dd787fc42b498be7_141.png)

对于 10 h，增益是 5/3，然后乘以。

![](img/7e929a646a3b3c13dd787fc42b498be7_143.png)

![](img/7e929a646a3b3c13dd787fc42b498be7_144.png)

我想我们要除以 fan in 的平方根，在这个例子中，fan in 最多是 10。

![](img/7e929a646a3b3c13dd787fc42b498be7_146.png)

我刚注意到，这里 w1 的 fan in 实际上是嵌入乘以块大小。大家还记得，这实际上是 30，因为每个字符是 10 维的，但我们有三个字符并且进行连接，所以这里的 fan in 实际上是 30，我应该在这里使用 30，但基本上我们需要 30 的平方根，所以这是这个数字，这就是我们要的。

我们希望标准差为，而这个数值恰好是 0.3，而在这里。通过调整和查看分布，确保它看起来不错，我们得出了 0.2，因此我们在这里想做的是使标准差为 5/3，也就是我们的增益，再将这个数值乘以 0.2 的平方根。

这些括号在这里并不是很必要，但我放在这里以便于理解。这基本上就是我们想要的，这在我们的 10H 非线性中是时间的表现，这就是我们如何初始化神经矩阵，因此我们乘以 0.3 而不是。

![](img/7e929a646a3b3c13dd787fc42b498be7_148.png)

乘以 0.2，因此我们可以这样初始化，然后我们可以进行训练。

![](img/7e929a646a3b3c13dd787fc42b498be7_150.png)

![](img/7e929a646a3b3c13dd787fc42b498be7_151.png)

神经矩阵，看看我们得到了什么，好的，我训练了神经矩阵，最终我们大致处于同一个位置，所以查看损失值，我们现在得到了 2.10，之前我们也得到了 2.10。有一点差异，但我怀疑那只是过程中的随机性。

![](img/7e929a646a3b3c13dd787fc42b498be7_153.png)

当然，最重要的是我们达到了同一个位置，但我们并不需要引入任何。

![](img/7e929a646a3b3c13dd787fc42b498be7_155.png)

我们从观察直方图和试验中得出的魔法数字，是一种半原则性的方法，能够扩展到更大的网络，并且可以作为我们的指导，因此我提到这些初始化的精确设置。

![](img/7e929a646a3b3c13dd787fc42b498be7_157.png)

由于一些现代创新，今天这些初始化的精确设置并不是那么重要，我认为现在是个不错的时机。

![](img/7e929a646a3b3c13dd787fc42b498be7_159.png)

介绍这些现代创新之一，即批量归一化，批量归一化在 2015 年由谷歌的一个团队提出，这是一篇极具影响力的论文，因为它使得训练非常深的神经网络变得相当可靠，基本上就是有效的，所以这里是。

![](img/7e929a646a3b3c13dd787fc42b498be7_161.png)

批量归一化的作用及其基本实现，我们有这些隐藏状态 H 预激活。对，我们在谈论如何不希望这些预激活状态太小，因为那样 10H 就没有任何作用，但我们也不希望它们太大，因为那样 10H 就饱和了，实际上我们希望它们大致为高斯，即零均值和单位。

或者至少在初始化时一个标准差，所以来自批量归一化论文的洞察是，好的，你有这些隐藏状态，你希望它们大致为高斯，那么为什么不直接将隐藏状态归一化为高斯呢？这听起来有点疯狂，但你可以这样做，因为将隐藏状态标准化，使其单位为高斯，是完全可以的。

可微分操作，正如我们将看到的，这就是这篇论文中的重大见解。当我第一次读到它时，我的脑海被震撼了，因为你可以规范化这些隐藏状态。如果你希望网络中的状态是单位高斯状态，至少在初始化时，你可以将它们标准化为单位高斯状态。现在，让我们看看这如何运作，我们将滚动到我们的预激活部分。

![](img/7e929a646a3b3c13dd787fc42b498be7_163.png)

在它们进入 10H 之前，想法再次是要记住我们试图让这些数值大致符合高斯分布。这是因为如果这些数值太小，那么这里的 10H 会变得有些不活跃；但如果这些数值非常大，那么 10H 就会饱和并影响流动，因此我们希望这些数值大致符合高斯分布。所以批量归一化的关键在于。

我们可以直接标准化这些激活，使它们完全符合高斯分布。因此 H 预激活的形状是 32 乘以 200，32 个样本，200 个神经元在最后一层。基本上，我们可以拿 H 预激活来计算均值，而均值的计算是沿着零维度进行的，并且我们还希望将它们保留为真实值，以便我们可以轻松地进行广播。

这个形状是 1 乘以 200，换句话说，我们是在对批次中所有元素进行均值计算。同样，我们也可以计算这些激活的标准差，这样的结果也是 1 乘以 200。现在在这篇论文中，他们有。

![](img/7e929a646a3b3c13dd787fc42b498be7_165.png)

这里的公式可以看到，我们正在计算均值，即取任意神经元激活的平均值，而标准差基本上是我们使用的扩散度量，表示每个值与均值的距离的平方平均，这就是方差。如果你想进行。

标准差是通过对方差开方来得到的，因此我们计算的就是这两个值。现在我们要通过减去均值并除以标准差来对这些 x 进行归一化或标准化，基本上我们是在处理 H 预激活。

![](img/7e929a646a3b3c13dd787fc42b498be7_167.png)

我们减去均值，然后除以标准差，这正是这两者所做的。

![](img/7e929a646a3b3c13dd787fc42b498be7_169.png)

标准差和均值的计算，抱歉，这里是均值，这里是方差。你会看到σ是标准差，通常情况下，这就是σ的平方，即方差，是标准差的平方。这就是我们如何标准化这些值的方式，这样做的结果是每个。

![](img/7e929a646a3b3c13dd787fc42b498be7_171.png)

单个神经元的激活率将在这 32 个示例中恰好是单位高斯分布，因此称其为批量归一化，我们正在对这些批次进行归一化，然后我们原则上可以训练这个。注意计算均值和标准差，这些只是数学公式，它们是完全可微分的，所有这些。

这是完全可微分的，我们可以直接训练这个，问题在于你实际上不会获得非常好的结果，原因是我们希望这些在初始化时大致呈高斯分布，但我们不希望这些始终被强制为高斯分布，我们希望允许神经网络调整这些分布，使其更加分散或更加尖锐。

使一些 10 个 h 神经元，可能更多，触发更频繁或不那么频繁，因此我们希望这个分布能够移动，我们希望反向传播告诉我们这个分布应该如何移动。因此，除了在任何时刻标准化激活的想法之外。

![](img/7e929a646a3b3c13dd787fc42b498be7_173.png)

在网络中，我们还必须引入这部分附加组件，文中对此进行了描述。

![](img/7e929a646a3b3c13dd787fc42b498be7_175.png)

进行缩放和平移，因此基本上我们所做的是将这些归一化的输入进行缩放，并通过某个增益进行附加缩放，并通过某个偏差进行偏移，以获得最终输出。

![](img/7e929a646a3b3c13dd787fc42b498be7_177.png)

从这一层开始，所以这意味着我们将允许批量归一化增益仅初始化一次，这一次将采用一行 n 个隐藏单元的形状。

![](img/7e929a646a3b3c13dd787fc42b498be7_179.png)

然后我们还将有一个 bn 偏差，它将初始化为零，形状为 n 乘以一乘以 n 个隐藏单元，然后这里的 bn 增益将乘以它，而 bn 偏差将对此进行偏移。

![](img/7e929a646a3b3c13dd787fc42b498be7_181.png)

由于在初始化时将其初始化为一，将其初始化为零，因此这一批次中每个神经元的激活值将恰好是单位高斯分布，无论 HP 激活的分布如何，输出都将是每个神经元的单位高斯分布。这大致是我们在初始化时想要的，然后在优化过程中，我们将能够进行反向传播。

将传播到 bn 增益和 bn 偏差，并进行更改，使网络获得充分的能力。内部处理时，网络可以随意使用这些，我们只需确保将这些包含在神经网络的参数中，因为它们将通过反向传播进行训练。因此，让我们初始化这些，然后我们应该能够进行训练。

然后我们还将复制这一行，即最佳归一化层。

![](img/7e929a646a3b3c13dd787fc42b498be7_183.png)

在这一行代码中，我们将下滑，并且在测试时也会做完全相同的事情，因此在训练时，我们将进行归一化，然后缩放，这将给我们带来我们的结果。

![](img/7e929a646a3b3c13dd787fc42b498be7_185.png)

训练和验证损失，我们稍后会看到我们实际上会对其进行一些更改，但现在我会保持这个状态，所以我会等着这个。

![](img/7e929a646a3b3c13dd787fc42b498be7_187.png)

允许神经网络在这里收敛，当我们向下滚动时，我们看到我们的验证损失大约是 2.10，我在这里写下了这一点，并且我们看到这实际上与我们之前取得的一些结果相当。现在我并不期待在这种情况下有改善，因为我们处理的是一个非常简单的神经网络。

只有一个隐藏层的神经网络，事实上在这个非常简单的情况下只有一个隐藏层。

![](img/7e929a646a3b3c13dd787fc42b498be7_189.png)

我们能够实际计算出 W 的规模应该是什么，以使这些预激活。

![](img/7e929a646a3b3c13dd787fc42b498be7_191.png)

这些激活已经大致呈高斯分布，因此合理化在这里没有发挥太多作用，但你可以想象，一旦你有一个更深的神经网络，包含许多不同类型的操作，以及我们将讨论的残差连接等，这将变得相对复杂。

![](img/7e929a646a3b3c13dd787fc42b498be7_193.png)

调整权重矩阵的规模非常困难，以使得整个神经网络的激活大致呈高斯分布，这将很快变得难以处理。

![](img/7e929a646a3b3c13dd787fc42b498be7_195.png)

但与此相比，在整个神经网络中散布合理化层将容易得多。

![](img/7e929a646a3b3c13dd787fc42b498be7_197.png)

在神经网络中，通常会查看每个线性层，比如这个。这是一个通过权重矩阵相乘并加上偏置的线性层，或者例如卷积层，我们将在后面讨论，它也基本上以更具空间结构的格式进行权重矩阵的乘法。

在它之后添加一个**合理化层**，以控制神经网络中每个点的激活规模。

![](img/7e929a646a3b3c13dd787fc42b498be7_199.png)

在整个神经网络中，这些激活的规模不需要我们做到完美。

![](img/7e929a646a3b3c13dd787fc42b498be7_201.png)

数学上关注这些不同类型神经网络的激活分布。

![](img/7e929a646a3b3c13dd787fc42b498be7_203.png)

网络或你可能想引入到神经网络中的乐高积木，并且它显著地。

![](img/7e929a646a3b3c13dd787fc42b498be7_205.png)

这稳定了训练，因此这些层现在相当流行，而归一化提供的稳定性实际上是以巨大的代价为代价的，如果你考虑这里发生的事情，某些事情非常奇怪且不自然，以前我们有一个单一的样本输入到神经网络中，然后计算这个激活及其。

logits 这是一个确定性的过程，因此你会得到这个样本的一些 logits，然后由于训练的效率，我们突然开始使用样本批次，但这些样本批次是独立处理的，这只是一种效率提升，但现在，批量归一化由于批次的归一化使得这些样本相互耦合。

在数学上，神经网络的前向传播和反向传播中的隐藏状态、激活值 HP Act 和任何一个输入样本的 logits 不仅仅是该样本及其输入的函数，还同时是该批次中其他所有样本的函数，这些样本是随机抽取的，因此发生的情况是，例如。

当你查看 HP Act 时，它将用于 H 隐藏状态激活，对于这些输入样本中的任何一个，其实际变化会根据批次中的其他样本而略有不同，因此，H 将会突然变化，如果你想象不同样本的抽取，它会像抖动一样。

均值和标准差的统计量将受到影响，因此你会得到 H 的抖动和 logits 的抖动，你会认为这会是一个错误或不希望的事情，但在某种非常奇怪的方式中，这实际上在神经网络训练中是有益的，副作用是你可以将其视为一种。

正则化器，因为发生的情况是你有你的输入并得到你的 H，然后根据其他样本，这会生成一些噪声，因此这实际上是在有效地填充这些输入样本中的任何一个，并引入一点熵，因为。

填充实际上有点像一种数据增强形式，我们将在未来讨论，这有点像对输入进行小幅增强并抖动，这使得神经网络更难以过拟合这些具体的样本，因此通过引入所有这些噪声，它实际上像是填充这些样本并且它正则化了神经网络，这就是为什么看似作为二阶效应的原因实际上这是一个正则化器。

这使得我们更难去除批量归一化的使用，因为基本上没有人喜欢这种特性，即批量中的示例在数学上是耦合的，在前向传递中也是如此。至少各种奇怪的结果我们稍后会详细讨论，这导致了很多错误，所以没有人喜欢这种特性，因此人们尝试过。

我们希望弃用批量归一化，转向不耦合示例的其他归一化技术，如线性归一化、实例归一化、组归一化等，我们稍后会详细讨论。但总之，批量归一化是第一种归一化技术。

引入后效果极佳，正好具有这种正则化效果，稳定了训练。人们一直在尝试去除它，转向一些其他归一化技术，但这很困难，因为它的效果相当好。而它能如此有效的原因之一正是因为这种正则化效果。

有效地控制激活值及其分布，这就是批量归一化的简要历史。我想给你展示一些其他奇怪的结果。

![](img/7e929a646a3b3c13dd787fc42b498be7_207.png)

这里是我之前提到的一个奇怪结果。

![](img/7e929a646a3b3c13dd787fc42b498be7_209.png)

当我在验证集上评估损失时，基本上一旦我们训练了神经网络。

![](img/7e929a646a3b3c13dd787fc42b498be7_211.png)

我们希望在某种环境中部署它，并且希望能够输入一个单独的示例并从我们的神经网络中得到预测。但当我们的示例之间存在耦合时，我们该如何做到这一点呢？

![](img/7e929a646a3b3c13dd787fc42b498be7_213.png)

神经网络现在在前向传递中估计一个批次的均值和标准差的统计量，神经网络现在期望批次作为输入。那么我们该如何输入一个单独的示例并获得合理的输出结果呢？因此，批量归一化论文中的提议如下。

![](img/7e929a646a3b3c13dd787fc42b498be7_215.png)

我们想要做的基本上是在训练后进行一步，计算并设置整个训练集上的批量均值和标准差，因此我写了这段代码以节省时间，我们将称之为校准批量统计。基本上我们做的是使用`torch.no_grad()`告诉`torch`。

我们不会称这个文档为反向文档，它将变得更加高效。我们将获取训练集，为每个训练示例获取预激活值。然后我们将一次性估算整个训练集的均值和标准差。接着我们将获得 B 和均值，以及 B 和标准差，这些都是固定的。

![](img/7e929a646a3b3c13dd787fc42b498be7_217.png)

估算整个训练集的数字，在这里我们不再动态估算，而是使用 B 和均值，并且我们将仅使用 B 和标准差。

![](img/7e929a646a3b3c13dd787fc42b498be7_219.png)

因此在测试时，我们将固定这些值，限制它们，并在推理期间使用它们，现在。

![](img/7e929a646a3b3c13dd787fc42b498be7_221.png)

你会发现我们基本上得到了相同的结果，但我们获得的好处是我们可以。

![](img/7e929a646a3b3c13dd787fc42b498be7_223.png)

现在也只需前向传播一个示例，因为均值和标准差现在是固定的。

![](img/7e929a646a3b3c13dd787fc42b498be7_225.png)

嗯，有一种张量的说法是没有人真正想在神经网络训练后的第二阶段估算这个均值和标准差，因为每个人都懒惰，因此这篇批量归一化的论文实际上引入了一个新想法，即我们可以在神经网络训练期间以运行的方式估算均值和标准差。

![](img/7e929a646a3b3c13dd787fc42b498be7_227.png)

网络，然后我们可以简单地进行一次训练阶段，并在该训练的旁边进行。

![](img/7e929a646a3b3c13dd787fc42b498be7_229.png)

我们正在估算运行中的均值和标准差，所以让我们看看这会是什么样子。

![](img/7e929a646a3b3c13dd787fc42b498be7_231.png)

让我基本上获取我们在批量上估算的均值，并将其称为 B 和第 i 次迭代的均值，然后这里是 B 和标准差 i，均值在这里，标准差在这里，所以到目前为止我什么都没做，只是移动了一下，创建了这些额外的均值和标准差变量，并把它们放在这里，所以到目前为止没有任何改变。

但我们现在要做的就是保持这两个值的运行均值。

![](img/7e929a646a3b3c13dd787fc42b498be7_233.png)

在训练期间，让我在这里稍微调整一下，创建一个 bn mean underscore running。我将其初始化为零，然后 bn STD running 将初始化为一。

![](img/7e929a646a3b3c13dd787fc42b498be7_235.png)

因为在开始时，由于我们初始化 w1 和 b1 的方式，h-criact 大致会是单位矩阵，因此均值大致为零，标准差大致为一，所以我将以这种方式初始化这些，但随后我会更新这些，在 PyTorch 中，这些运行中的均值和标准差实际上并不是梯度优化的一部分。

我们不会对它们推导梯度，它们是在训练的旁边更新的。因此，我们要做的是告诉 PyTorch 使用 torch.no_grad。

![](img/7e929a646a3b3c13dd787fc42b498be7_237.png)

此处的更新不应构建出一个图，因为没有反向传播。但这个运行均值基本上将是 0.999 乘以当前值，加上 0.001 乘以这个新均值。同样，bn STD 运行将大致保持原样，但会在当前值的方向上收到小更新。

标准差是，如你所见，这次更新在梯度优化的外部和旁边。它不是使用梯度下降进行更新，而是以一种类似平滑的运行均值方式被更新。因此，在网络训练期间，这些预激活值在反向传播中会发生变化和调整。

我们在跟踪典型的均值和标准差，并进行一次估计。

![](img/7e929a646a3b3c13dd787fc42b498be7_239.png)

当我现在运行这个时，我正在以一种运行的方式跟踪这个，而我们希望的是。

![](img/7e929a646a3b3c13dd787fc42b498be7_241.png)

当然，均值均值下划线运行和 bn 均值下划线 STD 将与我们之前计算的非常相似，因此我们不需要第二个阶段。

![](img/7e929a646a3b3c13dd787fc42b498be7_243.png)

我们将这两个阶段结合在一起，放在彼此旁边，你可以这样看待它。这也是在“混蛋归一化”层中实现的方式。

![](img/7e929a646a3b3c13dd787fc42b498be7_245.png)

在 PyTorch 中，所以在训练期间，将发生完全相同的事情，稍后当你使用。

![](img/7e929a646a3b3c13dd787fc42b498be7_247.png)

![](img/7e929a646a3b3c13dd787fc42b498be7_248.png)

推理时，将使用估计的运行均值，以及那些隐藏层的均值和标准差。

![](img/7e929a646a3b3c13dd787fc42b498be7_250.png)

状态，所以我们等待优化收敛，希望运行均值和导数大致等于这两个，然后我们可以简单地在这里使用它，我们不需要这个。

![](img/7e929a646a3b3c13dd787fc42b498be7_252.png)

明确校准的阶段结束，好的，优化完成后，我将重新运行明确的估计，然后来自明确估计的 bn 均值在这里，来自运行的 bn 均值。

![](img/7e929a646a3b3c13dd787fc42b498be7_254.png)

在优化过程中进行的估计，你可以看到它非常相似，并不是完全相同，但相当接近，以同样的方式，bn STD 是这个，bn STD 运行是这个，正如你所看到的，再次它们的值相当相似，不完全相同，但相当接近，因此在这里，我们可以用 bn mean 运行代替 bn mean，用 bn STD 运行代替 bn STD。

希望验证损失不会受到太大的影响，所以基本上是相同的。

![](img/7e929a646a3b3c13dd787fc42b498be7_256.png)

通过这种方式，我们消除了显式校准阶段的需要，因为我们正在进行中。

![](img/7e929a646a3b3c13dd787fc42b498be7_258.png)

在这里的行上，所以我们快完成批量归一化了，还有两个笔记我想提一下，第一，我跳过了关于这个正数 epsilon 的讨论，这个 epsilon 通常是某个小的固定数值，例如负五的默认值，而它的作用基本上是防止在方差为零的情况下出现除以零的情况。

在这种情况下，你的批次的结果恰好为零，这里我们通常会遇到除以零的情况，但由于有加上了 epsilon，这将使得分母变成一个很小的数，从而使情况变得更加稳定，因此可以在这里添加一个很小的正数 epsilon，实际上这并不会实质性地改变结果，我在我们的案例中会跳过它。

这种情况在我们非常简单的例子中不太可能发生，我希望你能注意到。

![](img/7e929a646a3b3c13dd787fc42b498be7_260.png)

注意到我们在这里有些浪费，这很微妙，但在这里我们将偏差添加到每个预激活中，这些偏差实际上是无用的，因为我们将它们添加到每个预激活中，但然后我们为这些神经元计算均值并进行减法，因此你在这里添加的任何偏差都会在这里被减去，所以这些偏差没有任何作用。

实际上它们被减去了，并且对其余计算没有影响，因此如果你查看 b1.grad，它实际上将为零，因为它被减去了，并且实际上没有。

![](img/7e929a646a3b3c13dd787fc42b498be7_262.png)

因此，每当你使用批量归一化层时，如果在前面有任何权重层，比如线性层或卷积层，最好来这里，直接不使用偏差，所以你不想使用偏差，在这里你也不想添加它，因为。

![](img/7e929a646a3b3c13dd787fc42b498be7_264.png)

这里的伪影变成了这个批量归一化偏差，现在这个批量归一化偏差负责这个分布的偏移，而不是我们最初的 b1。

![](img/7e929a646a3b3c13dd787fc42b498be7_266.png)

所以基本上批量归一化层有自己的偏置，而前一层就不需要再有偏置，因为那个偏置无论如何都会被减去。这是需要小心的另一个小细节，有时候它不会造成灾难性的后果，这个 b1 将会变得无用，它不会获得任何梯度，不会学习，会保持不变，这只是浪费。

但它并不会对其他任何事情产生实际影响。好的，所以我稍微调整了一下代码。

![](img/7e929a646a3b3c13dd787fc42b498be7_268.png)

我想对批量归一化层做一个非常简短的总结。

![](img/7e929a646a3b3c13dd787fc42b498be7_270.png)

我们使用批量归一化来控制神经网络中激活值的统计特性。通常会在神经网络中各处添加批量归一化层，通常会将其放在像线性层或卷积层等乘法层之后。

![](img/7e929a646a3b3c13dd787fc42b498be7_272.png)

批量归一化内部有增益和偏置的参数，这些参数通过反向传播进行训练。此外，它还有两个缓冲区，这些缓冲区分别是均值和标准差、运行均值和标准差。

![](img/7e929a646a3b3c13dd787fc42b498be7_274.png)

这些参数不是通过反向传播训练的，而是通过一种稍微不太正常的更新方式来训练的。

![](img/7e929a646a3b3c13dd787fc42b498be7_276.png)

运行均值更新，这些是批量归一化层的参数和缓冲区。它的实际操作是计算输入批次的激活值的均值和标准差，然后将该批次中心化为单位均值，再通过学习到的偏置和增益进行偏移和缩放。

![](img/7e929a646a3b3c13dd787fc42b498be7_278.png)

它跟踪输入的均值和标准差，并维护这个运行均值和标准差，这将在推理时使用，以免我们需要不断重新估计均值和标准差。此外，这还使我们在测试时能够基本上处理单个示例。这就是批量归一化层，它相当复杂。

这一层的内部运作是这样的，现在我想给你展示一个真实的例子。

![](img/7e929a646a3b3c13dd787fc42b498be7_280.png)

你可以搜索一下 ResNet，它是一种残差神经网络，这些是用于图像分类的神经网络架构。当然，我们还没有详细讨论 ResNet，所以我不会解释所有的部分，但现在请注意，图像在上方输入到 ResNet，并且有许多重复结构的层，一直到最终的预测。

在那个图像内部的结构是由这些块组成的，这些块在这个深度神经网络中是顺序堆叠的。这个块的代码基本上是用作并在序列中重复的，称为瓶颈块。瓶颈块有很多，这里都是 pytorch，当然我们还没有覆盖全部，但我想。

这里我想指出一些小的部分，初始化是在这里，我们初始化神经网络。

![](img/7e929a646a3b3c13dd787fc42b498be7_282.png)

这里的代码块基本上是我们正在做的事情，我们初始化所有的层。在前向传播中，我们指定神经网络在实际接收到输入后如何工作。

![](img/7e929a646a3b3c13dd787fc42b498be7_284.png)

所以这里的代码与我们正在做的事情是一致的，现在这些块被复制并串联堆叠，这就是残差网络的样子，因此请注意这里发生的事情。这些是卷积层，这些卷积层基本上与线性层是相同的，只是卷积层不应用卷积层。

对于图像，它们具有空间结构，基本上这个线性乘法和偏置。偏移是在图像块上进行的，而不是整个输入图，因此由于这些图像具有。结构，空间结构，卷积基本上就是做 wx 加 b，但它们是在输入的重叠块上进行的，但其他情况下就是 wx 加 b。然后我们有默认的归一化层。

这里被初始化为二维批归一化，所以是二维批归一化层。然后我们有一个非线性，例如 relu，因此在这里他们使用 relu，而在这种情况下我们使用的是 10h。但两者都是非线性，你可以相对可互换地使用它们。对于非常深的网络，relu 通常在经验上表现得更好，所以看看这个模式。

这里重复的部分是卷积批归一化，确切地说是卷积批归一化，等等。然后这里是我们还没有覆盖的残差连接，但基本上这就是我们这里的确切模式，我们有一个权重层，如卷积或线性层，批归一化，然后是 10h，非线性，但基本上是一个权重层。

一个归一化层和非线性，这正是你在创建这些深度神经网络时会堆叠的元素，正如这里所做的那样，还有一件事我想请你注意。

![](img/7e929a646a3b3c13dd787fc42b498be7_286.png)

这里在初始化 conv 层（如 conv 一对一）时，深度正是这里，因此它正在初始化一个 nn。conf2d，这是 pytorch 中的一个卷积层，还有许多关键字参数我还不打算解释，但你可以看到有 bias 等于 false，bias 等于 false 的原因和在我们案例中不使用偏差的原因完全相同。

![](img/7e929a646a3b3c13dd787fc42b498be7_288.png)

![](img/7e929a646a3b3c13dd787fc42b498be7_289.png)

我是如何消除偏差的使用，偏差的使用是虚假的，因为在这个权重层之后有一个批归一化，而批归一化减去那个偏差，并且它有自己的偏差。因此没有必要引入这些虚假的参数，这对性能没有影响，只是无用的，因此因为他们有这个 conv master 和 relu 的主题，他们不需要在这里添加偏差。

因为这里有一个偏差，所以这个例子很容易找到，只需搜索。 resnet pytorch，嗯，就是这个例子，所以这有点像 pytorch 中残差神经网络的基本实现，你可以在这里找到，但当然我还没有覆盖很多这些部分，我也想简要深入这些定义。

![](img/7e929a646a3b3c13dd787fc42b498be7_291.png)

pytorch 层及其参数，现在我们要看的是线性层，因为这就是我们在这里使用的。这是一个线性层，我还没有。

![](img/7e929a646a3b3c13dd787fc42b498be7_293.png)

还没有覆盖卷积，但正如我提到的，卷积基本上是线性层，只是在小块上进行处理，因此线性层执行的是 wx 加 b，而在这里他们称 w 为转置，因此称为 wx 加 b，和我们在这里初始化这个层的方法非常相似，你需要知道。

![](img/7e929a646a3b3c13dd787fc42b498be7_295.png)

fan in 和 fan out，这样他们就可以初始化这个 w，这是 fan in 和 fan out。

![](img/7e929a646a3b3c13dd787fc42b498be7_297.png)

他们知道权重矩阵应该有多大，你还需要传入是否希望有偏差，如果你将其设置为 false，那么这个层内部将没有偏差。你可能想这样做，就像在我们的案例中，如果你的层后面有一个归一化层，比如批归一化，这样可以基本上在初始化时禁用偏差。

![](img/7e929a646a3b3c13dd787fc42b498be7_299.png)

![](img/7e929a646a3b3c13dd787fc42b498be7_300.png)

如果我们向下看，这是报告在这个线性层中使用的变量，我们的线性层。

![](img/7e929a646a3b3c13dd787fc42b498be7_302.png)

这里有两个参数：权重和偏差，正如它们有一个权重和一个偏差一样。

![](img/7e929a646a3b3c13dd787fc42b498be7_304.png)

他们讨论如何默认初始化，所以默认情况下 pytorch 将通过获取输入的数量来初始化你的权重，然后做一个输入数量的平方根的倒数，而不是使用正常分布，他们使用均匀分布，所以非常相似，但。它们使用的是 1 而不是 5/3，因此没有计算增益。

这只是一个，但在其他方面与这里的完全相同，即为输入的平方根的倒数。所以，输入的平方根的倒数是权重的尺度，但当它们被绘制时。

![](img/7e929a646a3b3c13dd787fc42b498be7_306.png)

它们默认不使用高斯分布，而是使用均匀分布。

![](img/7e929a646a3b3c13dd787fc42b498be7_308.png)

所以它们在负平方根 k 到平方根 k 之间均匀绘制，但这是完全准确的。

![](img/7e929a646a3b3c13dd787fc42b498be7_310.png)

同样的事情，以及关于我们在这节课上看到的内容的相同动机。

![](img/7e929a646a3b3c13dd787fc42b498be7_312.png)

这样做的原因是，如果你有一个大致高斯输入，这将确保你从这个层输出一个大致高斯输出，基本上通过权重的缩放来实现。

![](img/7e929a646a3b3c13dd787fc42b498be7_314.png)

输入的平方根的倒数，所以这就是它所做的，第二件事是。

![](img/7e929a646a3b3c13dd787fc42b498be7_316.png)

归一化层，所以让我们看看在 pytorch 中它的样子，这里我们有一个。

![](img/7e929a646a3b3c13dd787fc42b498be7_318.png)

这里使用的是维度归一化层，并且有多个关键字参数输入，因此我们需要知道特征的数量，对我们来说是 200。这是必要的，以便初始化这些参数，包括增益、偏差和缓冲区，用于运行均值和标准差，然后它们需要知道这里的 epsilon 值。

默认情况下，这个值是 1 负 5，你通常不会太改动这个。然后它们需要知道动量，动量在这里被解释为基本使用。

![](img/7e929a646a3b3c13dd787fc42b498be7_320.png)

对于这些运行均值和运行标准差，默认动量是 0.1。我们在这个例子中使用的动量是 0.001，基本上你可能想要改变这个值。

![](img/7e929a646a3b3c13dd787fc42b498be7_322.png)

有时，粗略地说，如果你有一个非常大的批次大小，通常你会看到的是，当你估计每个批次大小的均值和标准差时，如果它足够大，你将得到大致相同的结果，因此你可以使用稍高的动量，如 0.1，但对于小至 32 的批次大小，这里的均值和标准差。

可能会采用略有不同的数值，因为我们使用的只有 32 个示例来估计均值和标准差，所以这个值变化很大，如果你的动量是 0.1，那可能不足以让这个值收敛到整个训练集的实际均值和标准差，因此如果你的批量大小非常小。

0.1 的动量是潜在危险的，可能导致运行均值和标准差在训练期间波动过大，而没有正确收敛。affine 为真决定这个批量归一化层是否具有可学习的仿射变换。

![](img/7e929a646a3b3c13dd787fc42b498be7_324.png)

参数增益和偏差几乎总是保持为真，我其实不太确定为什么你。

![](img/7e929a646a3b3c13dd787fc42b498be7_326.png)

如果你想将其更改为假，那么跟踪运行统计是决定是否。

![](img/7e929a646a3b3c13dd787fc42b498be7_328.png)

PyTorch 的批量归一化层会做这个，可能你会想跳过的一个原因。

![](img/7e929a646a3b3c13dd787fc42b498be7_330.png)

运行统计是因为你可能希望在结束时估计它们作为第二阶段。

![](img/7e929a646a3b3c13dd787fc42b498be7_332.png)

在这种情况下，你不希望批量归一化层进行所有这些额外的计算，因为你不会使用，最后我们需要知道我们要在哪个设备上运行这个批量归一化，是在 CPU 上还是 GPU 上，以及数据类型应该是半精度、单精度、双精度等等，所以这就是批量归一化层的要求。

![](img/7e929a646a3b3c13dd787fc42b498be7_334.png)

它们链接到论文，公式是我们实现的相同的一切也完全一样。

![](img/7e929a646a3b3c13dd787fc42b498be7_336.png)

我们在这里做了，这就是我想在这次讲座中覆盖的所有内容。

![](img/7e929a646a3b3c13dd787fc42b498be7_338.png)

我想谈的是理解激活值和梯度及其统计数据的重要性。在神经网络中，这变得越来越重要，尤其是当你让你的神经网络变得更大、更深时。我们基本上看了输出层的分布，我们看到如果你有两个置信度较高的错误预测，是因为激活值太。

在最后一层搞砸了，你可能会得到这些曲棍球棒损失，如果你修复了这个问题，你会在训练结束时获得更好的损失，因为你的训练没有做无用的工作，我们还看到我们需要控制激活值，我们不希望它们压缩为零或爆炸到无穷大，因为这样你会遇到很多麻烦。

这些神经网络中的非线性，基本上你希望整个网络保持相对均匀。你希望在整个神经网络中大致均匀地激活，让我谈谈，好吧。

![](img/7e929a646a3b3c13dd787fc42b498be7_340.png)

如果我们希望激活大致均匀，我们如何在神经网络初始化期间缩放这些权重矩阵和偏置，以便我们不会。让一切尽可能可控。

![](img/7e929a646a3b3c13dd787fc42b498be7_342.png)

![](img/7e929a646a3b3c13dd787fc42b498be7_343.png)

这给我们带来了巨大的改进，然后我谈到了这个策略对于更深的神经网络来说并不。实际上是可能的，因为当你有更深的神经网络和许多不同类型的层时，准确设置。权重和偏置变得非常困难，以至于激活在整个网络中大致均匀。

![](img/7e929a646a3b3c13dd787fc42b498be7_345.png)

神经网络，所以我引入了归一化层的概念，现在有许多归一化。层在实践中被人们使用，学士归一化层，层归一化，这就是归一化。组归一化，我们还没有涵盖大多数，但我引入了第一个，还有。

![](img/7e929a646a3b3c13dd787fc42b498be7_347.png)

我相信首先出现的是称为学士归一化的层，我们看到了。学士归一化是如何工作的，这是一个可以在深度神经网络中散布的层。基本想法是，如果你希望激活大致均匀，那么就取你的激活值并。取均值和标准差，并对数据进行中心化，你可以这样做，因为中心化。

操作是可微的，但除此之外，我们实际上还需要添加许多额外的功能。这让你感受到学士归一化层的复杂性，因为现在。我们正在对数据进行中心化，这很好，但突然间我们需要增益和偏置，而这些都是。可训练的，然后由于我们耦合了所有的训练示例，现在突然间问题是。

你如何进行推理，或者说我们现在需要估计这些均值和。

![](img/7e929a646a3b3c13dd787fc42b498be7_349.png)

标准差一次或整个训练集，然后在推理时使用这些，但没有人。

![](img/7e929a646a3b3c13dd787fc42b498be7_351.png)

喜欢进行第二阶段，因此我们在训练期间将所有内容折叠到学士归一化层中。并试图以运行的方式估计这些，以便一切变得更简单，这给了。我们学士归一化层，正如我提到的，没有人喜欢这个层，它会导致大量的。错误，直观上是因为它在神经网络的前向传递中耦合示例。

网络，我在生活中多次用这个层给自己带来了麻烦，我不希望你们遭受同样的痛苦，所以基本上尽量避免它。对于这些层的其他一些替代方案，例如群归一化或层归一化，最近在深度学习中变得更加常见，但我们还没有介绍这些。

但绝对来说，批量归一化在大约 2015 年推出时非常有影响力，因为那是你可以可靠地训练更深神经网络的第一次。根本原因是这个层在控制神经网络中的激活统计方面非常有效，所以到目前为止就是这个故事，这就是我想说的全部。

要覆盖的内容，以及未来的讲座，希望我们可以开始深入探讨递归神经网络，递归神经网络就像我们看到的，是非常非常深的网络，因为你解开了循环。当你真正优化这些神经网络时，这些激活统计和所有这些归一化层的分析就变得非常重要了。

性能，我们下次会看到这一点，拜拜，好吧，我说谎了，我想我们再做一个总结。

![](img/7e929a646a3b3c13dd787fc42b498be7_353.png)

这是作为一个额外的内容，我认为有一个额外的总结对我在这次讲座中展示的所有内容是有用的，但我还希望我们先将代码进行一些调整，使其更像在 PyTorch 中会遇到的样子。你会看到，我会将我们的代码结构分成这些模块，比如线性模块和批量归一化模块，我将代码放在这些模块内。

模块，以便我们可以构建神经网络，方式非常像我们在 PyTorch 中构建它们的方式。我会详细讲解这一点，因此我们将创建我们的神经网络，然后进行优化。

![](img/7e929a646a3b3c13dd787fc42b498be7_355.png)

![](img/7e929a646a3b3c13dd787fc42b498be7_356.png)

像之前那样进行循环，然后我在这里想做的另一件事是看一下。

![](img/7e929a646a3b3c13dd787fc42b498be7_358.png)

激活统计在前向传播和反向传播中都有，然后在这里我们有评估和采样，和之前一样，所以让我回到这里，再讲一点。

![](img/7e929a646a3b3c13dd787fc42b498be7_360.png)

慢一点，所以我在这里创建一个线性层，你会注意到`torch.nn`有很多不同类型的层，其中之一就是线性层。`torch.nn`接受一些输入特征、输出特征、是否需要偏置、我们希望将此层放置的设备以及数据类型，因此我将省略这两个参数，但其余的我们有完全相同的东西。

我们有输入的 fan in，输出的 fan out，以及我们是否想使用偏置。在这个层内部，有一个权重和一个偏置，如果你需要。通常使用从高斯分布中抽取的随机数来初始化权重，接下来就是我们在本讲中讨论的初始化，这是一种好的默认值。

我相信 PyTorch 使用的默认值，默认情况下偏置通常初始化为零。现在，当你调用这个模块时，它基本上会计算 w 乘以 x 加上 b，如果你有 nb。当你也调用这个模块上的参数时，它将返回该层的张量参数。接下来，我们有批量归一化层。

![](img/7e929a646a3b3c13dd787fc42b498be7_362.png)

我已经在这里写下来了，这与 PyTorch 的 dot batch norm 1d 层非常相似。

![](img/7e929a646a3b3c13dd787fc42b498be7_364.png)

在这里，我在获取这三个参数：维度、我们将在除法中使用的 epsilon，以及我们将在跟踪这些运行统计时使用的动量，运行均值和运行方差。实际上，PyTorch 会处理更多内容，但我假设一些他们的设置，因此我们认为会是正确的，这意味着我们将使用 gamma。

在归一化之后，跑步统计数据的 beta 将会是真实的，所以我们会进行跟踪。运行均值和运行方差在过去的房间中，我们的设备默认是 CPU，而数据类型默认是 float32，所以这些是默认值。否则，我们在这个批处理层中保持所有相同的参数，所以我现在只是在保存它们。

有一些新的东西，比如默认为 true 的 dot 训练，PyTorch 和模块也有这个属性，那就是训练。这是因为许多模块，包括批量归一化，具有不同的行为，具体取决于你是在训练还是在评估模式下运行，并计算评估损失，或者在一些测试示例上进行推断。

批量归一化就是这个例子的一个，因为当我们在训练时，我们将使用当前批次估计的均值和方差，但在推断时，我们使用运行均值和运行方差，因此，如果我们在训练，我们在更新均值和方差，但如果我们在测试，则这些不会被更新，而是保持不变，因此这个标志是必要的。

默认情况下为 true，就像在 PyTorch 中一样，Batch Norm 1D 的参数是 gamma 和 beta，然后运行均值和运行方差在 PyTorch 的命名法中称为缓冲区。

![](img/7e929a646a3b3c13dd787fc42b498be7_366.png)

这些缓冲区使用指数移动平均明确训练，它们不是反向传播的一部分，不是梯度下降的参数。因此，当我们有参数时，我们只返回 gamma 和 beta，而不返回均值和方差。这些在每次前向传递时都是内部训练的。

因此，这是初始化，现在在前向传递中，如果我们正在训练，那么我们使用由批次估计的均值和方差，或者查看论文，这里我们计算均值和方差。上面我估计了标准差并跟踪了运行标准差，而不是运行方差，但让我们。

完全按照论文的要求，这里计算方差，即标准差的平方。这是运行方差中跟踪的内容，而不是运行标准差。但我相信，如果我们不训练，这两者将非常相似。我们使用运行均值和方差进行归一化，然后在这里计算该层的输出。

将其分配给名为 dot out 的属性，现在 dot out 是我在模块中使用的。这不是您在 PyTorch 中会发现的，我们稍微偏离了它。我创建了一个 dot out，因为我希望非常方便地维护所有这些变量，以便我们可以创建。

![](img/7e929a646a3b3c13dd787fc42b498be7_368.png)

统计它们并绘制图形，但 PyTorch 和模块不会有 dot out 属性。

![](img/7e929a646a3b3c13dd787fc42b498be7_370.png)

最后，我们再次使用指数移动平均更新缓冲区。如我所提到的，提供了动量，并且重要的是，您会注意到我使用了 Torstop 无梯度上下文管理器。我这样做是因为如果不使用它，PyTorch 将开始根据这些张量构建整个计算图，因为它期望这样。

最终我们将调用反向传播，但我们永远不会在任何包含运行均值和运行方差的内容上调用它。因此，我们需要使用这个上下文管理器，以免使用额外的内存来维护它们，这样会更高效，并且它只是在告诉 PyTorch，在我们知道反向传播时，我们只有一堆张量。

我们想要更新它们，仅此而已，然后我们返回。现在向下滚动，我们有 10H 层。

![](img/7e929a646a3b3c13dd787fc42b498be7_372.png)

这与 Torstop 10H 非常相似，它的功能并不复杂，只是计算 10H，正如你所期望的那样，所以这就是 Torstop 10H，并且这一层没有参数，但由于这些是层，现在将它们堆叠起来变得很简单，基本上就是一个列表，我们可以进行所有我们习惯的初始化，所以我们有初步的嵌入。

![](img/7e929a646a3b3c13dd787fc42b498be7_374.png)

矩阵，我们有我们的层，我们可以顺序调用它们，然后再次使用 Torstop 无梯度。

![](img/7e929a646a3b3c13dd787fc42b498be7_376.png)

这里有一些初始化，所以我们希望让输出 softmax 稍微不那么自信。正如我们所看到的，此外，因为我们使用的是六层的多层感知器，所以你会看到我如何堆叠线性 10H、线性 10H 等，我将在这里使用游戏，并且我将在一秒钟内进行调整，所以你会看到当我们改变这个时，它会发生什么。

![](img/7e929a646a3b3c13dd787fc42b498be7_378.png)

统计信息，最后，基元基本上是嵌入矩阵，以及所有层中的所有基元，请注意，我在使用双重列表推导，如果你愿意这样称呼的话，但对于每一层中的每一个参数，我们只是在堆叠所有这些片段，所有这些参数，现在总共有 46000 个参数，我在告诉。

所有这些模式都需要梯度，这里我们有一切，实际上我们大多数是习惯于抽样批次，我们在进行前向传播，前向传播现在只是所有层的线性应用，随后是交叉熵，然后在反向传播中，你会注意到对于每一层，我现在迭代所有输出，并告诉模式。

保留它们的梯度，这里我们已经习惯了所有梯度，发送为无，进行反向传播以填充梯度，使用随机梯度进行更新。

![](img/7e929a646a3b3c13dd787fc42b498be7_380.png)

然后跟踪一些统计信息，之后我将在单次迭代后中断，现在在这里。

![](img/7e929a646a3b3c13dd787fc42b498be7_382.png)

在这个图表中的单元格中，我正在可视化前向传播激活的直方图，我特别在 10h 层中进行，所以迭代所有层，除了最后一层，基本上就是 softmax 层，如果这是一个 10h 层，我使用 10h 层只是因为它们有有限的输出，从负一到一，所以在这里可视化非常简单。

所以你看到从负一到一，这是一个有限范围，且可以进行操作。我将从那个层输出的张量取出，赋值给 t，然后我计算 t 的均值、标准差和百分比饱和度。我定义百分比饱和度的方式是 t 的绝对值大于 0.97。所以这意味着我们在 10H 的尾部，记住当我们在尾部时。

10H 实际上会停止梯度，因此我们不希望这个值过高。现在在这里我调用 torch.dot.histogram，然后我绘制这个直方图，因此基本上这正在做的事情是。

![](img/7e929a646a3b3c13dd787fc42b498be7_384.png)

每种不同类型的层都有不同的颜色，我们正在查看这些测试者中有多少值取这个轴上任何值，因此第一层在 20%处相当饱和，所以你可以看到它有尾部，但之后一切都会稳定，如果我们这里有更多层，它实际上会在标准附近稳定。

偏差约为 0.65，饱和度大约为 5%，而这能稳定并给我们带来良好分布的原因是增益设置为 5/3，现在在这里你看到这个增益。

![](img/7e929a646a3b3c13dd787fc42b498be7_386.png)

默认情况下，我们用 1 除以 fan in 的平方根进行初始化，但在这里初始化时，我会遍历所有层，如果是线性层，我就会通过增益进行提升。现在我们看到 1，所以基本上如果我们不使用增益，那么重新绘制这个会发生什么。

![](img/7e929a646a3b3c13dd787fc42b498be7_388.png)

你会看到标准差在缩小，饱和度接近于零。基本上，第一层还算不错，但进一步的层就像是缩小到零，并且这一过程缓慢发生，但它正在缩小到零。原因是当你仅仅有一层线性层的“夹心”时，那么就会。

![](img/7e929a646a3b3c13dd787fc42b498be7_390.png)

以这种方式初始化我们的权重，之前我们看到过，这会保持标准差为 1，但因为我们有这个交错的 10H 层，这些 10 个线性层是压缩函数，因此它们会稍微压缩你的分布，所以需要一些增益来持续扩展，以对抗压缩，因此结果就是 5/3。

这是一个好的值，所以如果我们有一个太小的值，比如 1，我们看到事物会向零靠近。但如果是一个太高的值，比如 2，那么我们看到。

![](img/7e929a646a3b3c13dd787fc42b498be7_392.png)

好吧，让我做一些更极端的事情，因为这样会更明显，让我们试试三。

![](img/7e929a646a3b3c13dd787fc42b498be7_394.png)

![](img/7e929a646a3b3c13dd787fc42b498be7_395.png)

好的，所以我们看到这里的饱和度会过大，好的，所以三会产生。

![](img/7e929a646a3b3c13dd787fc42b498be7_397.png)

过于饱和的激活，所以 5/3 是线性层组合的良好设置。

![](img/7e929a646a3b3c13dd787fc42b498be7_399.png)

![](img/7e929a646a3b3c13dd787fc42b498be7_400.png)

使用 10H 激活，标准差在合理范围内大致稳定。

![](img/7e929a646a3b3c13dd787fc42b498be7_402.png)

说实话，我不知道在 PyTorch 中 5/3 是从哪里来的，当时我们在查看这些内容。

![](img/7e929a646a3b3c13dd787fc42b498be7_404.png)

从初始化来看，我经验上发现它稳定了这个线性和 10H 的组合，并且饱和度处于良好范围内，但我实际上并不知道这是否来自某个数学公式。我曾尝试简单搜索一下这个来源，但没有找到任何东西，但可以肯定的是我们。

![](img/7e929a646a3b3c13dd787fc42b498be7_406.png)

实证数据显示这些都是非常好的范围，我们的饱和度大约是 5%，这是个不错的数字，在这种情况下这是增益的良好设置。同样，我们可以用梯度做完全相同的事情，这里是一个非常相同的循环，如果是 10H，但我不再取出层，而是取出梯度，然后展示均值和标准偏差，我。

![](img/7e929a646a3b3c13dd787fc42b498be7_408.png)

绘制这些值的直方图，你会看到梯度分布相当合理，特别是我们希望的是，这个组合中的不同层的梯度大致相同，既没有缩小也没有爆炸，因此我们可以。

![](img/7e929a646a3b3c13dd787fc42b498be7_410.png)

来这里，我们可以看看如果这个增益过小会发生什么，比如设置为 0.5。这时你会看到所有激活都在缩小到零，而梯度也出现了奇怪的现象，梯度从这里开始，而现在它们像是扩展出去。同样，如果我们有一个过高的增益，比如三，那么我们也会看到。

![](img/7e929a646a3b3c13dd787fc42b498be7_412.png)

梯度存在一些不对称现象，随着层数加深而加剧。

![](img/7e929a646a3b3c13dd787fc42b498be7_414.png)

激活也在变化，所以这不是我们想要的。在这种情况下，我们看到，如果不使用批量归一化，如今我们必须非常小心地设置。

![](img/7e929a646a3b3c13dd787fc42b498be7_416.png)

这些增益在前向传递和反向传递中都能产生良好的激活。

![](img/7e929a646a3b3c13dd787fc42b498be7_418.png)

在我们继续进行归一化之前，我还想看看会发生什么。

![](img/7e929a646a3b3c13dd787fc42b498be7_420.png)

当我们这里没有 10H 单元时，去掉所有 10H 的非线性，但保持增益为 5/3。我们现在只剩下一个巨大的线性夹心层，所以让我们看看激活发生了什么。正如我们之前看到的，这里的正确增益是保持标准差的增益，因此 1.667 太高了，因此接下来将发生以下情况。

哦，我得把这个改成线性，因为没有更多的 10H 单元了。让我也把这个改成线性，所以我们看到的激活一开始是蓝色的。到第四层时变得非常分散，所以激活发生了什么呢？

![](img/7e929a646a3b3c13dd787fc42b498be7_422.png)

在最上层的梯度激活中，梯度统计是紫色的。然后随着层数的加深，它们逐渐减小，因此基本上在神经网络中存在不对称性。你可以想象，如果你有非常深的神经网络，比如 50 层，或者类似的层数，这显然不是一个好的状态，所以在这之前。

![](img/7e929a646a3b3c13dd787fc42b498be7_424.png)

批量归一化之前，这种设置非常棘手，尤其是当这个值过大时。

![](img/7e929a646a3b3c13dd787fc42b498be7_426.png)

增益再次发生，如果增益过小，那么也会发生相反的情况。

![](img/7e929a646a3b3c13dd787fc42b498be7_428.png)

基本上在这里发生了，我们有一个缩小和扩散，具体取决于我们观察的方向。

![](img/7e929a646a3b3c13dd787fc42b498be7_430.png)

![](img/7e929a646a3b3c13dd787fc42b498be7_431.png)

从中可以看出，这显然不是你想要的，在这种情况下，正确的设置是。

![](img/7e929a646a3b3c13dd787fc42b498be7_433.png)

增益正好是 1，就像我们在初始化时做的那样，然后我们看到统计数据。

![](img/7e929a646a3b3c13dd787fc42b498be7_435.png)

正向和反向传播表现良好，所以我想向你展示这个原因是。

![](img/7e929a646a3b3c13dd787fc42b498be7_437.png)

但基本上，就像让神经网络在这些归一化层之前进行训练一样。

![](img/7e929a646a3b3c13dd787fc42b498be7_439.png)

使用像 Adam 这样的高级优化器，我们仍需涵盖的残差连接等。训练神经网络基本上看起来就是这样，这就像一场完全的平衡行动，你必须确保一切都精确协调，你必须关注激活和其统计数据，然后也许你能训练出一些东西，但基本上是这样的。

训练非常深的网络几乎是不可能的，这基本上就是原因所在。你必须非常非常小心地进行初始化，嗯，另一点是。你可能会问，顺便说一下，我不确定我是否讲过，为什么我们需要这些 10H 层呢？我们为什么要包括它们，然后还要担心增益？原因就在于此。

![](img/7e929a646a3b3c13dd787fc42b498be7_441.png)

当然，如果你只是有一堆线性层，那么我们肯定会非常容易。

![](img/7e929a646a3b3c13dd787fc42b498be7_443.png)

好的激活等等，但这只是一个庞大的线性三明治，事实证明。

![](img/7e929a646a3b3c13dd787fc42b498be7_445.png)

在表示能力方面，模型最终简化为一个单一的线性层，因此如果你绘制输入的函数输出，无论你堆叠多少线性层，得到的结果都是一个线性函数。所有的 wx 加上 b 最终都合并为一个大的 wx 加上 b，只是 w 和 b 可能有所不同，但有趣的是。

尽管前向传播简化为仅一个线性层，但由于反向传播和反向传播的动态，优化过程实际上并不完全相同。你实际上会在反向传播中遇到各种有趣的动态，这是因为链式法则的计算方式。因此，优化单个线性层和优化十个线性层的“三明治”是不同的。

在这两种情况下，前向传播都是线性变换，但训练动态是不同的，实际上有整篇论文分析像无限层线性层等内容，这里有很多可以尝试的事情，但基本上这十个线性层允许我们将这个三明治从仅仅是线性变换。

函数可以嵌入到神经网络中，原则上可以近似任何任意函数。好的，现在我将代码重置为使用大约 10 个线性层的“三明治”结构，并重置了所有内容。

![](img/7e929a646a3b3c13dd787fc42b498be7_447.png)

![](img/7e929a646a3b3c13dd787fc42b498be7_448.png)

所以增益为五除以三，我们可以运行一次优化步骤，并观察。

![](img/7e929a646a3b3c13dd787fc42b498be7_450.png)

前向传播和反向传播的激活统计，但我在这里增加了一个图。

![](img/7e929a646a3b3c13dd787fc42b498be7_452.png)

我认为这是在训练神经网络时需要关注的重要因素，最终我们所做的是更新神经网络的参数，因此我们关心的是参数及其值和梯度。因此，我实际上是在遍历所有可用的参数，然后我只将其限制在二维。

这些参数基本上是这些线性层的权重，我跳过了偏置项，也跳过了简单起见的伽玛和贝塔，但你也可以查看它们。权重的变化本身就很有启发性。所以这里我们有所有不同权重的形状，这里是嵌入层的第一个。

线性层一直到最后一层，然后我们有所有这些参数的均值和标准差的直方图，你可以看到这实际上看起来并不那么出色。因此，即使这些梯度看起来还可以，依然有些奇怪。

![](img/7e929a646a3b3c13dd787fc42b498be7_454.png)

这里发生了什么，我稍后会解释，最后一点是数据与梯度的比率。因此，有时我也喜欢将其可视化，因为这可以让你感知梯度的规模与实际值的规模相比如何，这很重要，因为我们将进行一次步长更新，即学习率乘以梯度。

数据，因此梯度的大小过大，如果里面的数字与数据相比太大，那么你会陷入困境，但在这种情况下，数据的梯度是较低的数字，所以在这些权重中，grad 的值比 data 中的值小一千倍，尤其是在最后一层。

最后一层实际上是有点麻烦，因为目前的安排使得这一层（粉色）取值比神经元内部的一些值大得多。所以标准差大致在负三附近，除了最后一层。

梯度的标准差大约在负二附近，所以最后一层的梯度目前大约是其他神经元权重的 100 倍，抱歉，是 10 倍，这很成问题，因为在最简单的随机梯度下降设置中，你将训练最后一层的速度比训练其他层快约 10 倍。

其他层在初始化时，现在实际上如果你进行训练，这会有点自我修复。

![](img/7e929a646a3b3c13dd787fc42b498be7_456.png)

稍微长一点，例如，如果我大于 1000，那么就中断，让我。

![](img/7e929a646a3b3c13dd787fc42b498be7_458.png)

重新初始化，然后让我进行 1000 步，经过 1000 步后我们可以查看。

![](img/7e929a646a3b3c13dd787fc42b498be7_460.png)

前向传播还可以，所以你可以看到神经元有一点饱和，我们也可以观察一下。

![](img/7e929a646a3b3c13dd787fc42b498be7_462.png)

在反向传播时，除此之外它们看起来良好，基本相等，没有收缩。

![](img/7e929a646a3b3c13dd787fc42b498be7_464.png)

除零或爆炸到无穷大，你可以在权重中看到这里的东西也在稳定。

![](img/7e929a646a3b3c13dd787fc42b498be7_466.png)

有一点，最后一层的尾部实际上是在优化期间进入的。但这确实有点令人困扰，特别是如果你使用的是非常简单的更新。

![](img/7e929a646a3b3c13dd787fc42b498be7_468.png)

像随机梯度下降这样的规则，而不是现代优化器像 Adam。现在我想展示。

![](img/7e929a646a3b3c13dd787fc42b498be7_470.png)

这是我在训练神经网络时通常会查看的另一个图，基本上，梯度与数据的比率实际上并没有那么有信息量，因为最终重要的不是梯度与数据的比率，而是更新与数据的比率，因为这才是我们实际会用到的量。

![](img/7e929a646a3b3c13dd787fc42b498be7_472.png)

在这些张量中更改数据，所以我想做的是引入一个新的更新与数据的比率，它会比我们每次迭代要构建的量要少，并且我希望跟踪基本上每次迭代的比率。因此，在没有任何成分的情况下，我比较的是更新，即学习率乘以。

乘以梯度，这是我们将应用于每个参数的更新。社会物质世界的参数，然后我基本上取更新的标准差，并除以该参数的实际内容及其。标准差，因此这基本上是更新与这些值的比率。

然后我们将取对数，实际上我想取以 10 为底的对数，以便它是一个。漂亮的服务化实现，所以我们基本上将查看这个除法的指数，然后将这个项弹出浮点数，我们将为所有参数跟踪。并将其添加到这个 UD 张量中，所以现在让我重新强调一下。

![](img/7e929a646a3b3c13dd787fc42b498be7_474.png)

一千次迭代后，我们可以查看激活值、梯度和参数梯度，因为我们。

![](img/7e929a646a3b3c13dd787fc42b498be7_476.png)

之前我做过，但是现在我这里有一个新的图要介绍，发生在这里的情况是每次。

![](img/7e929a646a3b3c13dd787fc42b498be7_478.png)

间隔将是参数，我再次像我在这里做的那样，将其限制为仅两个权重。

![](img/7e929a646a3b3c13dd787fc42b498be7_480.png)

这些传感器的维度是两个，然后我基本上在绘制所有这些。更新比率随时间变化，所以当我绘制这个时，我绘制这些比率，你可以看到它们。

![](img/7e929a646a3b3c13dd787fc42b498be7_482.png)

![](img/7e929a646a3b3c13dd787fc42b498be7_483.png)

在初始化期间随时间演变，它们会计算某些值，然后这些更新。通常在训练期间开始稳定，另外我在这里绘制的是一个近似值，作为粗略的指导，它应该大约是负三左右，这意味着基本上有一些值。

在这个张量中，它们具有某些值，并且在每一次迭代中的更新不超过那些张量实际大小的千分之一。如果这个值大得多，比如说，如果这个对数是负一，那么它实际上在更新这些值相当多，它们经历了很大的变化，但。

最后一层是异常值的原因是因为这一层被人为缩小，以保持 softmax 的输出不自信，所以这里你会看到我们将权重乘以点。

![](img/7e929a646a3b3c13dd787fc42b498be7_485.png)

在初始化时将最后一层的预测变得不那么自信，这是人为造成的。

![](img/7e929a646a3b3c13dd787fc42b498be7_487.png)

![](img/7e929a646a3b3c13dd787fc42b498be7_488.png)

使得那个张量内部的值过低，这就是为什么我们暂时得到一个非常高的比率，但你会看到，随着时间的推移它会稳定下来，一旦权重开始学习。基本上，我喜欢观察这个更新比率的演变，通常针对我的所有参数。我喜欢确保它不超过负三。

在这个对数图上，如果低于负三，通常意味着参数没有。

![](img/7e929a646a3b3c13dd787fc42b498be7_490.png)

训练速度太快，如果我们的学习率非常低，那就进行这个实验，初始化一下。然后我们实际设置一个学习率，比如这里的负三，即 0.001。如果你的学习太低，这个图通常会显示出来，你会看到所有的更新都太小了，所以更新的大小基本上是 10,000 倍于。

在那个张量中的数字首先是一个训练速度过慢的症状。所以这也是有时设置学习率的一种方式，以了解学习率应该是什么，最终这是你需要跟踪的内容。

![](img/7e929a646a3b3c13dd787fc42b498be7_492.png)

如果说这里的学习率有点高，那是因为你可以看到我们在负三的黑线之上，差不多在负 2.5 左右，感觉还可以。

![](img/7e929a646a3b3c13dd787fc42b498be7_494.png)

一切看起来都有点稳定，这看起来是一个相当不错的学习率设置，但这是需要关注的，当事情误校准时，你会很快看到，比如说，一切看起来都相当正常，对吧。

![](img/7e929a646a3b3c13dd787fc42b498be7_496.png)

但作为比较，当事情没有正确校准时，情况是什么样的呢？让我上来，比如说我们做了什么，假设我们忘记应用这个风扇归一化，所以线性层内部的权重只是从高斯分布中采样。在所有这些阶段，我们如何注意到有什么问题呢？激活。

图表会告诉你，哇，你的神经元过于饱和，梯度会完全混乱，这些权重的直方图也会搞砸，存在很多不对称。如果我们在这里看，我怀疑这也会相当混乱，所以你会看到，这些层学习的速度有很大的差异，有些正在学习。

太快了，所以负 1 和负 1.5，这些在这个比例上是非常大的数字。你应该在负 3 左右，而不是更多，这就是神经网络误校准的表现，这些图表是一个很好的指示。

![](img/7e929a646a3b3c13dd787fc42b498be7_498.png)

一种引起你注意并解决那些误校准的方法。

![](img/7e929a646a3b3c13dd787fc42b498be7_500.png)

![](img/7e929a646a3b3c13dd787fc42b498be7_501.png)

到目前为止，我们已经看到，当我们有这个线性 10-H 夹心时，我们实际上可以精确校准增益，使得激活、梯度、参数和更新看起来相当不错，但确实感觉有点像在你的手指上平衡铅笔，因为这个增益必须非常精确地校准，所以现在让我们引入。

将单身女郎的元素加入其中，让我们看看如何修复这个问题。所以在这里，我将把单身女郎 1D 类放进去。正如我之前提到的，典型的放置位置是在线性层之间，所以在其之后但在非线性之前，但人们确实尝试过其他方式。

实际上，即使你在非线性之后放置它，你也可以得到非常相似的结果。我想提到的另一件事是，将其放在最后一个线性层之后和损失函数之前也是完全可以的，所以在这种情况下，这样的输出将是词汇大小。

![](img/7e929a646a3b3c13dd787fc42b498be7_503.png)

由于最后一层是一个最佳层，我们不会通过改变权重来降低 softmax 的置信度，而是会改变 gamma，因为在最佳层中，gamma 是与归一化输出以乘法方式交互的变量。所以我们可以初始化这个“夹心层”，然后进行训练，观察激活值。

![](img/7e929a646a3b3c13dd787fc42b498be7_505.png)

当然会看起来非常好，因为现在在每个 10H 层之前都有一个批归一化，所以毫无疑问，所有的效果看起来都不错，标准差大约为 0.652%，并且在整个层中大致相等。因此，所有东西看起来非常均匀。

梯度看起来不错，权重也很好，它们的分布以及更新的结果。

![](img/7e929a646a3b3c13dd787fc42b498be7_507.png)

同样也看起来相当合理，尽管我们稍微超过了负三，但没有太多。因此，所有参数的训练速率大致相同。

![](img/7e929a646a3b3c13dd787fc42b498be7_509.png)

但现在我们获得的好处是，对于这些增益，我们将会稍微不那么脆弱。例如，我可以将增益设为 0.2。

![](img/7e929a646a3b3c13dd787fc42b498be7_511.png)

![](img/7e929a646a3b3c13dd787fc42b498be7_512.png)

这比使用 10H 时要慢得多，但正如我们将看到的，激活值将会...

![](img/7e929a646a3b3c13dd787fc42b498be7_514.png)

实际上将会完全不受影响，这又是由于这种显式的归一化。梯度看起来不错，权重梯度也看起来不错。

![](img/7e929a646a3b3c13dd787fc42b498be7_516.png)

但实际上更新会发生变化，因此尽管前向和反向传播看起来相当不错。

![](img/7e929a646a3b3c13dd787fc42b498be7_518.png)

在很大程度上，反向传播的批归一化和传入激活值的规模如何相互作用，使得反向传播实际上在改变这些参数的更新规模，因此这些权重的梯度受到了影响。

![](img/7e929a646a3b3c13dd787fc42b498be7_520.png)

所以我们仍然不能完全自由地传入任意权重，但其他一切都...

![](img/7e929a646a3b3c13dd787fc42b498be7_522.png)

在前向和反向传播以及权重梯度方面，模型的稳健性显著增强。只不过，如果你有足够的变化，可能需要重新调整学习率。这里的激活值规模进入批归一化的过程，例如，我们将这些线性层的增益调整得更大，因此我们看到更新效果。

由于这个原因，结果是更新会降低。如果我们使用批归一化，我们不会...

![](img/7e929a646a3b3c13dd787fc42b498be7_524.png)

事实上，我们不需要一定将其重置为 1，因此没有增益，我们甚至不一定需要。 有时不进行归一化，所以如果我去掉输入，那么这些现在只是。

![](img/7e929a646a3b3c13dd787fc42b498be7_526.png)

一个随机的输入将显示出由于批归一化，这实际上会相对表现良好。因此，前向传播的表现不错，梯度看起来也很好。反向传播的权重更新也不错，但某些层中有一些极端值。

![](img/7e929a646a3b3c13dd787fc42b498be7_528.png)

![](img/7e929a646a3b3c13dd787fc42b498be7_529.png)

这看起来也还可以，但正如你所见，我们明显低于负三。因此，我们需要提高这个批归一化的学习率，以便更好地训练。特别是看来我们需要将学习率提高到约 10 倍。

![](img/7e929a646a3b3c13dd787fc42b498be7_531.png)

在负三，所以我们来这里，我们将其更改为更新 1.0。如果。

![](img/7e929a646a3b3c13dd787fc42b498be7_533.png)

你和我重新初始化，那么我们会看到一切仍然看起来不错，现在。

![](img/7e929a646a3b3c13dd787fc42b498be7_535.png)

我们大致在这里，我们期待这将是一次不错的训练运行。长话短说，我们是。

![](img/7e929a646a3b3c13dd787fc42b498be7_537.png)

对这些线性层的增益显著更具鲁棒性，无论我们是否需要应用。

![](img/7e929a646a3b3c13dd787fc42b498be7_539.png)

输入和增益可以更改，但我们确实需要稍微担心一下。

![](img/7e929a646a3b3c13dd787fc42b498be7_541.png)

更新缩放，并确保学习率在这里得到适当校准，但前向和反向传播的激活以及更新都表现得明显更好。

![](img/7e929a646a3b3c13dd787fc42b498be7_543.png)

除了可能在这里被调整的全局规模，好吧，现在让我总结一下。我希望通过这一部分实现三件事，第一，我想介绍。你了解学士归一化，这是我们正在研究的首批现代创新之一。

![](img/7e929a646a3b3c13dd787fc42b498be7_545.png)

这有助于稳定非常深的神经网络及其训练，我希望你能理解。学士化是如何工作的，以及它如何在你的网络中使用。我原本希望能。

![](img/7e929a646a3b3c13dd787fc42b498be7_547.png)

将我们的部分代码转换为 PyTorch，并将其包装到这些模块中，比如线性层、学士、星期一、10h 等。

![](img/7e929a646a3b3c13dd787fc42b498be7_549.png)

这些层或模块可以像乐高积木一样堆叠成神经网络。

![](img/7e929a646a3b3c13dd787fc42b498be7_551.png)

这些层实际上存在于 PyTorch 中，如果你导入 torch，那么你可以实际地。

![](img/7e929a646a3b3c13dd787fc42b498be7_553.png)

我构建的方式是，你只需在所有不同的层前添加一个 endot，就可以简单地使用 pytorch。实际上一切都会正常工作，因为我在这里开发的 API 与 pytorch 使用的 API 是完全相同的，而实现也基本上是我所了解的。

![](img/7e929a646a3b3c13dd787fc42b498be7_555.png)

![](img/7e929a646a3b3c13dd787fc42b498be7_556.png)

与 pytorch 中的那个相同，第三点我试图向你介绍一些诊断工具。

![](img/7e929a646a3b3c13dd787fc42b498be7_558.png)

你可以用来了解你的神经网络是否在动态上处于良好状态，所以我们在。

![](img/7e929a646a3b3c13dd787fc42b498be7_560.png)

查看统计数据和直方图，以及正向传播的激活。

![](img/7e929a646a3b3c13dd787fc42b498be7_562.png)

反向传播的梯度，然后我们也在关注将要更新的权重。作为随机梯度上升的一部分，我们查看它们的均值和标准差。

![](img/7e929a646a3b3c13dd787fc42b498be7_564.png)

还有梯度与数据的比率，或者更好的是更新与数据的比率，我们看到通常我们。

![](img/7e929a646a3b3c13dd787fc42b498be7_566.png)

实际上不要把它视为某个特定迭代下冻结的单一快照。通常人们把它看作是随着时间的推移，就像我在这里做的一样，他们查看这些数据更新比率，并确保一切看起来正常，特别是我提到的，-3 或者基本上说是在对数尺度上的 -3 是一个不错的粗略值。

这是你想要这个比率的启发式，如果它过高，可能学习率或更新过大；如果它过低，学习率可能过小。所以这只是你在尝试时可能想要调整的一些事项。

![](img/7e929a646a3b3c13dd787fc42b498be7_568.png)

尝试让你的神经网络运作得非常好，现在有很多事情我没有尝试。

![](img/7e929a646a3b3c13dd787fc42b498be7_570.png)

我没有尝试通过引入浴室层来打破我们之前的表现作为一个例子，实际上我确实尝试了，我发现我使用了之前描述的学习率查找机制。我试着训练一个浴室层的神经网络，结果。

![](img/7e929a646a3b3c13dd787fc42b498be7_572.png)

最终的结果与我们之前获得的非常非常相似，这就是因为。

![](img/7e929a646a3b3c13dd787fc42b498be7_574.png)

我们的性能现在不再受到优化的瓶颈，这是 bash norm 帮助的。此阶段的性能瓶颈可能是我们上下文的上下文长度。因此目前我们正在用三个字符预测第四个字符，我认为我们需要超越这一点，并需要查看更强大的架构，例如循环神经网络。

转换器进行全面解释，以进一步推动我们在这个数据集上所取得的能力。

![](img/7e929a646a3b3c13dd787fc42b498be7_576.png)

而且我也没有尝试对所有这些激活、梯度和。

![](img/7e929a646a3b3c13dd787fc42b498be7_578.png)

反向传播和所有这些梯度的统计，因此你可能发现了一些。

![](img/7e929a646a3b3c13dd787fc42b498be7_580.png)

这里的某些部分不太直观，可能你会稍微困惑，好吧，如果我在这里改变增益。

![](img/7e929a646a3b3c13dd787fc42b498be7_582.png)

为什么我们需要不同的学习率，我没有详细讨论，因为你可能。

![](img/7e929a646a3b3c13dd787fc42b498be7_584.png)

我们必须实际查看所有这些不同层的反向传播，并直观理解这一切是如何运作的，而我在这次讲座中并没有深入探讨这一点，目的真的。

![](img/7e929a646a3b3c13dd787fc42b498be7_586.png)

只是为了向你介绍诊断工具及其外观，但在直观层面上，理解初始化、反向传播及其相互作用仍有很多工作要做，但你不应该感到太糟糕，因为老实说，我们正处于前沿领域。

![](img/7e929a646a3b3c13dd787fc42b498be7_588.png)

在这个领域，我们可以肯定地说，初始化问题尚未解决，反向传播也没有解决，这些仍然是非常活跃的研究领域，人们仍在努力弄清楚初始化这些网络的最佳方法，使用什么是最佳更新规则等等，所以这些问题并没有真正解决，我们并没有对所有。

![](img/7e929a646a3b3c13dd787fc42b498be7_590.png)

情况有确切的答案，但至少你知道我们在取得进展，至少我们有一些。

![](img/7e929a646a3b3c13dd787fc42b498be7_592.png)

工具可以告诉我们目前事情是否在正确的轨道上，所以我认为我们取得了一些进展。

![](img/7e929a646a3b3c13dd787fc42b498be7_594.png)

在这次讲座中取得了积极的进展，我希望你喜欢这次讲座，下次再见。

![](img/7e929a646a3b3c13dd787fc42b498be7_596.png)
