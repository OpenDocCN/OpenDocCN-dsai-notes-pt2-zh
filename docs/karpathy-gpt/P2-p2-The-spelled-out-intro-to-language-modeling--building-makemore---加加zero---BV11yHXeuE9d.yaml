- en: P2：p2 The spelled-out intro to language modeling： building makemore - 加加zero
    - BV11yHXeuE9d
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P2：p2 语言模型的详细介绍：构建 makemore - 加加zero - BV11yHXeuE9d
- en: Hi everyone， hope you're well。 And next up what I'd like to do is I'd like to
    build out MakeMore。 Like Micrograd before it， MakeMore is a repository that I
    have on my GitHub webpage。 You can look at it。 But just like with Micrograd， I'm
    going to build it out step by step。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 大家好，希望你们都很好。接下来我想做的是构建 MakeMore。像之前的 Micrograd 一样，MakeMore 是我在 GitHub 网页上的一个仓库。你可以查看它。但就像
    Micrograd 一样，我将一步一步地构建它。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_1.png)'
- en: and I'm going to spell everything out。 So we're going to build it out slowly
    and together。 Now。 what is MakeMore？ MakeMore， as the name suggests， makes more
    of things that you give it。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我将把所有内容详细说明。所以我们将慢慢一起构建它。现在，什么是 MakeMore？MakeMore，如其名所示，可以生成你给它的事物。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_3.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_3.png)'
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_4.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_4.png)'
- en: So here's an example。 Names。txt is an example data set to MakeMore。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个示例。names.txt 是一个给 MakeMore 的示例数据集。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_6.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_6.png)'
- en: And when you look at names。txt， you'll find that it's a very large data set
    of names。 So here's lots of different types of names。 In fact， I believe there
    are 32，000 names。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当你查看 names.txt 时，你会发现这是一个非常大的名字数据集。所以这里有很多不同类型的名字。事实上，我相信有 32,000 个名字。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_8.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_8.png)'
- en: that I've sort of found randomly on the government website。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我在政府网站上随机找到的一些东西。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_10.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_10.png)'
- en: And if you train MakeMore on this data set， it will learn to make more of things
    like this。 And in particular， in this case， that will mean more things that sound
    name-like。 but are actually unique names。 And maybe if you have a baby and you're
    trying to assign a name。 maybe you're looking for a cool new sounding unique name，
    MakeMore might help you。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在这个数据集上训练 MakeMore，它将学习生成更多这样的事物。特别是在这种情况下，这将意味着生成更多听起来像名字的东西，但实际上是独特的名字。如果你有个宝宝并试图给他起个名字，也许你在寻找一个很酷的新颖独特的名字，MakeMore
    可能会帮助你。
- en: So here are some example generations from the neural network once we train it
    on our data set。 So here's some example unique names that it will generate。 Don't
    tell。 I rot。 Zen-di。 And so on。 And so all these sort of sound name-like， but
    they're not， of course， names。 So under the hood。 MakeMore is a character-level
    language model。 So what that means is that it is treating every single line here
    as an example。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这里是一些从神经网络生成的示例结果，一旦我们在我们的数据集上训练它。这里是一些它将生成的独特名字示例。不要告诉。I rot。Zen-di。等等。这些名字听起来像名字，但当然不是名字。所以在底层，MakeMore
    是一个字符级语言模型。这意味着它将这里的每一行视为一个示例。
- en: And within each example， it's treating them all as sequences of individual characters。
    So R-E-E-S-E is this example。 And that's the sequence of characters。 And that's
    the level in which we are building out MakeMore。 And what it means to be a character-level
    language model then is that it's just sort of modeling those sequences of characters。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个示例中，它将所有内容视为单个字符的序列。所以 R-E-E-S-E 是这个示例。这是字符的序列。这是我们构建 MakeMore 的层次。而作为字符级语言模型的意义在于，它只是对这些字符序列进行建模。
- en: and it knows how to predict the next character in the sequence。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 它知道如何预测序列中的下一个字符。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_12.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_12.png)'
- en: Now， we're actually going to implement a large number of character-level language
    models。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们实际上要实现大量的字符级语言模型。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_14.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_14.png)'
- en: in terms of the neural networks that are involved in predicting the next character
    in a sequence。 So very simple， by-gram and bag-of-word models， multi-level perceptrons，
    recurring neural networks。 all the way to modern transformers。 In fact， a transformer
    that we will build will be basically the equivalent transformer to GPT-2。 if you
    have heard of GPT。 So that's kind of a big deal。 It's a modern network。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 关于参与预测序列中下一个字符的神经网络。所以非常简单的 bi-gram 和 bag-of-word 模型，多层感知器，递归神经网络，一直到现代的 transformers。实际上，我们将构建的
    transformer 基本上是 GPT-2 的等效 transformer。如果你听说过 GPT。这个确实是个大事情。这是一个现代网络。
- en: And by the end of the series， you will actually understand how that works on
    the level of characters。 Now， to give you a sense of the extensions here， after
    characters。 we will probably spend some time on the next level。 We will probably
    spend some time on the word level so that we can generate documents of words。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 到了系列的最后，你将真正理解这一切在字符层面的运作。现在，为了给你一个关于这里扩展的感觉，在字符之后。我们可能会花一些时间在下一个层面。我们可能会花一些时间在单词层面，以便能够生成单词的文档。
- en: not just little segments of characters， but we can generate entire large， much
    larger documents。 And then we're probably going to go into images and image text
    networks， such as Dali。 Stable Diffusion and so on。 But for now， we have to start
    here， character-level language modeling。 Let's go。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅仅是字符的小片段，我们可以生成整个更大、规模更大的文档。然后，我们可能会进入图像和图像文本网络，比如Dali、Stable Diffusion等等。但现在，我们必须从这里开始，字符级语言建模。开始吧。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_16.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_16.png)'
- en: So like before， we are starting with a completely blank， duplicate page。 The
    first thing is I would like to basically load up the dataset， named。txt。 So we're
    going to open up names。txt for reading。 And we're going to read in everything
    into a massive string。 And then because it's a massive string， we'd only like
    the individual words and put them in the list。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前一样，我们从一张完全空白的重复页面开始。首先，我想基本上加载名为.txt的数据集。所以我们将打开names.txt进行读取。然后我们会把所有内容读入一个大型字符串中。由于这是一个大型字符串，我们只想要单独的单词，并将它们放入列表中。
- en: So let's call split lines on that string to get all of our words as a Python
    list of strings。 So basically， we can look at， for example， the first 10 words。
    And we have that it's a list of Emma。 Olivia， Eva， and so on。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们在那个字符串上调用split lines，以获得我们所有的单词作为Python字符串列表。因此，我们可以查看，例如，前10个单词。我们发现这是一个包括Emma、Olivia、Eva等的列表。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_18.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_18.png)'
- en: And if we look at the top of the page here， that is indeed what we see。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们看看页面的顶部，确实就是我们所看到的。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_20.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_20.png)'
- en: So that's good。 This list actually makes me feel that this is probably sorted
    by frequency。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这很好。这个列表实际上让我觉得它可能是按频率排序的。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_22.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_22.png)'
- en: But， okay， so these are the words。 Now， we'd like to actually like learn a little
    bit more about this dataset。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，好吧，这些就是单词。现在，我们实际上想多了解一些这个数据集。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_24.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_24.png)'
- en: Let's look at the total number of words。 We expect this to be roughly 32，000。
    And then what is the。 for example， shortest word？ So min of， line of each word
    for w in words。 So the shortest word will be length two。 And max of one w for
    w in words。 So the longest word will be 15 characters。 So let's now think through
    our very first language model。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下总词数。我们预计大约为32,000。那么，最短的单词是什么呢？例如，计算每个单词的最小长度。最短的单词将是两个字母。最长的单词将是15个字符。现在让我们思考一下我们的第一个语言模型。
- en: As I mentioned， a character level language model is predicting the next character
    in a sequence。 given already some concrete sequence of characters before it。 Now。
    what we have to realize here is that every single word here， like a Zabella。 is
    actually quite a few examples packed in to that single word。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我提到的，字符级语言模型是在预测序列中的下一个字符，给定之前的一些具体字符序列。现在，我们必须意识到这里的每一个单词，例如Zabella，实际上包含了几个示例。
- en: Because what is an existence of a word like a Zabella in the dataset telling
    us really？
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，像Zabella这样的单词在数据集中存在，实际上告诉我们什么呢？
- en: It's saying that the character i is a very likely character to come first in
    a sequence of a name。 The character S is likely to come after i。 The character
    A is likely to come after iS。 The character B is very likely to come after iS
    A。 And so on all the way to A following a Zabella。 And then there's one more example
    actually packed in here。 And that is that after there's a Zabella。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明字符i在一个名字的序列中是非常可能首先出现的字符。字符S可能会在i后面出现。字符A可能会在iS后面出现。字符B非常可能会在iSA后面出现。以此类推，直到A在Zabella之后出现。实际上，这里还有一个例子，就是在Zabella之后。
- en: the word is very likely to end。 So that's one more sort of explicit piece of
    information that we have here。 That we have to be careful with。 And so there's
    a lot packed into a single individual word in terms of the statistical structure。
    of what's likely to follow in these character sequences。 And then of course we
    don't have just an individual word。 We actually have 32，000 of these。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这个单词很可能结束。所以这是我们在这里有的另外一个明确的信息点。我们必须小心。因此，在这些字符序列中，单个单词中包含了大量的统计结构，以预测哪些字符可能会跟随。并且当然，我们不仅仅有一个单独的单词。实际上我们有32,000个这样的单词。
- en: And so there's a lot of structure here to model。 Now in the beginning what I'd
    like to start with is I'd like to start with building a MIGRAM language model。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这里有很多结构需要建模。现在一开始，我想从构建一个MIGRAM语言模型开始。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_26.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_26.png)'
- en: Now in a MIGRAM language model we're always working with just two characters
    at a time。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在一个MIGRAM语言模型中，我们始终一次处理两个字符。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_28.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_28.png)'
- en: So we're only looking at one character that we are given and we're trying to
    predict the next character in the sequence。 So what characters are likely to follow
    are what characters are likely to follow A and so on。 And we're just modeling
    that kind of a little local structure。 And we're forgetting the fact that we may
    have a lot more information。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们只关注一个给定的字符，并试图预测序列中的下一个字符。哪些字符可能会跟随A等等。我们仅仅是在建模这种局部结构。我们忽略了可能还有很多更多信息的事实。
- en: We're always just looking at the previous character to predict the next one。
    So it's a very simple and weak language model， but I think it's a great place
    to start。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们总是只看前一个字符来预测下一个字符。所以这是一个非常简单且弱的语言模型，但我认为这是一个很好的起点。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_30.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_30.png)'
- en: So now let's begin by looking at these BIGRAMS in our dataset and what they
    look like。 And these BIGRAMS again are just two characters in a row。 So for W
    and words。 each W here is an individual word string。 We want to iterate this word
    with consecutive characters。 So two characters at a time sliding it through the
    word。 Now a interesting nice way。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们开始查看数据集中的这些BIGRAMS，以及它们的样子。这些BIGRAMS再次只是连续两个字符。所以对于W和单词。这里的每个W都是一个单独的单词字符串。我们想要用连续字符迭代这个单词。所以每次滑动两个字符。现在有一种有趣的好方法。
- en: cute way to do this in Python by the way， is doing something like this。 For
    character one。 character two in zip of W and W at one。 One call。 Print， character
    one， character two。 And let's not do all the words。 Let's just do the first three
    words。 And I'm going to show you in a second how this works。 But for now， basically
    as an example。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，有一种有趣的做法，就是这样做。对于character one，character two，在zip的W和W的一列中。打印，character
    one，character two。我们不做所有单词。我们只做前三个单词。我将稍后向你展示这是如何工作的。但现在，基本上作为一个示例。
- en: let's just do the very first word alone， Emma。 You see how we have a Emma？
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们单独处理第一个单词，Emma。你看到我们有Emma吗？
- en: And this will just print EM， MMA。 And the reason this works is because W is
    the string Emma。 W at one column is the string MMA。 And zip takes two iterators。
    And it pairs them up and then creates an iterator over the tuples of their consecutive
    entries。 And if any one of these lists is shorter than the other， then it will
    just halt and return。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印出EM，MMA。之所以这样有效，是因为W是字符串Emma。W在第一列是字符串MMA。zip接受两个迭代器。它将它们配对，然后创建一个迭代器，返回它们连续条目的元组。如果其中任何一个列表比另一个短，它将停止并返回。
- en: So basically that's why we return EM， MMA， MMA。 But then because this iterator，
    second one here。 runs out of elements， zip just ends。 And that's why we only get
    these tuples。 So pretty cute。 So these are the consecutive elements in the first
    word。 Now we have to be careful because we actually have more information here
    than just these three examples。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 所以基本上这就是我们为什么返回EM，MMA，MMA。可是因为这个迭代器，这里的第二个，元素耗尽，zip就结束了。这就是为什么我们只得到这些元组。所以相当可爱。这些是第一个单词中的连续元素。现在我们必须小心，因为我们实际上这里有比这三个示例更多的信息。
- en: As I mentioned， we know that E is very likely to come first and we know that
    A in this case is coming last。 So one way to do this is basically we're going
    to create a special array here， our characters。 And we're going to hallucinate
    a special start token here。 I'm going to call it like special start。 So this is
    a list of one element plus W and then plus a special end character。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我提到的，我们知道E很可能是第一个出现的，而在这种情况下A是最后一个。因此，做这件事的一种方法是基本上在这里创建一个特殊的数组，我们的字符。我们将在这里幻想一个特殊的开始标记，我称之为特殊开始。因此，这是一个包含一个元素加W和一个特殊结束字符的列表。
- en: And the reason I'm wrapping the list of W here is because W is a string MMA。
    List of W will just have the individual characters in the list。 And then doing
    this again now。 but not iterating over Ws but over the characters will give us
    something like this。 So E is likely。 so this is a by gram of the start character
    and E。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我将W的列表包裹起来是因为W是一个字符串MMA。W的列表将仅包含列表中的单个字符。然后再次执行这个，但不是迭代W，而是迭代字符，将给我们像这样。因此E是可能的。所以这是开始字符和E的字对。
- en: And this is a by gram of the A in the special end character。 And now we can
    look at， for example。 what this looks like for Olivia or Eva。 And indeed we can
    actually。 potentially this for the entire dataset。 But we won't print that， that's
    going to be too much。 But these are the individual character by grams and we can
    print them。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这是字对A和特殊结束字符。现在我们可以查看，例如，对于Olivia或Eva这看起来像什么。实际上，我们可以对整个数据集进行这样的处理。但我们不会打印，这会太多。但这些是单个字符的字对，我们可以打印它们。
- en: Now in order to learn the statistics about which characters are likely to follow
    other characters。 the simplest way in the by gram language models is to simply
    do it by counting。 So we're basically just going to count how often any one of
    these combinations occurs in the training set in these words。 So we're going to
    need some kind of a dictionary that's going to maintain some counts for every
    one of these by grams。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了了解哪些字符可能跟随其他字符的统计信息。在字对语言模型中，最简单的方法就是简单地通过计数来实现。因此，我们基本上只是在训练集中的这些单词里统计这些组合出现的频率。所以我们需要某种字典来维护每一个字对的计数。
- en: So let's use a dictionary B and this will map these by grams。 So by gram is
    a tuple of character on character two。 And then B at by gram will be beat up get
    of by gram， which is basically the same as B at by gram。 But in the case that
    by gram is not in the dictionary B。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们来使用字典B，它将映射这些字对。字对是字符元组（character on character）。然后B在字对中的值将是字对的计数，这基本上与B在字对中的值相同，但在字对不在字典B中的情况下。
- en: we would like to buy default return zero plus one。 So this will basically add
    up all the by grams and count how often they occur。 Let's get rid of printing
    or rather， let's keep the printing and let's just inspect what B is in this case。
    And we see that many by grams occur just a single time。 This one allegedly occurred
    three times。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想购买默认返回零加一。因此，这基本上会将所有的字对加起来并计算它们出现的频率。我们来去掉打印，或者说，我们保留打印，仅仅检查一下在这种情况下B是什么。我们发现许多字对只出现了一次。这一个据称出现了三次。
- en: So A was an ending character three times and that's true for all of these words。
    All of Emma。 Olivia and Eva and with A。 So that's why this occurred three times。
    Now let's do it for all the words。 Oops， I should not have printed it。 I'm going
    to erase that。 Let's kill this。 Let's just run and now B will have the statistics
    of the entire data set。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 所以A是结束字符出现了三次，这对所有这些单词都是如此。所有的Emma、Olivia和Eva，以及A。所以这就是为什么它出现三次。现在让我们对所有单词做这个。哎呀，我不应该打印这个。我将把它删掉。让我们停止这个。让我们运行，现在B将拥有整个数据集的统计信息。
- en: So these are the counts across all the words of the individual by grams。 And
    we could， for example。 look at some of the most common ones and least common ones。
    This kind of grows in Python。 but the way to do this， the simplest way I like
    is we just use B dot items。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这些是各个字的单独字对的计数。我们可以，例如，查看一些最常见和最少见的字对。这在Python中是可以扩展的，但我喜欢的最简单方法是使用B.dot items。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_32.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_32.png)'
- en: B dot items returns the tuples of key value。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: B.dot items返回键值元组。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_34.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_34.png)'
- en: In this case， the keys are the character by grams and the values are the counts。
    And so then what we want to do is we want to do sorted of this。 But the default
    sort is on the first item of a tuple。 But we want to sort by the values which
    are the second element of a tuple that is the key value。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，键是字符组合，值是计数。因此，我们想要对这个进行排序。但是默认的排序是基于元组的第一个项目。但我们想按值排序，即元组的第二个元素，也就是键值。
- en: So we want to use the key equals lambda that takes the key value and returns
    the key value at one。 not at zero but at one， which is the count。 So we want to
    sort by the count of these elements。 And actually we want it to go backwards。
    So here we have is the by gram Q and R occurs only a single time。 DZ occurred
    only a single time。 And when we sort this the other way around。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们想使用 key 等于 lambda，该函数接受键值并返回第一个值，而不是零，而是返回计数。因此，我们想按这些元素的计数进行排序。实际上，我们希望它反向排序。所以这里有
    by gram Q 和 R 仅出现一次。DZ 也仅出现一次。当我们反向排序时。
- en: we're going to see the most likely by grams。 So we see that N was very often
    an ending character many。 many times。 And apparently N almost always follows an
    A and that's a very likely combination as well。 So this is kind of the individual
    counts that we achieve over the entire DZ。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到按字符组合的概率。因此，我们看到 N 很多时候是结束字符，许多次。而且显然 N 几乎总是跟随 A，这是一个非常可能的组合。因此，这是我们在整个
    DZ 中获得的个别计数。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_36.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_36.png)'
- en: Now it's actually going to be significantly more convenient for us to keep this
    information in a two dimensional array instead of a Python dictionary。 So we're
    going to store this information in a 2D array。 And the rows are going to be the
    first character of the by gram and the columns are going to be the second character。
    And each entry in the student machine array will tell us how often that first
    character follows the second character in the DZ。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，将这些信息保存在二维数组中比保存在 Python 字典中要方便得多。因此，我们将把这些信息存储在一个 2D 数组中。行将是字符组合的第一个字符，列将是第二个字符。学生机器数组中的每个条目将告诉我们第一个字符在
    DZ 中跟随第二个字符的频率。
- en: So in particular the array representation that we're going to use or the library
    is that of PyTorch。 And PyTorch is a deep learning neural work framework。 But
    part of it is also this torch。tensor which allows us to create multi dimensional
    arrays and manipulate them very efficiently。 So let's import PyTorch which you
    can do by import torch。 And then we can create arrays。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是我们将使用的数组表示或库是 PyTorch。PyTorch 是一个深度学习神经网络框架。但其中的一部分也是这个 torch.tensor，它允许我们创建多维数组并高效地进行操作。因此，让我们导入
    PyTorch，可以通过 import torch 来完成。然后我们可以创建数组。
- en: So let's create a array of zeros and we give it a size of this array。 Let's
    create a 3x5 array as an example。 And this is a 3x5 array of zeros。 And by default
    you'll notice 8 of D type which is short for data type is float 32。 So these are
    single precision floating point numbers。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们创建一个零数组，并给它一个大小。我们以 3x5 数组为例。这是一个 3x5 的零数组。默认情况下，你会注意到 D 类型的值为 8，代表数据类型为
    float 32。因此，这些是单精度浮点数。
- en: Because we are going to represent counts let's actually use D type as torch。in32。
    So these are 32 bit integers。 So now you see that we have integer data inside
    this tensor。 Now tensors allow us to really manipulate all the individual entries
    and do it very efficiently。 So for example if we want to change this bit we have
    to index into the tensor。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们将表示计数，所以实际上使用 D 类型为 torch.in32。因此，这些是 32 位整数。因此，现在你会看到这个张量中有整数数据。张量允许我们高效地操作所有个别条目。例如，如果我们想改变这个位，我们必须对张量进行索引。
- en: And in particular here this is the first row because it's zero indexed。 So this
    is row index one and column index 0， 1， 2， 3。 So A at 1。 3 we can set that to
    1 and then A will have a 1 over there。 We can of course also do things like this。
    So now A will be 2 over there or 3。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是在这里，这是第一行，因为它是从零开始索引的。所以这是行索引 1 和列索引 0、1、2、3。因此，我们可以将 A 在 1.3 位置设置为 1，然后
    A 将在那有一个 1。我们当然也可以这样做。因此现在 A 在那里是 2 或 3。
- en: And also we can for example say A00 is 5 and then A will have a 5 over here。
    So that's how we can index into the arrays。 Now of course the array that we are
    interested in is much much bigger。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以例如说 A00 是 5，那么 A 在这里将有一个 5。这就是我们如何索引数组。现在当然我们感兴趣的数组要大得多。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_38.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_38.png)'
- en: So for our purposes we have 26 letters of the alphabet and then we have two
    special characters S and E。 So we want 26 plus 2 or 28 by 28 array。 And let's
    call it the capital N because it's going to represent sort of the counts。 Let
    me erase this stuff。 So that's the array that starts at 0s 28 by 28。 And now let's
    copy paste this here。 But instead of having a dictionary B which we're going to
    erase we now have an N。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 就我们的目的而言，我们有 26 个字母的字母表，然后有两个特殊字符 S 和 E。因此我们想要一个 28 乘 28 的数组。我们称之为大写 N，因为它将表示计数。让我擦掉这些东西。所以这是一个从
    0 开始的 28 乘 28 的数组。现在让我们把这个复制粘贴到这里。但是我们现在没有字典 B，我们将擦掉它，而是有一个 N。
- en: Now the problem here is that we have these characters which are strings but
    we have to now。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的问题是我们有这些字符，它们是字符串，但我们现在必须。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_40.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_40.png)'
- en: basically index into a array and we have to index using integers。 So we need
    some kind of a lookup table from characters to integers。 So let's construct such
    a character array。 And the way we're going to do this is we're going to take all
    the words which is a list of strings。 We're going to concatenate all of it into
    a massive string。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上是对数组进行索引，我们必须使用整数进行索引。因此我们需要某种从字符到整数的查找表。让我们构建这样一个字符数组。我们将采取所有单词，这是一系列字符串。我们将把它们连接成一个庞大的字符串。
- en: So this is just simply the entire dataset as a single string。 We're going to
    pass this to the set constructor which takes this massive string and throws out。
    duplicates because sets do not allow duplicates。 So set of this will just be the
    set of all the lowercase characters。 And there should be a total of 26 of them。
    And now we actually don't want a set we want a list。
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是整个数据集作为一个单一字符串。我们将把它传递给集合构造函数，该函数接受这个庞大的字符串并去掉重复项，因为集合不允许重复。因此这个集合将是所有小写字符的集合。总共应该有
    26 个。现在我们实际上不想要一个集合，我们想要一个列表。
- en: But we don't want a list sorted in some weird arbitrary way we want it to be
    sorted from A to Z。 So a sorted list。 So those are our characters。 Now what we
    want is this lookup table as I mentioned。 So let's create a special S to I， I
    will call it S is string or character。 And this will be an S to I mapping for
    is in enumerate of these characters。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我们不想要一个以某种奇怪的任意方式排序的列表，我们希望它从 A 排序到 Z。因此是一个排序列表。这就是我们的字符。现在我们想要的是这个查找表，正如我提到的。让我们创建一个特殊的
    S 到 I，我将称之为 S 是字符串或字符。这将是这些字符的 S 到 I 映射。
- en: So enumerate basically gives us this iterator over the integer index and the
    actual element of the list。 And then we are mapping the character to the integer。
    So S to I is a mapping from A to zero B to one， etc。 all the way from Z to 25。
    And that's going to be useful here， but we actually also have to specifically
    set that S will be 26。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 所以 enumerate 基本上给我们提供了一个迭代器，遍历整数索引和列表的实际元素。然后我们将字符映射到整数。因此 S 到 I 是从 A 到零，B 到一，等等，一直到
    Z 到 25 的映射。这在这里会很有用，但我们实际上还必须特别设置 S 为 26。
- en: And S to I at E will be 27， right， because Z was 25。 So those are the lookups。
    And now we can come here and we can map both character one and character two to
    their integers。 So this will be S to I character one and I X two will be S to
    I of character two。 And now we should be able to do this line but using our array。
    So an at X one， I X two。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 所以 S 到 I 的值将是 27，对吧，因为 Z 是 25。因此这些就是查找值。现在我们可以在这里将字符一和字符二映射到它们的整数值。所以这将是 S 到
    I 字符一，而 I X 二将是 S 到 I 字符二。现在我们应该能够使用我们的数组来处理这一行。所以在 X 一，I X 二。
- en: This is the two dimensional array indexing I've shown you before。 And honestly
    just plus equals one because everything starts at zero。 So this should work and
    give us a large 28 by 20 array of all these counts。 So if we print N this is the
    array， but of course it looks ugly。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我之前向您展示的二维数组索引。老实说，只需加一，因为一切从零开始。所以这应该能工作，并给我们一个包含所有计数的大型 28 乘 20 数组。因此，如果我们打印
    N，这就是数组，但当然看起来很丑。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_42.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_42.png)'
- en: So let's erase this ugly mess and let's try to visualize it a bit more nicer。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们擦掉这堆丑陋的东西，试着把它可视化得更好一点。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_44.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_44.png)'
- en: So for that we're going to use a library called matplotlib。 So matplotlib allows
    us to create figures。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将使用一个名为 matplotlib 的库。matplotlib 允许我们创建图形。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_46.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_46.png)'
- en: So we can do things like PLT， I'm show of the counter a。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 因此我们可以做一些事情，比如 PLT，我会展示计数器 a。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_48.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_48.png)'
- en: So this is the 20 by 20 array and this is structure。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个20乘20的数组，这是结构。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_50.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_50.png)'
- en: But even this I would say is still pretty ugly。 So we're going to try to create
    a much nicer visualization of it and I wrote a bunch of code for that。 The first
    thing we're going to need is we're going to need to invert this array here。 this
    dictionary。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 但即便如此，我认为这仍然相当丑陋。因此我们将尝试创建一个更漂亮的可视化，我为此写了一些代码。我们首先需要做的是反转这个数组，这个字典。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_52.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_52.png)'
- en: S to I is a mapping from S to I and in I to S we're going to reverse this dictionary。
    So it rated over all the items and just reverse that array。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: S到I是从S到I的映射，而在I到S中，我们将反转这个字典。所以它遍历所有项目并反转那个数组。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_54.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_54.png)'
- en: So I to S maps inversely from zero to A， one to B， etc。
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 所以I到S的映射是从零到A，1到B，等等。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_56.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_56.png)'
- en: So we'll need that。 And then here's the code that I came up with to try to make
    this a little bit nicer。
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们需要那个。然后这是我想出的代码，试图让这个变得更好看。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_58.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_58.png)'
- en: We create a figure we plot M and then we do and then we visualize a bunch of
    things later。
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一个图形，绘制M，然后稍后可视化一些东西。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_60.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_60.png)'
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_61.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_61.png)'
- en: Let me just run it so you get a sense of what it is。 So you see here that we
    have the array spaced out and every one of these is basically like B follows G
    zero times。 B follows H 41 times。 So A follows J one 75 times。 And so what you
    can see that I'm doing here is first I show that entire array and then I iterate
    over all the individual cells here。
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我运行一下，这样你就能感受到它是什么。你会看到这里的数组是分开的，每一个基本上都是B跟随G零次。B跟随H 41次。A跟随J一次75次。因此你可以看到我在这里做的事情是，首先显示整个数组，然后迭代所有单独的单元。
- en: And I create a character string here， which is the inverse mapping I to S of
    integer I and integer J。 So that's those are the biograms in a character representation。
    And then I plot just the diagram text and then I plot the number of times that
    is by grammar curse。 Now the reason that there's a dot item here is because when
    you index into these arrays。
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里创建了一个字符字符串，这是整数I和整数J的反向映射I到S。所以这就是字符表示中的双字母组合。然后我仅绘制图表文本，并根据语法计数绘制出现次数。这里有一个点项的原因是当你索引这些数组时。
- en: these are torch tensors。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是torch张量。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_63.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_63.png)'
- en: You see that we still get a tensor back。 So the type of this thing you think
    it would be just an integer 149。 but it's actually a torch that tensor。 And so
    if you do dot item。 then it will pop out that individual integer。 So it'll just
    be 149。 So that's what's happening there。 And these are just some options to make
    it look nice。
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到我们仍然返回一个张量。所以你认为这东西的类型应该只是整数149，但它实际上是一个torch张量。因此，如果你使用dot item，然后它将弹出那个单独的整数。因此，它只是149。这就是发生的事情。这些只是一些选项来让它看起来不错。
- en: So what is the structure of this array？ We have all these counts and we see
    that some of them occur often and some of them do not occur often。 Now if you
    scrutinize this carefully， you will notice that we're not actually being very
    clever。 That's because when you come over here， you'll notice that for example
    we have an entire row of completely zeros。 And that's because the end character
    is never possibly going to be the first character of a biogram。
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 那么这个数组的结构是什么？我们有所有这些计数，我们看到其中一些经常出现，而一些则不常出现。仔细审视，你会注意到我们其实并没有很聪明。因为当你过来时，你会发现例如我们有一整行完全是零。这是因为结束字符不可能是双字母组合的第一个字符。
- en: because we're always placing these end tokens all at the end of a biogram。 Similarly。
    we have entire column zeros here because the S character will never possibly be
    the second element of a biogram。 because we always start with S and we end with
    E and we only have the words in between。 So we have an entire column of zeros，
    an entire row of zeros。
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们总是把这些结束标记放在双字组的末尾。同样，我们在这里有整列的零，因为S字符永远不可能是双字组的第二个元素。因为我们总是以S开始，以E结束，中间只有单词。因此，我们有一整列的零，一整行的零。
- en: and in this little 2x2 matrix here as well， the only one that can possibly happen
    is if S directly follows E。 That can be nonzero if we have a word that has no
    letters。 So in that case。 there's no letters in the word。 It's an empty word and
    we just have S follows E。 But the other ones are just not possible。 And so we're
    basically wasting space and not only that。
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个小的2x2矩阵中，唯一可能发生的情况是S直接跟随E。如果我们有一个没有字母的单词，这可以是非零的。所以在这种情况下，单词中没有字母。它是一个空单词，我们只有S跟随E。但其他的情况是不可行的。因此，我们基本上是在浪费空间，不仅如此。
- en: but the S and the E are getting very crowded here。 I was using these brackets
    because there's convention in natural language processing to use these kinds of
    brackets。 to denote special tokens， but we're going to use something else。 So
    let's fix all this and make it prettier。 We're not actually going to have two
    special tokens。
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 但是S和E在这里变得非常拥挤。我使用这些括号是因为在自然语言处理领域有使用这类括号的惯例，以表示特殊标记，但我们将使用其他东西。所以让我们修正这一切，让它变得更漂亮。我们实际上不会有两个特殊标记。
- en: we're only going to have one special token。 So we're going to have n by n array
    of 27 by 27 instead。 Instead of having two， we will just have one and I will call
    it a dot。 Okay。 Let me swing this over here。 Now one more thing that I would like
    to do is I would actually like to make this special character half position zero。
    And I would like to offset all the other letters off。 I find that a little bit
    more pleasing。
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只会有一个特殊标记。因此，我们将有一个27乘以27的n乘以n数组。我们只会有一个，我称之为点。好吧。让我把这个移到这里。现在我想做的另一件事是，我实际上想把这个特殊字符的半位置设为零，并希望将所有其他字母偏移。我发现这样更令人愉悦。
- en: So we need a plus one here so that the first character， which is a， will start
    at one。 So S to I will now be a starts at one and dot is zero。 And I to S， of
    course。 we're not changing this because I to S just creates a reverse mapping
    and this will work fine。 So one is a two is B zero is dot。 So we reverse that
    here， we have a dot and a dot。
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们需要在这里加一，这样第一个字符a就会从1开始。所以S到I现在a从1开始，点是0。而I到S，当然，我们不改变这个，因为I到S只是创建了一个反向映射，这将很好地工作。所以1是a，2是B，0是点。我们在这里反转它，我们得到了一个点和一个点。
- en: This should work fine。 Make sure I started zeros count and then here we don't
    go up to 28。 we go up to 27。 And this should just work。
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该没问题。确保我从零开始计数，然后这里我们不去到28，而是去到27。这应该可以正常工作。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_65.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_65.png)'
- en: Okay， so we see that dot dot never happened。 It's a zero because we don't have
    empty words。 Then this row here now is just very simply the counts for all the
    first letters。 So J starts a word。 H starts a word， I start a word， etc。 And then
    these are all the ending characters。 And in between。 we have the structure of
    what characters follow each other。
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，所以我们看到点点从未发生。这是0，因为我们没有空单词。那么这一行现在只是所有首字母的简单计数。所以J开始一个单词。H开始一个单词，I开始一个单词，等等。然后这些都是结束字符。在之间，我们有字符彼此跟随的结构。
- en: So this is the counts array of our entire data set。 So this array actually has
    all the information necessary for us to actually sample from this。 by gram character
    level language model。 And roughly speaking。 what we're going to do is we're just
    going to start following these probabilities and these counts。
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们整个数据集的计数数组。这个数组实际上包含了我们从双字组字符级语言模型中进行采样所需的所有信息。大致而言，我们要做的就是开始遵循这些概率和这些计数。
- en: And we're going to start sampling from the from model。 So in the beginning，
    of course。 we start with the dot， the start token dot。 So to sample the first
    character of a name。 we're looking at this row here。 So we see that we have the
    counts and those counts externally are telling us how often any one of these characters
    is to start a word。 So if we take this N and we grab the first row， we can do
    that by using just indexing and zero。
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从模型开始抽样。因此，在一开始，当然，我们从点开始，开始标记点。为了抽样名称的第一个字符，我们要查看这一行。所以我们看到这些计数，它们外部告诉我们任何一个字符开始一个单词的频率。如果我们取这个N并抓取第一行，我们可以通过简单的索引和零来做到这一点。
- en: And then using this notation column for the rest of that row。 So N zero column
    is indexing into the zero row and then it's grabbing all the columns。 And so this
    will give us a one dimensional array of the first row。 So zero， four， four， ten。
    you know， zero， four， four， ten， one， three， oh， six， one， five， four， two， etc。
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用这个符号列来获取该行的其余部分。因此，N零列是在零行中进行索引，然后抓取所有列。这将给我们第一行的一维数组。所以零，四，四，十。你知道，零，四，四，十，一，三，哦，六，一，五，四，二，等等。
- en: It's just the first row。 The shape of this is twenty seven。 It's just the row
    of twenty seven。 And the other way that you can do this also is you just， you
    don't actually give this。 you just grab the zero row like this。 This is equivalent。
    Now these are the counts and now what we'd like to do is we'd like to basically
    sample from this。
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是第一行。它的形状是二十七。这只是二十七的行。你也可以这样做，你只需抓取零行。这是等效的。现在这些是计数，而我们想做的是基本上从中抽样。
- en: Since these are the raw counts， we actually have to convert this to probabilities。
    So we create a probability vector。 So we'll take N of zero and we'll actually
    convert this to float first。 Okay， so these integers are converted to float， floating
    point numbers。 And the reason we're creating floats is because we're about to
    normalize these counts。
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些是原始计数，我们实际上需要将其转换为概率。所以我们创建一个概率向量。我们将取N的零，并先将其转换为浮点数。好的，这些整数被转换为浮点数。我们创建浮点数的原因是因为我们即将对这些计数进行归一化。
- en: So to create a probability distribution here， we want to divide。 We basically
    want to do p。 p divide p that sum。 Now we get a vector of smaller numbers and
    these are now probabilities。 So of course because we divided by the sum， the sum
    of p now is one。 So this is a nice proper probability distribution。
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在这里创建概率分布，我们想要进行除法。我们基本上想要做p。p除以p的总和。现在我们得到了一个较小数字的向量，这些现在是概率。因此，当然因为我们是通过总和除以的，所以p的总和现在是1。这是一个良好的概率分布。
- en: It sums to one and this is giving us the probability for any single character
    to be the first character of a word。 So now we can try to sample from this distribution。
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 它的总和为1，这为任何单个字符成为单词的第一个字符提供了概率。因此，现在我们可以尝试从这个分布中抽样。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_67.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_67.png)'
- en: To sample from these distributions， we're going to use Torre-Tut Multinomial。
    which I've pulled up here。 So Torre-Tut Multinomial returns samples from the Multinomial
    probability distribution。 which is a complicated way of saying， you give me probabilities
    and I will give you integers。 which are sampled according to the probability distribution。
    So this is the signature of the method。
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从这些分布中抽样，我们将使用Torre-Tut多项分布，我在这里已经打开了。Torre-Tut多项分布从多项式概率分布中返回样本，这是一种复杂的说法，意思是你给我概率，我就给你根据概率分布抽样的整数。这就是该方法的特征。
- en: And to make everything deterministic， we're going to use a generator object
    in PyTorch。
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使一切确定性，我们将使用PyTorch中的生成器对象。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_69.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_69.png)'
- en: So this makes everything deterministic。 So when you run this on your computer。
    you're going to get the exact same results that I'm getting here on my computer。
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做使一切都是确定性的。所以当你在你的计算机上运行这个时，你会得到与我在我的计算机上获得的完全相同的结果。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_71.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_71.png)'
- en: So let me show you how this works。 Here's the deterministic way of creating
    a Torre-Tut generator object。
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我给你展示一下这是如何工作的。这是创建Torre-Tut生成器对象的确定性方法。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_73.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_73.png)'
- en: Seating it with some number that we can agree on。 So that seeds a generator。
    It gives us an object G。 And then we can pass that G to a function that creates
    here random numbers。 Torre-Tut random creates random numbers， three of them。 And
    it's using this generator object as a source of randomness。 So without normalizing
    it。
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 用我们可以达成一致的某个数字来设置种子。这样就种下了一个生成器。它给我们一个对象 G。然后我们可以将 G 传递给一个创建随机数的函数。Torre-Tut
    随机数生成三个随机数，并且它使用这个生成器对象作为随机性来源。所以不进行归一化。
- en: I can just print。 This is sort of like numbers between zero and one that are
    random according to this thing。 And whenever I run it again， I'm always going
    to get the same result because I keep using the same generator object which I'm
    seeding here。 And then if I divide to normalize， I'm going to get a nice probability
    distribution of just three elements。
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以直接打印。这有点像是根据这个东西生成的随机数在零和一之间。而每次我再次运行时，我总会得到相同的结果，因为我一直在使用相同的生成器对象，这里我正在设置种子。如果我进行归一化，我会得到三个元素的良好概率分布。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_75.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_75.png)'
- en: And then we can use Torre-Tut multinomial to draw samples from it。
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以使用 Torre-Tut 多项式从中抽取样本。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_77.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_77.png)'
- en: So this is what that looks like。 Torre-Tut multinomial will take the Torre-Tensor
    of probability distributions。 Then we can ask for a number of samples like say
    20。 Replacement equals true means that when we draw an element。 we can draw it
    and then we can put it back into the list of eligible indices to draw again。
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这看起来就是这样。 Torre-Tut 多项式将会获取概率分布的 Torre-Tensor。然后我们可以请求多个样本，比如说 20。替换为 true
    意味着当我们抽取一个元素时，可以将其放回合格索引的列表中再次抽取。
- en: And we have to specify replacement as true because by default for some reason
    it's false。
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须将替换指定为 true，因为默认情况下由于某种原因是 false。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_79.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_79.png)'
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_80.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_80.png)'
- en: And I think it's just something to be careful with。 And the generator is passed
    in here。 So we're going to always get deterministic results in the same results。
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这只是需要小心的事情。生成器在这里传入。所以我们总会得到确定的结果。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_82.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_82.png)'
- en: So if I run these two， we're going to get a bunch of samples from this distribution。
    Now you'll notice here that the probability for the first element in this tensor
    is 60%。 So in these 20 samples， we'd expect 60% of them to be zero。 We'd expect
    30% of them to be one。 And because the element index two has only 10% probability，
    very few of these samples should be two。
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果我运行这两个，我们将从这个分布中获得一堆样本。现在你会注意到，这个张量中第一个元素的概率是 60%。所以在这 20 个样本中，我们预计 60%
    会是零。我们预计 30% 会是 1。由于元素索引 2 只有 10% 的概率，这些样本中几乎没有应该是二。
- en: And indeed we only have a small number of twos。 And we can sample as many as
    we would like。
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，我们只有少量的二。我们可以根据需要抽取任意数量。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_84.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_84.png)'
- en: And the more we sample， the more these numbers should roughly have the distribution
    here。 So we should have lots of zeros， half as many ones。
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们抽取的样本越多，这些数字就越应该大致符合这里的分布。因此我们应该有很多零，数量是少量一的两倍。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_86.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_86.png)'
- en: And we should have three times S few ones and three times S few twos。
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该有三倍 S 的少量一和三倍 S 的少量二。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_88.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_88.png)'
- en: So you see that we have very few twos。 We have some ones and most of them are
    zero。 So that's what Torx-Dynote-Nal is doing。
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你看到我们几乎没有二。我们有一些一，大多数都是零。这就是 Torx-Dynote-Nal 的作用。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_90.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_90.png)'
- en: For us here， we aren't just in this row， we've created this P here， and now
    we can sample from it。
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们来说，我们不仅仅在这一行，我们在这里创建了这个 P，现在可以从中抽取样本。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_92.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_92.png)'
- en: So if we use the same seed and then we sample from this distribution， let's
    just get one sample。 Then we see that the sample is say 13。 So this will be the
    index。 And you see how it's a tensor that wraps 13？ We again have to use 。item
    to pop out that integer。 And now index would be just the number 13。 And of course
    we can map the I2S of Ix to figure out exactly which character we're sampling
    here。
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果我们使用相同的种子，然后从这个分布中抽样，我们只获取一个样本。然后我们看到样本是 13。所以这将是索引。你看到它是一个包裹 13 的张量吗？我们再次需要使用
    .item 来取出那个整数。现在索引就是数字 13。我们当然可以将 I2S 的 Ix 映射，准确找出我们在这里抽样的字符。
- en: We're sampling M。 So we're saying that the first character is M in our generation。
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在抽样 M。也就是说，我们的生成中的第一个字符是 M。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_94.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_94.png)'
- en: And just looking at the row here， M was drawn。 And we can see that M actually
    starts a large number of words。 M started 2，500 words out of 32，000 words。 So
    almost a bit less than 10% of the words start with M。
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 看着这一行，M 被抽样。我们可以看到 M 实际上开始了大量的单词。M 开头的单词有 2,500 个，占 32,000 个单词的数量。因此，几乎有不到 10%
    的单词是以 M 开头的。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_96.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_96.png)'
- en: So this was actually a fairly likely character to draw。 So that would be the
    first character of our word。 And now we can continue to sample more characters。
    Because now we know that M started。
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这个字符实际上是一个相当可能被抽样的字符。这将是我们单词的第一个字符。现在我们可以继续抽样更多字符。因为我们现在知道是以 M 开头。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_98.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_98.png)'
- en: M is already sampled。 So now to draw the next character。 we will come back here
    and we will look for the row that starts with M。 So you see M and we have a row
    here。 So we see that M。 is 516， M。A。 is this many， M。B。 is this many。 etc。 So
    these are the counts for the next row。 And that's the next character that we are
    going to now generate。
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: M 已经被抽样了。现在为了绘制下一个字符，我们将返回这里，查找以 M 开头的行。所以你看 M，并且我们这里有一行。所以我们看到 M 的计数是 516，M.A
    的计数是这个，M.B 的计数是这个，等等。这些是下一行的计数。这就是我们现在要生成的下一个字符。
- en: So I think we are ready to actually just write out the loop。 Because I think
    you're starting to get a sense of how this is going to go。 We always begin at
    index 0， because that's the start token。 And then while true。 we're going to grab
    the row corresponding to index that we're currently on。 So that's P。
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我想我们实际上已经准备好写出循环。因为我想你开始理解这是怎么进行的。我们总是从索引 0 开始，因为那是开始标记。然后在循环中，我们将抓取与当前索引对应的行。那就是
    P。
- en: So that's N， array at Ix。 So the way to float is R P。 Then we normalize this
    P to sum to 1。
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是 N，数组在 Ix。浮动的方法是 R P。然后我们将这个 P 归一化，使其总和为 1。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_100.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_100.png)'
- en: Accidentally random infinite loop。 We normalize P to sum to 1。
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 意外的随机无限循环。我们将 P 归一化，使其总和为 1。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_102.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_102.png)'
- en: Then we need this generator object。 And we're going to initialize up here。
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们需要这个生成器对象。我们将在这里初始化。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_104.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_104.png)'
- en: And we're going to draw a single sample from this distribution。
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从这个分布中抽取一个样本。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_106.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_106.png)'
- en: And then this is going to tell us what index is going to be next。 If the index
    sampled is 0。 then that's now the end token。 So we will break。 Otherwise we are
    going to print S2I of Ix。
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 然后这将告诉我们下一个索引是什么。如果抽样的索引是 0，那么这就是结束标记。所以我们将中断。否则，我们将打印 S2I 的 Ix。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_108.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_108.png)'
- en: I2S of Ix。 And that's pretty much it。
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: I2S 的 Ix。这差不多就是全部内容。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_110.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_110.png)'
- en: We're just， this should work。 Okay， more。 So that's the name that we've sampled。
    We start it with M。 the next stop was O， then R， and then dot。 And this dot， we
    print it here as well。 So let's now do this a few times。
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只是，这应该可以工作。好的，更多。所以这是我们抽样的名字。我们以 M 开头，下一步是 O，然后是 R，最后是点。我们也在这里打印这个点。所以现在我们来做几次。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_112.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_112.png)'
- en: So let's actually create an out list here。
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们实际上在这里创建一个输出列表。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_114.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_114.png)'
- en: And instead of printing， we're going to append。 So out that append this character。
    And then here。 let's just print it at the end。
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是打印，我们将附加。因此输出将附加这个字符。然后在这里。让我们在最后打印出来。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_116.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_116.png)'
- en: So let's just join up all the outs。 And we're just going to print more。 Okay。
    now we're always getting the same result because of the generator。 So if we want
    to do this a few times， we can go for high and range 10。 We can sample 10 names。
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们只需将所有的输出连接起来。我们将会打印更多。好吧。现在我们总是得到相同的结果，因为生成器的原因。如果我们想多做几次，我们可以设置为高和范围为10。我们可以抽样10个名字。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_118.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_118.png)'
- en: And we can just do that 10 times。 And these are the names that we're getting
    out。 Let's do 20。 I'll be honest with you， this doesn't look right。 So I'll stare
    at it a few minutes to convince myself that it actually is right。 The reason these
    samples are so terrible is that by gram language model is actually。 just like
    really terrible。 We can generate a few more here。
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以这样做10次。这些是我们得到的名字。我们再做20次。老实告诉你，这看起来不太对。所以我会盯着它几分钟，以说服自己这确实是正确的。这些样本如此糟糕的原因是n-gram语言模型真的。就像是非常糟糕。我们可以再生成几个。
- en: And you can see that they're kind of like their name like a little bit like，
    Yannu， Erile， etc。 But they're just like totally messed up。 And I mean， the reason
    that this is so bad。 like we're generating H as a name。 But you have to think
    through it from the model's eyes。 It doesn't know that this H is the very first
    H。 All it knows is that H was previously。
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到这些名字有点像，比如Yannu、Erile等。但它们真的完全搞错了。我的意思是，这么糟糕的原因是，我们生成了H作为名字。但你必须从模型的视角来看待这个问题。它并不知道这个H是第一个H。它只知道之前有H。
- en: And now how likely is H the last character？ Well， it's somewhat likely。 And
    so it just makes it last character。 It doesn't know that there were other things
    before it or there were not other things before it。 And so that's why I'm generating
    all these nonsense names。 Another way to do this is to convince yourself that
    it's actually doing something， reasonable。
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 那么H作为最后一个字符的可能性有多大呢？嗯，它是有些可能的。所以它就把它作为最后一个字符。它不知道之前是否有其他东西。因此这就是为什么我生成了这些无意义的名字。另一种做法是让自己相信它实际上在做一些合理的事情。
- en: even though it's so terrible， is these little piece here are 27， right？ Like
    27。 So how about if we did something like this？ Instead of having any structure
    whatsoever。 How about if P was just a torch dot ones of 27？ By default， this is
    a float 32。 So this is fine。 Divide 27。 So what I'm doing here is this is the
    uniform distribution。
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这很糟糕，但这里的这一小部分是27，对吗？像27。那么如果我们这样做呢？而不是拥有任何结构。如果P只是torch的全1矩阵，大小为27呢？默认情况下，这是float32。所以这没问题。除以27。所以我在这里做的是均匀分布。
- en: which will make everything equally likely。 And we can sample from that。
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这会让所有结果同样可能。我们可以从中抽样。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_120.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_120.png)'
- en: So let's see if that does any better。 Okay？ So this is what you have from a
    model that is completely untrained。 where everything is equally likely。
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们看看这样是否能更好。好吗？这是一个完全未训练的模型的结果，其中每个结果都是同样可能的。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_122.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_122.png)'
- en: So it's obviously garbage。
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 所以显然这是垃圾。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_124.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_124.png)'
- en: And then if we have a trained model， which is trained on just by grams， this
    is what we get。
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 然后如果我们有一个训练好的模型，仅仅通过n-gram训练，这就是我们得到的结果。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_126.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_126.png)'
- en: So you can see that it is more name like it is actually working。
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可以看到这更像是名字，它实际上在工作。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_128.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_128.png)'
- en: It's just by gram is so terrible and we have to do better。
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅使用n-gram是如此糟糕，我们必须做得更好。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_130.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_130.png)'
- en: Now next， I would like to fix an inefficiency that we have going on here。 Because
    what we're doing here is we're always fetching a row of N from the counts matrix
    up ahead。
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 现在接下来，我想修复我们在这里的低效。因为我们在这里做的是总是从计数矩阵中提取一行N。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_132.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_132.png)'
- en: And then we're always doing the same things。 We're converting to float and we're
    dividing。 And we're doing this every single iteration of this loop。 And we just
    keep renormalizing these rows over and over again and it's extremely inefficient
    and wasteful。 So what I'd like to do is I'd like to actually prepare a matrix
    capital P that will just have the probabilities in it。
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们总是在做相同的事情。我们在进行浮点转换并进行除法。我们在这个循环的每一个迭代中都在做这件事。我们不断地对这些行进行重新归一化，这极其低效且浪费。因此，我想要做的是准备一个包含概率的矩阵
    P。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_134.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_134.png)'
- en: So in other words， it's going to be the same as the capital N matrix here of
    counts。 But every single row will have the row of probabilities that is normalized
    to one。 indicating the probability distribution for the next character given the
    character before it。 As defined by which row we're in。
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，它将与这里的计数矩阵 N 相同。但每一行都会有一行概率，其总和归一化为1，表示给定前一个字符的下一个字符的概率分布。由我们所处的行来定义。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_136.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_136.png)'
- en: So basically what we'd like to do is we'd like to just do it up front here。
    And then we would like to just use that row here。 So here we would like to just
    do P equals P of I X instead。 Okay。 The other reason I want to do this is not
    just proficiency。 but also I would like us to practice these indimensional tensors。
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上我们想要做的是在这里直接进行操作。然后我们想要直接使用这一行。因此在这里我们希望将 P 设为 P 的 I X，而不是其他方式。好的。我想这样做的另一个原因不仅仅是为了熟练，还希望我们能够练习这些多维张量。
- en: And I'd like us to practice their manipulation and especially something that's
    called broadcasting that we'll go into in a second。 We're actually going to have
    to become very good at these tensor manipulations because we're going to build
    out all the way to transformers。
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要练习它们的操作，特别是有一个叫做广播的概念，我们马上会涉及到。实际上，我们需要非常擅长这些张量操作，因为我们要构建到变换器。
- en: We're going to be doing some pretty complicated array operations for efficiency。
    And we need to really understand that and be very good at it。 So intuitively what
    we want to do is we first want to grab the floating point copy of N。 And I'm mimicking
    the line here basically。 And then we want to divide all the rows so that they
    sum to one。
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将进行一些相当复杂的数组操作以提高效率。我们需要真正理解这一点并且要非常擅长。因此，直观上我们想要做的首先是获取 N 的浮点副本。我在这里基本上是在模拟这一行。然后我们希望将所有行除以它们的总和，使其总和为1。
- en: So we like to do something like this， P divide P dot sum。 But now we have to
    be careful because P dot sum actually produces a sum。
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们想要做一些类似 P 除以 P 的总和。但现在我们必须小心，因为 P 的总和实际上会产生一个总和。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_138.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_138.png)'
- en: Sorry， P equals and that float copy。 P dot sum produces a sums up all of the
    counts of this entire matrix N and gives us a single number of just the summation
    of everything。
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 抱歉，P 等于并且是这个浮点副本。P 的总和会将整个矩阵 N 的所有计数相加，并给出一个只包含所有内容的总和的单一数字。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_140.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_140.png)'
- en: So that's not the way we want to divide。 We want to simultaneously and imperil
    divide all the rows by their respective sums。
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这不是我们想要的除法方式。我们想要同时并且逐行地将所有行除以各自的总和。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_142.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_142.png)'
- en: So what we have to do now is we have to go into documentation for Torx dot sum。
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们现在需要做的是查看 Torx 的文档，了解 dot sum。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_144.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_144.png)'
- en: And we can scroll down here to a definition that is relevant to us。 which is
    where we don't only provide an input array that we want to sum。 but we also provide
    the dimension along which we want to sum。 And in particular we want to sum up
    over rows。 Now one more argument that I want you to pay attention to here is the
    keep them as false。
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以向下滚动到与我们相关的定义。在这里，我们不仅提供一个我们想要求和的输入数组，还提供我们希望沿着哪个维度进行求和。特别是我们想对行进行求和。还有一个我希望你注意的参数是
    keep them as false。
- en: If keep them is true， then the output tensor is of the same size as input。 except
    of course the dimension along which you summed， which will become just one。 But
    if you pass in keep them as false， then this dimension is squeezed out。 And so
    Torx dot sum not only does the sum and collapses the dimension to be of size one。
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`keepdims`为真，那么输出张量的大小与输入相同，当然，沿着求和的维度将变为1。但如果你将`keepdims`传入为假，那么这个维度就被挤压了。因此，`torch.sum`不仅进行了求和，还将维度压缩到大小为1。
- en: but in addition it does what's called a squeeze， where it squeezes out that
    dimension。
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 但除此之外，它还会进行一个叫做挤压的操作，挤出那个维度。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_146.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_146.png)'
- en: So basically what we want here is we instead want to do p dot sum of sum axis。
    And in particular。 notice that p dot shape is 27 by 27。 So when we sum up across
    axis zero。 then we would be taking the zero dimension and we would be summing
    across it。 So when keep them as true， then this thing will not only give us the
    counts across along the columns。
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上我们想要的是进行`p`的求和操作，而不是简单的相加。特别地，注意`p`的形状是27乘27。因此，当我们在轴零上求和时，我们将取零维度并对其进行求和。因此，当保持`keepdims`为真时，这个操作不仅会给我们沿列的计数。
- en: but notice that basically the shape of this is one by 27。 We just get a row
    vector。 And the reason we get a row vector here again is because we pass in zero
    dimension。 So this zero dimension becomes one and we've done a sum and we get
    a row。 And so basically we've done the sum this way vertically and arrived at
    just a single one by 27 vector of counts。
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 但注意，这里的形状基本上是1乘27。我们得到一个行向量。之所以得到行向量，是因为我们传入了零维度。因此，这个零维度变为1，我们进行了求和，得到了一个行向量。基本上我们以这种方式竖直求和，得到了一个单一的1乘27的计数向量。
- en: What happens when you take out keep them is that we just get 27。 So it squeezes
    out that dimension and we just get one dimensional vector of size 27。 Now we don't
    actually want one by twenty seven row vector because that gives us the counts
    or the sums across the columns。 We actually want to sum the other way along dimension
    one。
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 当你去掉`keepdims`时，我们只得到27。因此，它压缩了那个维度，我们只得到大小为27的一维向量。现在我们实际上并不想要1乘27的行向量，因为那会给我们列上的计数或求和。我们实际上想要沿维度1进行反向求和。
- en: And you'll see that the shape of this is 27 by one。 So it's a column vector。
    It's a twenty seven by one vector of counts。 And that's because what's happened
    here is that we're going horizontally and this twenty seven by twenty seven matrix
    becomes a twenty seven by one array。
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到它的形状是27乘1。因此，这是一个列向量。它是一个27乘1的计数向量。这是因为这里发生的情况是，我们在水平方向上进行，而这个27乘27的矩阵变成了27乘1的数组。
- en: Now you'll notice by the way that the actual numbers of these counts are identical。
    And that's because this special array of counts here comes from by grams to the
    sticks。 And actually it just so happens by chance or because of the way this array
    is constructed that this sounds along the columns or along the rows horizontally
    or vertically is identical。 But actually what we want to do in this case is we
    want to sum across the rows horizontally。
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下，你会注意到这些计数的实际数字是相同的。这是因为这个特殊的计数数组来自于双语法。实际上，由于这个数组的构造方式，沿列或沿行的计数是相同的。但在这种情况下，我们想要的是沿着行水平求和。
- en: So what we want here is be that some of one with keep them true。 Twenty seven
    by one column vector。 And now what we want to do is we want to divide by that。
    Now we have to be careful here again。 Is it possible to take what's a p dot shape
    you see here is twenty seven by twenty seven。 Is it possible to take a twenty
    seven by twenty seven array and divided by what is a twenty seven by one array。
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要的是保持`keepdims`为真的情况下，得到一个27乘1的列向量。现在我们想要做的是对其进行除法。我们在这里需要小心。这里的`p`形状是27乘27，是否可以将一个27乘27的数组除以一个27乘1的数组。
- en: Is that an operation that you can do？ And whether or not you can perform this
    operation is determined by what's called broadcasting rules。
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个可以执行的操作吗？是否能够执行这个操作取决于所谓的广播规则。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_148.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_148.png)'
- en: So if you just search broadcasting semantics in torch。
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果你只搜索`torch`中的广播语义。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_150.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_150.png)'
- en: You'll notice that there's a special definition for what's called broadcasting
    that for whether or not these two arrays can be combined in a binary operation
    like division。 So the first condition is each tensor has at least one dimension
    which is the case for us。
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到有一个特别的定义，关于所谓的广播，即这两个数组是否可以在像除法这样的二元操作中结合。第一个条件是每个张量至少有一个维度，这对我们来说是成立的。
- en: And then when iterating over the dimension sizes starting at the trailing dimension。
    The dimension sizes must either be equal one of them is one or one of them does
    not exist。
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在迭代维度大小时，从最后一个维度开始。维度大小必须相等，或者其中一个是1，或者其中一个不存在。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_152.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_152.png)'
- en: Okay， so let's do that we need to align the two arrays and their shapes which
    is very easy because both of these shapes have two elements so they're aligned。
    Then we iterate over from the from the right and going to the left。 Each dimension
    must be either equal one of them is a one or one of them does not exist。 So in
    this case they're not equal but one of them is a one so this is fine。
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，让我们这样做，我们需要对齐这两个数组及其形状，这非常简单，因为这两个形状都有两个元素，因此它们是对齐的。然后我们从右往左进行迭代。每个维度必须相等，或者其中一个是1，或者其中一个不存在。因此在这种情况下，它们不相等，但其中一个是1，所以这没有问题。
- en: And then this dimension they're both equal so this is fine。 So all the dimensions
    are fine and therefore the this operation is broadcastable。 So that means that
    this operation is allowed and what is it that these arrays do when you divide
    27 by 27 by 27 by one。 What it does is that it takes this dimension one and it
    stretches it out it copies it to match 27 here in this case。
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 然后这个维度它们都是相等的，所以这没问题。因此所有维度都正常，因此这个操作是可广播的。这意味着这个操作是允许的，那么这些数组在你将27除以27再除以27再除以1时会发生什么呢？它会将这个维度1拉伸，并将其复制以匹配这里的27。
- en: So in our case it takes this column vector which is 27 by one and it copies
    it 27 times to make these both be 27 by 27 internally。 You can think of it that
    way。 And so it copies those counts and then it does an element wise division which
    is what we want because these counts we want to divide by them on every single
    one of these columns in this matrix。
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，它接受这个27行1列的列向量，并将其复制27次，使得这两者在内部都变为27行27列。你可以这样理解。因此，它复制这些计数，然后进行逐元素除法，这正是我们想要的，因为我们希望在这个矩阵的每一列上都对这些计数进行除法。
- en: So this actually we expect will normalize every single row。 And we can check
    that this is true by taking the first row for example and taking it some。 We expect
    this to be one because it's not normalized。 And then we expect this now because
    if we actually correctly normalize all the rows we expect to get the exact same
    result here。
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们实际上期望会对每一行进行归一化。我们可以通过拿第一行来检查这一点，比如说，取它的总和。我们期望这应该是1，因为它没有被归一化。然后我们现在期望这个，因为如果我们确实正确归一化了所有行，我们期望在这里得到完全相同的结果。
- en: So let's run this。 It's the exact same result。 So this is correct。 So now I
    would like to scare you a little bit。 You actually have to like I basically encourage
    you very strongly to read through broadcasting semantics。
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们来运行这个。这是完全相同的结果。所以这是正确的。现在我想吓唬你一点。实际上，我强烈鼓励你仔细阅读广播语义。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_154.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_154.png)'
- en: And I encourage you to treat this with respect。
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我鼓励你尊重这一点。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_156.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_156.png)'
- en: And it's not something to play fast and loose with it's something to really
    respect really understand and look up maybe some tutorials for broadcasting and
    practice it and be careful with it because you can very quickly run it to box。
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是可以随便处理的事情，而是需要真正尊重、理解，并查找一些广播的教程，进行练习，并小心处理，因为你很快就会遇到麻烦。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_158.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_158.png)'
- en: Let me show you what I mean。 You see how here we have Pete at some of one Pete
    then this true。 The shape of this is twenty seven by one。 Let me take out this
    line just so we have the N and then we can see the counts。 We can see that this
    is all the counts across all the rows and it's twenty seven by one column vector
    right。 Now suppose that I tried to do the following but I erase keep them just
    true here。
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 让我给你展示我所说的。你看这里我们有Pete在维度一的Pete，这是真的。它的形状是二十七乘一。让我去掉这一行，这样我们就有N，然后我们可以看到计数。我们可以看到这是所有行的计数，它是一个二十七乘一的列向量，对吧？现在假设我试着这样做，但我把保留它的true删掉了。
- en: What does that do if keep them is not true it's false。 Then remember according
    to the documentation it gets rid of this dimension one it squeezes it out。 So
    basically we just get all the same counts the same result except the shape of
    it is not twenty seven by one it is just twenty seven the one disappears。 But
    all the counts are the same。 So you'd think that this divide that would work。
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 如果保持它们不为真而为假会有什么影响呢？然后根据文档记住，它会去掉这个维度，将其挤压出去。所以基本上我们得到的都是相同的计数，结果相同，只是它的形状不是二十七乘一，而只是二十七，那个一消失了。但所有的计数都是相同的。所以你会认为这个除法应该可以工作。
- en: First of all can we even write this and will it even is it even expected to
    run is it broadcastable。 Let's determine if this result is broadcastable。 Pete
    at summit one is shape is twenty seven。 This is twenty seven by twenty seven。
    So twenty seven by twenty seven broadcasting into twenty seven。 So now rules of
    broadcasting number one align all the dimensions on the right done。
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们能否写出这个，它是否期望能够运行，是不是可广播的？让我们确定这个结果是否可广播。Pete在维度一的形状是二十七。这是二十七乘二十七。所以二十七乘二十七广播到二十七。因此广播的规则之一是将所有维度右对齐，完成。
- en: Now iteration over all the dimensions started from the right going to the left。
    All the dimensions math either be equal one of them must be one or one that does
    not exist。 So here they are equal here the dimension does not exist。 So internally
    what broadcasting will do is it will create a one here and then we see that one
    of them is a one and this will get copied and this will run this will broadcast。
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 现在从右到左对所有维度进行迭代。所有维度的数学关系要么相等，要么其中一个必须是1，或者一个不存在。所以在这里它们相等，这里维度不存在。因此，内部广播会在这里创建一个1，然后我们看到其中一个是1，这将被复制，这将运行并进行广播。
- en: Okay so you'd expect this to work because we are this broadcast and this we
    can divide this。 Now if I run this you'd expect it to work but it doesn't。 You
    actually get garbage you get a wrong result because this is actually a bug。 This
    keep them equal equals true makes it work。 This is a bug。
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，你会期望这个可以工作，因为我们正在广播，而这个我们可以进行除法。现在如果我运行这个，你会期望它能够工作，但实际上并不能。你会得到垃圾数据，结果是错误的，因为这实际上是一个bug。这个保留它等于true使得它能够工作。这是一个bug。
- en: In both cases we are doing the correct counts we are summing up across the rows
    but keep them as saving us and making it work。 So in this case I'd like you to
    encourage you to potentially like pause this video at this point and try to think
    about why this is buggy and why the keep them was necessary here。
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，我们都在正确地计算，我们在行间求和，但保留它们使得它能够工作。所以在这种情况下，我希望你能暂停这个视频，思考一下为什么这个是有bug的，为什么在这里需要保留它。
- en: Okay so the reason to do for this is I'm trying to hint it here when I was sort
    of giving you a bit of a hint on how this works。 This twenty seven vector internally
    inside the broadcasting this becomes a one by twenty seven and one by twenty seven
    is a row vector。
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，这样做的原因是我在试图暗示它，这样可以帮助你理解它是如何工作的。这个二十七的向量在广播内部变成了一个一乘二十七，而一乘二十七是一个行向量。
- en: And now we are dividing twenty seven by twenty seven by one by twenty seven
    and torch will replicate this dimension。 So basically it will take it will take
    this row vector and it will copy it vertically now。 Twenty seven times so the
    twenty seven by twenty seven lies exactly and element wise divides。 And so basically
    what's happening here is we are actually normalizing the columns instead of normalizing
    the rows。
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们是在将二十七除以二十七，再除以一再除以二十七，而torch会复制这个维度。因此，它会将这个行向量垂直复制，重复二十七次。所以这个二十七乘二十七的相应元素进行逐元素除法。因此这里发生的情况是我们实际上是在对列进行归一化，而不是对行进行归一化。
- en: So you can check what's happening here is that P at zero which is the first
    row of P。 That sum is not one it's seven。 It is the first column as an example
    that sums to one。 So to summarize where does the issue come from the issue comes
    from the silent adding of the dimension here because in broadcasting rules you
    align on the right and go from right to left and if the dimension doesn't exist
    you create it。 So that's where the problem happens we still did the counts correctly
    we did the counts across the rows and we got the counts on the right here as a
    column vector。
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可以检查这里发生了什么，P 在零处，即 P 的第一行。它的总和不是 1，而是 7。作为一个例子，第一列的总和是 1。那么总结一下，问题出在哪里？问题来自于这里的维度默默添加，因为在广播规则中，你是从右到左对齐，如果维度不存在，就会创建它。所以问题发生在这里，我们仍然正确地进行了计数，我们在行上进行了计数，并且得到了作为列向量的正确计数。
- en: But because the key things was true this dimension was discarded and now we
    just have a vector of twenty seven。 And because of broadcasting the way it works
    this vector of twenty seven suddenly becomes a row vector and then this row vector
    gets replicated vertically and that every single point we are dividing by the
    count in the opposite direction。
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 但由于关键因素，这个维度被丢弃了，现在我们只剩下一个二十七的向量。由于广播的工作方式，这个二十七的向量突然变成了一个行向量，然后这个行向量被垂直复制，而在每一个点我们都是在用相反方向的计数进行除法。
- en: So this thing just doesn't work this needs to be keep them simple in this case。
    So then we have that P at zero is normalized and conversely the first column you'd
    expect to potentially not be normalized。 And this is what makes it work。
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这个东西就是不工作，需要保持简单。在这种情况下，P 在零处是归一化的，而第一列你可能会期望它不被归一化。这就是它能够工作的原因。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_160.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_160.png)'
- en: So pretty subtle and hopefully this helps to scare you that you should have
    respect for broadcasting be careful check your work and understand how it works
    under the hood and make sure that it's broadcasting in the direction that you
    like。 Otherwise you're going to introduce very subtle bugs very hard to find bugs
    and just be careful。
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这很微妙，希望这能让你意识到你应该对广播保持尊重，要小心，检查你的工作，理解它是如何在底层工作的，并确保它的广播方向是你想要的。否则，你将引入非常微妙且难以找到的错误，要小心。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_162.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_162.png)'
- en: One more note on efficiency we don't want to be doing this here because this
    creates a completely new tensor that we store into P we prefer to use in place
    operations if possible。 So this would be an in place operation has the potential
    to be faster it doesn't create new memory under the hood and then let's erase
    this we don't need it and let's also just do fewer just so I'm not wasting space。
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 关于效率还有一点要注意，我们不想在这里做这个，因为这会创建一个完全新的张量并存储到 P 中，如果可能的话，我们更倾向于使用就地操作。因此，这将是一个就地操作，有潜力更快，它不会在底层创建新的内存，然后我们把这个擦掉，我们不需要它，同时也只做更少的操作，以免浪费空间。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_164.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_164.png)'
- en: Okay so we're actually in the pretty good spot now we trained a by gram language
    model and we trained it really just by counting how frequently any pairing occurs
    and then normalizing so that we get a nice property distribution。 So really these
    elements of this array P are really the parameters of our by gram language model
    giving us and summarizing the statistics of these by grams。
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，所以我们现在实际上处于一个相当不错的状态，我们训练了一个双元语言模型，主要是通过计算任意配对出现的频率，然后进行归一化，以获得良好的概率分布。因此，这个数组
    P 的元素实际上是我们双元语言模型的参数，总结了这些双元的统计信息。
- en: So we train the model and then we know how to sample from the model we just
    iteratively sampled the next character and feed it in each time and get a next
    character。 Now what I'd like to do is I'd like to somehow evaluate the quality
    of this model。 We'd like to somehow summarize the quality of this model into a
    single number how good is it at predicting the training set。 And as an example
    so in the training set we can evaluate now the training loss and this training
    loss is telling us about sort of the quality of this model in a single number
    just like we saw in micro grad。
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们训练模型，然后我们知道如何从模型中抽样，我们每次迭代地抽取下一个字符并将其输入，从而得到下一个字符。现在我想做的是评估这个模型的质量。我们希望将这个模型的质量总结成一个单一的数字，以便评估它在训练集上的预测能力。作为例子，在训练集中我们现在可以评估训练损失，这个训练损失告诉我们这个模型的质量，用一个单一的数字表示，就像我们在微梯度中看到的那样。
- en: So let's try to think through the quality of the model and how we would evaluate
    it。 Basically what we're going to do is we're going to copy paste this code that
    we previously used for counting。 Okay， and let me just print these by grams first
    we're going to use F strings and I'm going to print character one followed by
    character two these are the by grams。 And then I don't want to do it for all the
    words just do first three words。
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试着考虑模型的质量以及如何评估它。基本上我们要做的是复制之前用于计数的代码。好的，让我先打印这些二元组，我们将使用F字符串打印字符一后跟字符二，这就是二元组。然后我不想为所有单词做，只需做前三个单词。
- en: So here we have Emma Olivia and Eva by grams。 Now what we'd like to do is we'd
    like to basically look at the probability that the model assigns to every one
    of these by grams。 So in other words we can look at the probability which is summarized
    in the matrix B of Ix1， Ix2。
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这里我们有Emma、Olivia和Eva的二元组。现在我们希望基本上查看模型为每个二元组分配的概率。换句话说，我们可以查看在矩阵B中的概率，Ix1，Ix2。
- en: And then we can print it here as probability。 And because these probabilities
    are way too large let me percent or call on point 4f to like truncate it a bit。
    So what do we have here right we're looking at the probabilities that the model
    assigns to everyone on these by grams in the data set。
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以在这里打印出概率。因为这些概率实在太大了，让我用百分比或者调用点4f来缩短一下。那么我们现在看到的是什么呢？我们在查看模型在数据集中为这些二元组分配的概率。
- en: And so we can see some of them are 4% 3% etc。 just to have a measuring stick
    in our mind by the way。 We have 27 possible characters or tokens and if everything
    was equally likely then you'd expect all these probabilities to be 4% roughly。
    So anything above 4% means that we've learned something useful from these by grams
    statistics。 And you see that roughly some of these are 4% but some of them are
    as high as 40%， 35% and so on。
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 因此我们可以看到一些概率为4%、3%等。顺便提一下，我们有27个可能的字符或符号，如果一切都是同样可能的，那么你会期望这些概率大约为4%。所以任何超过4%的概率意味着我们从这些二元组统计中学到了有用的东西。你会看到大致上有一些是4%，但有一些高达40%、35%等。
- en: So you see that the model actually assigned a pretty high probability to whatever
    is in the training set。 And so that's a good thing。 Basically if you have a very
    good model you'd expect that these probabilities should be near 1 because that
    means that your model is correctly predicting what's going to come next。
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你会看到模型实际上给训练集中的任何内容分配了相当高的概率。这是好事。基本上，如果你有一个非常好的模型，你会期望这些概率接近1，因为这意味着你的模型正确预测了接下来会发生什么。
- en: especially in the training set where you train your model。 So now we'd like
    to think about how can we summarize these probabilities into a single number that
    measures the quality of this model。
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是在训练集上，你训练模型的地方。因此，现在我们希望思考如何将这些概率总结成一个数字来衡量模型的质量。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_166.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_166.png)'
- en: Now when you look at the literature into maximum likelihood estimation and statistical
    modeling and so on。
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 现在当你查看最大似然估计和统计建模等文献时。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_168.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_168.png)'
- en: you'll see that what's typically used here is something called the likelihood。
    And the likelihood is the product of all of these probabilities。 And so the product
    of all of these probabilities is the likelihood and it's really telling us about
    the probability of the entire data set assigned by the model that we've trained。
    And that is the measure of quality。 So the product of these should be as high
    as possible。
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到，这里通常使用的是一种叫做似然的东西。似然是所有这些概率的乘积。因此，所有这些概率的乘积就是似然，它实际上告诉我们模型所训练的整个数据集的概率。这是质量的衡量标准。因此，这些的乘积应该尽可能高。
- en: When you are training the model and when you have a good model。 your product
    of these probabilities should be very high。 Now because the product of these probabilities
    is an unwieldy thing to work with。 you can see that all of them are between 0
    and 1。 So your product of these probabilities will be a very tiny number。
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 当你训练模型并且有一个好的模型时，这些概率的乘积应该非常高。由于这些概率的乘积是一个难以处理的东西，你可以看到它们都在0到1之间。因此，这些概率的乘积将是一个非常小的数字。
- en: So for convenience what people work with usually is not the likelihood。 but
    they work with what's called the log likelihood。 So the product of these is the
    likelihood。 To get the log likelihood， we just have to take the log of the probability。
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 所以为了方便，人们通常处理的不是似然，而是称为对数似然的东西。因此，这些的乘积是似然。要得到对数似然，我们只需对概率取对数。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_170.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_170.png)'
- en: And so the log of the probability here， the log of x from 0 to 1， the log is
    a。 you see here monotonic transformation of the probability， where if you pass
    in 1， you get 0。 So probability 1 gets your log probability of 0。 And then as
    you go lower and lower probability。 the log will grow more and more negative until
    all the way to negative infinity at 0。
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这里概率的对数，从0到1的对数，你会看到这是一个概率的单调变换，当你传入1时，你得到0。所以概率1会给你对数概率0。然后，随着概率越来越低，对数将变得越来越负，直到在0时达到负无穷大。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_172.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_172.png)'
- en: So here we have a log problem， which is really just a torch dot log of probability。
    Let's print it out to get a sense of what that looks like。 Log problem also 0。4f。
    Okay。 so as you can see， when we plug in numbers that are very close， some of
    our higher numbers。 we get closer and closer to 0。 And then if we plug in very
    bad probabilities。
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这里我们有一个对数问题，这实际上只是概率的 torch dot log。让我们打印出来，看看那是什么样子。对数问题也是0.4f。好的，正如你所看到的，当我们输入非常接近的数字时，一些较大的数字，我们会越来越接近0。如果我们输入非常糟糕的概率。
- en: we get more and more negative number。 That's bad。 So。 and the reason we work
    with this is for large extent convenience， right？
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到越来越多的负数。这不好。所以，我们之所以处理这个，主要是出于方便，对吧？
- en: Because we have mathematically that if you have some product a times b times
    c of all these probabilities。 right， or the likelihood is the product of all these
    probabilities。 then the log of these is just log of a plus log of b plus log of
    c。 If you remember your logs from your high school or undergrad and so on。
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们在数学上知道，如果你有一些概率的乘积 a 乘以 b 乘以 c，或者说似然是所有这些概率的乘积，那么这些的对数就是 a 的对数加上 b 的对数加上
    c 的对数。如果你还记得高中或本科时学过的对数。
- en: So we have that basically the likelihood of the product of probabilities。 the
    log likelihood is just the sum of the logs of the individual probabilities。 So
    log likelihood starts at 0。 And then log likelihood here， we can just accumulate
    simply。 And then the end， we can print this。 Print the log likelihood。 Have strings。
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 所以基本上，概率的乘积的似然，对数似然就是各个概率的对数之和。所以对数似然从0开始。然后对数似然在这里，我们可以简单地累积。最后，我们可以打印出来。打印对数似然。拥有字符串。
- en: maybe you're familiar with this。 So log likelihood is negative 38。 Okay。 Now，
    we actually want。 so how high can log likelihood get？ It can go to zero。 So when
    all the probabilities are one。 log likelihood will be zero。 And then when all
    the probabilities are lower。 this will grow more and more negative。 Now， we don't
    actually like this because what we'd like is a loss function。
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 也许你对此比较熟悉。所以对数似然是负38。好的，现在我们实际上想知道，对数似然能达到多高？它可以达到零。所以当所有概率都是1时，对数似然将为零。当所有概率更低时，这个值将变得越来越负。现在，我们其实不喜欢这样，因为我们希望有一个损失函数。
- en: And a loss function has the semantics that low is good， because we're trying
    to minimize the loss。 So we actually need to invert this。 And that's what gives
    us something called the negative log likelihood。 Negative log likelihood is just
    negative of the log likelihood。 These are F strings， by the way。 if you'd like
    to look this up。 Negative log likelihood equals。
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数的语义是低是好的，因为我们试图最小化损失。所以我们实际上需要对其进行反转。这就是我们得到所谓负对数似然的原因。负对数似然只是对数似然的负值。顺便提一下，这些是F字符串，如果你想查一下。负对数似然等于。
- en: So negative log likelihood now is just negative of it。 And so the negative log
    likelihood is a very nice loss function because the lowest it can get is zero。
    And the higher it is， the worse off the predictions are that you're making。 And
    then one more modification to this that sometimes people do is that for convenience。
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，负对数似然现在只是它的负值。因此，负对数似然是一个非常好的损失函数，因为它能达到的最低值是零。而且它越高，你所做的预测就越糟糕。然后，有一个人们有时会做的修改，为了方便。
- en: they actually like to normalize by， they like to make it an average instead
    of a sum。 And so here。 let's just keep some counts as well。 So n plus equals one
    starts at zero。 And then here。 we can have sort of like a normalized log likelihood。
    If we just normalize it by the count。 then we will sort of get the average log
    likelihood。
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 他们实际上喜欢进行归一化，他们更喜欢将其变为平均值而不是总和。因此在这里，我们也可以保持一些计数。所以n加等于一，从零开始。然后在这里，我们可以得到类似于归一化对数似然的东西。如果我们只是通过计数来归一化，那么我们将得到平均对数似然。
- en: So this would be usually our loss function here。 This is what we would use。
    So our loss function for the training set assigned by the model is 2。4。 That's
    the quality of this model。 And the lower it is， the better off we are。 And the
    higher it is。 the worse off we are。 And the job of our training is to find the
    parameters that minimize the negative log likelihood loss。
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这通常就是我们的损失函数。这就是我们要使用的。因此，模型为训练集分配的损失函数是2.4。这是该模型的质量。它越低，我们的效果就越好；而它越高，我们的效果就越差。我们训练的任务是找到能够最小化负对数似然损失的参数。
- en: And that would be like a high quality model。 Okay， so to summarize， I actually
    wrote it out here。
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 这将像是一个高质量的模型。好吧，简单总结一下，我实际上在这里写下来了。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_174.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_174.png)'
- en: So our goal is to maximize likelihood， which is the product of all the probabilities
    assigned by the model。 And we want to maximize this likelihood with respect to
    the model parameters。 In our case。 the model parameters here are defined in the
    table。
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们的目标是最大化似然，即模型分配的所有概率的乘积。我们希望在模型参数方面最大化这个似然。在我们的例子中，这里的模型参数在表中定义。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_176.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_176.png)'
- en: These numbers， the probabilities are the model parameters sort of in our Bagram
    language models so far。
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数字，即概率，是到目前为止我们Bagram语言模型中的模型参数。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_178.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_178.png)'
- en: But you have to keep in mind that here we are storing everything in a table
    format。 the probabilities。 But what's coming up as a brief preview is that these
    numbers will not be kept explicitly。 but these numbers will be calculated by a
    neural network。 So that's coming up。 And we want to change and tune the parameters
    of these neural works。
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 但你必须记住，这里我们将所有内容存储在表格格式中，即概率。但是，接下来会简要预览的是，这些数字不会被显式保留，而是由神经网络计算得出。所以这是即将发生的。我们想要改变和调整这些神经网络的参数。
- en: We want to change these parameters to maximize the likelihood， the product of
    the probabilities。
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望改变这些参数以最大化似然，即概率的乘积。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_180.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_180.png)'
- en: Now， maximizing the likelihood is equivalent to maximizing the log likelihood
    because log is a monotonic function。
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，最大化似然等同于最大化对数似然，因为对数是单调函数。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_182.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_182.png)'
- en: Here's the graph of log。 And basically， all it is doing is it's just scaling
    your。 you can look at it as just a scaling of the loss function。
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对数的图形。基本上，它所做的就是缩放你的，你可以将其视为对损失函数的缩放。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_184.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_184.png)'
- en: And so the optimization problem here and here are actually equivalent because
    this is just a scaling。 You can look at it that way。 And so these are two identical
    optimization problems。 Maximizing the log likelihood is equivalent to minimizing
    the negative log likelihood。
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这里的优化问题实际上是等价的，因为这只是一个缩放。你可以这么看。所以这两个优化问题是相同的。最大化对数似然等同于最小化负对数似然。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_186.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_186.png)'
- en: And then in practice， people actually minimize the average negative log likelihood
    to get numbers like 2。4。 And then this summarizes the quality of your model。 And
    we'd like to minimize it and make it as small as possible。 And the lowest it can
    get is zero。 And the lower it is， the better off your model is because it's signing
    high probabilities to your data。
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际操作中，人们实际上最小化平均负对数似然，以得到像2.4这样的数字。然后这总结了你模型的质量。我们希望最小化它，并尽可能小。它的最低值可以是零。数值越低，模型越好，因为它给你的数据分配了高概率。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_188.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_188.png)'
- en: Now let's estimate the probability over the entire training set just to make
    sure that we get something around 2。4。
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们估计整个训练集上的概率，以确保我们得到大约2.4的结果。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_190.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_190.png)'
- en: Let's run this over the entire， oops， let's take out the print statement as
    well。 Okay， 2。45 or the entire training set。 Now what I'd like to show you is
    that you can actually evaluate the probability for any word that you want。 Like
    for example， if we just test a single word， Andre， and bring back the print statement。
    then you see that Andre is actually kind of like an unlikely word。 And like on
    average。
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在整个训练集上运行这个，哦，先把打印语句去掉。好的，2.45或整个训练集。现在我想给你展示的是，你实际上可以评估你想要的任何单词的概率。比如说，如果我们只测试一个单词，Andre，并恢复打印语句。那么你会看到Andre实际上是一个不太可能的单词。平均而言。
- en: we take three log probability to represent it。 And roughly that's because EJ
    apparently is very uncommon as an example。 Now think through this， when I take
    Andre and I append Q and I test the probability of it， Andre Q。 We actually get
    infinity。 And that's because JQ has a 0% probability according to our model。 So
    the log likelihood。 So the log of zero will be negative infinity。 We get infinite
    loss。
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 我们取三个对数概率来表示它。大致上是因为EJ显然是一个非常不常见的例子。现在思考一下，当我把Andre和Q拼接在一起并测试它的概率，Andre Q。我们实际上得到的是无限大。这是因为根据我们的模型，JQ的概率是0%。因此，对数似然，零的对数将是负无穷大。我们得到无限损失。
- en: So this is kind of undesirable， right？ Because we plugged in a string that could
    be like a somewhat reasonable name。 But basically what this is saying is that
    this model is exactly 0% likely to predict this name。 And our loss is infinity
    on this example。 And really what the reason for that is that J is followed by
    Q zero times。 where is Q？ JQ is zero。 And so JQ is 0% likely。 So it's actually
    kind of gross and people don't like this too much。
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这有点不理想，对吧？因为我们输入了一个可能是合理名字的字符串。但基本上，这表示该模型对预测这个名字的可能性为0%。而在这个例子中，我们的损失是无限大的。真正的原因是J后面跟着Q的次数为0。Q在哪里？JQ为0。因此，JQ的可能性是0%。这实际上是有点恶心，人们对此并不太喜欢。
- en: To fix this， there's a very simple fix that people like to do to sort of like
    smooth out your model a little bit。
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 要解决这个问题，有一个非常简单的解决方案，大家喜欢用它来稍微平滑一下你的模型。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_192.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_192.png)'
- en: And it's called model smoothing。 And roughly what's happening is that we will
    add some fake accounts。
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为模型平滑。大致上发生的事情是我们会增加一些虚假的计数。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_194.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_194.png)'
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_195.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_195.png)'
- en: So imagine adding a count of one to everything。 So we add a count of one like
    this。 And then we recalculate the probabilities。
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 所以想象一下，对所有内容加上一个计数。所以我们像这样加上一个计数。然后我们重新计算概率。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_197.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_197.png)'
- en: And that's model smoothing。 And you can add as much as you like。 You can add
    five and it will give you a smoother model。 And the more you add here。 the more
    uniform model you're going to have。 And the less you add。 the more peaked model
    you're going to have， of course。 So one is like a pretty decent count to add。
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是模型平滑。你可以根据需要添加任意多的内容。你可以添加五个，它会给你一个更平滑的模型。而你添加得越多，模型就会越均匀。当然，添加得越少，模型就会越尖锐。所以，一个适度的计数是相当不错的选择。
- en: And that will ensure that there will be no zeros in our probability matrix P。
    And so this will of course change the generations a little bit。 In this case it
    didn't。 but it in principle it could。
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 这将确保我们的概率矩阵 P 中没有零。因此，这当然会稍微改变生成结果。在这种情况下没有，但原则上它可能会。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_199.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_199.png)'
- en: But what that's going to do now is that nothing will be infinity unlikely。 So
    now our model will predict some other probability。 And we see that JQ now has
    a very small probability。 So the model still finds it very surprising that this
    was a word or a by gram。
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 但这将意味着没有什么是无穷小可能的。因此，现在我们的模型将预测其他概率。我们看到 JQ 现在的概率非常小。因此，模型仍然发现这个单词或字节是非常令人惊讶的。
- en: but we don't get negative infinity。
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们不会得到负无穷大。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_201.png)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_201.png)'
- en: So it's kind of like a nice fix that people like to apply sometimes and it's
    called models moving。
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是一种人们有时喜欢应用的不错修复，称为模型平滑。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_203.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_203.png)'
- en: Okay， so we've now trained a respectable by gram character level language model。
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们现在已经训练了一个相当不错的字节级字符语言模型。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_205.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_205.png)'
- en: And we saw that we both sort of trained the model by looking at the counts of
    all the by grams and normalizing the rows to get probability distributions。 We
    saw that we can also then use those parameters of this model to perform sampling
    of new words。
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，我们都通过查看所有的字节计数并归一化行来训练模型，以获得概率分布。我们还看到，我们可以使用这个模型的参数来对新单词进行采样。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_207.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_207.png)'
- en: So we sample new names according to those distributions。
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们根据这些分布来采样新的名称。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_209.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_209.png)'
- en: And we also saw that we can evaluate the quality of this model。 And the quality
    of this model is summarized in a single number which is the negative log likelihood。
    And the lower this number is， the better the model is because it is giving high
    probabilities to the actual next characters in all the by grams in our training
    set。 So that's all well and good， but we've arrived at this model explicitly by
    doing something that felt sensible。
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还看到，我们可以评估这个模型的质量。这个模型的质量用一个数字来总结，即负对数似然。这个数字越低，模型越好，因为它对训练集中所有字节的实际下一个字符赋予了高概率。所以这一切都很好，但我们通过做一些合理的事情显式地得到了这个模型。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_211.png)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_211.png)'
- en: We were just performing counts and then we were normalizing those counts。
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只是进行了计数，然后对这些计数进行了归一化。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_213.png)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_213.png)'
- en: Now what I would like to do is I would like to take an alternative approach。
    We will end up in a very very similar position， but the approach will look very
    different。 Because I would like to cast the problem of by gram character level
    language modeling into the neural network framework。 And in the neural network
    framework we're going to approach things slightly differently。
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我想做的是采取另一种方法。我们将达到一个非常相似的位置，但方法看起来会非常不同。因为我想将字节级字符语言建模的问题转化为神经网络框架。在神经网络框架中，我们将略有不同地处理这些问题。
- en: but again end up in a very similar spot。 I'll go into that later。 Now our neural
    network is going to be a still a by gram character level language model。 So it
    receives a single character as an input。 Then there's neural network with some
    weights or some parameters W。 And it's going to output the probability distribution
    over the next character in a sequence。
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 但最后又会陷入非常相似的境地。我稍后会讲到这一点。现在我们的神经网络仍然是一个字节级字符语言模型。因此，它接收一个字符作为输入。接着有一个带有一些权重或参数
    W 的神经网络。它将输出序列中下一个字符的概率分布。
- en: It's going to make guesses as to what is likely to follow this character that
    was input to the model。 And then in addition to that we're going to be able to
    evaluate any setting of the parameters of the neural network。 because we have
    a loss function。 The negative lot likelihood。 So we're going to take a look at
    its probability distributions and we're going to use the labels。
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 它将对输入到模型中的字符后面可能出现的字符进行猜测。此外，我们还将能够评估神经网络参数的任何设置，因为我们有一个损失函数。负对数似然。所以我们将查看其概率分布，并使用这些标签。
- en: Which are basically just the identity of the next character in that by gram，
    the second character。 So knowing what second character actually comes next in
    the by gram allows us to then look at what how high probability the model assigns
    to that character。
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 这些标签基本上只是该双字组中下一个字符的身份，即第二个字符。因此，知道双字组中实际接下来的第二个字符，允许我们查看模型对该字符分配的高概率。
- en: And then we of course want the probability to be very high。 And that is another
    way of saying that the loss is low。 So we're going to use gradient based optimization
    then to tune the parameters of this network。 Because we have the loss function
    and we're going to minimize it。
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们当然希望概率非常高。这也是说损失低的另一种方式。所以我们将使用基于梯度的优化来调整这个网络的参数。因为我们有损失函数，并将其最小化。
- en: So we're going to tune the weights so that the neural net is correctly predicting
    the probabilities for the next character。 So let's get started。 The first thing
    I want to do is I want to compile the training set of this neural network。 So
    create the training set of all the by grams。
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们要调整权重，使得神经网络能够正确预测下一个字符的概率。我们开始吧。首先我想做的是编译这个神经网络的训练集。所以创建所有的双字组的训练集。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_215.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_215.png)'
- en: Okay。 And here I'm going to copy paste this code because this code iterates
    over all the by grams。
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。这里我将复制粘贴这段代码，因为这段代码遍历所有的双字组。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_217.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_217.png)'
- en: So here we start with the words we iterate over all the by grams and previously
    as you recall we did the counts。 But now we're not going to do counts。 We're just
    creating a training set。 Now this training set will be made up of two lists。 We
    have the inputs and the targets， the labels。 And these by grams will denote x，
    y。 Those are the characters， right？
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这里我们从单词开始，我们遍历所有的双字组，之前你还记得我们进行了计数。但现在我们不再计数。我们只是创建一个训练集。这个训练集将由两个列表组成。我们有输入和目标，标签。这些双字组将表示x，y。它们就是字符，对吧？
- en: And so we're given the first character of the by gram and then we're trying
    to predict the next one。 Both of these are going to be integers。 So here we'll
    take x's that append is just x1。 Why is that append ix2？ And then here we actually
    don't want lists of integers。 We will create tensors out of these。 So x is torched
    dot tensor of x's and y is torched dot tensor of y's。
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们得到了双字组的第一个字符，然后我们试图预测下一个字符。这两个都将是整数。所以这里我们将x的追加就是x1。为什么追加ix2？然后这里我们实际上不想要整数列表。我们将从这些中创建张量。所以x是torch.tensor的x，y是torch.tensor的y。
- en: And then we don't actually want to take all the words just yet because I want
    everything to be manageable。 So let's just do the first word which is Emma。 And
    then it's clear what these x's and y's would be。 Here let me print character one
    character two just so you see what's going on here。 So the by grams of these characters
    is dot E E M M M A dot。
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们实际上不想现在就取所有的单词，因为我希望一切都能管理。所以我们先做第一个单词Emma。然后很明显这些x和y会是什么。让我打印一下字符一字符二，让你看到这里发生了什么。所以这些字符的双字组是.dot
    E E M M M A.dot。
- en: So this single word as I mentioned has one， two， three， four， five examples
    for our neural network。 There are five separate examples in Emma。 And those examples
    are here。 When the input to the neural network is integer zero。 the desired label
    is integer five which corresponds to E。
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这个单词，正如我提到的，有一个，两个，三个，四个，五个例子供我们的神经网络使用。Emma中有五个独立的例子。这里是这些例子。当神经网络的输入为整数零时，期望的标签是整数五，对应于E。
- en: When the input to the neural network is five， we want its weights to be arranged
    so that 13 gets a very high probability。 When 13 is put in， we want 13 to have
    a high probability。 When 13 is put in。 we also want one to have a high probability。
    When one is input。 we want zero to have a very high probability。 So there are
    five separate input examples to a neural net in this data set。
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 当神经网络的输入为五时，我们希望其权重安排得当，以便13获得非常高的概率。当13被输入时，我们希望13具有高概率。当输入13时，我们也希望1具有高概率。当输入1时，我们希望0具有非常高的概率。所以在这个数据集中，神经网络有五个独立的输入示例。
- en: I wanted to add a tangent of a note of caution to be careful with a lot of the
    APIs of some of these frameworks。 You saw me silently use torch dot tensor with
    a lowercase t and the output looked right。
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 我想添加一个警告，提醒对某些框架的API要小心。你看到我默默地使用小写t的torch张量，输出看起来正确。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_219.png)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_219.png)'
- en: But you should be aware that there's actually two ways of constructing a tensor。
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 但是你应该知道其实有两种构建张量的方法。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_221.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_221.png)'
- en: There's a torch dot lowercase tensor and there's also a torch dot capital tensor
    class which you can also construct。
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个torch的小写张量，还有一个torch的大写张量类，你也可以构建。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_223.png)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_223.png)'
- en: So you can actually call both。 You can also do torch dot capital tensor and
    you get an Xs and Ys as well。 So that's not confusing at all。
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你实际上可以调用两者。你也可以使用torch的大写张量，你同样会得到Xs和Ys。因此，这一点并不混淆。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_225.png)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_225.png)'
- en: There are threads on what is the difference between these two。 Unfortunately。
    the docs are just not clear on the difference。
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 有关这两者之间区别的讨论很多。不幸的是，文档对此并不清晰。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_227.png)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_227.png)'
- en: When you look at the docs of lowercase tensor， construct tensor with no autograph
    history by copying data。
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 当你查看小写张量的文档时，构建张量时没有签名历史是通过复制数据。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_229.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_229.png)'
- en: It's just like it doesn't make sense。
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 这听起来完全没有意义。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_231.png)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_231.png)'
- en: So the actual difference as far as I can tell is explained eventually in this
    random thread that you can Google。 And really it comes down to I believe that，
    um， what is this？
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 所以据我所知，实际区别最终在这个随机的讨论中有解释，你可以谷歌一下。真的，我认为这主要是，嗯，这是什么？
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_233.png)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_233.png)'
- en: Torch dot tensor in first the D type， the data type automatically。 Well torch
    dot tensor just returns a float tensor。 I would recommend stick to torch dot lowercase
    tensor。
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: torch张量首先自动识别数据类型。好吧，torch张量只返回float张量。我建议你坚持使用torch的小写张量。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_235.png)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_235.png)'
- en: So indeed we see that when I construct this with a capital T， the data type
    here of Xs is float 32。 But torch dot lowercase tensor。 You see how it's now X
    dot D type is now integer。 So it's advised that you use lowercase T and you can
    read more about it if you like in some of these threads。 But basically I'm pointing
    out some of these things because I want to caution you and I want you to get used
    to reading a lot of documentation and reading through a lot of Q and A's and threads
    like this。
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 所以确实，当我用大写T构建时，Xs的数据类型是float 32。但是torch的小写张量，你可以看到现在Xs的数据类型是整数。因此建议你使用小写T，如果你想了解更多，可以在一些讨论中阅读。但基本上，我提到这些是因为我想提醒你，并希望你习惯于阅读大量文档，以及像这样的问答和讨论。
- en: And， you know， some of this stuff is unfortunately not easy and not very well
    documented and you have to be careful out there。 What we want here is integers
    because that's what makes sense。 And so lowercase tensor is what we are using。
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 而且，你知道，有些东西确实不容易，而且文档并不充分，你必须谨慎处理。我们在这里需要整数，因为这才有意义。所以我们使用小写张量。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_237.png)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_237.png)'
- en: Okay， now we want to think through how we're going to feed in these examples
    into a neural network。 Now it's not quite as straightforward as plugging it in
    because these examples right now are integers。 So there's like a zero five or
    thirteen。 It gives us the index of the character and you can't just plug an integer
    index into a neural net。 These neural nets， right， are sort of made up of these
    neurons and these neurons have weights。
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在我们想要考虑如何将这些示例输入到神经网络中。实际上，这并不像直接插入那么简单，因为这些示例现在是整数。所以像零、五或十三这样的数字。这给出了字符的索引，而你不能仅仅将一个整数索引插入到神经网络中。这些神经网络是由这些神经元组成的，这些神经元有权重。
- en: And as you saw in micro grad， these weights act multiplicatively on the inputs
    W X plus B。 there's 10 Hs and so on。 And so it doesn't really make sense to make
    an input neuron take on integer values that you feed in and then multiply on with
    weights。 So instead， a common way of encoding integers is what's called one-hot
    encoding。 In one-hot encoding， we take an integer like thirteen and we create
    a vector that is all zeros except for the thirteenth dimension which returned
    to a one。
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在微观梯度中看到的，这些权重对输入W X加B是乘法作用的。有10个H等等。因此，让输入神经元接受整数值并与权重相乘并没有多大意义。因此，一种常见的编码整数的方法是所谓的独热编码。在独热编码中，我们取一个整数，比如十三，然后创建一个向量，除了第十三维外，其他都是零。
- en: And then that vector can feed into a neural net。
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 然后该向量可以输入到神经网络中。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_239.png)'
  id: totrans-367
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_239.png)'
- en: Now， conveniently， PyTorch actually has something called the one-hot function
    inside Torch and in functional。 It takes a tensor made up of integers long as
    an integer。 And it also takes a number of classes which is how large you want
    your tensor vector to be。
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，方便的是，PyTorch实际上在Torch和functional中有一个名为one-hot的函数。它接受由整数长构成的张量。它还接受类别数量，即你希望张量向量的大小。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_241.png)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_241.png)'
- en: So here， let's import Torch。 and in that functional SF， this is a common way
    of importing it。 And then let's do F dot one-hot and we feed in the integers that
    we want to encode。 So we can actually feed in the entire array of Xs。 And we can
    tell it that num class is 27。 So it doesn't have to try to guess it。 It may have
    guessed that it's only 13 and would give us an incorrect result。
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在这里，让我们导入Torch。在这个功能SF中，这是一个常见的导入方式。然后让我们做F.dot.one-hot，输入我们想要编码的整数。因此，我们实际上可以输入整个Xs数组。我们可以告诉它num
    class是27。这样它就不必去猜测了。它可能会猜测只有13，从而给出错误的结果。
- en: So this is the one-hot。 Let's call this X in for X encoded。 And then we see
    that X encoded that shape is 5 by 27。 And we can also visualize it PLT。IMS show
    of X-ink to make it a little bit more clear because this is a little messy。
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是独热编码。我们称之为X输入，表示X编码。然后我们看到X编码的形状是5乘27。我们还可以用PLT.IMS可视化X输入，使其更清晰，因为这有点混乱。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_243.png)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_243.png)'
- en: So we see that we've encoded all the five examples into vectors。 We have five
    examples。 so we have five rows and each row here is now an example into a neural
    net。 And we see that the appropriate bit is turned on as a one and everything
    else is zero。 So here。 for example， the zero-th bit is turned on， the fifth bit
    is turned on。
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们看到我们已经将所有五个示例编码为向量。我们有五个示例，所以我们有五行，每一行现在都是一个输入到神经网络的示例。我们看到相应的比特被打开为1，其他都是零。因此，比如这里，第零位被打开，第五位被打开。
- en: the thirteenth bits are turned on for both of these examples。
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 第十三位对于这两个示例都是开启的。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_245.png)'
  id: totrans-375
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_245.png)'
- en: And the first bit here is turned on。 So that's how we can encode integers into
    vectors。 And then these vectors can feed in to neural nets。
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的第一个比特被打开。这就是我们如何将整数编码为向量。然后这些向量可以输入到神经网络中。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_247.png)'
  id: totrans-377
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_247.png)'
- en: One more issue to be careful with here， by the way， is let's look at the data
    type of econcoding。 We always want to be careful with data types。 What would you
    expect X encoding data type to be？
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 另外一个需要注意的问题是，让我们看一下编码的数据类型。我们总是要小心数据类型。你期望X编码的数据类型是什么？
- en: When we're plugging numbers into neural nets， we don't want them to be integers。
    We want them to be floating point numbers that can take on various values。 But
    the dtype here is actually 64-bit integer。
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将数字输入到神经网络时，我们不希望它们是整数。我们希望它们是可以取不同值的浮点数。但这里的 dtype 实际上是 64 位整数。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_249.png)'
  id: totrans-380
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_249.png)'
- en: And the reason for that I suspect is that one-hot received a 64-bit integer
    here。 and it returned the same data type。
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 我怀疑这是因为 one-hot 接收了一个 64 位整数，并返回了相同的数据类型。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_251.png)'
  id: totrans-382
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_251.png)'
- en: And when you look at the signature of one-hot， it doesn't even take a dtype。
    a desired data type of the output tensor。
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 当你查看 one-hot 的签名时，它甚至不需要 dtype。所需的输出张量的数据类型。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_253.png)'
  id: totrans-384
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_253.png)'
- en: And so we can't， in a lot of functions in Torter， we'd be able to do something
    like dtype equals Torx。flow32， which is what we want， but one-hot does not support
    that。 So instead。 we're going to want to cast this to float like this。
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在 Torter 的许多函数中，我们能够做类似 dtype 等于 Torx。flow32 的事情，这正是我们想要的，但 one-hot 不支持这个。所以我们将把它转换为浮点数，像这样。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_255.png)'
  id: totrans-386
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_255.png)'
- en: So that these， everything is the same。
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这样一来，所有的内容都是一样的。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_257.png)'
  id: totrans-388
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_257.png)'
- en: Everything looks the same， but the dtype is float32。 And floats can feed into
    neural nets。 So now let's construct our first neuron。 This neuron will look at
    these input vectors。 And as you remember from Micrograd， these neurons basically
    perform a very simple function。 WX plus B， where WX is a dot product。 Right？ So
    we can achieve the same thing here。
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 一切看起来都一样，但 dtype 是 float32。浮点数可以输入到神经网络中。因此现在让我们构建第一个神经元。这个神经元将查看这些输入向量。如你所记得的，来自
    Micrograd，这些神经元基本上执行一个非常简单的函数。WX 加 B，其中 WX 是点积。对吧？所以我们可以在这里实现同样的功能。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_259.png)'
  id: totrans-390
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_259.png)'
- en: Let's first define the weights of this neuron， basically。 Where are the initial
    weights at initialization for this neuron？
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们定义这个神经元的权重，基本上。这个神经元在初始化时的初始权重在哪里？
- en: Let's initialize them with torch dot random。
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用 torch dot random 初始化它们。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_261.png)'
  id: totrans-393
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_261.png)'
- en: Torx dot random is fills a tensor with random numbers。
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: Torx dot random 是用随机数填充张量。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_263.png)'
  id: totrans-395
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_263.png)'
- en: drawn from a normal distribution。 And a normal distribution has a probability
    density function like this。 And so most of the numbers drawn from this distribution
    will be around zero。 but some of them will be as high as almost three and so on。
    And very few numbers will be above three in magnitude。
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 从正态分布中抽取。正态分布的概率密度函数是这样的。因此，从这个分布中抽取的大多数数字将围绕零，但其中一些将高达接近三等等。绝大多数数字的绝对值将高于三的情况非常少。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_265.png)'
  id: totrans-397
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_265.png)'
- en: So we need to take size as an input here。 And I'm going to use size as to be
    27 by one。 So 27 by one， and then let's visualize W。
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们需要在这里将 size 作为输入。我将使用 size 为 27 乘 1。因此是 27 乘 1，然后让我们可视化 W。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_267.png)'
  id: totrans-399
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_267.png)'
- en: So W is a column vector of 27 numbers。
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 所以 W 是一个包含 27 个数字的列向量。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_269.png)'
  id: totrans-401
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_269.png)'
- en: And these weights are then multiplied by the inputs。
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 然后这些权重与输入相乘。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_271.png)'
  id: totrans-403
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_271.png)'
- en: So now to perform this multiplication， we can take X encoding， and we can multiply
    it with W。 This is a matrix multiplication operator in PyTorch。
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 现在为了执行这个乘法，我们可以将 X 编码，并且可以将其与 W 相乘。这是 PyTorch 中的矩阵乘法操作符。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_273.png)'
  id: totrans-405
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_273.png)'
- en: And the output of this operation is five by one。
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 此操作的输出是 5 乘 1。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_275.png)'
  id: totrans-407
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_275.png)'
- en: The reason it's five by five is the following。 We took X encoding， which is
    five by 27。 and we multiplied it by 27 by one。 And in matrix multiplication。 you
    see that the output will become five by one， because these 27 will multiply and
    add。 So basically what we're seeing here out of this operation is we are seeing
    the five。
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 五乘以五的原因如下。我们取X编码，五乘以27，然后将其乘以27乘以1。在矩阵乘法中，你会看到输出变成五乘以1，因为这27将进行乘法和加法。因此，基本上我们在这个操作中看到的是五个。
- en: activations of this neuron on these five inputs。 And we've evaluated all of
    them in parallel。 So we can feed in just a single input to the single neuron。
    We fed in simultaneously all the five inputs into the same neuron。 And in parallel。
    PyTorch has evaluated the WX plus B， but here it's just the WX。 There's no bias。
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 我们并行评估了所有这些。因此，我们可以只向单个神经元输入一个输入。我们同时将所有五个输入输入到同一个神经元中。并行的，PyTorch已经评估了WX加B，但在这里它只是WX。没有偏差。
- en: It has valued W times X for all of them independently。 Now instead of a single
    neuron though。 I would like to have 27 neurons。 And I'll show you in a second
    why I've got 27 neurons。 So instead of having just a one here， which is indicating
    this presence of one single， neuron。 we can use 27。 And then when W is 27 by 27，
    this will in parallel evaluate all the 27 neurons on all。
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 它对所有的W乘以X进行了独立计算。现在，我想要27个神经元，而不是单个神经元。我稍后会告诉你为什么我有27个神经元。所以不是只有一个在这里，表示一个单一神经元的存在，而是我们可以使用27个。当W是27乘以27时，这将并行评估所有27个神经元在所有五个输入上的激活。
- en: the five inputs， giving us a much better， much， much bigger result。
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 五个输入，给我们一个更好，更大，更大的结果。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_277.png)'
  id: totrans-412
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_277.png)'
- en: So now what we've done is five by 27 multiplied， 27 by 27， and the output of
    this is now five。 by 27。 So we can see that the shape of this is five by 27。
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们所做的是五乘以27，再乘以27，输出现在是五乘以27。所以我们可以看到这个形状是五乘以27。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_279.png)'
  id: totrans-414
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_279.png)'
- en: So what is every element here telling us？ It's telling us for every one of 27
    neurons that we created。 what is the firing rate of， those neurons on every one
    of those five examples？ So the element。 for example， 3，13 is giving us the firing
    rate of the 13th neuron looking， at the third input。 And the way this was achieved
    is by a dot product between the third input and the 13th。
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 每个元素告诉我们什么？它告诉我们，对于我们创建的27个神经元，每个神经元在这五个示例中的发射率是什么？例如，3，13这个元素给我们的是第13个神经元在第三个输入下的发射率。这是通过第三个输入和第13个神经元之间的点积实现的。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_281.png)'
  id: totrans-416
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_281.png)'
- en: column of this W matrix here。
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 这个W矩阵的列。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_283.png)'
  id: totrans-418
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_283.png)'
- en: So using matrix multiplication， we can very efficiently evaluate the dot product
    between。 lots of input examples in a batch and lots of neurons where all those
    neurons have weights。 in the columns of those W's。 And in matrix multiplication。
    we're just doing those dot products in parallel。 Just to show you that this is
    the case。
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 所以使用矩阵乘法，我们可以非常高效地评估批量中大量输入示例与许多神经元之间的点积，所有这些神经元的权重都在这些W的列中。在矩阵乘法中，我们只是并行地进行这些点积。为了向你展示确实如此。
- en: we can take X and we can take the third row。
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以取X，并取第三行。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_285.png)'
  id: totrans-421
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_285.png)'
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_286.png)'
  id: totrans-422
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_286.png)'
- en: And we can take the W and take its 13th column。
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以取W并获取它的第13列。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_288.png)'
  id: totrans-424
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_288.png)'
- en: And then we can do X and get three， element-wise multiply with W of 13 and sum
    that up。 That's W of X plus B。 Well， there's no plus B。 It's just W of X dot product。
    And that's this number。 So you see that this is just being done efficiently by
    the matrix multiplication operation for all。
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以做X，并用第13个W进行元素逐个相乘并求和。这是WX加B。实际上，没有加B。只是WX点积。这就是这个数字。所以你可以看到，这是通过矩阵乘法操作高效完成的。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_290.png)'
  id: totrans-426
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_290.png)'
- en: the input examples and for all the output neurons of this first layer。 Okay。
    so we fed our 27 dimensional inputs into a first layer of a neural net that has
    27。
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 输入示例以及这一层所有输出神经元的情况。好吧。所以我们将27维输入送入具有27个神经元的神经网络的第一层。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_292.png)'
  id: totrans-428
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_292.png)'
- en: neurons。 So we have 27 inputs and now we have 27 neurons。 These neurons perform
    W times X。 They don't have a bias and they don't have a nonlinearity like 10H。
    We're going to lead them to be a linear layer。 In addition to that。 we're not
    going to have any other layers。 This is going to be it。 It's just going to be
    the dumbest。
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元。因此，我们有 27 个输入，现在我们有 27 个神经元。这些神经元执行 W 乘以 X。它们没有偏置，也没有像 10H 那样的非线性激活函数。我们将它们设为线性层。除此之外，我们不会有其他层。这将是它。这将是最简单的。
- en: smallest， simplest neural net， which is just a single linear layer。
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 最小、最简单的神经网络，仅仅是一个线性层。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_294.png)'
  id: totrans-431
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_294.png)'
- en: And now I'd like to explain what I want those 27 outputs to be。
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我想解释我希望这 27 个输出是什么。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_296.png)'
  id: totrans-433
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_296.png)'
- en: Intuitively， what we're trying to produce here for every single input example
    is we're trying to produce some kind of a probability。
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 从直观上看，我们要为每一个输入示例生成某种概率。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_298.png)'
  id: totrans-435
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_298.png)'
- en: distribution for the next character in a sequence。 And there's 27 of them。
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 用于序列中下一个字符的分布。总共有 27 个这样的字符。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_300.png)'
  id: totrans-437
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_300.png)'
- en: But we have to come up with like precise semantics for exactly how we're going
    to interpret these 27 numbers。
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们必须准确地定义如何解释这 27 个数字的语义。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_302.png)'
  id: totrans-439
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_302.png)'
- en: and we're going to make these two neurons take a whole。
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使这两个神经元占据整体。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_304.png)'
  id: totrans-441
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_304.png)'
- en: Now intuitively， you see here that these numbers are negative and some of them
    are positive， etc。
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 从直观上看，你会发现这些数字有的为负数，有的为正数，等等。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_306.png)'
  id: totrans-443
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_306.png)'
- en: And that's because these are coming out of a neural net layer initialized with
    these。
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为这些是来自一个用这些初始化的神经网络层。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_308.png)'
  id: totrans-445
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_308.png)'
- en: normal distribution parameters。 But what we want is we want something like we
    had here。 like each row here told us the counts。
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 正态分布参数。但我们想要的是像这里的结果一样。每一行告诉我们计数。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_310.png)'
  id: totrans-447
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_310.png)'
- en: and then we normalize the counts to get probabilities。 We want something similar
    to come out of a neural net。 But what we just have right now is just some negative
    and positive numbers。 Now we want those numbers to somehow represent the probabilities
    for the next character。
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们对计数进行归一化以获得概率。我们希望从神经网络中得到类似的东西。但我们现在所拥有的只是一些负数和正数。现在我们希望这些数字能以某种方式表示下一个字符的概率。
- en: But you see that probabilities， they have a special structure。 They're positive
    numbers and they sum to one。 And so that doesn't just come out of a neural net。
    And then they can't be counts because these counts are positive and counts are
    integers。 So counts are also not really a good thing to output from a neural net。
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 但你会看到概率有一个特殊结构。它们是正数，并且它们的总和为一。因此，这不仅仅来自神经网络。而且它们不能是计数，因为这些计数是正数，而计数是整数。所以，计数实际上并不是神经网络输出的一个好的选择。
- en: So instead what the neural net is going to output and how we are going to interpret
    the 27 numbers。 is that these 27 numbers are giving us log counts， basically。
    So instead of giving us counts directly， like in this table， they're giving us
    log counts。
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 所以神经网络将输出的内容，以及我们如何解释这 27 个数字，是这些数字给我们提供了对数计数。因此，它们并不是直接给我们计数，像这个表格那样，而是给我们对数计数。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_312.png)'
  id: totrans-451
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_312.png)'
- en: And to get the counts， we're going to take the log counts and we're going to
    exponentiate them。 Now exponentiation takes the following form。 It takes numbers
    that are negative or they are positive。 It takes the entire real line。 And then
    if you plug in negative numbers。 you're going to get e to the x， which is always
    below one。 So you're getting numbers lower than one。
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得计数，我们将对对数计数进行指数运算。指数运算的形式如下。它可以处理负数或正数，涵盖整个实数范围。如果你输入负数，将得到 e 的 x 次方，这总是小于一。因此，你得到的数字小于一。
- en: And if you plug in numbers greater than zero， you're getting numbers greater
    than one。 all the way growing to the infinity。
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你输入大于零的数字，你会得到大于一的数字，一直增长到无穷大。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_314.png)'
  id: totrans-454
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_314.png)'
- en: And this here grows to zero。 So basically we're going to take these numbers
    here。
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 而这里增长到零。所以基本上我们将采用这些数字。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_316.png)'
  id: totrans-456
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_316.png)'
- en: And instead of them being positive and negative and all over the place。
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是它们正负混杂。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_318.png)'
  id: totrans-458
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_318.png)'
- en: we're going to interpret them as log counts。 And then we're going to element
    twice exponentiate these numbers。
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把它们解释为对数计数。然后我们将对这些数字进行两次指数处理。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_320.png)'
  id: totrans-460
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_320.png)'
- en: Exponentiating them now gives us something like this。 And you see that these
    numbers now。 because of that they went through an exponent， all the negative numbers
    turned into numbers below one。 like 0。338。 And all the positive numbers， originally，
    turned into even more positive numbers。 so we're greater than one。
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 指数化它们现在给我们一些类似的东西。你会看到这些数字，因为经过指数处理，所有的负数变成了小于一的数字，比如0.338。而所有的正数，最初变成了更大的正数，所以我们大于一。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_322.png)'
  id: totrans-462
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_322.png)'
- en: So like for example， seven is some positive number over here。 That is greater
    than zero。 But exponentiated outputs here basically give us something that we
    can use and interpret as the equivalent of counts。 originally。 So you see these
    counts here， one twelve， seven， fifty one， one， etc。
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，七是这里的一个正数。这大于零。但是这里的指数输出基本上给我们一些可以用来解释为计数的等价物。所以你会看到这些计数，121，7，51，1等等。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_324.png)'
  id: totrans-464
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_324.png)'
- en: The neural light is kind of now predicting counts。 And these counts are positive
    numbers。 They can never be below zero， so that makes sense。
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络现在预测的是计数。这些计数是正数。它们永远不会小于零，所以这很有意义。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_326.png)'
  id: totrans-466
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_326.png)'
- en: And they can now take on various values depending on the settings of W。
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 它们现在可以根据W的设置取不同的值。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_328.png)'
  id: totrans-468
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_328.png)'
- en: So let me break this down。 We're going to interpret these to be the log counts。
    In other words。 for this， that is often used is so called logits。 These are logits
    log counts。
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 让我来分解一下。我们将把这些解释为对数计数。换句话说，对于这个，常用的被称为对数值。这些就是对数计数。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_330.png)'
  id: totrans-470
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_330.png)'
- en: Then these will be sort of the counts。 Logits exponentiated。 And this is equivalent
    to the n matrix。 sort of the n array that we used previously。 Remember this was
    the n？ This is the array of counts。 And each row here are the counts for the next
    character， sort of。 So those are the counts。 And now the probabilities are just
    the counts normalized。 And so I'm not going to find the same。
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 那么这些将是某种计数。对数值的指数化。这相当于n矩阵，类似于我们之前使用的n数组。记得这个是n吗？这是计数的数组。这里的每一行都是下一个字符的计数，差不多。所以这些就是计数。现在的概率只是计数的归一化。因此我不会找到相同的。
- en: but basically I'm not going to scroll all over the place。
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 但基本上我不会到处滚动。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_332.png)'
  id: totrans-473
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_332.png)'
- en: We've already done this。 We want to counts that sum along the first dimension。
    and we want to keep them as true。 We've went over this。 and this is how we normalize
    the rows of our counts matrix to get our probabilities。 So now these are the probabilities，
    and these are the counts that we have currently。
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经做过这个。我们希望计数沿第一个维度求和。我们希望保持它们的真实性。我们已经讨论过了。这就是我们如何归一化计数矩阵的行，以获得我们的概率。所以现在这些是概率，而这些是我们目前拥有的计数。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_334.png)'
  id: totrans-475
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_334.png)'
- en: And now when I show the probabilities， you see that every row here， of course，
    will sum to 1。 because they're normalized。 And the shape of this is 5 by 27。
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我展示概率时，你会看到这里的每一行当然会加起来等于1，因为它们是归一化的。它的形状是5乘27。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_336.png)'
  id: totrans-477
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_336.png)'
- en: And so really what we've achieved is for every one of our five examples。 we
    now have a row that came out of a neural net。 And because of the transformations
    here。 we made sure that this output of this neural net now are probabilities。
    or we can interpret to be probabilities。
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 所以实际上我们所取得的成果是，对于我们的每一个五个示例，我们现在有一行来自神经网络的输出。由于这里的变换，我们确保这个神经网络的输出现在是概率，或者我们可以将其解释为概率。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_338.png)'
  id: totrans-479
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_338.png)'
- en: So our WX here gave us logits， and then we interpret those to be log counts。
    And we exponentiate to get something that looks like counts。 and then we normalize
    those counts to get a probability distribution。 And all of these are differentiable
    operations。 So what we've done now is we are taking inputs。
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们的 WX 在这里给了我们 logits，然后我们将其解释为日志计数。我们通过指数运算得到看起来像计数的东西，然后我们对这些计数进行归一化，以获得概率分布。而这些都是可微分的操作。所以我们现在所做的是接受输入。
- en: We have differentiable operations that we can back propagate through。 and we're
    getting out probability distributions。
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有可微分的操作，可以进行反向传播，并且我们得到了概率分布。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_340.png)'
  id: totrans-482
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_340.png)'
- en: So， for example， for the 0th example that fed in， which was the 0th example
    here was a 1-half vector of 0。
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，例如，对于第0个输入示例，它的第0个示例是一个值为0的1/2向量。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_342.png)'
  id: totrans-484
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_342.png)'
- en: And it basically corresponded to feeding in this example here。
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上对应于将这个示例输入到这里。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_344.png)'
  id: totrans-486
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_344.png)'
- en: So we're feeding in a dot into a neural net。 And the way we fed the dot into
    a neural net is that we first got its index。 Then we one-hot encoded it。
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们将一个点输入到神经网络中。我们将点输入到神经网络的方法是首先获取其索引，然后进行独热编码。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_346.png)'
  id: totrans-488
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_346.png)'
- en: Then it went into the neural net， and out came this distribution of probabilities。
    And its shape is 27。 There's 27 numbers。 And we're going to interpret this as
    the neural net's assignment for how likely every one of these characters。
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 然后它进入神经网络，输出了这个概率分布。它的形状是27。有27个数字。我们将其解释为神经网络对每个字符的可能性分配。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_348.png)'
  id: totrans-490
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_348.png)'
- en: the 27 characters， are to come next。 And as we tune the weights W， we're going
    to be， of course。 getting different probabilities out for any character that you
    input。
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是27个字符。当我们调整权重 W 时，当然会为你输入的任何字符输出不同的概率。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_350.png)'
  id: totrans-492
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_350.png)'
- en: And so now the question is just， can we optimize and find a good W such that
    the probabilities coming out are pretty good？
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 所以现在的问题是，我们能否优化并找到一个好的 W，使得输出的概率相当不错？
- en: And the way we measure pretty good is by the loss function。 Okay。 so I organized
    everything into a single summary so that hopefully it's a bit more clear。 So it
    starts here。 We have an input data set。 We have some inputs to the neural net。
    And we have some labels for the correct next character in a sequence。 These are
    integers。
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 我们衡量良好的方式是通过损失函数。好的。所以我将所有内容整理成一个单一的摘要，希望它能更清晰一点。它从这里开始。我们有一个输入数据集。我们有一些输入到神经网络中。我们还有一些关于序列中下一个正确字符的标签。这些都是整数。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_352.png)'
  id: totrans-495
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_352.png)'
- en: Here I'm using Torx generators now so that you see the same numbers that I see。
    And I'm generating 27 neurons weights。 And each neuron here receives 27 inputs。
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我现在使用 Torx 生成器，这样你就能看到我所看到的相同数字。我正在生成27个神经元的权重。每个神经元在这里接收27个输入。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_354.png)'
  id: totrans-497
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_354.png)'
- en: Then here we're going to plug in all the input examples X's into a neural net。
    So here this is a forward pass。 First we have to encode all of the inputs into
    one-hot representations。 So we have 27 classes。 We pass in these integers。 And
    X， Inc。 becomes a array that is 5 by 27。 The zeros， except for a few ones。 We
    then multiply this in the first layer of a neural net to get logits。
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在这里我们将所有输入示例 X 插入到神经网络中。所以这里是一个前向传递。首先我们必须将所有输入编码为独热表示。我们有27个类别。我们传入这些整数。X，Inc。变成一个5乘27的数组。除了几个1，其他都是0。然后我们在神经网络的第一层中乘以这个以得到
    logits。
- en: Exponentiate the logits to get fake counts sort of。 And normalize these counts
    to get probabilities。 So these last two lines by the way here are called a softmax。
    Which I pulled up here。
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 将 logits 进行指数运算以获得伪计数。并对这些计数进行归一化以得到概率。顺便提一下，这最后两行被称为 softmax。这里我提到的就是这个。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_356.png)'
  id: totrans-500
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_356.png)'
- en: Softmax is a very often used layer in a neural net that takes these z's which
    are logits。 Exponentiates them and divides and normalizes。 It's a way of taking
    outputs of a neural net layer。 And these outputs can be positive or negative。
    And it outputs probability distributions。 It outputs something that always sums
    to one in our positive numbers。 Just like probabilities。
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax 是神经网络中一个非常常用的层，它接受这些 z 值，也就是 logits。对它们进行指数运算并进行归一化。它是处理神经网络层输出的一种方式。这些输出可以是正数或负数。它输出概率分布。它的输出总是加和为
    1，且为正数。就像概率一样。
- en: So this is kind of like a normalization function if you want to think of it
    that way。 And you can put it on top of any other linear layer inside a neural
    net。 And it basically makes a neural net output probabilities。 That's very often
    used。 And we used it as well here。
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这有点像一个归一化函数，如果你想这么理解的话。你可以把它放在神经网络内部的任何其他线性层上。它基本上使神经网络输出概率。这种用法非常常见。我们在这里也使用了它。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_358.png)'
  id: totrans-503
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_358.png)'
- en: So this is the forward pass and that's how we made a neural net output probability。
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是前向传播，这就是我们让神经网络输出概率的方式。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_360.png)'
  id: totrans-505
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_360.png)'
- en: Now you'll notice that all of these， this entire forward pass is made up of
    differentiable layers。 Everything here we can back propagate through。 And we saw
    some of the back propagation in micrograd。 This is just multiplication and addition。
    All that's happening here is just multiplying and add。 And we know how to back
    propagate through them。 Exponentiation we know how to back propagate through。
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你会注意到，所有这些，整个前向传播都是由可微分层组成的。这里的一切我们都可以进行反向传播。我们在 micrograd 中看到了一些反向传播。这只是乘法和加法。这里发生的所有事情只是乘法和加法。我们知道如何对它们进行反向传播。对于指数运算，我们也知道如何进行反向传播。
- en: And then here we are summing and sum is easily back propagated as well。 And
    division as well。 So everything here is the differentiable operation。 And we can
    back propagate through。
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 然后这里我们在求和，和也是很容易进行反向传播的。除法也是如此。所以这里的一切都是可微分操作。我们可以进行反向传播。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_362.png)'
  id: totrans-508
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_362.png)'
- en: Now we achieve these probabilities which are 5 by 27。
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们得到了这些概率，大小为 5 乘 27。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_364.png)'
  id: totrans-510
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_364.png)'
- en: For every single example we have a vector of probabilities that's on to one。
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每一个示例，我们有一个概率向量，映射到一个。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_366.png)'
  id: totrans-512
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_366.png)'
- en: And then here I wrote a bunch of stuff to sort of like break down the examples。
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在这里我写了一堆东西，试图分解这些示例。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_368.png)'
  id: totrans-514
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_368.png)'
- en: So we have 5 examples making up Emma。 And there are 5 by grams inside Emma。
    So by gram example one is that E is the beginning character right after dot。
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们有 5 个示例构成 Emma。Emma 中有 5 个字母组。因此，字母组的示例一是 E 是紧跟在点后面的第一个字符。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_370.png)'
  id: totrans-516
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_370.png)'
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_371.png)'
  id: totrans-517
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_371.png)'
- en: And the indexes for these are 0 and 5。 So then we feed in a 0。 That's the input
    of the neural net。 We get probabilities from the neural net that are 27 numbers。
    And then the label is 5 because E actually comes after dot。
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 这些的索引是 0 和 5。因此我们输入 0。这是神经网络的输入。我们从神经网络得到 27 个数字的概率。然后标签是 5，因为 E 实际上是在点后面。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_373.png)'
  id: totrans-519
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_373.png)'
- en: So that's the label。 And then we use this label 5 to index into the probability
    distribution here。 So this index 5 here is 0， 1， 2， 3， 4， 5。 It's this number
    here。 Which is here。 So that's basically the probability assigned by the neural
    net to the actual correct character。
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是标签。然后我们使用这个标签 5 在概率分布中索引。因此这里的索引 5 是 0，1，2，3，4，5。就是这个数字。它在这里。所以这基本上是神经网络分配给实际正确字符的概率。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_375.png)'
  id: totrans-521
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_375.png)'
- en: You see that the net work currently thinks that this next character that E following
    dot is only 1% likely。 Which is of course not very good right because this actually
    is a training example。 And the network thinks that it's currently very very unlikely。
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到网络目前认为这个字符E在点之后的可能性只有1%。这当然不是很好，因为这实际上是一个训练示例。而且网络认为它目前非常不可能。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_377.png)'
  id: totrans-523
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_377.png)'
- en: But that's just because we didn't get very lucky in generating a good setting
    of W。
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 但这只是因为我们在生成一个好的W设置时没有运气。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_379.png)'
  id: totrans-525
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_379.png)'
- en: So right now this network thinks it's unlikely and 0。01 is not a good outcome。
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这个网络认为不太可能，0.01不是一个好的结果。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_381.png)'
  id: totrans-527
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_381.png)'
- en: So the log likelihood then is very negative。
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 所以对数似然非常负。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_383.png)'
  id: totrans-529
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_383.png)'
- en: And the negative log likelihood is very positive。 And so 4 is a very high negative
    log likelihood。 And that means we're going to have a high loss。 Because what is
    the loss？
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 负对数似然非常积极。所以4是一个非常高的负对数似然。这意味着我们会有很高的损失。因为损失是什么？
- en: The loss is just the average negative log likelihood。
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 损失就是平均负对数似然。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_385.png)'
  id: totrans-532
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_385.png)'
- en: So the second character is EM。 And you see here that also the network thought
    that M following E is very unlikely 1%。
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 所以第二个字符是EM。你会看到网络认为E之后的M也很不可能，只有1%。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_387.png)'
  id: totrans-534
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_387.png)'
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_388.png)'
  id: totrans-535
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_388.png)'
- en: For M following M it thought it was 2%。 And for A following M it actually thought
    it was 7% likely。
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 对于M之后的M，它认为是2%。对于A之后的M，它实际上认为是7%的可能性。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_390.png)'
  id: totrans-537
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_390.png)'
- en: So just by chance this one actually has a pretty good probability and therefore
    a pretty low negative log likelihood。
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 所以恰好这个实际上有一个相当好的概率，因此负对数似然也相当低。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_392.png)'
  id: totrans-539
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_392.png)'
- en: And finally here it thought this was 1% likely。 So overall our average negative
    log likelihood which is the loss。 The total loss that summarizes basically how
    well this network currently works。 At least on this one word not on the full data
    set just the one word is 3。76。 Which is actually very fairly high loss。 This is
    not a very good setting of W's。
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 最后这里它认为这个可能性是1%。所以总体来说，我们的平均负对数似然就是损失。总结基本上是这个网络当前工作效果的总损失。至少在这个单词上，而不是在整个数据集上，仅这个单词是3.76。这个损失实际上相当高。这不是一个很好设置的W。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_394.png)'
  id: totrans-541
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_394.png)'
- en: Now here's what we can do。 We're currently getting 3。76。
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以做什么。我们当前得到的是3.76。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_396.png)'
  id: totrans-543
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_396.png)'
- en: We can actually come here and we can change our W。 We can resample it。 So let
    me just add one to have a different seed。 And then we get a different W。 And then
    we can rerun this。
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上可以来到这里，改变我们的W。我们可以重新采样它。所以让我加一来有一个不同的种子。然后我们得到了一个不同的W。然后我们可以重新运行这个。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_398.png)'
  id: totrans-545
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_398.png)'
- en: And with this different C with this different setting of W's。
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 以及这个不同的C和这个不同的W设置。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_400.png)'
  id: totrans-547
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_400.png)'
- en: We now get 3。37。 So this is a much better W。 And it's better because the probability
    just happened to come out higher for the characters that actually are next。
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在得到了3.37。所以这是一个更好的W。它更好是因为这个概率恰好对下一个字符更高。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_402.png)'
  id: totrans-549
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_402.png)'
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_403.png)'
  id: totrans-550
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_403.png)'
- en: And so you can imagine actually just resampling this。
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可以想象重新采样这个。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_405.png)'
  id: totrans-552
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_405.png)'
- en: We can try the two。 So okay this feels not very good。
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以尝试两个。所以，好的，这感觉不好。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_407.png)'
  id: totrans-554
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_407.png)'
- en: Let's try one more。 We can try three。 Okay this was a terrible setting because
    we have a very high loss。
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再试一次。我们可以尝试三次。好的，这个设置非常糟糕，因为我们的损失非常高。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_409.png)'
  id: totrans-556
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_409.png)'
- en: So anyway I'm going to erase this。
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 不管怎样，我要把这个擦掉。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_411.png)'
  id: totrans-558
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_411.png)'
- en: What I'm doing here which is just guess and check of randomly assigning parameters
    and seeing if the network is good。
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里所做的就是随机分配参数并检查网络的好坏。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_413.png)'
  id: totrans-560
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_413.png)'
- en: That is amateur hour。 That's not how you optimize a neural net。 The way you
    optimize a neural net is you start with some random guess and we're going to commit
    to this one even though it's not very good。 But now the big deal is we have a
    loss function。
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 这就像是业余时光。这不是优化神经网络的方式。优化神经网络的方法是你从某个随机猜测开始，我们会坚持这个猜测，即使它并不好。但现在关键是我们有一个损失函数。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_415.png)'
  id: totrans-562
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_415.png)'
- en: So this loss is made up only of differentiable operations。
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这个损失仅由可微操作构成。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_417.png)'
  id: totrans-564
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_417.png)'
- en: And we can minimize the loss by tuning W's by computing the gradients of the
    loss with respect to these W matrices。 And so then we can tune W to minimize the
    loss and find a good setting of W using gradient based optimization。
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过计算损失相对于这些W矩阵的梯度来最小化损失，从而调整W。因此我们可以调整W以最小化损失，并使用基于梯度的优化找到W的良好设置。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_419.png)'
  id: totrans-566
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_419.png)'
- en: So let's see how that will work。 Now things are actually going to look almost
    identical to what we had with micrograd。
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看这将如何运作。现在的东西实际上看起来几乎与micrograd中的一样。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_421.png)'
  id: totrans-568
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_421.png)'
- en: So here I pulled up the lecture from micrograd， the notebook。
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我这里调出了来自micrograd的讲座，那个笔记本。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_423.png)'
  id: totrans-570
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_423.png)'
- en: It's from this repository。 And when I scroll all the way to the end where we
    left off with micrograd we had something very very similar。 We had a number of
    input examples。 In this case we had four input examples inside X's。 And we had
    their targets。
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 这是来自这个代码库。当我向下滚动到我们与micrograd停下的地方时，我们有一些非常非常相似的东西。我们有多个输入示例。在这种情况下，我们在`X`中有四个输入示例。我们还有它们的目标。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_425.png)'
  id: totrans-572
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_425.png)'
- en: These are our targets。 Just like here we have our X's now but we have five of
    them。 And they're now integers instead of vectors。 But we're going to convert
    our integers to vectors。
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们的目标。就像这里我们有`X`一样，但现在我们有五个。它们现在是整数，而不是向量。但我们会将我们的整数转换为向量。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_427.png)'
  id: totrans-574
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_427.png)'
- en: Except our vectors will be 27 large instead of 3 large。 And then here what we
    did is first we did a forward pass where we ran a neural net on all of the inputs
    to get predictions。 Our neural net at the time， this N of X was a net or multi-layer
    perceptron。 Our neural net is going to look different because our neural net is
    just a single layer。
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们的向量将是27维，而不是3维。然后在这里我们首先进行了前向传播，在所有输入上运行神经网络以获取预测。那时我们的神经网络，`N of X`是一个多层感知器。我们的神经网络将看起来不同，因为我们的神经网络只是一个单层。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_429.png)'
  id: totrans-576
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_429.png)'
- en: single linear layer followed by a softmax。
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 单个线性层后跟一个softmax。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_431.png)'
  id: totrans-578
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_431.png)'
- en: So that's our neural net。 And the loss here was the mean squared error。 So we
    simply subtracted the prediction from the ground truth and squared it and summed
    it all up。 And that was the loss。 And the loss was the single number that summarized
    the quality of the neural net。 And when loss is low， like almost zero， that means
    the neural net is predicting correctly。
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是我们的神经网络。这里的损失是均方误差。我们简单地将预测值从真实值中减去，然后平方并将其全部相加。这就是损失。损失是总结神经网络质量的单一数字。当损失很低，几乎为零时，意味着神经网络的预测是正确的。
- en: So we had a single number that summarized the performance of the neural net。
    And everything here was differentiable and was stored in a massive compute graph。
    And then we iterated over all the parameters。 We made sure that the gradients
    are set to zero。 And we called loss。backward。 And loss。backward initiated back
    propagation at the final output node of loss。
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们有一个数字来总结神经网络的性能。这里的一切都是可微的，并且存储在一个庞大的计算图中。然后我们遍历所有参数。我们确保梯度被设置为零。然后我们调用`loss.backward`。而`loss.backward`在损失的最终输出节点启动了反向传播。
- en: Right。 So yeah， remember these expressions？ We had loss all the way at the end。
    We start back propagation and we went all the way back。 And we made sure that
    we populated all the parameters dot grad。 So that grad started at zero。 but back
    propagation filled it in。 And then in the update， we iterated over all the parameters。
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 对。是的，记得这些表达式吗？我们在最后有损失。我们开始反向传播，然后一路向后。并确保填充了所有参数的 dot grad。于是 grad 从零开始，但反向传播填充了它。然后在更新中，我们遍历了所有参数。
- en: And we simply did a parameter update where every single element of our parameters
    was nudged in the opposite direction of the gradient。 And so we're going to do
    the exact same thing here。
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 我们简单地进行了参数更新，其中我们参数的每个元素都朝着梯度的相反方向微调。所以我们将在这里做完全相同的事情。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_433.png)'
  id: totrans-583
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_433.png)'
- en: So I'm going to pull this up on the side here。
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我将把它拉到旁边。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_435.png)'
  id: totrans-585
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_435.png)'
- en: So that we have it availed and we're actually going to do the exact same thing。
    So this was the forward pass。 So where we did this。 And props is our white thread。
    So now we have to evaluate the loss， but we're not using the mean squared error。
    We're using the negative log likelihood because we are doing classification。
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们有了它，我们实际上会做完全相同的事情。这是前向传播。在这里我们做了这个。并且 props 是我们的白色线程。现在我们需要评估损失，但我们不使用均方误差。我们使用负对数似然，因为我们正在进行分类。
- en: We're not doing regression as it's called。 So here we want to calculate loss。
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 我们并没有进行所谓的回归。所以在这里，我们想计算损失。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_437.png)'
  id: totrans-588
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_437.png)'
- en: Now the way we calculated is just this average negative log likelihood。
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们计算的方式就是这个平均负对数似然。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_439.png)'
  id: totrans-590
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_439.png)'
- en: Now this props here has a shape of five by twenty seven。
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这个 props 的形状是五乘以二十七。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_441.png)'
  id: totrans-592
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_441.png)'
- en: And so to get all the we basically want to pluck out the probabilities at the
    correct indices here。 So in particular because the labels are stored here in the
    array wise。 Basically what we're after is for the first example we're looking
    at probability of five at index five。 For the second example at the second row
    or row index one。
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 所以为了获取所有的概率，我们基本上想要提取出正确索引处的概率。特别是因为标签是以数组的形式存储的。基本上我们关注的是第一个示例中索引五处的概率。在第二个示例中是在第二行或行索引一。
- en: we are interested in the probability assigned to index thirteen。 At the second
    example we also have thirteen。 At the third row we want one。 And at the last row
    which is four we want zero。
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对索引十三分配的概率感兴趣。在第二个示例中，我们也有十三。在第三行，我们想要一个。在最后一行，即四，我们想要零。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_443.png)'
  id: totrans-595
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_443.png)'
- en: So these are the probabilities we're interested in。 And you can see that they're
    not amazing as we saw above。 So these are the probabilities we want。 but we want
    like a more efficient way to access these probabilities。 Not just listing them
    out in a tuple like this。 So it turns out that the way to do this in PyTorch。
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这些是我们感兴趣的概率。你可以看到它们并不是很优秀，正如我们之前看到的那样。所以这些是我们想要的概率。但我们想要更高效的方法来访问这些概率，而不是像这样列出它们。所以结果是，在
    PyTorch 中做这件事的方式是。
- en: one of the ways at least， is we can basically pass in all of these integers，
    in a vector。 So these ones you see how they're just 0， 1， 2， 3， 4。 We can actually
    create that using empty。 not empty， sorry， torch。arrange of five。 0， 1， 2， 3，
    4。 So we can index here with torch。arrange of five。 And here we index with wise。
    And you see that that gives us exactly these numbers。
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 至少其中一种方法是我们可以基本上传入所有这些整数，以一个向量的形式。所以你看到的这些就是 0，1，2，3，4。我们实际上可以使用 empty 创建它。不是
    empty，抱歉，是 torch.arrange 的五个数字：0，1，2，3，4。所以我们可以用 torch.arrange 的五个数字进行索引。在这里我们按数组索引。你看到这正好给我们这些数字。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_445.png)'
  id: totrans-598
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_445.png)'
- en: So that plucks out the probabilities of that the neural network assigns to the
    correct next character。
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这提取出神经网络分配给正确下一个字符的概率。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_447.png)'
  id: totrans-600
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_447.png)'
- en: Now we take those probabilities and we don't， we actually look at the log probability。
    So we want to dot log。 And then we want to just average that up。 So take the mean
    of all that。 And then it's the negative average log likelihood。 That is the loss。
    So the loss here is 3。7 something。
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们取这些概率，我们不直接使用，而是查看对数概率。所以我们想要计算对数。然后我们想要简单地平均这些值。所以取所有的均值。然后是负的平均对数似然。这就是损失。因此这里的损失是3.7左右。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_449.png)'
  id: totrans-602
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_449.png)'
- en: And you see that this loss 3。76， 3。76 is exactly as we've obtained before。 But
    this is a vectorized form of that expression。 So we get the same loss。 And the
    same loss we can consider sort of as part of this forward pass and we've achieved
    here now loss。
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到这个损失3.76，3.76正是我们之前获得的。但是这是该表达式的向量化形式。因此我们得到了相同的损失。相同的损失我们可以视为前向传播的一部分，我们现在实现了损失。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_451.png)'
  id: totrans-604
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_451.png)'
- en: Okay， so we made our way all the way to loss。 We defined the forward pass。 We
    forwarded the network and the loss。 Now we're ready to do the backward pass。 So
    backward pass。 We want to first make sure that all the gradients are reset。 So
    they're at zero。 Now in PyTorch you can set the gradients to be zero， but you
    can also just set it to none。
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，所以我们一路走到了损失。我们定义了前向传播。我们推进了网络和损失。现在我们准备进行反向传播。对于反向传播，我们首先要确保所有的梯度都被重置为零。在PyTorch中，你可以将梯度设置为零，但也可以直接设置为none。
- en: And setting it to none is more efficient。 And PyTorch will interpret none as
    like a lack of a gradient and it's the same as zeros。 So this is a way to set
    to zero， the gradient。 And now we do loss。backward。 Before we do loss。backward
    we need one more thing。 If you remember from micrograd PyTorch actually requires
    that we pass in。 requires grad is true。 So that we tell PyTorch that we are interested
    in calculating gradients for this leaf tensor。
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 设置为none更高效。PyTorch会将none解释为缺乏梯度，这与零相同。所以这是一种将梯度设置为零的方法。现在我们进行loss.backward。在执行loss.backward之前，我们需要再确认一件事。如果你还记得在微梯度中，PyTorch实际上要求我们传入.requires_grad为true。这样我们就告诉PyTorch我们希望计算这个叶子张量的梯度。
- en: By default this is false。 So let me recalculate with that。
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下这是假。让我用这个重新计算一下。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_453.png)'
  id: totrans-608
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_453.png)'
- en: And then setting none and loss。backward。 Now something magical happened when
    loss。backward was run。 Because PyTorch just like micrograd， when we did the forward
    pass here。 it keeps track of all the operations under the hood。
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 然后设置为none并调用损失的.backward。现在在运行loss.backward时发生了奇妙的事情。因为PyTorch和微梯度一样，当我们在这里进行前向传播时，它会跟踪所有底层的操作。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_455.png)'
  id: totrans-610
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_455.png)'
- en: It builds a full computational graph。 Just like the graphs we produced in micrograd。
    those graphs exist inside PyTorch。 And so it knows all the dependencies and all
    the mathematical operations of everything。 And when you then calculate the loss，
    we can call a。backward on it。 And that backward then fills in the gradients of
    all the intermediates all the way back to W's。
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 它构建了一个完整的计算图。就像我们在微梯度中生成的图。这些图存在于PyTorch内部。因此，它知道所有的依赖关系和所有的数学运算。当你计算损失时，我们可以对其调用.backward。这时反向传播会填充所有中间变量的梯度，一直回溯到W。
- en: Which are the parameters of our neural net。 So now we can do WL grad and we
    see that it has structure。 There's stuff inside it。 And these gradients， every
    single element here。 So W。shape is 27 by 27。 W grad's shape is the same。 27 by
    27。
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们神经网络的参数。现在我们可以查看WL.grad，看到它具有结构。里面有东西。这些梯度，这里的每个元素。所以W的形状是27乘27。W.grad的形状也是一样。27乘27。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_457.png)'
  id: totrans-613
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_457.png)'
- en: And every element of W。grad is telling us the influence of that weight on the
    loss function。 So for example this number all the way here。 If this element， the
    zero zero element of W。 Because the gradient is positive， it's telling us that
    this has a positive influence on the loss。 Slightly nudging W。 Slightly taking
    W zero zero。 And adding a small h to it would increase the loss。
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: W.grad的每个元素都告诉我们该权重对损失函数的影响。例如，这个数字。如果这是W的零零元素。因为梯度是正的，它告诉我们这对损失有正面影响。稍微推动W。稍微调整W的零零元素，加一个小的h会增加损失。
- en: Mildly。 Because this gradient is positive。 Some of these gradients are also
    negative。
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 略微。因为这个梯度是正的。一些梯度也是负的。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_459.png)'
  id: totrans-616
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_459.png)'
- en: So that's telling us about the gradient information。
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们关于梯度的信息。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_461.png)'
  id: totrans-618
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_461.png)'
- en: And we can use this gradient information to update the weights of this neural
    network。 So let's not do the update。 It's going to be very similar to what we
    had in micro grad。 We need no loop over all the parameters because we only have
    one parameter tensor and that is W。 So we simply do W。data plus equals。 We can
    actually copy this almost exactly。 Negative 0。1 times W。
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这个梯度信息来更新这个神经网络的权重。所以我们不要进行更新。这将与我们在micro grad中拥有的非常相似。我们不需要遍历所有参数，因为我们只有一个参数张量，那就是W。因此我们简单地做`W.data
    +=`。我们实际上可以几乎完全复制这个。负的0.1乘以W。
- en: grad。
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: grad。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_463.png)'
  id: totrans-621
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_463.png)'
- en: And that would be the update to the tensor。 So that updates the tensor。 And
    because the tensor is updated， we would expect that now the loss should decrease。
    So here if I print loss。item， it was 3。76。 So we've updated the W here。 So if
    I recalculate forward pass， loss now should be slightly lower。 So 3。76 goes to
    3。74。
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是张量的更新。所以更新了张量。由于张量被更新，我们预计现在损失应该降低。所以这里如果我打印`loss.item`，它是3.76。因此我们在这里更新了W。如果我重新计算前向传播，损失现在应该稍微低一些。所以3.76变成3.74。
- en: And then we can again set to set grad to none and backward， update。 And now
    the parameters changed again。 So if we recalculate the forward pass。 we expect
    a lower loss again 3。72。 And this is again doing the， we're now doing reading
    the set。 And when we achieve a low loss， that will mean that the network is assigning
    high probabilities to the correct next characters。
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以再次设置梯度为零并进行反向传播，更新。现在参数再次改变。如果我们重新计算前向传播，我们再次期待损失降低到3.72。这又是在进行读取集。当我们实现低损失时，这意味着网络正在将高概率分配给正确的下一个字符。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_465.png)'
  id: totrans-624
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_465.png)'
- en: Okay， so I rearranged everything and I put it all together from scratch。 So
    here is where we construct our data set of by grams。 You see that we are still
    iterating only over the first word Emma。 I'm going to change that in a second。
    I added a number that counts the number of elements in Xs so that we explicitly
    see that number of examples is 5。
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我重新整理了一切，从头开始把所有内容组合在一起。这是我们构建按字组的数据集的地方。你会看到我们仍然只对第一个词“Emma”进行迭代。我马上会改变这一点。我添加了一个数字来计算Xs中的元素数量，这样我们就可以明确看到示例数量是5。
- en: Because currently we were just working with Emma and there's 5 by grams there。
    And here I added a loop of exactly what we had before。 So we had 10 iterations
    of very new descent of forward pass， backward pass and update。 And so running
    these two cells initialization and creating descent gives us some improvement
    on the loss function。
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 因为目前我们只在使用“Emma”，那里有5个字组。在这里我添加了一个循环，正是我们之前所拥有的。所以我们进行了10次迭代的非常新下降，前向传播，反向传播和更新。因此运行这两个单元的初始化和创建下降给我们在损失函数上带来了一些改善。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_467.png)'
  id: totrans-627
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_467.png)'
- en: But now I want to use all the words。 And there's not 5 but 228，000 by grams
    now。 However。 this should require no modification whatsoever。 Everything should
    just run because all the code we wrote doesn't care if there's 5 by grams or 228。000
    by grams。 And with everything we should just work。 So you see that this will just
    run。 But now we are optimizing the entire training set of all the by grams。
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 但现在我想使用所有单词。而不是5，而是228,000的字组。然而，这不应该需要任何修改。一切应该都能运行，因为我们编写的所有代码都不在乎字组是5还是228,000。而且所有内容应该都能正常工作。所以你会看到这将会运行。但现在我们正在优化所有字组的整个训练集。
- en: And you see now that we are decreasing very slightly。 So actually we can probably
    afford a larger learning rate。 And probably afford even larger learning rate。
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在会看到我们正在非常轻微地减少。因此我们实际上可能可以承受更大的学习率。并且可能承受更大的学习率。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_469.png)'
  id: totrans-630
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_469.png)'
- en: Even 50 seems to work on this very， very simple example。 So let me reinitialize
    and let's run 100 iterations。 See what happens。
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是50在这个非常简单的示例中似乎也能工作。所以让我重新初始化，进行100次迭代。看看会发生什么。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_471.png)'
  id: totrans-632
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_471.png)'
- en: Okay。 We seem to be coming up to some pretty good losses here。
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧。我们似乎在这里得到了相当不错的损失。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_473.png)'
  id: totrans-634
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_473.png)'
- en: 2。47。 Let me run 100 more。 What is the number that we expect by the way in the
    loss？
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 2.47。让我再运行100次。顺便问一下，我们预计损失应该是多少？
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_475.png)'
  id: totrans-636
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_475.png)'
- en: We expect to get something around what we had originally actually。 So all the
    way back if you remember in the beginning of this video。 when we optimized just
    by counting， our loss was roughly 2。47 after we added smoothing。 But before smoothing
    we had roughly 2。45 likely at sorry loss。
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 我们期望得到的结果大约和我们最初的一样。如果你还记得在视频开始时，我们通过计数优化，损失大约是2.47，在添加平滑后。但在没有平滑之前，我们的损失大约是2.45。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_477.png)'
  id: totrans-638
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_477.png)'
- en: And so that's actually roughly the vicinity of what we expect to achieve。 But
    before we achieved it by counting and here we are achieving roughly the same result。
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这实际上大致是我们期望实现的目标。但是在我们通过计数实现之前，这里我们大致上达到了相同的结果。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_479.png)'
  id: totrans-640
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_479.png)'
- en: but with gradient based optimization。
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: 但使用基于梯度的优化。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_481.png)'
  id: totrans-642
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_481.png)'
- en: So we come to about 2。46， 2。45， et cetera。
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们达到了大约2.46、2.45等等。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_483.png)'
  id: totrans-644
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_483.png)'
- en: And that makes sense because fundamentally we're not taking in any additional
    information。 We're still just taking in the previous character and trying to predict
    the next one。 But instead of doing it explicitly by counting and normalizing。
    we are doing it with gradient based learning。 And it just so happens that the
    explicit approach happens to very well optimize the loss function。
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 这很有道理，因为从根本上讲，我们并没有获取任何额外的信息。我们仍然只是获取前一个字符并尝试预测下一个字符。但我们不是通过计数和归一化明确进行，而是通过基于梯度的学习进行。而且恰好是显式方法非常好地优化了损失函数。
- en: without any need for a gradient based optimization。 Because the setup for Bagram
    language models is so straightforward， it's so simple。 We can just afford to estimate
    those probabilities directly and maintain them in a table。
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 而不需要任何基于梯度的优化。因为Bagram语言模型的设置非常简单。我们可以直接估计这些概率并将它们保存在表格中。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_485.png)'
  id: totrans-647
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_485.png)'
- en: But the gradient based approach is significantly more flexible。 So we've actually
    gained a lot because what we can do now is we can expand this approach。 and complexify
    the neural net。 So currently we're just taking a single character and feeding
    into a neural net。 and the neural net is extremely simple。 But we're about to
    iterate on this substantially。
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 但基于梯度的方法灵活性显著更高。因此，我们实际上获得了很多，因为现在我们可以扩展这种方法，并复杂化神经网络。所以目前我们只是在输入一个字符并送入一个非常简单的神经网络。但我们即将大幅迭代这个过程。
- en: We're going to be taking multiple previous characters and we're going to be
    feeding them。 into increasingly more complex neural nets。 But fundamentally。 the
    output of the neural net will always just be logits。 And those logits will go
    through the exact same transformation。
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将会处理多个之前的字符，并将它们输入到越来越复杂的神经网络中。但从根本上讲，神经网络的输出将始终只是logits。这些logits将经过完全相同的转换。
- en: We are going to take them through a softmax， calculate the loss function and
    the negative log likelihood。 and do gradient based optimization。 And so actually
    as we complexify the neural nets and work all the way up to transformers。 none
    of this will really fundamentally change。 None of this will fundamentally change。
    The only thing that will change is the way we do the forward pass。
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对它们进行softmax计算，计算损失函数和负对数似然，并进行基于梯度的优化。因此，实际上随着我们复杂化神经网络并逐步提高到变换器，这些基本上不会改变。唯一会改变的是我们进行前向传播的方式。
- en: where we take in some previous characters and calculate the logits for the next
    character in a sequence。 That will become more complex and I will use the same
    machinery to optimize it。 And it's not obvious how we would have extended this
    by-gram approach into the case。
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将获取一些之前的字符，并计算下一个字符在序列中的logits。这将变得更加复杂，我将使用相同的机制来优化它。而且不明显的是，我们将如何将这种by-gram方法扩展到这种情况。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_487.png)'
  id: totrans-652
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_487.png)'
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_488.png)'
  id: totrans-653
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_488.png)'
- en: where there are many more characters at the input。 Because eventually these
    tables would get way too large because there's way too many combinations。 of what
    previous characters could be。 If you only have one previous character。 we can
    just keep everything in a table， the counts。
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 输入中有许多更多的字符。因为最终这些表会变得太大，因为有太多组合。前一个字符可能是什么。如果你只有一个前一个字符，我们可以把所有内容都保存在一个表中，计数。
- en: But if you have the last 10 characters that are input。 we can't actually keep
    everything in a table anymore。 So this is fundamentally an unscalable approach。
    And the neural network approach is significantly more scalable。
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果你有最后10个输入字符，我们实际上不能再把所有东西保存在一个表中。因此，这从根本上来说是一种不可扩展的方法。而神经网络的方法则显著更具可扩展性。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_490.png)'
  id: totrans-656
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_490.png)'
- en: And it's something that actually we can improve on over time。
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 而且这实际上是我们可以随着时间的推移改进的东西。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_492.png)'
  id: totrans-658
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_492.png)'
- en: So that's where we will be digging next。 I wanted to point out two more things。
    Number one。 I want you to notice that this x-ank here， this is made up of one-hot
    vectors。 And then those one-hot vectors are multiplied by this w matrix。 And we
    think of this as multiple neurons being forwarded in a fully connected manner。
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是我们接下来要深入探讨的地方。我想指出两件事。第一，我希望你注意到这个x-ank，这由一个个热编码向量构成。然后这些个热编码向量与这个w矩阵相乘。我们把这看作多个神经元以完全连接的方式进行前向传播。
- en: But actually what's happening here is that， for example。 if you have a one-hot
    vector here that has a one， let's say the fifth dimension。 then because of the
    way the matrix multiplication works。 multiplying that one-hot vector with w actually
    ends up plucking out the fifth row of w。
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: 但实际上这里发生的是，例如，如果你在这里有一个个热编码向量，其中有一个，假设在第五维。那么由于矩阵乘法的方式，与w相乘的个热编码向量实际上会提取出w的第五行。
- en: Logits would become just the fifth row of w。 And that's because of the way the
    matrix multiplication works。 So that's actually what ends up happening。 But that's
    actually exactly what happened before。 Because remember all the way up here， we
    have a by-gram， we took the first character。
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: Logits将变成w的第五行。这是由于矩阵乘法的方式。所以实际上发生的就是这样。但这实际上和之前完全一样。因为记得在这里，我们有一个by-gram，我们取了第一个字符。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_494.png)'
  id: totrans-662
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_494.png)'
- en: and then that first character indexed into a row of this array here。 And that
    row gave us the probability distribution for the next character。 So the first
    character was used as a lookup into a matrix here to get the probability distribution。
    Well， that's actually exactly what's happening here。 So we're taking the index。
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: 然后第一个字符索引到了这个数组的一行。那一行给了我们下一个字符的概率分布。因此，第一个字符被用作查找这里的一个矩阵以获得概率分布。嗯，这实际上正是这里发生的事情。因此，我们在获取索引。
- en: we're encoding it as one-hot and multiplying it by w。 So logits literally becomes
    the appropriate row of w。 And that gets just as before。 exponentiated to create
    the counts， and then normalized and becomes probability。
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将其编码为个热编码并与w相乘。所以logits实际上变成了w的适当行。然后就像之前一样，指数化以创建计数，然后进行归一化，成为概率。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_496.png)'
  id: totrans-665
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_496.png)'
- en: So this w here is literally the same as this array here。 But w， remember， is
    the log counts。 not the counts。 So it's more precise to say that w exponentiated
    w。x is this array。 But this array was filled in by counting and by basically populating
    the counts of by-grams。 Whereas in the gradient-based framework， we initialize
    it randomly。
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这里的w实际上和这个数组是一样的。但记住，w是日志计数，而不是计数。因此，更准确地说，w的指数化w.x就是这个数组。但这个数组是通过计数和基本上填充by-grams的计数而生成的。而在基于梯度的框架中，我们是随机初始化的。
- en: and then we let the loss guide us to arrive at the exact same array。 So this
    array exactly here is basically the array w at the end of optimization。 except
    we arrived at it piece by piece by following the loss。 And that's why we also
    obtain the same loss function at the end。
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们让损失引导我们到达完全相同的数组。所以这个数组实际上就是在优化结束时的数组w。只不过我们是通过逐步跟随损失得出的。这也是为什么我们在最后得到相同的损失函数。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_498.png)'
  id: totrans-668
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_498.png)'
- en: And the second note is if I come here， remember the smoothing where we added
    fake counts to our counts。 in order to smooth out and make more uniform the distributions
    of these probabilities。 And that prevented us from assigning zero probability
    to any one by-gram。 Now。 if I increase the count here， what's happening to the
    probability？ As I increase the count。
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个注意事项是如果我来到这里，记住我们添加虚假计数以平滑和均匀这些概率分布的平滑。这样可以防止我们将零概率分配给任何一个字对。现在，如果我增加这里的计数，概率会发生什么变化？随着计数的增加。
- en: probability becomes more and more uniform。
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: 概率变得越来越均匀。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_500.png)'
  id: totrans-671
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_500.png)'
- en: Because these counts go only up to like 900 or whatever。 So if I'm adding plus
    a million to every single number here。 you can see how the row and its probability
    then when you divide。 it is just going to become more and more close to exactly
    even probability in the form distribution。
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这些计数最多大约为900。所以如果我在这里每个数字上加上百万，你可以看到这一行及其概率在除法后将会变得越来越接近完全均匀的概率分布。
- en: It turns out that the gradient-based framework has an equivalent to smoothing。
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，基于梯度的框架与平滑是等价的。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_502.png)'
  id: totrans-674
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_502.png)'
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_503.png)'
  id: totrans-675
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_503.png)'
- en: In particular， think through these w's here， which we initialize randomly。
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，思考这些w，我们是随机初始化的。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_505.png)'
  id: totrans-677
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_505.png)'
- en: We could also think about initializing w's to be zero。
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以考虑将w初始化为零。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_507.png)'
  id: totrans-679
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_507.png)'
- en: If all the entries of w are zero， then you'll see that logits will become all
    zero。 And then exponentiating those logits becomes all one and then the probabilities
    turn out to be exactly uniform。
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: 如果w的所有条目都是零，那么你会看到logits都变为零。然后对这些logits进行指数运算会变成全一，然后概率正好是均匀的。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_509.png)'
  id: totrans-681
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_509.png)'
- en: So basically when w's are all equal to each other or say especially zero。
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: 所以基本上当w都相等，或者特别是等于零时。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_511.png)'
  id: totrans-683
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_511.png)'
- en: then the probability is not completely uniform。 So trying to incentivize w to
    be near zero is basically equivalent to label smoothing。 And the more you incentivize
    that in a loss function。 the more smooth distribution you're going to achieve。
    So this brings us to something that's called regularization。
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: 那么概率就不是完全均匀的。所以尝试鼓励w接近零基本上等同于标签平滑。而在损失函数中越是鼓励这一点，就会获得越平滑的分布。这引出了一个叫做正则化的概念。
- en: where we can actually augment the loss function to have a small component that
    we call a regularization loss。 In particular， what we're going to do is we can
    take w and we can， for example。 square all of its entries。 And then we can， whoops，
    sorry about that。 we can take all the entries of w and we can sum them。
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上可以增强损失函数，使其包含一个小组件，我们称之为正则化损失。具体而言，我们要做的是可以取w，并且我们可以，例如，平方它的所有条目。然后我们可以，哦，抱歉，我们可以取w的所有条目并将其求和。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_513.png)'
  id: totrans-686
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_513.png)'
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_514.png)'
  id: totrans-687
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_514.png)'
- en: And because we're squaring， there will be no signs anymore。 Natives and positives
    all get squashed because of numbers。 And then the way this works is you achieve
    zero loss if w is exactly or zero。 But if w has nonzero numbers， you accumulate
    loss。
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们是在平方，所以将不再有符号。负数和正数都被数字压缩了。然后这样做的方式是如果w正好为零或零，损失为零。但如果w有非零数值，你就会累积损失。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_516.png)'
  id: totrans-689
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_516.png)'
- en: And so we can actually take this and we can add it on here。 So we can do something
    like loss plus w square dot sum。 Or let's actually instead of sum。 let's take
    a mean because otherwise the sum gets too large。 So mean is like a little bit
    more manageable。 And then we have a regularization loss here。
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们实际上可以将其添加到这里。我们可以做一些比如损失加上w的平方总和。或者我们实际上可以改为取均值，因为否则总和会变得太大。均值更易于管理。然后我们有一个正则化损失。
- en: like say 0。01 times or something like that。 You can choose the regularization
    strength。 And then we can just optimize this。
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: 比如说0.01倍或类似的东西。你可以选择正则化强度。然后我们可以优化这个。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_518.png)'
  id: totrans-692
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_518.png)'
- en: And now this optimization actually has two components。 Not only is it trying
    to make all the probabilities work out， but in addition to that。 there's an additional
    component that simultaneously tries to make all w's be zero。 Because if w's are
    nonzero， you feel a loss。 And so minimizing this。
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这个优化实际上有两个组成部分。不仅试图使所有概率正常工作，而且还有一个附加组成部分，试图同时使所有 w 为零。因为如果 w 不为零，你会感受到损失。因此，要最小化这个。
- en: the only way to achieve that is for w to be zero。 And so you can think of this
    as adding like a spring force or like a gravity force that pushes w to be zero。
    So w wants to be zero and the probabilities want to be uniform。
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一实现这一点的方法是让 w 等于零。因此，你可以将其视为增加一个像弹簧力或重力的力，使 w 推向零。因此，w 想要等于零，而概率希望是均匀的。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_520.png)'
  id: totrans-695
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_520.png)'
- en: But they also simultaneously want to match up your probabilities as indicated
    by the data。 And so the strength of this regularization is exactly controlling
    the amount of counts that you add here。
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: 但他们也希望与你的数据所指示的概率相匹配。因此，这种正则化的强度正好控制你在这里添加的计数量。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_522.png)'
  id: totrans-697
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_522.png)'
- en: Adding a lot more counts here corresponds to increasing this number。
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里添加更多计数相当于增加这个数字。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_524.png)'
  id: totrans-699
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_524.png)'
- en: Because the more you increase it， the more this part of the loss function dominates
    this part。 And the more these weights will be unable to grow。 Because as they
    grow。 they accumulate way too much loss。 And so if this is strong enough。 then
    we are not able to overcome the force of this loss。 And we will never。
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: 因为你越是增加它，这个损失函数的这一部分就会主导这一部分。而且这些权重将无法增长。因为当它们增长时，它们会累积过多的损失。因此，如果这个强度足够大，那么我们将无法克服这个损失的力量。我们将永远无法做到。
- en: and basically everything will be uniform predictions。 So I thought that's kind
    of cool。
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上所有的预测都是均匀的。所以我觉得这有点酷。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_526.png)'
  id: totrans-702
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_526.png)'
- en: Okay， and lastly， before we wrap up， I wanted to show you how you would sample
    from this neural net model。 And I copy pasted the sampling code from before。 Where
    remember that we sampled five times and all we did is we started zero。 we grabbed
    the current iX row of P。 And that was our probability row from which we sampled
    the next index and just accumulated that and break when zero。 And running this
    gave us these results。 I still have the P in memory， so this is fine。 Now。
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，最后，在我们结束之前，我想向你展示如何从这个神经网络模型中进行采样。我复制了之前的采样代码。记得我们采样了五次，所有做的就是从零开始。我们抓取了当前
    iX 行的 P。这就是我们的概率行，从中采样下一个索引，并累加，直到遇到零。运行这个给我们带来了这些结果。我仍然在内存中有 P，所以这没问题。现在。
- en: this P doesn't come from the row of P。 Instead it comes from this neural net。
    First we take iX and we encode it into a one-hot row of X-ink。 This X-ink multiplies
    our W。 which really just plucks out the row of W corresponding to iX。 Really that's
    what's happening。 And that gets our logits and then we normalize those logits。
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 P 不是来自 P 的行，而是来自这个神经网络。首先我们取 iX，将其编码为一个 X-ink 的独热行。这个 X-ink 乘以我们的 W。实际上，它只是提取出与
    iX 对应的 W 的行。实际上就是这样。然后我们得到 logits，并对这些 logits 进行归一化。
- en: Exponentiate to get counts and then normalize to get the distribution and then
    we can sample from the distribution。 So if I run this kind of anti-climatic or
    climatic， depending how you look at it。 but we get the exact same result。 And
    that's because this is in the identical model。 Not only does it achieve the same
    loss， but as I mentioned these are identical models and this W is the log counts
    of what we've estimated before。
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: 进行指数运算以获得计数，然后归一化以获得分布，然后我们可以从该分布中采样。所以如果我运行这个，取决于你的看法，它有点反高潮或高潮，但我们得到了完全相同的结果。这是因为这在同一模型中。不仅实现了相同的损失，正如我提到的，这些是相同的模型，这个
    W 是我们之前估计的对数计数。
- en: But we came to this answer in a very different way and it's got a very different
    interpretation。 But fundamentally this is basically the same model and gives the
    same samples here。 And so that's kind of cool。
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我们以非常不同的方式得出了这个答案，并且它有非常不同的解释。但从根本上讲，这基本上是同一个模型，并且在这里给出了相同的样本。所以这有点酷。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_528.png)'
  id: totrans-707
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_528.png)'
- en: Okay， so we've actually covered a lot of ground。 We introduced the bygram character
    level language model。 We saw how we can train the model， how we can sample from
    the model and how we can evaluate the quality of the model using the negative
    log likelihood loss。
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们实际上已经覆盖了很多内容。我们介绍了基于字符的二元组语言模型。我们看到了如何训练模型，如何从模型中采样，以及如何使用负对数似然损失来评估模型的质量。
- en: And then we actually trained the model in two completely different ways that
    actually get the same result and the same model。 In the first way we just counted
    up the frequency of all the bygrams and normalized。 In the second way we used
    the negative log likelihood loss as a guide to optimizing the counts matrix or
    the counts array so that the loss is minimized in the gradient based framework。
    And we saw that both of them give the same result and that's it。
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们实际上用两种完全不同的方式训练了模型，结果却得到了相同的结果和相同的模型。第一种方法我们只是计算所有二元组的频率并进行归一化。第二种方法我们使用负对数似然损失作为优化计数矩阵或计数数组的指导，以便在基于梯度的框架中最小化损失。我们发现两者都得到了相同的结果，仅此而已。
- en: Now the second one of these the gradient based framework is much more flexible。
    And right now our neural net part is super simple。 We're taking a single previous
    character and we're taking it through a single linear layer to calculate the logits。
    This is about to complexify。 So in the follow up videos we're going to be taking
    more and more of these characters and we're going to be feeding them into a neural
    net。
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，第二种基于梯度的框架更加灵活。而目前我们的神经网络部分非常简单。我们只使用一个之前的字符，并通过一个线性层来计算对数值。这个过程即将变得复杂。因此，在后续视频中，我们将会引入越来越多的字符，并将它们输入到神经网络中。
- en: But this neural net will still output the exact same thing。 The neural net will
    output logits。 And these logits will still be normalized in the exact same way
    and all the loss and everything else and the gradient based framework。 everything
    stays identical。
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: 但这个神经网络仍然会输出完全相同的内容。神经网络将输出对数值（logits）。这些对数值仍然会以完全相同的方式进行归一化，所有的损失及其他一切，以及基于梯度的框架，所有内容保持不变。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_530.png)'
  id: totrans-712
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_530.png)'
- en: It's just that this neural net will now complexify all the way to transformers。
    So that's going to be pretty awesome and I'm looking forward to it。
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: 只是这个神经网络现在会复杂化到变换器（transformers）。这将会非常令人兴奋，我对此充满期待。
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_532.png)'
  id: totrans-714
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b0f3ae8b78024dba93e80534d70c09b_532.png)'
- en: For now。 Bye。
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: 现在先这样。再见。
