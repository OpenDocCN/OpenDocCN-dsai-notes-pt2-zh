- en: P2：p2 The spelled-out intro to language modeling： building makemore - 加加zero
    - BV11yHXeuE9d
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hi everyone， hope you're well。 And next up what I'd like to do is I'd like to
    build out MakeMore。 Like Micrograd before it， MakeMore is a repository that I
    have on my GitHub webpage。 You can look at it。 But just like with Micrograd， I'm
    going to build it out step by step。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_1.png)'
  prefs: []
  type: TYPE_IMG
- en: and I'm going to spell everything out。 So we're going to build it out slowly
    and together。 Now。 what is MakeMore？ MakeMore， as the name suggests， makes more
    of things that you give it。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_3.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_4.png)'
  prefs: []
  type: TYPE_IMG
- en: So here's an example。 Names。txt is an example data set to MakeMore。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_6.png)'
  prefs: []
  type: TYPE_IMG
- en: And when you look at names。txt， you'll find that it's a very large data set
    of names。 So here's lots of different types of names。 In fact， I believe there
    are 32，000 names。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_8.png)'
  prefs: []
  type: TYPE_IMG
- en: that I've sort of found randomly on the government website。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_10.png)'
  prefs: []
  type: TYPE_IMG
- en: And if you train MakeMore on this data set， it will learn to make more of things
    like this。 And in particular， in this case， that will mean more things that sound
    name-like。 but are actually unique names。 And maybe if you have a baby and you're
    trying to assign a name。 maybe you're looking for a cool new sounding unique name，
    MakeMore might help you。
  prefs: []
  type: TYPE_NORMAL
- en: So here are some example generations from the neural network once we train it
    on our data set。 So here's some example unique names that it will generate。 Don't
    tell。 I rot。 Zen-di。 And so on。 And so all these sort of sound name-like， but
    they're not， of course， names。 So under the hood。 MakeMore is a character-level
    language model。 So what that means is that it is treating every single line here
    as an example。
  prefs: []
  type: TYPE_NORMAL
- en: And within each example， it's treating them all as sequences of individual characters。
    So R-E-E-S-E is this example。 And that's the sequence of characters。 And that's
    the level in which we are building out MakeMore。 And what it means to be a character-level
    language model then is that it's just sort of modeling those sequences of characters。
  prefs: []
  type: TYPE_NORMAL
- en: and it knows how to predict the next character in the sequence。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_12.png)'
  prefs: []
  type: TYPE_IMG
- en: Now， we're actually going to implement a large number of character-level language
    models。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_14.png)'
  prefs: []
  type: TYPE_IMG
- en: in terms of the neural networks that are involved in predicting the next character
    in a sequence。 So very simple， by-gram and bag-of-word models， multi-level perceptrons，
    recurring neural networks。 all the way to modern transformers。 In fact， a transformer
    that we will build will be basically the equivalent transformer to GPT-2。 if you
    have heard of GPT。 So that's kind of a big deal。 It's a modern network。
  prefs: []
  type: TYPE_NORMAL
- en: And by the end of the series， you will actually understand how that works on
    the level of characters。 Now， to give you a sense of the extensions here， after
    characters。 we will probably spend some time on the next level。 We will probably
    spend some time on the word level so that we can generate documents of words。
  prefs: []
  type: TYPE_NORMAL
- en: not just little segments of characters， but we can generate entire large， much
    larger documents。 And then we're probably going to go into images and image text
    networks， such as Dali。 Stable Diffusion and so on。 But for now， we have to start
    here， character-level language modeling。 Let's go。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_16.png)'
  prefs: []
  type: TYPE_IMG
- en: So like before， we are starting with a completely blank， duplicate page。 The
    first thing is I would like to basically load up the dataset， named。txt。 So we're
    going to open up names。txt for reading。 And we're going to read in everything
    into a massive string。 And then because it's a massive string， we'd only like
    the individual words and put them in the list。
  prefs: []
  type: TYPE_NORMAL
- en: So let's call split lines on that string to get all of our words as a Python
    list of strings。 So basically， we can look at， for example， the first 10 words。
    And we have that it's a list of Emma。 Olivia， Eva， and so on。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_18.png)'
  prefs: []
  type: TYPE_IMG
- en: And if we look at the top of the page here， that is indeed what we see。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_20.png)'
  prefs: []
  type: TYPE_IMG
- en: So that's good。 This list actually makes me feel that this is probably sorted
    by frequency。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_22.png)'
  prefs: []
  type: TYPE_IMG
- en: But， okay， so these are the words。 Now， we'd like to actually like learn a little
    bit more about this dataset。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_24.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's look at the total number of words。 We expect this to be roughly 32，000。
    And then what is the。 for example， shortest word？ So min of， line of each word
    for w in words。 So the shortest word will be length two。 And max of one w for
    w in words。 So the longest word will be 15 characters。 So let's now think through
    our very first language model。
  prefs: []
  type: TYPE_NORMAL
- en: As I mentioned， a character level language model is predicting the next character
    in a sequence。 given already some concrete sequence of characters before it。 Now。
    what we have to realize here is that every single word here， like a Zabella。 is
    actually quite a few examples packed in to that single word。
  prefs: []
  type: TYPE_NORMAL
- en: Because what is an existence of a word like a Zabella in the dataset telling
    us really？
  prefs: []
  type: TYPE_NORMAL
- en: It's saying that the character i is a very likely character to come first in
    a sequence of a name。 The character S is likely to come after i。 The character
    A is likely to come after iS。 The character B is very likely to come after iS
    A。 And so on all the way to A following a Zabella。 And then there's one more example
    actually packed in here。 And that is that after there's a Zabella。
  prefs: []
  type: TYPE_NORMAL
- en: the word is very likely to end。 So that's one more sort of explicit piece of
    information that we have here。 That we have to be careful with。 And so there's
    a lot packed into a single individual word in terms of the statistical structure。
    of what's likely to follow in these character sequences。 And then of course we
    don't have just an individual word。 We actually have 32，000 of these。
  prefs: []
  type: TYPE_NORMAL
- en: And so there's a lot of structure here to model。 Now in the beginning what I'd
    like to start with is I'd like to start with building a MIGRAM language model。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_26.png)'
  prefs: []
  type: TYPE_IMG
- en: Now in a MIGRAM language model we're always working with just two characters
    at a time。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_28.png)'
  prefs: []
  type: TYPE_IMG
- en: So we're only looking at one character that we are given and we're trying to
    predict the next character in the sequence。 So what characters are likely to follow
    are what characters are likely to follow A and so on。 And we're just modeling
    that kind of a little local structure。 And we're forgetting the fact that we may
    have a lot more information。
  prefs: []
  type: TYPE_NORMAL
- en: We're always just looking at the previous character to predict the next one。
    So it's a very simple and weak language model， but I think it's a great place
    to start。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_30.png)'
  prefs: []
  type: TYPE_IMG
- en: So now let's begin by looking at these BIGRAMS in our dataset and what they
    look like。 And these BIGRAMS again are just two characters in a row。 So for W
    and words。 each W here is an individual word string。 We want to iterate this word
    with consecutive characters。 So two characters at a time sliding it through the
    word。 Now a interesting nice way。
  prefs: []
  type: TYPE_NORMAL
- en: cute way to do this in Python by the way， is doing something like this。 For
    character one。 character two in zip of W and W at one。 One call。 Print， character
    one， character two。 And let's not do all the words。 Let's just do the first three
    words。 And I'm going to show you in a second how this works。 But for now， basically
    as an example。
  prefs: []
  type: TYPE_NORMAL
- en: let's just do the very first word alone， Emma。 You see how we have a Emma？
  prefs: []
  type: TYPE_NORMAL
- en: And this will just print EM， MMA。 And the reason this works is because W is
    the string Emma。 W at one column is the string MMA。 And zip takes two iterators。
    And it pairs them up and then creates an iterator over the tuples of their consecutive
    entries。 And if any one of these lists is shorter than the other， then it will
    just halt and return。
  prefs: []
  type: TYPE_NORMAL
- en: So basically that's why we return EM， MMA， MMA。 But then because this iterator，
    second one here。 runs out of elements， zip just ends。 And that's why we only get
    these tuples。 So pretty cute。 So these are the consecutive elements in the first
    word。 Now we have to be careful because we actually have more information here
    than just these three examples。
  prefs: []
  type: TYPE_NORMAL
- en: As I mentioned， we know that E is very likely to come first and we know that
    A in this case is coming last。 So one way to do this is basically we're going
    to create a special array here， our characters。 And we're going to hallucinate
    a special start token here。 I'm going to call it like special start。 So this is
    a list of one element plus W and then plus a special end character。
  prefs: []
  type: TYPE_NORMAL
- en: And the reason I'm wrapping the list of W here is because W is a string MMA。
    List of W will just have the individual characters in the list。 And then doing
    this again now。 but not iterating over Ws but over the characters will give us
    something like this。 So E is likely。 so this is a by gram of the start character
    and E。
  prefs: []
  type: TYPE_NORMAL
- en: And this is a by gram of the A in the special end character。 And now we can
    look at， for example。 what this looks like for Olivia or Eva。 And indeed we can
    actually。 potentially this for the entire dataset。 But we won't print that， that's
    going to be too much。 But these are the individual character by grams and we can
    print them。
  prefs: []
  type: TYPE_NORMAL
- en: Now in order to learn the statistics about which characters are likely to follow
    other characters。 the simplest way in the by gram language models is to simply
    do it by counting。 So we're basically just going to count how often any one of
    these combinations occurs in the training set in these words。 So we're going to
    need some kind of a dictionary that's going to maintain some counts for every
    one of these by grams。
  prefs: []
  type: TYPE_NORMAL
- en: So let's use a dictionary B and this will map these by grams。 So by gram is
    a tuple of character on character two。 And then B at by gram will be beat up get
    of by gram， which is basically the same as B at by gram。 But in the case that
    by gram is not in the dictionary B。
  prefs: []
  type: TYPE_NORMAL
- en: we would like to buy default return zero plus one。 So this will basically add
    up all the by grams and count how often they occur。 Let's get rid of printing
    or rather， let's keep the printing and let's just inspect what B is in this case。
    And we see that many by grams occur just a single time。 This one allegedly occurred
    three times。
  prefs: []
  type: TYPE_NORMAL
- en: So A was an ending character three times and that's true for all of these words。
    All of Emma。 Olivia and Eva and with A。 So that's why this occurred three times。
    Now let's do it for all the words。 Oops， I should not have printed it。 I'm going
    to erase that。 Let's kill this。 Let's just run and now B will have the statistics
    of the entire data set。
  prefs: []
  type: TYPE_NORMAL
- en: So these are the counts across all the words of the individual by grams。 And
    we could， for example。 look at some of the most common ones and least common ones。
    This kind of grows in Python。 but the way to do this， the simplest way I like
    is we just use B dot items。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_32.png)'
  prefs: []
  type: TYPE_IMG
- en: B dot items returns the tuples of key value。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_34.png)'
  prefs: []
  type: TYPE_IMG
- en: In this case， the keys are the character by grams and the values are the counts。
    And so then what we want to do is we want to do sorted of this。 But the default
    sort is on the first item of a tuple。 But we want to sort by the values which
    are the second element of a tuple that is the key value。
  prefs: []
  type: TYPE_NORMAL
- en: So we want to use the key equals lambda that takes the key value and returns
    the key value at one。 not at zero but at one， which is the count。 So we want to
    sort by the count of these elements。 And actually we want it to go backwards。
    So here we have is the by gram Q and R occurs only a single time。 DZ occurred
    only a single time。 And when we sort this the other way around。
  prefs: []
  type: TYPE_NORMAL
- en: we're going to see the most likely by grams。 So we see that N was very often
    an ending character many。 many times。 And apparently N almost always follows an
    A and that's a very likely combination as well。 So this is kind of the individual
    counts that we achieve over the entire DZ。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_36.png)'
  prefs: []
  type: TYPE_IMG
- en: Now it's actually going to be significantly more convenient for us to keep this
    information in a two dimensional array instead of a Python dictionary。 So we're
    going to store this information in a 2D array。 And the rows are going to be the
    first character of the by gram and the columns are going to be the second character。
    And each entry in the student machine array will tell us how often that first
    character follows the second character in the DZ。
  prefs: []
  type: TYPE_NORMAL
- en: So in particular the array representation that we're going to use or the library
    is that of PyTorch。 And PyTorch is a deep learning neural work framework。 But
    part of it is also this torch。tensor which allows us to create multi dimensional
    arrays and manipulate them very efficiently。 So let's import PyTorch which you
    can do by import torch。 And then we can create arrays。
  prefs: []
  type: TYPE_NORMAL
- en: So let's create a array of zeros and we give it a size of this array。 Let's
    create a 3x5 array as an example。 And this is a 3x5 array of zeros。 And by default
    you'll notice 8 of D type which is short for data type is float 32。 So these are
    single precision floating point numbers。
  prefs: []
  type: TYPE_NORMAL
- en: Because we are going to represent counts let's actually use D type as torch。in32。
    So these are 32 bit integers。 So now you see that we have integer data inside
    this tensor。 Now tensors allow us to really manipulate all the individual entries
    and do it very efficiently。 So for example if we want to change this bit we have
    to index into the tensor。
  prefs: []
  type: TYPE_NORMAL
- en: And in particular here this is the first row because it's zero indexed。 So this
    is row index one and column index 0， 1， 2， 3。 So A at 1。 3 we can set that to
    1 and then A will have a 1 over there。 We can of course also do things like this。
    So now A will be 2 over there or 3。
  prefs: []
  type: TYPE_NORMAL
- en: And also we can for example say A00 is 5 and then A will have a 5 over here。
    So that's how we can index into the arrays。 Now of course the array that we are
    interested in is much much bigger。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_38.png)'
  prefs: []
  type: TYPE_IMG
- en: So for our purposes we have 26 letters of the alphabet and then we have two
    special characters S and E。 So we want 26 plus 2 or 28 by 28 array。 And let's
    call it the capital N because it's going to represent sort of the counts。 Let
    me erase this stuff。 So that's the array that starts at 0s 28 by 28。 And now let's
    copy paste this here。 But instead of having a dictionary B which we're going to
    erase we now have an N。
  prefs: []
  type: TYPE_NORMAL
- en: Now the problem here is that we have these characters which are strings but
    we have to now。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_40.png)'
  prefs: []
  type: TYPE_IMG
- en: basically index into a array and we have to index using integers。 So we need
    some kind of a lookup table from characters to integers。 So let's construct such
    a character array。 And the way we're going to do this is we're going to take all
    the words which is a list of strings。 We're going to concatenate all of it into
    a massive string。
  prefs: []
  type: TYPE_NORMAL
- en: So this is just simply the entire dataset as a single string。 We're going to
    pass this to the set constructor which takes this massive string and throws out。
    duplicates because sets do not allow duplicates。 So set of this will just be the
    set of all the lowercase characters。 And there should be a total of 26 of them。
    And now we actually don't want a set we want a list。
  prefs: []
  type: TYPE_NORMAL
- en: But we don't want a list sorted in some weird arbitrary way we want it to be
    sorted from A to Z。 So a sorted list。 So those are our characters。 Now what we
    want is this lookup table as I mentioned。 So let's create a special S to I， I
    will call it S is string or character。 And this will be an S to I mapping for
    is in enumerate of these characters。
  prefs: []
  type: TYPE_NORMAL
- en: So enumerate basically gives us this iterator over the integer index and the
    actual element of the list。 And then we are mapping the character to the integer。
    So S to I is a mapping from A to zero B to one， etc。 all the way from Z to 25。
    And that's going to be useful here， but we actually also have to specifically
    set that S will be 26。
  prefs: []
  type: TYPE_NORMAL
- en: And S to I at E will be 27， right， because Z was 25。 So those are the lookups。
    And now we can come here and we can map both character one and character two to
    their integers。 So this will be S to I character one and I X two will be S to
    I of character two。 And now we should be able to do this line but using our array。
    So an at X one， I X two。
  prefs: []
  type: TYPE_NORMAL
- en: This is the two dimensional array indexing I've shown you before。 And honestly
    just plus equals one because everything starts at zero。 So this should work and
    give us a large 28 by 20 array of all these counts。 So if we print N this is the
    array， but of course it looks ugly。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_42.png)'
  prefs: []
  type: TYPE_IMG
- en: So let's erase this ugly mess and let's try to visualize it a bit more nicer。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_44.png)'
  prefs: []
  type: TYPE_IMG
- en: So for that we're going to use a library called matplotlib。 So matplotlib allows
    us to create figures。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_46.png)'
  prefs: []
  type: TYPE_IMG
- en: So we can do things like PLT， I'm show of the counter a。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_48.png)'
  prefs: []
  type: TYPE_IMG
- en: So this is the 20 by 20 array and this is structure。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_50.png)'
  prefs: []
  type: TYPE_IMG
- en: But even this I would say is still pretty ugly。 So we're going to try to create
    a much nicer visualization of it and I wrote a bunch of code for that。 The first
    thing we're going to need is we're going to need to invert this array here。 this
    dictionary。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_52.png)'
  prefs: []
  type: TYPE_IMG
- en: S to I is a mapping from S to I and in I to S we're going to reverse this dictionary。
    So it rated over all the items and just reverse that array。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_54.png)'
  prefs: []
  type: TYPE_IMG
- en: So I to S maps inversely from zero to A， one to B， etc。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_56.png)'
  prefs: []
  type: TYPE_IMG
- en: So we'll need that。 And then here's the code that I came up with to try to make
    this a little bit nicer。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_58.png)'
  prefs: []
  type: TYPE_IMG
- en: We create a figure we plot M and then we do and then we visualize a bunch of
    things later。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_60.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_61.png)'
  prefs: []
  type: TYPE_IMG
- en: Let me just run it so you get a sense of what it is。 So you see here that we
    have the array spaced out and every one of these is basically like B follows G
    zero times。 B follows H 41 times。 So A follows J one 75 times。 And so what you
    can see that I'm doing here is first I show that entire array and then I iterate
    over all the individual cells here。
  prefs: []
  type: TYPE_NORMAL
- en: And I create a character string here， which is the inverse mapping I to S of
    integer I and integer J。 So that's those are the biograms in a character representation。
    And then I plot just the diagram text and then I plot the number of times that
    is by grammar curse。 Now the reason that there's a dot item here is because when
    you index into these arrays。
  prefs: []
  type: TYPE_NORMAL
- en: these are torch tensors。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_63.png)'
  prefs: []
  type: TYPE_IMG
- en: You see that we still get a tensor back。 So the type of this thing you think
    it would be just an integer 149。 but it's actually a torch that tensor。 And so
    if you do dot item。 then it will pop out that individual integer。 So it'll just
    be 149。 So that's what's happening there。 And these are just some options to make
    it look nice。
  prefs: []
  type: TYPE_NORMAL
- en: So what is the structure of this array？ We have all these counts and we see
    that some of them occur often and some of them do not occur often。 Now if you
    scrutinize this carefully， you will notice that we're not actually being very
    clever。 That's because when you come over here， you'll notice that for example
    we have an entire row of completely zeros。 And that's because the end character
    is never possibly going to be the first character of a biogram。
  prefs: []
  type: TYPE_NORMAL
- en: because we're always placing these end tokens all at the end of a biogram。 Similarly。
    we have entire column zeros here because the S character will never possibly be
    the second element of a biogram。 because we always start with S and we end with
    E and we only have the words in between。 So we have an entire column of zeros，
    an entire row of zeros。
  prefs: []
  type: TYPE_NORMAL
- en: and in this little 2x2 matrix here as well， the only one that can possibly happen
    is if S directly follows E。 That can be nonzero if we have a word that has no
    letters。 So in that case。 there's no letters in the word。 It's an empty word and
    we just have S follows E。 But the other ones are just not possible。 And so we're
    basically wasting space and not only that。
  prefs: []
  type: TYPE_NORMAL
- en: but the S and the E are getting very crowded here。 I was using these brackets
    because there's convention in natural language processing to use these kinds of
    brackets。 to denote special tokens， but we're going to use something else。 So
    let's fix all this and make it prettier。 We're not actually going to have two
    special tokens。
  prefs: []
  type: TYPE_NORMAL
- en: we're only going to have one special token。 So we're going to have n by n array
    of 27 by 27 instead。 Instead of having two， we will just have one and I will call
    it a dot。 Okay。 Let me swing this over here。 Now one more thing that I would like
    to do is I would actually like to make this special character half position zero。
    And I would like to offset all the other letters off。 I find that a little bit
    more pleasing。
  prefs: []
  type: TYPE_NORMAL
- en: So we need a plus one here so that the first character， which is a， will start
    at one。 So S to I will now be a starts at one and dot is zero。 And I to S， of
    course。 we're not changing this because I to S just creates a reverse mapping
    and this will work fine。 So one is a two is B zero is dot。 So we reverse that
    here， we have a dot and a dot。
  prefs: []
  type: TYPE_NORMAL
- en: This should work fine。 Make sure I started zeros count and then here we don't
    go up to 28。 we go up to 27。 And this should just work。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_65.png)'
  prefs: []
  type: TYPE_IMG
- en: Okay， so we see that dot dot never happened。 It's a zero because we don't have
    empty words。 Then this row here now is just very simply the counts for all the
    first letters。 So J starts a word。 H starts a word， I start a word， etc。 And then
    these are all the ending characters。 And in between。 we have the structure of
    what characters follow each other。
  prefs: []
  type: TYPE_NORMAL
- en: So this is the counts array of our entire data set。 So this array actually has
    all the information necessary for us to actually sample from this。 by gram character
    level language model。 And roughly speaking。 what we're going to do is we're just
    going to start following these probabilities and these counts。
  prefs: []
  type: TYPE_NORMAL
- en: And we're going to start sampling from the from model。 So in the beginning，
    of course。 we start with the dot， the start token dot。 So to sample the first
    character of a name。 we're looking at this row here。 So we see that we have the
    counts and those counts externally are telling us how often any one of these characters
    is to start a word。 So if we take this N and we grab the first row， we can do
    that by using just indexing and zero。
  prefs: []
  type: TYPE_NORMAL
- en: And then using this notation column for the rest of that row。 So N zero column
    is indexing into the zero row and then it's grabbing all the columns。 And so this
    will give us a one dimensional array of the first row。 So zero， four， four， ten。
    you know， zero， four， four， ten， one， three， oh， six， one， five， four， two， etc。
  prefs: []
  type: TYPE_NORMAL
- en: It's just the first row。 The shape of this is twenty seven。 It's just the row
    of twenty seven。 And the other way that you can do this also is you just， you
    don't actually give this。 you just grab the zero row like this。 This is equivalent。
    Now these are the counts and now what we'd like to do is we'd like to basically
    sample from this。
  prefs: []
  type: TYPE_NORMAL
- en: Since these are the raw counts， we actually have to convert this to probabilities。
    So we create a probability vector。 So we'll take N of zero and we'll actually
    convert this to float first。 Okay， so these integers are converted to float， floating
    point numbers。 And the reason we're creating floats is because we're about to
    normalize these counts。
  prefs: []
  type: TYPE_NORMAL
- en: So to create a probability distribution here， we want to divide。 We basically
    want to do p。 p divide p that sum。 Now we get a vector of smaller numbers and
    these are now probabilities。 So of course because we divided by the sum， the sum
    of p now is one。 So this is a nice proper probability distribution。
  prefs: []
  type: TYPE_NORMAL
- en: It sums to one and this is giving us the probability for any single character
    to be the first character of a word。 So now we can try to sample from this distribution。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_67.png)'
  prefs: []
  type: TYPE_IMG
- en: To sample from these distributions， we're going to use Torre-Tut Multinomial。
    which I've pulled up here。 So Torre-Tut Multinomial returns samples from the Multinomial
    probability distribution。 which is a complicated way of saying， you give me probabilities
    and I will give you integers。 which are sampled according to the probability distribution。
    So this is the signature of the method。
  prefs: []
  type: TYPE_NORMAL
- en: And to make everything deterministic， we're going to use a generator object
    in PyTorch。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_69.png)'
  prefs: []
  type: TYPE_IMG
- en: So this makes everything deterministic。 So when you run this on your computer。
    you're going to get the exact same results that I'm getting here on my computer。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_71.png)'
  prefs: []
  type: TYPE_IMG
- en: So let me show you how this works。 Here's the deterministic way of creating
    a Torre-Tut generator object。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_73.png)'
  prefs: []
  type: TYPE_IMG
- en: Seating it with some number that we can agree on。 So that seeds a generator。
    It gives us an object G。 And then we can pass that G to a function that creates
    here random numbers。 Torre-Tut random creates random numbers， three of them。 And
    it's using this generator object as a source of randomness。 So without normalizing
    it。
  prefs: []
  type: TYPE_NORMAL
- en: I can just print。 This is sort of like numbers between zero and one that are
    random according to this thing。 And whenever I run it again， I'm always going
    to get the same result because I keep using the same generator object which I'm
    seeding here。 And then if I divide to normalize， I'm going to get a nice probability
    distribution of just three elements。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_75.png)'
  prefs: []
  type: TYPE_IMG
- en: And then we can use Torre-Tut multinomial to draw samples from it。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_77.png)'
  prefs: []
  type: TYPE_IMG
- en: So this is what that looks like。 Torre-Tut multinomial will take the Torre-Tensor
    of probability distributions。 Then we can ask for a number of samples like say
    20。 Replacement equals true means that when we draw an element。 we can draw it
    and then we can put it back into the list of eligible indices to draw again。
  prefs: []
  type: TYPE_NORMAL
- en: And we have to specify replacement as true because by default for some reason
    it's false。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_79.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_80.png)'
  prefs: []
  type: TYPE_IMG
- en: And I think it's just something to be careful with。 And the generator is passed
    in here。 So we're going to always get deterministic results in the same results。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_82.png)'
  prefs: []
  type: TYPE_IMG
- en: So if I run these two， we're going to get a bunch of samples from this distribution。
    Now you'll notice here that the probability for the first element in this tensor
    is 60%。 So in these 20 samples， we'd expect 60% of them to be zero。 We'd expect
    30% of them to be one。 And because the element index two has only 10% probability，
    very few of these samples should be two。
  prefs: []
  type: TYPE_NORMAL
- en: And indeed we only have a small number of twos。 And we can sample as many as
    we would like。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_84.png)'
  prefs: []
  type: TYPE_IMG
- en: And the more we sample， the more these numbers should roughly have the distribution
    here。 So we should have lots of zeros， half as many ones。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_86.png)'
  prefs: []
  type: TYPE_IMG
- en: And we should have three times S few ones and three times S few twos。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_88.png)'
  prefs: []
  type: TYPE_IMG
- en: So you see that we have very few twos。 We have some ones and most of them are
    zero。 So that's what Torx-Dynote-Nal is doing。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_90.png)'
  prefs: []
  type: TYPE_IMG
- en: For us here， we aren't just in this row， we've created this P here， and now
    we can sample from it。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_92.png)'
  prefs: []
  type: TYPE_IMG
- en: So if we use the same seed and then we sample from this distribution， let's
    just get one sample。 Then we see that the sample is say 13。 So this will be the
    index。 And you see how it's a tensor that wraps 13？ We again have to use 。item
    to pop out that integer。 And now index would be just the number 13。 And of course
    we can map the I2S of Ix to figure out exactly which character we're sampling
    here。
  prefs: []
  type: TYPE_NORMAL
- en: We're sampling M。 So we're saying that the first character is M in our generation。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_94.png)'
  prefs: []
  type: TYPE_IMG
- en: And just looking at the row here， M was drawn。 And we can see that M actually
    starts a large number of words。 M started 2，500 words out of 32，000 words。 So
    almost a bit less than 10% of the words start with M。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_96.png)'
  prefs: []
  type: TYPE_IMG
- en: So this was actually a fairly likely character to draw。 So that would be the
    first character of our word。 And now we can continue to sample more characters。
    Because now we know that M started。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_98.png)'
  prefs: []
  type: TYPE_IMG
- en: M is already sampled。 So now to draw the next character。 we will come back here
    and we will look for the row that starts with M。 So you see M and we have a row
    here。 So we see that M。 is 516， M。A。 is this many， M。B。 is this many。 etc。 So
    these are the counts for the next row。 And that's the next character that we are
    going to now generate。
  prefs: []
  type: TYPE_NORMAL
- en: So I think we are ready to actually just write out the loop。 Because I think
    you're starting to get a sense of how this is going to go。 We always begin at
    index 0， because that's the start token。 And then while true。 we're going to grab
    the row corresponding to index that we're currently on。 So that's P。
  prefs: []
  type: TYPE_NORMAL
- en: So that's N， array at Ix。 So the way to float is R P。 Then we normalize this
    P to sum to 1。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_100.png)'
  prefs: []
  type: TYPE_IMG
- en: Accidentally random infinite loop。 We normalize P to sum to 1。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_102.png)'
  prefs: []
  type: TYPE_IMG
- en: Then we need this generator object。 And we're going to initialize up here。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_104.png)'
  prefs: []
  type: TYPE_IMG
- en: And we're going to draw a single sample from this distribution。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_106.png)'
  prefs: []
  type: TYPE_IMG
- en: And then this is going to tell us what index is going to be next。 If the index
    sampled is 0。 then that's now the end token。 So we will break。 Otherwise we are
    going to print S2I of Ix。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_108.png)'
  prefs: []
  type: TYPE_IMG
- en: I2S of Ix。 And that's pretty much it。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_110.png)'
  prefs: []
  type: TYPE_IMG
- en: We're just， this should work。 Okay， more。 So that's the name that we've sampled。
    We start it with M。 the next stop was O， then R， and then dot。 And this dot， we
    print it here as well。 So let's now do this a few times。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_112.png)'
  prefs: []
  type: TYPE_IMG
- en: So let's actually create an out list here。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_114.png)'
  prefs: []
  type: TYPE_IMG
- en: And instead of printing， we're going to append。 So out that append this character。
    And then here。 let's just print it at the end。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_116.png)'
  prefs: []
  type: TYPE_IMG
- en: So let's just join up all the outs。 And we're just going to print more。 Okay。
    now we're always getting the same result because of the generator。 So if we want
    to do this a few times， we can go for high and range 10。 We can sample 10 names。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_118.png)'
  prefs: []
  type: TYPE_IMG
- en: And we can just do that 10 times。 And these are the names that we're getting
    out。 Let's do 20。 I'll be honest with you， this doesn't look right。 So I'll stare
    at it a few minutes to convince myself that it actually is right。 The reason these
    samples are so terrible is that by gram language model is actually。 just like
    really terrible。 We can generate a few more here。
  prefs: []
  type: TYPE_NORMAL
- en: And you can see that they're kind of like their name like a little bit like，
    Yannu， Erile， etc。 But they're just like totally messed up。 And I mean， the reason
    that this is so bad。 like we're generating H as a name。 But you have to think
    through it from the model's eyes。 It doesn't know that this H is the very first
    H。 All it knows is that H was previously。
  prefs: []
  type: TYPE_NORMAL
- en: And now how likely is H the last character？ Well， it's somewhat likely。 And
    so it just makes it last character。 It doesn't know that there were other things
    before it or there were not other things before it。 And so that's why I'm generating
    all these nonsense names。 Another way to do this is to convince yourself that
    it's actually doing something， reasonable。
  prefs: []
  type: TYPE_NORMAL
- en: even though it's so terrible， is these little piece here are 27， right？ Like
    27。 So how about if we did something like this？ Instead of having any structure
    whatsoever。 How about if P was just a torch dot ones of 27？ By default， this is
    a float 32。 So this is fine。 Divide 27。 So what I'm doing here is this is the
    uniform distribution。
  prefs: []
  type: TYPE_NORMAL
- en: which will make everything equally likely。 And we can sample from that。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_120.png)'
  prefs: []
  type: TYPE_IMG
- en: So let's see if that does any better。 Okay？ So this is what you have from a
    model that is completely untrained。 where everything is equally likely。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_122.png)'
  prefs: []
  type: TYPE_IMG
- en: So it's obviously garbage。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_124.png)'
  prefs: []
  type: TYPE_IMG
- en: And then if we have a trained model， which is trained on just by grams， this
    is what we get。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_126.png)'
  prefs: []
  type: TYPE_IMG
- en: So you can see that it is more name like it is actually working。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_128.png)'
  prefs: []
  type: TYPE_IMG
- en: It's just by gram is so terrible and we have to do better。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_130.png)'
  prefs: []
  type: TYPE_IMG
- en: Now next， I would like to fix an inefficiency that we have going on here。 Because
    what we're doing here is we're always fetching a row of N from the counts matrix
    up ahead。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_132.png)'
  prefs: []
  type: TYPE_IMG
- en: And then we're always doing the same things。 We're converting to float and we're
    dividing。 And we're doing this every single iteration of this loop。 And we just
    keep renormalizing these rows over and over again and it's extremely inefficient
    and wasteful。 So what I'd like to do is I'd like to actually prepare a matrix
    capital P that will just have the probabilities in it。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_134.png)'
  prefs: []
  type: TYPE_IMG
- en: So in other words， it's going to be the same as the capital N matrix here of
    counts。 But every single row will have the row of probabilities that is normalized
    to one。 indicating the probability distribution for the next character given the
    character before it。 As defined by which row we're in。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_136.png)'
  prefs: []
  type: TYPE_IMG
- en: So basically what we'd like to do is we'd like to just do it up front here。
    And then we would like to just use that row here。 So here we would like to just
    do P equals P of I X instead。 Okay。 The other reason I want to do this is not
    just proficiency。 but also I would like us to practice these indimensional tensors。
  prefs: []
  type: TYPE_NORMAL
- en: And I'd like us to practice their manipulation and especially something that's
    called broadcasting that we'll go into in a second。 We're actually going to have
    to become very good at these tensor manipulations because we're going to build
    out all the way to transformers。
  prefs: []
  type: TYPE_NORMAL
- en: We're going to be doing some pretty complicated array operations for efficiency。
    And we need to really understand that and be very good at it。 So intuitively what
    we want to do is we first want to grab the floating point copy of N。 And I'm mimicking
    the line here basically。 And then we want to divide all the rows so that they
    sum to one。
  prefs: []
  type: TYPE_NORMAL
- en: So we like to do something like this， P divide P dot sum。 But now we have to
    be careful because P dot sum actually produces a sum。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_138.png)'
  prefs: []
  type: TYPE_IMG
- en: Sorry， P equals and that float copy。 P dot sum produces a sums up all of the
    counts of this entire matrix N and gives us a single number of just the summation
    of everything。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_140.png)'
  prefs: []
  type: TYPE_IMG
- en: So that's not the way we want to divide。 We want to simultaneously and imperil
    divide all the rows by their respective sums。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_142.png)'
  prefs: []
  type: TYPE_IMG
- en: So what we have to do now is we have to go into documentation for Torx dot sum。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_144.png)'
  prefs: []
  type: TYPE_IMG
- en: And we can scroll down here to a definition that is relevant to us。 which is
    where we don't only provide an input array that we want to sum。 but we also provide
    the dimension along which we want to sum。 And in particular we want to sum up
    over rows。 Now one more argument that I want you to pay attention to here is the
    keep them as false。
  prefs: []
  type: TYPE_NORMAL
- en: If keep them is true， then the output tensor is of the same size as input。 except
    of course the dimension along which you summed， which will become just one。 But
    if you pass in keep them as false， then this dimension is squeezed out。 And so
    Torx dot sum not only does the sum and collapses the dimension to be of size one。
  prefs: []
  type: TYPE_NORMAL
- en: but in addition it does what's called a squeeze， where it squeezes out that
    dimension。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_146.png)'
  prefs: []
  type: TYPE_IMG
- en: So basically what we want here is we instead want to do p dot sum of sum axis。
    And in particular。 notice that p dot shape is 27 by 27。 So when we sum up across
    axis zero。 then we would be taking the zero dimension and we would be summing
    across it。 So when keep them as true， then this thing will not only give us the
    counts across along the columns。
  prefs: []
  type: TYPE_NORMAL
- en: but notice that basically the shape of this is one by 27。 We just get a row
    vector。 And the reason we get a row vector here again is because we pass in zero
    dimension。 So this zero dimension becomes one and we've done a sum and we get
    a row。 And so basically we've done the sum this way vertically and arrived at
    just a single one by 27 vector of counts。
  prefs: []
  type: TYPE_NORMAL
- en: What happens when you take out keep them is that we just get 27。 So it squeezes
    out that dimension and we just get one dimensional vector of size 27。 Now we don't
    actually want one by twenty seven row vector because that gives us the counts
    or the sums across the columns。 We actually want to sum the other way along dimension
    one。
  prefs: []
  type: TYPE_NORMAL
- en: And you'll see that the shape of this is 27 by one。 So it's a column vector。
    It's a twenty seven by one vector of counts。 And that's because what's happened
    here is that we're going horizontally and this twenty seven by twenty seven matrix
    becomes a twenty seven by one array。
  prefs: []
  type: TYPE_NORMAL
- en: Now you'll notice by the way that the actual numbers of these counts are identical。
    And that's because this special array of counts here comes from by grams to the
    sticks。 And actually it just so happens by chance or because of the way this array
    is constructed that this sounds along the columns or along the rows horizontally
    or vertically is identical。 But actually what we want to do in this case is we
    want to sum across the rows horizontally。
  prefs: []
  type: TYPE_NORMAL
- en: So what we want here is be that some of one with keep them true。 Twenty seven
    by one column vector。 And now what we want to do is we want to divide by that。
    Now we have to be careful here again。 Is it possible to take what's a p dot shape
    you see here is twenty seven by twenty seven。 Is it possible to take a twenty
    seven by twenty seven array and divided by what is a twenty seven by one array。
  prefs: []
  type: TYPE_NORMAL
- en: Is that an operation that you can do？ And whether or not you can perform this
    operation is determined by what's called broadcasting rules。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_148.png)'
  prefs: []
  type: TYPE_IMG
- en: So if you just search broadcasting semantics in torch。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_150.png)'
  prefs: []
  type: TYPE_IMG
- en: You'll notice that there's a special definition for what's called broadcasting
    that for whether or not these two arrays can be combined in a binary operation
    like division。 So the first condition is each tensor has at least one dimension
    which is the case for us。
  prefs: []
  type: TYPE_NORMAL
- en: And then when iterating over the dimension sizes starting at the trailing dimension。
    The dimension sizes must either be equal one of them is one or one of them does
    not exist。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_152.png)'
  prefs: []
  type: TYPE_IMG
- en: Okay， so let's do that we need to align the two arrays and their shapes which
    is very easy because both of these shapes have two elements so they're aligned。
    Then we iterate over from the from the right and going to the left。 Each dimension
    must be either equal one of them is a one or one of them does not exist。 So in
    this case they're not equal but one of them is a one so this is fine。
  prefs: []
  type: TYPE_NORMAL
- en: And then this dimension they're both equal so this is fine。 So all the dimensions
    are fine and therefore the this operation is broadcastable。 So that means that
    this operation is allowed and what is it that these arrays do when you divide
    27 by 27 by 27 by one。 What it does is that it takes this dimension one and it
    stretches it out it copies it to match 27 here in this case。
  prefs: []
  type: TYPE_NORMAL
- en: So in our case it takes this column vector which is 27 by one and it copies
    it 27 times to make these both be 27 by 27 internally。 You can think of it that
    way。 And so it copies those counts and then it does an element wise division which
    is what we want because these counts we want to divide by them on every single
    one of these columns in this matrix。
  prefs: []
  type: TYPE_NORMAL
- en: So this actually we expect will normalize every single row。 And we can check
    that this is true by taking the first row for example and taking it some。 We expect
    this to be one because it's not normalized。 And then we expect this now because
    if we actually correctly normalize all the rows we expect to get the exact same
    result here。
  prefs: []
  type: TYPE_NORMAL
- en: So let's run this。 It's the exact same result。 So this is correct。 So now I
    would like to scare you a little bit。 You actually have to like I basically encourage
    you very strongly to read through broadcasting semantics。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_154.png)'
  prefs: []
  type: TYPE_IMG
- en: And I encourage you to treat this with respect。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_156.png)'
  prefs: []
  type: TYPE_IMG
- en: And it's not something to play fast and loose with it's something to really
    respect really understand and look up maybe some tutorials for broadcasting and
    practice it and be careful with it because you can very quickly run it to box。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_158.png)'
  prefs: []
  type: TYPE_IMG
- en: Let me show you what I mean。 You see how here we have Pete at some of one Pete
    then this true。 The shape of this is twenty seven by one。 Let me take out this
    line just so we have the N and then we can see the counts。 We can see that this
    is all the counts across all the rows and it's twenty seven by one column vector
    right。 Now suppose that I tried to do the following but I erase keep them just
    true here。
  prefs: []
  type: TYPE_NORMAL
- en: What does that do if keep them is not true it's false。 Then remember according
    to the documentation it gets rid of this dimension one it squeezes it out。 So
    basically we just get all the same counts the same result except the shape of
    it is not twenty seven by one it is just twenty seven the one disappears。 But
    all the counts are the same。 So you'd think that this divide that would work。
  prefs: []
  type: TYPE_NORMAL
- en: First of all can we even write this and will it even is it even expected to
    run is it broadcastable。 Let's determine if this result is broadcastable。 Pete
    at summit one is shape is twenty seven。 This is twenty seven by twenty seven。
    So twenty seven by twenty seven broadcasting into twenty seven。 So now rules of
    broadcasting number one align all the dimensions on the right done。
  prefs: []
  type: TYPE_NORMAL
- en: Now iteration over all the dimensions started from the right going to the left。
    All the dimensions math either be equal one of them must be one or one that does
    not exist。 So here they are equal here the dimension does not exist。 So internally
    what broadcasting will do is it will create a one here and then we see that one
    of them is a one and this will get copied and this will run this will broadcast。
  prefs: []
  type: TYPE_NORMAL
- en: Okay so you'd expect this to work because we are this broadcast and this we
    can divide this。 Now if I run this you'd expect it to work but it doesn't。 You
    actually get garbage you get a wrong result because this is actually a bug。 This
    keep them equal equals true makes it work。 This is a bug。
  prefs: []
  type: TYPE_NORMAL
- en: In both cases we are doing the correct counts we are summing up across the rows
    but keep them as saving us and making it work。 So in this case I'd like you to
    encourage you to potentially like pause this video at this point and try to think
    about why this is buggy and why the keep them was necessary here。
  prefs: []
  type: TYPE_NORMAL
- en: Okay so the reason to do for this is I'm trying to hint it here when I was sort
    of giving you a bit of a hint on how this works。 This twenty seven vector internally
    inside the broadcasting this becomes a one by twenty seven and one by twenty seven
    is a row vector。
  prefs: []
  type: TYPE_NORMAL
- en: And now we are dividing twenty seven by twenty seven by one by twenty seven
    and torch will replicate this dimension。 So basically it will take it will take
    this row vector and it will copy it vertically now。 Twenty seven times so the
    twenty seven by twenty seven lies exactly and element wise divides。 And so basically
    what's happening here is we are actually normalizing the columns instead of normalizing
    the rows。
  prefs: []
  type: TYPE_NORMAL
- en: So you can check what's happening here is that P at zero which is the first
    row of P。 That sum is not one it's seven。 It is the first column as an example
    that sums to one。 So to summarize where does the issue come from the issue comes
    from the silent adding of the dimension here because in broadcasting rules you
    align on the right and go from right to left and if the dimension doesn't exist
    you create it。 So that's where the problem happens we still did the counts correctly
    we did the counts across the rows and we got the counts on the right here as a
    column vector。
  prefs: []
  type: TYPE_NORMAL
- en: But because the key things was true this dimension was discarded and now we
    just have a vector of twenty seven。 And because of broadcasting the way it works
    this vector of twenty seven suddenly becomes a row vector and then this row vector
    gets replicated vertically and that every single point we are dividing by the
    count in the opposite direction。
  prefs: []
  type: TYPE_NORMAL
- en: So this thing just doesn't work this needs to be keep them simple in this case。
    So then we have that P at zero is normalized and conversely the first column you'd
    expect to potentially not be normalized。 And this is what makes it work。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_160.png)'
  prefs: []
  type: TYPE_IMG
- en: So pretty subtle and hopefully this helps to scare you that you should have
    respect for broadcasting be careful check your work and understand how it works
    under the hood and make sure that it's broadcasting in the direction that you
    like。 Otherwise you're going to introduce very subtle bugs very hard to find bugs
    and just be careful。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_162.png)'
  prefs: []
  type: TYPE_IMG
- en: One more note on efficiency we don't want to be doing this here because this
    creates a completely new tensor that we store into P we prefer to use in place
    operations if possible。 So this would be an in place operation has the potential
    to be faster it doesn't create new memory under the hood and then let's erase
    this we don't need it and let's also just do fewer just so I'm not wasting space。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_164.png)'
  prefs: []
  type: TYPE_IMG
- en: Okay so we're actually in the pretty good spot now we trained a by gram language
    model and we trained it really just by counting how frequently any pairing occurs
    and then normalizing so that we get a nice property distribution。 So really these
    elements of this array P are really the parameters of our by gram language model
    giving us and summarizing the statistics of these by grams。
  prefs: []
  type: TYPE_NORMAL
- en: So we train the model and then we know how to sample from the model we just
    iteratively sampled the next character and feed it in each time and get a next
    character。 Now what I'd like to do is I'd like to somehow evaluate the quality
    of this model。 We'd like to somehow summarize the quality of this model into a
    single number how good is it at predicting the training set。 And as an example
    so in the training set we can evaluate now the training loss and this training
    loss is telling us about sort of the quality of this model in a single number
    just like we saw in micro grad。
  prefs: []
  type: TYPE_NORMAL
- en: So let's try to think through the quality of the model and how we would evaluate
    it。 Basically what we're going to do is we're going to copy paste this code that
    we previously used for counting。 Okay， and let me just print these by grams first
    we're going to use F strings and I'm going to print character one followed by
    character two these are the by grams。 And then I don't want to do it for all the
    words just do first three words。
  prefs: []
  type: TYPE_NORMAL
- en: So here we have Emma Olivia and Eva by grams。 Now what we'd like to do is we'd
    like to basically look at the probability that the model assigns to every one
    of these by grams。 So in other words we can look at the probability which is summarized
    in the matrix B of Ix1， Ix2。
  prefs: []
  type: TYPE_NORMAL
- en: And then we can print it here as probability。 And because these probabilities
    are way too large let me percent or call on point 4f to like truncate it a bit。
    So what do we have here right we're looking at the probabilities that the model
    assigns to everyone on these by grams in the data set。
  prefs: []
  type: TYPE_NORMAL
- en: And so we can see some of them are 4% 3% etc。 just to have a measuring stick
    in our mind by the way。 We have 27 possible characters or tokens and if everything
    was equally likely then you'd expect all these probabilities to be 4% roughly。
    So anything above 4% means that we've learned something useful from these by grams
    statistics。 And you see that roughly some of these are 4% but some of them are
    as high as 40%， 35% and so on。
  prefs: []
  type: TYPE_NORMAL
- en: So you see that the model actually assigned a pretty high probability to whatever
    is in the training set。 And so that's a good thing。 Basically if you have a very
    good model you'd expect that these probabilities should be near 1 because that
    means that your model is correctly predicting what's going to come next。
  prefs: []
  type: TYPE_NORMAL
- en: especially in the training set where you train your model。 So now we'd like
    to think about how can we summarize these probabilities into a single number that
    measures the quality of this model。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_166.png)'
  prefs: []
  type: TYPE_IMG
- en: Now when you look at the literature into maximum likelihood estimation and statistical
    modeling and so on。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_168.png)'
  prefs: []
  type: TYPE_IMG
- en: you'll see that what's typically used here is something called the likelihood。
    And the likelihood is the product of all of these probabilities。 And so the product
    of all of these probabilities is the likelihood and it's really telling us about
    the probability of the entire data set assigned by the model that we've trained。
    And that is the measure of quality。 So the product of these should be as high
    as possible。
  prefs: []
  type: TYPE_NORMAL
- en: When you are training the model and when you have a good model。 your product
    of these probabilities should be very high。 Now because the product of these probabilities
    is an unwieldy thing to work with。 you can see that all of them are between 0
    and 1。 So your product of these probabilities will be a very tiny number。
  prefs: []
  type: TYPE_NORMAL
- en: So for convenience what people work with usually is not the likelihood。 but
    they work with what's called the log likelihood。 So the product of these is the
    likelihood。 To get the log likelihood， we just have to take the log of the probability。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_170.png)'
  prefs: []
  type: TYPE_IMG
- en: And so the log of the probability here， the log of x from 0 to 1， the log is
    a。 you see here monotonic transformation of the probability， where if you pass
    in 1， you get 0。 So probability 1 gets your log probability of 0。 And then as
    you go lower and lower probability。 the log will grow more and more negative until
    all the way to negative infinity at 0。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_172.png)'
  prefs: []
  type: TYPE_IMG
- en: So here we have a log problem， which is really just a torch dot log of probability。
    Let's print it out to get a sense of what that looks like。 Log problem also 0。4f。
    Okay。 so as you can see， when we plug in numbers that are very close， some of
    our higher numbers。 we get closer and closer to 0。 And then if we plug in very
    bad probabilities。
  prefs: []
  type: TYPE_NORMAL
- en: we get more and more negative number。 That's bad。 So。 and the reason we work
    with this is for large extent convenience， right？
  prefs: []
  type: TYPE_NORMAL
- en: Because we have mathematically that if you have some product a times b times
    c of all these probabilities。 right， or the likelihood is the product of all these
    probabilities。 then the log of these is just log of a plus log of b plus log of
    c。 If you remember your logs from your high school or undergrad and so on。
  prefs: []
  type: TYPE_NORMAL
- en: So we have that basically the likelihood of the product of probabilities。 the
    log likelihood is just the sum of the logs of the individual probabilities。 So
    log likelihood starts at 0。 And then log likelihood here， we can just accumulate
    simply。 And then the end， we can print this。 Print the log likelihood。 Have strings。
  prefs: []
  type: TYPE_NORMAL
- en: maybe you're familiar with this。 So log likelihood is negative 38。 Okay。 Now，
    we actually want。 so how high can log likelihood get？ It can go to zero。 So when
    all the probabilities are one。 log likelihood will be zero。 And then when all
    the probabilities are lower。 this will grow more and more negative。 Now， we don't
    actually like this because what we'd like is a loss function。
  prefs: []
  type: TYPE_NORMAL
- en: And a loss function has the semantics that low is good， because we're trying
    to minimize the loss。 So we actually need to invert this。 And that's what gives
    us something called the negative log likelihood。 Negative log likelihood is just
    negative of the log likelihood。 These are F strings， by the way。 if you'd like
    to look this up。 Negative log likelihood equals。
  prefs: []
  type: TYPE_NORMAL
- en: So negative log likelihood now is just negative of it。 And so the negative log
    likelihood is a very nice loss function because the lowest it can get is zero。
    And the higher it is， the worse off the predictions are that you're making。 And
    then one more modification to this that sometimes people do is that for convenience。
  prefs: []
  type: TYPE_NORMAL
- en: they actually like to normalize by， they like to make it an average instead
    of a sum。 And so here。 let's just keep some counts as well。 So n plus equals one
    starts at zero。 And then here。 we can have sort of like a normalized log likelihood。
    If we just normalize it by the count。 then we will sort of get the average log
    likelihood。
  prefs: []
  type: TYPE_NORMAL
- en: So this would be usually our loss function here。 This is what we would use。
    So our loss function for the training set assigned by the model is 2。4。 That's
    the quality of this model。 And the lower it is， the better off we are。 And the
    higher it is。 the worse off we are。 And the job of our training is to find the
    parameters that minimize the negative log likelihood loss。
  prefs: []
  type: TYPE_NORMAL
- en: And that would be like a high quality model。 Okay， so to summarize， I actually
    wrote it out here。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_174.png)'
  prefs: []
  type: TYPE_IMG
- en: So our goal is to maximize likelihood， which is the product of all the probabilities
    assigned by the model。 And we want to maximize this likelihood with respect to
    the model parameters。 In our case。 the model parameters here are defined in the
    table。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_176.png)'
  prefs: []
  type: TYPE_IMG
- en: These numbers， the probabilities are the model parameters sort of in our Bagram
    language models so far。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_178.png)'
  prefs: []
  type: TYPE_IMG
- en: But you have to keep in mind that here we are storing everything in a table
    format。 the probabilities。 But what's coming up as a brief preview is that these
    numbers will not be kept explicitly。 but these numbers will be calculated by a
    neural network。 So that's coming up。 And we want to change and tune the parameters
    of these neural works。
  prefs: []
  type: TYPE_NORMAL
- en: We want to change these parameters to maximize the likelihood， the product of
    the probabilities。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_180.png)'
  prefs: []
  type: TYPE_IMG
- en: Now， maximizing the likelihood is equivalent to maximizing the log likelihood
    because log is a monotonic function。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_182.png)'
  prefs: []
  type: TYPE_IMG
- en: Here's the graph of log。 And basically， all it is doing is it's just scaling
    your。 you can look at it as just a scaling of the loss function。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_184.png)'
  prefs: []
  type: TYPE_IMG
- en: And so the optimization problem here and here are actually equivalent because
    this is just a scaling。 You can look at it that way。 And so these are two identical
    optimization problems。 Maximizing the log likelihood is equivalent to minimizing
    the negative log likelihood。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_186.png)'
  prefs: []
  type: TYPE_IMG
- en: And then in practice， people actually minimize the average negative log likelihood
    to get numbers like 2。4。 And then this summarizes the quality of your model。 And
    we'd like to minimize it and make it as small as possible。 And the lowest it can
    get is zero。 And the lower it is， the better off your model is because it's signing
    high probabilities to your data。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_188.png)'
  prefs: []
  type: TYPE_IMG
- en: Now let's estimate the probability over the entire training set just to make
    sure that we get something around 2。4。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_190.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's run this over the entire， oops， let's take out the print statement as
    well。 Okay， 2。45 or the entire training set。 Now what I'd like to show you is
    that you can actually evaluate the probability for any word that you want。 Like
    for example， if we just test a single word， Andre， and bring back the print statement。
    then you see that Andre is actually kind of like an unlikely word。 And like on
    average。
  prefs: []
  type: TYPE_NORMAL
- en: we take three log probability to represent it。 And roughly that's because EJ
    apparently is very uncommon as an example。 Now think through this， when I take
    Andre and I append Q and I test the probability of it， Andre Q。 We actually get
    infinity。 And that's because JQ has a 0% probability according to our model。 So
    the log likelihood。 So the log of zero will be negative infinity。 We get infinite
    loss。
  prefs: []
  type: TYPE_NORMAL
- en: So this is kind of undesirable， right？ Because we plugged in a string that could
    be like a somewhat reasonable name。 But basically what this is saying is that
    this model is exactly 0% likely to predict this name。 And our loss is infinity
    on this example。 And really what the reason for that is that J is followed by
    Q zero times。 where is Q？ JQ is zero。 And so JQ is 0% likely。 So it's actually
    kind of gross and people don't like this too much。
  prefs: []
  type: TYPE_NORMAL
- en: To fix this， there's a very simple fix that people like to do to sort of like
    smooth out your model a little bit。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_192.png)'
  prefs: []
  type: TYPE_IMG
- en: And it's called model smoothing。 And roughly what's happening is that we will
    add some fake accounts。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_194.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_195.png)'
  prefs: []
  type: TYPE_IMG
- en: So imagine adding a count of one to everything。 So we add a count of one like
    this。 And then we recalculate the probabilities。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_197.png)'
  prefs: []
  type: TYPE_IMG
- en: And that's model smoothing。 And you can add as much as you like。 You can add
    five and it will give you a smoother model。 And the more you add here。 the more
    uniform model you're going to have。 And the less you add。 the more peaked model
    you're going to have， of course。 So one is like a pretty decent count to add。
  prefs: []
  type: TYPE_NORMAL
- en: And that will ensure that there will be no zeros in our probability matrix P。
    And so this will of course change the generations a little bit。 In this case it
    didn't。 but it in principle it could。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_199.png)'
  prefs: []
  type: TYPE_IMG
- en: But what that's going to do now is that nothing will be infinity unlikely。 So
    now our model will predict some other probability。 And we see that JQ now has
    a very small probability。 So the model still finds it very surprising that this
    was a word or a by gram。
  prefs: []
  type: TYPE_NORMAL
- en: but we don't get negative infinity。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_201.png)'
  prefs: []
  type: TYPE_IMG
- en: So it's kind of like a nice fix that people like to apply sometimes and it's
    called models moving。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_203.png)'
  prefs: []
  type: TYPE_IMG
- en: Okay， so we've now trained a respectable by gram character level language model。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_205.png)'
  prefs: []
  type: TYPE_IMG
- en: And we saw that we both sort of trained the model by looking at the counts of
    all the by grams and normalizing the rows to get probability distributions。 We
    saw that we can also then use those parameters of this model to perform sampling
    of new words。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_207.png)'
  prefs: []
  type: TYPE_IMG
- en: So we sample new names according to those distributions。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_209.png)'
  prefs: []
  type: TYPE_IMG
- en: And we also saw that we can evaluate the quality of this model。 And the quality
    of this model is summarized in a single number which is the negative log likelihood。
    And the lower this number is， the better the model is because it is giving high
    probabilities to the actual next characters in all the by grams in our training
    set。 So that's all well and good， but we've arrived at this model explicitly by
    doing something that felt sensible。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_211.png)'
  prefs: []
  type: TYPE_IMG
- en: We were just performing counts and then we were normalizing those counts。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_213.png)'
  prefs: []
  type: TYPE_IMG
- en: Now what I would like to do is I would like to take an alternative approach。
    We will end up in a very very similar position， but the approach will look very
    different。 Because I would like to cast the problem of by gram character level
    language modeling into the neural network framework。 And in the neural network
    framework we're going to approach things slightly differently。
  prefs: []
  type: TYPE_NORMAL
- en: but again end up in a very similar spot。 I'll go into that later。 Now our neural
    network is going to be a still a by gram character level language model。 So it
    receives a single character as an input。 Then there's neural network with some
    weights or some parameters W。 And it's going to output the probability distribution
    over the next character in a sequence。
  prefs: []
  type: TYPE_NORMAL
- en: It's going to make guesses as to what is likely to follow this character that
    was input to the model。 And then in addition to that we're going to be able to
    evaluate any setting of the parameters of the neural network。 because we have
    a loss function。 The negative lot likelihood。 So we're going to take a look at
    its probability distributions and we're going to use the labels。
  prefs: []
  type: TYPE_NORMAL
- en: Which are basically just the identity of the next character in that by gram，
    the second character。 So knowing what second character actually comes next in
    the by gram allows us to then look at what how high probability the model assigns
    to that character。
  prefs: []
  type: TYPE_NORMAL
- en: And then we of course want the probability to be very high。 And that is another
    way of saying that the loss is low。 So we're going to use gradient based optimization
    then to tune the parameters of this network。 Because we have the loss function
    and we're going to minimize it。
  prefs: []
  type: TYPE_NORMAL
- en: So we're going to tune the weights so that the neural net is correctly predicting
    the probabilities for the next character。 So let's get started。 The first thing
    I want to do is I want to compile the training set of this neural network。 So
    create the training set of all the by grams。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_215.png)'
  prefs: []
  type: TYPE_IMG
- en: Okay。 And here I'm going to copy paste this code because this code iterates
    over all the by grams。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_217.png)'
  prefs: []
  type: TYPE_IMG
- en: So here we start with the words we iterate over all the by grams and previously
    as you recall we did the counts。 But now we're not going to do counts。 We're just
    creating a training set。 Now this training set will be made up of two lists。 We
    have the inputs and the targets， the labels。 And these by grams will denote x，
    y。 Those are the characters， right？
  prefs: []
  type: TYPE_NORMAL
- en: And so we're given the first character of the by gram and then we're trying
    to predict the next one。 Both of these are going to be integers。 So here we'll
    take x's that append is just x1。 Why is that append ix2？ And then here we actually
    don't want lists of integers。 We will create tensors out of these。 So x is torched
    dot tensor of x's and y is torched dot tensor of y's。
  prefs: []
  type: TYPE_NORMAL
- en: And then we don't actually want to take all the words just yet because I want
    everything to be manageable。 So let's just do the first word which is Emma。 And
    then it's clear what these x's and y's would be。 Here let me print character one
    character two just so you see what's going on here。 So the by grams of these characters
    is dot E E M M M A dot。
  prefs: []
  type: TYPE_NORMAL
- en: So this single word as I mentioned has one， two， three， four， five examples
    for our neural network。 There are five separate examples in Emma。 And those examples
    are here。 When the input to the neural network is integer zero。 the desired label
    is integer five which corresponds to E。
  prefs: []
  type: TYPE_NORMAL
- en: When the input to the neural network is five， we want its weights to be arranged
    so that 13 gets a very high probability。 When 13 is put in， we want 13 to have
    a high probability。 When 13 is put in。 we also want one to have a high probability。
    When one is input。 we want zero to have a very high probability。 So there are
    five separate input examples to a neural net in this data set。
  prefs: []
  type: TYPE_NORMAL
- en: I wanted to add a tangent of a note of caution to be careful with a lot of the
    APIs of some of these frameworks。 You saw me silently use torch dot tensor with
    a lowercase t and the output looked right。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_219.png)'
  prefs: []
  type: TYPE_IMG
- en: But you should be aware that there's actually two ways of constructing a tensor。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_221.png)'
  prefs: []
  type: TYPE_IMG
- en: There's a torch dot lowercase tensor and there's also a torch dot capital tensor
    class which you can also construct。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_223.png)'
  prefs: []
  type: TYPE_IMG
- en: So you can actually call both。 You can also do torch dot capital tensor and
    you get an Xs and Ys as well。 So that's not confusing at all。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_225.png)'
  prefs: []
  type: TYPE_IMG
- en: There are threads on what is the difference between these two。 Unfortunately。
    the docs are just not clear on the difference。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_227.png)'
  prefs: []
  type: TYPE_IMG
- en: When you look at the docs of lowercase tensor， construct tensor with no autograph
    history by copying data。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_229.png)'
  prefs: []
  type: TYPE_IMG
- en: It's just like it doesn't make sense。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_231.png)'
  prefs: []
  type: TYPE_IMG
- en: So the actual difference as far as I can tell is explained eventually in this
    random thread that you can Google。 And really it comes down to I believe that，
    um， what is this？
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_233.png)'
  prefs: []
  type: TYPE_IMG
- en: Torch dot tensor in first the D type， the data type automatically。 Well torch
    dot tensor just returns a float tensor。 I would recommend stick to torch dot lowercase
    tensor。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_235.png)'
  prefs: []
  type: TYPE_IMG
- en: So indeed we see that when I construct this with a capital T， the data type
    here of Xs is float 32。 But torch dot lowercase tensor。 You see how it's now X
    dot D type is now integer。 So it's advised that you use lowercase T and you can
    read more about it if you like in some of these threads。 But basically I'm pointing
    out some of these things because I want to caution you and I want you to get used
    to reading a lot of documentation and reading through a lot of Q and A's and threads
    like this。
  prefs: []
  type: TYPE_NORMAL
- en: And， you know， some of this stuff is unfortunately not easy and not very well
    documented and you have to be careful out there。 What we want here is integers
    because that's what makes sense。 And so lowercase tensor is what we are using。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_237.png)'
  prefs: []
  type: TYPE_IMG
- en: Okay， now we want to think through how we're going to feed in these examples
    into a neural network。 Now it's not quite as straightforward as plugging it in
    because these examples right now are integers。 So there's like a zero five or
    thirteen。 It gives us the index of the character and you can't just plug an integer
    index into a neural net。 These neural nets， right， are sort of made up of these
    neurons and these neurons have weights。
  prefs: []
  type: TYPE_NORMAL
- en: And as you saw in micro grad， these weights act multiplicatively on the inputs
    W X plus B。 there's 10 Hs and so on。 And so it doesn't really make sense to make
    an input neuron take on integer values that you feed in and then multiply on with
    weights。 So instead， a common way of encoding integers is what's called one-hot
    encoding。 In one-hot encoding， we take an integer like thirteen and we create
    a vector that is all zeros except for the thirteenth dimension which returned
    to a one。
  prefs: []
  type: TYPE_NORMAL
- en: And then that vector can feed into a neural net。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_239.png)'
  prefs: []
  type: TYPE_IMG
- en: Now， conveniently， PyTorch actually has something called the one-hot function
    inside Torch and in functional。 It takes a tensor made up of integers long as
    an integer。 And it also takes a number of classes which is how large you want
    your tensor vector to be。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_241.png)'
  prefs: []
  type: TYPE_IMG
- en: So here， let's import Torch。 and in that functional SF， this is a common way
    of importing it。 And then let's do F dot one-hot and we feed in the integers that
    we want to encode。 So we can actually feed in the entire array of Xs。 And we can
    tell it that num class is 27。 So it doesn't have to try to guess it。 It may have
    guessed that it's only 13 and would give us an incorrect result。
  prefs: []
  type: TYPE_NORMAL
- en: So this is the one-hot。 Let's call this X in for X encoded。 And then we see
    that X encoded that shape is 5 by 27。 And we can also visualize it PLT。IMS show
    of X-ink to make it a little bit more clear because this is a little messy。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_243.png)'
  prefs: []
  type: TYPE_IMG
- en: So we see that we've encoded all the five examples into vectors。 We have five
    examples。 so we have five rows and each row here is now an example into a neural
    net。 And we see that the appropriate bit is turned on as a one and everything
    else is zero。 So here。 for example， the zero-th bit is turned on， the fifth bit
    is turned on。
  prefs: []
  type: TYPE_NORMAL
- en: the thirteenth bits are turned on for both of these examples。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_245.png)'
  prefs: []
  type: TYPE_IMG
- en: And the first bit here is turned on。 So that's how we can encode integers into
    vectors。 And then these vectors can feed in to neural nets。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_247.png)'
  prefs: []
  type: TYPE_IMG
- en: One more issue to be careful with here， by the way， is let's look at the data
    type of econcoding。 We always want to be careful with data types。 What would you
    expect X encoding data type to be？
  prefs: []
  type: TYPE_NORMAL
- en: When we're plugging numbers into neural nets， we don't want them to be integers。
    We want them to be floating point numbers that can take on various values。 But
    the dtype here is actually 64-bit integer。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_249.png)'
  prefs: []
  type: TYPE_IMG
- en: And the reason for that I suspect is that one-hot received a 64-bit integer
    here。 and it returned the same data type。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_251.png)'
  prefs: []
  type: TYPE_IMG
- en: And when you look at the signature of one-hot， it doesn't even take a dtype。
    a desired data type of the output tensor。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_253.png)'
  prefs: []
  type: TYPE_IMG
- en: And so we can't， in a lot of functions in Torter， we'd be able to do something
    like dtype equals Torx。flow32， which is what we want， but one-hot does not support
    that。 So instead。 we're going to want to cast this to float like this。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_255.png)'
  prefs: []
  type: TYPE_IMG
- en: So that these， everything is the same。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_257.png)'
  prefs: []
  type: TYPE_IMG
- en: Everything looks the same， but the dtype is float32。 And floats can feed into
    neural nets。 So now let's construct our first neuron。 This neuron will look at
    these input vectors。 And as you remember from Micrograd， these neurons basically
    perform a very simple function。 WX plus B， where WX is a dot product。 Right？ So
    we can achieve the same thing here。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_259.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's first define the weights of this neuron， basically。 Where are the initial
    weights at initialization for this neuron？
  prefs: []
  type: TYPE_NORMAL
- en: Let's initialize them with torch dot random。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_261.png)'
  prefs: []
  type: TYPE_IMG
- en: Torx dot random is fills a tensor with random numbers。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_263.png)'
  prefs: []
  type: TYPE_IMG
- en: drawn from a normal distribution。 And a normal distribution has a probability
    density function like this。 And so most of the numbers drawn from this distribution
    will be around zero。 but some of them will be as high as almost three and so on。
    And very few numbers will be above three in magnitude。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_265.png)'
  prefs: []
  type: TYPE_IMG
- en: So we need to take size as an input here。 And I'm going to use size as to be
    27 by one。 So 27 by one， and then let's visualize W。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_267.png)'
  prefs: []
  type: TYPE_IMG
- en: So W is a column vector of 27 numbers。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_269.png)'
  prefs: []
  type: TYPE_IMG
- en: And these weights are then multiplied by the inputs。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_271.png)'
  prefs: []
  type: TYPE_IMG
- en: So now to perform this multiplication， we can take X encoding， and we can multiply
    it with W。 This is a matrix multiplication operator in PyTorch。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_273.png)'
  prefs: []
  type: TYPE_IMG
- en: And the output of this operation is five by one。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_275.png)'
  prefs: []
  type: TYPE_IMG
- en: The reason it's five by five is the following。 We took X encoding， which is
    five by 27。 and we multiplied it by 27 by one。 And in matrix multiplication。 you
    see that the output will become five by one， because these 27 will multiply and
    add。 So basically what we're seeing here out of this operation is we are seeing
    the five。
  prefs: []
  type: TYPE_NORMAL
- en: activations of this neuron on these five inputs。 And we've evaluated all of
    them in parallel。 So we can feed in just a single input to the single neuron。
    We fed in simultaneously all the five inputs into the same neuron。 And in parallel。
    PyTorch has evaluated the WX plus B， but here it's just the WX。 There's no bias。
  prefs: []
  type: TYPE_NORMAL
- en: It has valued W times X for all of them independently。 Now instead of a single
    neuron though。 I would like to have 27 neurons。 And I'll show you in a second
    why I've got 27 neurons。 So instead of having just a one here， which is indicating
    this presence of one single， neuron。 we can use 27。 And then when W is 27 by 27，
    this will in parallel evaluate all the 27 neurons on all。
  prefs: []
  type: TYPE_NORMAL
- en: the five inputs， giving us a much better， much， much bigger result。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_277.png)'
  prefs: []
  type: TYPE_IMG
- en: So now what we've done is five by 27 multiplied， 27 by 27， and the output of
    this is now five。 by 27。 So we can see that the shape of this is five by 27。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_279.png)'
  prefs: []
  type: TYPE_IMG
- en: So what is every element here telling us？ It's telling us for every one of 27
    neurons that we created。 what is the firing rate of， those neurons on every one
    of those five examples？ So the element。 for example， 3，13 is giving us the firing
    rate of the 13th neuron looking， at the third input。 And the way this was achieved
    is by a dot product between the third input and the 13th。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_281.png)'
  prefs: []
  type: TYPE_IMG
- en: column of this W matrix here。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_283.png)'
  prefs: []
  type: TYPE_IMG
- en: So using matrix multiplication， we can very efficiently evaluate the dot product
    between。 lots of input examples in a batch and lots of neurons where all those
    neurons have weights。 in the columns of those W's。 And in matrix multiplication。
    we're just doing those dot products in parallel。 Just to show you that this is
    the case。
  prefs: []
  type: TYPE_NORMAL
- en: we can take X and we can take the third row。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_285.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_286.png)'
  prefs: []
  type: TYPE_IMG
- en: And we can take the W and take its 13th column。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_288.png)'
  prefs: []
  type: TYPE_IMG
- en: And then we can do X and get three， element-wise multiply with W of 13 and sum
    that up。 That's W of X plus B。 Well， there's no plus B。 It's just W of X dot product。
    And that's this number。 So you see that this is just being done efficiently by
    the matrix multiplication operation for all。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_290.png)'
  prefs: []
  type: TYPE_IMG
- en: the input examples and for all the output neurons of this first layer。 Okay。
    so we fed our 27 dimensional inputs into a first layer of a neural net that has
    27。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_292.png)'
  prefs: []
  type: TYPE_IMG
- en: neurons。 So we have 27 inputs and now we have 27 neurons。 These neurons perform
    W times X。 They don't have a bias and they don't have a nonlinearity like 10H。
    We're going to lead them to be a linear layer。 In addition to that。 we're not
    going to have any other layers。 This is going to be it。 It's just going to be
    the dumbest。
  prefs: []
  type: TYPE_NORMAL
- en: smallest， simplest neural net， which is just a single linear layer。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_294.png)'
  prefs: []
  type: TYPE_IMG
- en: And now I'd like to explain what I want those 27 outputs to be。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_296.png)'
  prefs: []
  type: TYPE_IMG
- en: Intuitively， what we're trying to produce here for every single input example
    is we're trying to produce some kind of a probability。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_298.png)'
  prefs: []
  type: TYPE_IMG
- en: distribution for the next character in a sequence。 And there's 27 of them。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_300.png)'
  prefs: []
  type: TYPE_IMG
- en: But we have to come up with like precise semantics for exactly how we're going
    to interpret these 27 numbers。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_302.png)'
  prefs: []
  type: TYPE_IMG
- en: and we're going to make these two neurons take a whole。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_304.png)'
  prefs: []
  type: TYPE_IMG
- en: Now intuitively， you see here that these numbers are negative and some of them
    are positive， etc。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_306.png)'
  prefs: []
  type: TYPE_IMG
- en: And that's because these are coming out of a neural net layer initialized with
    these。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_308.png)'
  prefs: []
  type: TYPE_IMG
- en: normal distribution parameters。 But what we want is we want something like we
    had here。 like each row here told us the counts。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_310.png)'
  prefs: []
  type: TYPE_IMG
- en: and then we normalize the counts to get probabilities。 We want something similar
    to come out of a neural net。 But what we just have right now is just some negative
    and positive numbers。 Now we want those numbers to somehow represent the probabilities
    for the next character。
  prefs: []
  type: TYPE_NORMAL
- en: But you see that probabilities， they have a special structure。 They're positive
    numbers and they sum to one。 And so that doesn't just come out of a neural net。
    And then they can't be counts because these counts are positive and counts are
    integers。 So counts are also not really a good thing to output from a neural net。
  prefs: []
  type: TYPE_NORMAL
- en: So instead what the neural net is going to output and how we are going to interpret
    the 27 numbers。 is that these 27 numbers are giving us log counts， basically。
    So instead of giving us counts directly， like in this table， they're giving us
    log counts。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_312.png)'
  prefs: []
  type: TYPE_IMG
- en: And to get the counts， we're going to take the log counts and we're going to
    exponentiate them。 Now exponentiation takes the following form。 It takes numbers
    that are negative or they are positive。 It takes the entire real line。 And then
    if you plug in negative numbers。 you're going to get e to the x， which is always
    below one。 So you're getting numbers lower than one。
  prefs: []
  type: TYPE_NORMAL
- en: And if you plug in numbers greater than zero， you're getting numbers greater
    than one。 all the way growing to the infinity。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_314.png)'
  prefs: []
  type: TYPE_IMG
- en: And this here grows to zero。 So basically we're going to take these numbers
    here。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_316.png)'
  prefs: []
  type: TYPE_IMG
- en: And instead of them being positive and negative and all over the place。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_318.png)'
  prefs: []
  type: TYPE_IMG
- en: we're going to interpret them as log counts。 And then we're going to element
    twice exponentiate these numbers。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_320.png)'
  prefs: []
  type: TYPE_IMG
- en: Exponentiating them now gives us something like this。 And you see that these
    numbers now。 because of that they went through an exponent， all the negative numbers
    turned into numbers below one。 like 0。338。 And all the positive numbers， originally，
    turned into even more positive numbers。 so we're greater than one。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_322.png)'
  prefs: []
  type: TYPE_IMG
- en: So like for example， seven is some positive number over here。 That is greater
    than zero。 But exponentiated outputs here basically give us something that we
    can use and interpret as the equivalent of counts。 originally。 So you see these
    counts here， one twelve， seven， fifty one， one， etc。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_324.png)'
  prefs: []
  type: TYPE_IMG
- en: The neural light is kind of now predicting counts。 And these counts are positive
    numbers。 They can never be below zero， so that makes sense。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_326.png)'
  prefs: []
  type: TYPE_IMG
- en: And they can now take on various values depending on the settings of W。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_328.png)'
  prefs: []
  type: TYPE_IMG
- en: So let me break this down。 We're going to interpret these to be the log counts。
    In other words。 for this， that is often used is so called logits。 These are logits
    log counts。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_330.png)'
  prefs: []
  type: TYPE_IMG
- en: Then these will be sort of the counts。 Logits exponentiated。 And this is equivalent
    to the n matrix。 sort of the n array that we used previously。 Remember this was
    the n？ This is the array of counts。 And each row here are the counts for the next
    character， sort of。 So those are the counts。 And now the probabilities are just
    the counts normalized。 And so I'm not going to find the same。
  prefs: []
  type: TYPE_NORMAL
- en: but basically I'm not going to scroll all over the place。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_332.png)'
  prefs: []
  type: TYPE_IMG
- en: We've already done this。 We want to counts that sum along the first dimension。
    and we want to keep them as true。 We've went over this。 and this is how we normalize
    the rows of our counts matrix to get our probabilities。 So now these are the probabilities，
    and these are the counts that we have currently。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_334.png)'
  prefs: []
  type: TYPE_IMG
- en: And now when I show the probabilities， you see that every row here， of course，
    will sum to 1。 because they're normalized。 And the shape of this is 5 by 27。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_336.png)'
  prefs: []
  type: TYPE_IMG
- en: And so really what we've achieved is for every one of our five examples。 we
    now have a row that came out of a neural net。 And because of the transformations
    here。 we made sure that this output of this neural net now are probabilities。
    or we can interpret to be probabilities。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_338.png)'
  prefs: []
  type: TYPE_IMG
- en: So our WX here gave us logits， and then we interpret those to be log counts。
    And we exponentiate to get something that looks like counts。 and then we normalize
    those counts to get a probability distribution。 And all of these are differentiable
    operations。 So what we've done now is we are taking inputs。
  prefs: []
  type: TYPE_NORMAL
- en: We have differentiable operations that we can back propagate through。 and we're
    getting out probability distributions。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_340.png)'
  prefs: []
  type: TYPE_IMG
- en: So， for example， for the 0th example that fed in， which was the 0th example
    here was a 1-half vector of 0。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_342.png)'
  prefs: []
  type: TYPE_IMG
- en: And it basically corresponded to feeding in this example here。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_344.png)'
  prefs: []
  type: TYPE_IMG
- en: So we're feeding in a dot into a neural net。 And the way we fed the dot into
    a neural net is that we first got its index。 Then we one-hot encoded it。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_346.png)'
  prefs: []
  type: TYPE_IMG
- en: Then it went into the neural net， and out came this distribution of probabilities。
    And its shape is 27。 There's 27 numbers。 And we're going to interpret this as
    the neural net's assignment for how likely every one of these characters。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_348.png)'
  prefs: []
  type: TYPE_IMG
- en: the 27 characters， are to come next。 And as we tune the weights W， we're going
    to be， of course。 getting different probabilities out for any character that you
    input。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_350.png)'
  prefs: []
  type: TYPE_IMG
- en: And so now the question is just， can we optimize and find a good W such that
    the probabilities coming out are pretty good？
  prefs: []
  type: TYPE_NORMAL
- en: And the way we measure pretty good is by the loss function。 Okay。 so I organized
    everything into a single summary so that hopefully it's a bit more clear。 So it
    starts here。 We have an input data set。 We have some inputs to the neural net。
    And we have some labels for the correct next character in a sequence。 These are
    integers。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_352.png)'
  prefs: []
  type: TYPE_IMG
- en: Here I'm using Torx generators now so that you see the same numbers that I see。
    And I'm generating 27 neurons weights。 And each neuron here receives 27 inputs。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_354.png)'
  prefs: []
  type: TYPE_IMG
- en: Then here we're going to plug in all the input examples X's into a neural net。
    So here this is a forward pass。 First we have to encode all of the inputs into
    one-hot representations。 So we have 27 classes。 We pass in these integers。 And
    X， Inc。 becomes a array that is 5 by 27。 The zeros， except for a few ones。 We
    then multiply this in the first layer of a neural net to get logits。
  prefs: []
  type: TYPE_NORMAL
- en: Exponentiate the logits to get fake counts sort of。 And normalize these counts
    to get probabilities。 So these last two lines by the way here are called a softmax。
    Which I pulled up here。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_356.png)'
  prefs: []
  type: TYPE_IMG
- en: Softmax is a very often used layer in a neural net that takes these z's which
    are logits。 Exponentiates them and divides and normalizes。 It's a way of taking
    outputs of a neural net layer。 And these outputs can be positive or negative。
    And it outputs probability distributions。 It outputs something that always sums
    to one in our positive numbers。 Just like probabilities。
  prefs: []
  type: TYPE_NORMAL
- en: So this is kind of like a normalization function if you want to think of it
    that way。 And you can put it on top of any other linear layer inside a neural
    net。 And it basically makes a neural net output probabilities。 That's very often
    used。 And we used it as well here。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_358.png)'
  prefs: []
  type: TYPE_IMG
- en: So this is the forward pass and that's how we made a neural net output probability。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_360.png)'
  prefs: []
  type: TYPE_IMG
- en: Now you'll notice that all of these， this entire forward pass is made up of
    differentiable layers。 Everything here we can back propagate through。 And we saw
    some of the back propagation in micrograd。 This is just multiplication and addition。
    All that's happening here is just multiplying and add。 And we know how to back
    propagate through them。 Exponentiation we know how to back propagate through。
  prefs: []
  type: TYPE_NORMAL
- en: And then here we are summing and sum is easily back propagated as well。 And
    division as well。 So everything here is the differentiable operation。 And we can
    back propagate through。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_362.png)'
  prefs: []
  type: TYPE_IMG
- en: Now we achieve these probabilities which are 5 by 27。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_364.png)'
  prefs: []
  type: TYPE_IMG
- en: For every single example we have a vector of probabilities that's on to one。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_366.png)'
  prefs: []
  type: TYPE_IMG
- en: And then here I wrote a bunch of stuff to sort of like break down the examples。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_368.png)'
  prefs: []
  type: TYPE_IMG
- en: So we have 5 examples making up Emma。 And there are 5 by grams inside Emma。
    So by gram example one is that E is the beginning character right after dot。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_370.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_371.png)'
  prefs: []
  type: TYPE_IMG
- en: And the indexes for these are 0 and 5。 So then we feed in a 0。 That's the input
    of the neural net。 We get probabilities from the neural net that are 27 numbers。
    And then the label is 5 because E actually comes after dot。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_373.png)'
  prefs: []
  type: TYPE_IMG
- en: So that's the label。 And then we use this label 5 to index into the probability
    distribution here。 So this index 5 here is 0， 1， 2， 3， 4， 5。 It's this number
    here。 Which is here。 So that's basically the probability assigned by the neural
    net to the actual correct character。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_375.png)'
  prefs: []
  type: TYPE_IMG
- en: You see that the net work currently thinks that this next character that E following
    dot is only 1% likely。 Which is of course not very good right because this actually
    is a training example。 And the network thinks that it's currently very very unlikely。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_377.png)'
  prefs: []
  type: TYPE_IMG
- en: But that's just because we didn't get very lucky in generating a good setting
    of W。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_379.png)'
  prefs: []
  type: TYPE_IMG
- en: So right now this network thinks it's unlikely and 0。01 is not a good outcome。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_381.png)'
  prefs: []
  type: TYPE_IMG
- en: So the log likelihood then is very negative。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_383.png)'
  prefs: []
  type: TYPE_IMG
- en: And the negative log likelihood is very positive。 And so 4 is a very high negative
    log likelihood。 And that means we're going to have a high loss。 Because what is
    the loss？
  prefs: []
  type: TYPE_NORMAL
- en: The loss is just the average negative log likelihood。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_385.png)'
  prefs: []
  type: TYPE_IMG
- en: So the second character is EM。 And you see here that also the network thought
    that M following E is very unlikely 1%。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_387.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_388.png)'
  prefs: []
  type: TYPE_IMG
- en: For M following M it thought it was 2%。 And for A following M it actually thought
    it was 7% likely。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_390.png)'
  prefs: []
  type: TYPE_IMG
- en: So just by chance this one actually has a pretty good probability and therefore
    a pretty low negative log likelihood。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_392.png)'
  prefs: []
  type: TYPE_IMG
- en: And finally here it thought this was 1% likely。 So overall our average negative
    log likelihood which is the loss。 The total loss that summarizes basically how
    well this network currently works。 At least on this one word not on the full data
    set just the one word is 3。76。 Which is actually very fairly high loss。 This is
    not a very good setting of W's。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_394.png)'
  prefs: []
  type: TYPE_IMG
- en: Now here's what we can do。 We're currently getting 3。76。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_396.png)'
  prefs: []
  type: TYPE_IMG
- en: We can actually come here and we can change our W。 We can resample it。 So let
    me just add one to have a different seed。 And then we get a different W。 And then
    we can rerun this。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_398.png)'
  prefs: []
  type: TYPE_IMG
- en: And with this different C with this different setting of W's。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_400.png)'
  prefs: []
  type: TYPE_IMG
- en: We now get 3。37。 So this is a much better W。 And it's better because the probability
    just happened to come out higher for the characters that actually are next。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_402.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_403.png)'
  prefs: []
  type: TYPE_IMG
- en: And so you can imagine actually just resampling this。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_405.png)'
  prefs: []
  type: TYPE_IMG
- en: We can try the two。 So okay this feels not very good。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_407.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's try one more。 We can try three。 Okay this was a terrible setting because
    we have a very high loss。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_409.png)'
  prefs: []
  type: TYPE_IMG
- en: So anyway I'm going to erase this。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_411.png)'
  prefs: []
  type: TYPE_IMG
- en: What I'm doing here which is just guess and check of randomly assigning parameters
    and seeing if the network is good。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_413.png)'
  prefs: []
  type: TYPE_IMG
- en: That is amateur hour。 That's not how you optimize a neural net。 The way you
    optimize a neural net is you start with some random guess and we're going to commit
    to this one even though it's not very good。 But now the big deal is we have a
    loss function。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_415.png)'
  prefs: []
  type: TYPE_IMG
- en: So this loss is made up only of differentiable operations。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_417.png)'
  prefs: []
  type: TYPE_IMG
- en: And we can minimize the loss by tuning W's by computing the gradients of the
    loss with respect to these W matrices。 And so then we can tune W to minimize the
    loss and find a good setting of W using gradient based optimization。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_419.png)'
  prefs: []
  type: TYPE_IMG
- en: So let's see how that will work。 Now things are actually going to look almost
    identical to what we had with micrograd。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_421.png)'
  prefs: []
  type: TYPE_IMG
- en: So here I pulled up the lecture from micrograd， the notebook。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_423.png)'
  prefs: []
  type: TYPE_IMG
- en: It's from this repository。 And when I scroll all the way to the end where we
    left off with micrograd we had something very very similar。 We had a number of
    input examples。 In this case we had four input examples inside X's。 And we had
    their targets。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_425.png)'
  prefs: []
  type: TYPE_IMG
- en: These are our targets。 Just like here we have our X's now but we have five of
    them。 And they're now integers instead of vectors。 But we're going to convert
    our integers to vectors。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_427.png)'
  prefs: []
  type: TYPE_IMG
- en: Except our vectors will be 27 large instead of 3 large。 And then here what we
    did is first we did a forward pass where we ran a neural net on all of the inputs
    to get predictions。 Our neural net at the time， this N of X was a net or multi-layer
    perceptron。 Our neural net is going to look different because our neural net is
    just a single layer。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_429.png)'
  prefs: []
  type: TYPE_IMG
- en: single linear layer followed by a softmax。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_431.png)'
  prefs: []
  type: TYPE_IMG
- en: So that's our neural net。 And the loss here was the mean squared error。 So we
    simply subtracted the prediction from the ground truth and squared it and summed
    it all up。 And that was the loss。 And the loss was the single number that summarized
    the quality of the neural net。 And when loss is low， like almost zero， that means
    the neural net is predicting correctly。
  prefs: []
  type: TYPE_NORMAL
- en: So we had a single number that summarized the performance of the neural net。
    And everything here was differentiable and was stored in a massive compute graph。
    And then we iterated over all the parameters。 We made sure that the gradients
    are set to zero。 And we called loss。backward。 And loss。backward initiated back
    propagation at the final output node of loss。
  prefs: []
  type: TYPE_NORMAL
- en: Right。 So yeah， remember these expressions？ We had loss all the way at the end。
    We start back propagation and we went all the way back。 And we made sure that
    we populated all the parameters dot grad。 So that grad started at zero。 but back
    propagation filled it in。 And then in the update， we iterated over all the parameters。
  prefs: []
  type: TYPE_NORMAL
- en: And we simply did a parameter update where every single element of our parameters
    was nudged in the opposite direction of the gradient。 And so we're going to do
    the exact same thing here。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_433.png)'
  prefs: []
  type: TYPE_IMG
- en: So I'm going to pull this up on the side here。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_435.png)'
  prefs: []
  type: TYPE_IMG
- en: So that we have it availed and we're actually going to do the exact same thing。
    So this was the forward pass。 So where we did this。 And props is our white thread。
    So now we have to evaluate the loss， but we're not using the mean squared error。
    We're using the negative log likelihood because we are doing classification。
  prefs: []
  type: TYPE_NORMAL
- en: We're not doing regression as it's called。 So here we want to calculate loss。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_437.png)'
  prefs: []
  type: TYPE_IMG
- en: Now the way we calculated is just this average negative log likelihood。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_439.png)'
  prefs: []
  type: TYPE_IMG
- en: Now this props here has a shape of five by twenty seven。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_441.png)'
  prefs: []
  type: TYPE_IMG
- en: And so to get all the we basically want to pluck out the probabilities at the
    correct indices here。 So in particular because the labels are stored here in the
    array wise。 Basically what we're after is for the first example we're looking
    at probability of five at index five。 For the second example at the second row
    or row index one。
  prefs: []
  type: TYPE_NORMAL
- en: we are interested in the probability assigned to index thirteen。 At the second
    example we also have thirteen。 At the third row we want one。 And at the last row
    which is four we want zero。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_443.png)'
  prefs: []
  type: TYPE_IMG
- en: So these are the probabilities we're interested in。 And you can see that they're
    not amazing as we saw above。 So these are the probabilities we want。 but we want
    like a more efficient way to access these probabilities。 Not just listing them
    out in a tuple like this。 So it turns out that the way to do this in PyTorch。
  prefs: []
  type: TYPE_NORMAL
- en: one of the ways at least， is we can basically pass in all of these integers，
    in a vector。 So these ones you see how they're just 0， 1， 2， 3， 4。 We can actually
    create that using empty。 not empty， sorry， torch。arrange of five。 0， 1， 2， 3，
    4。 So we can index here with torch。arrange of five。 And here we index with wise。
    And you see that that gives us exactly these numbers。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_445.png)'
  prefs: []
  type: TYPE_IMG
- en: So that plucks out the probabilities of that the neural network assigns to the
    correct next character。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_447.png)'
  prefs: []
  type: TYPE_IMG
- en: Now we take those probabilities and we don't， we actually look at the log probability。
    So we want to dot log。 And then we want to just average that up。 So take the mean
    of all that。 And then it's the negative average log likelihood。 That is the loss。
    So the loss here is 3。7 something。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_449.png)'
  prefs: []
  type: TYPE_IMG
- en: And you see that this loss 3。76， 3。76 is exactly as we've obtained before。 But
    this is a vectorized form of that expression。 So we get the same loss。 And the
    same loss we can consider sort of as part of this forward pass and we've achieved
    here now loss。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_451.png)'
  prefs: []
  type: TYPE_IMG
- en: Okay， so we made our way all the way to loss。 We defined the forward pass。 We
    forwarded the network and the loss。 Now we're ready to do the backward pass。 So
    backward pass。 We want to first make sure that all the gradients are reset。 So
    they're at zero。 Now in PyTorch you can set the gradients to be zero， but you
    can also just set it to none。
  prefs: []
  type: TYPE_NORMAL
- en: And setting it to none is more efficient。 And PyTorch will interpret none as
    like a lack of a gradient and it's the same as zeros。 So this is a way to set
    to zero， the gradient。 And now we do loss。backward。 Before we do loss。backward
    we need one more thing。 If you remember from micrograd PyTorch actually requires
    that we pass in。 requires grad is true。 So that we tell PyTorch that we are interested
    in calculating gradients for this leaf tensor。
  prefs: []
  type: TYPE_NORMAL
- en: By default this is false。 So let me recalculate with that。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_453.png)'
  prefs: []
  type: TYPE_IMG
- en: And then setting none and loss。backward。 Now something magical happened when
    loss。backward was run。 Because PyTorch just like micrograd， when we did the forward
    pass here。 it keeps track of all the operations under the hood。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_455.png)'
  prefs: []
  type: TYPE_IMG
- en: It builds a full computational graph。 Just like the graphs we produced in micrograd。
    those graphs exist inside PyTorch。 And so it knows all the dependencies and all
    the mathematical operations of everything。 And when you then calculate the loss，
    we can call a。backward on it。 And that backward then fills in the gradients of
    all the intermediates all the way back to W's。
  prefs: []
  type: TYPE_NORMAL
- en: Which are the parameters of our neural net。 So now we can do WL grad and we
    see that it has structure。 There's stuff inside it。 And these gradients， every
    single element here。 So W。shape is 27 by 27。 W grad's shape is the same。 27 by
    27。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_457.png)'
  prefs: []
  type: TYPE_IMG
- en: And every element of W。grad is telling us the influence of that weight on the
    loss function。 So for example this number all the way here。 If this element， the
    zero zero element of W。 Because the gradient is positive， it's telling us that
    this has a positive influence on the loss。 Slightly nudging W。 Slightly taking
    W zero zero。 And adding a small h to it would increase the loss。
  prefs: []
  type: TYPE_NORMAL
- en: Mildly。 Because this gradient is positive。 Some of these gradients are also
    negative。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_459.png)'
  prefs: []
  type: TYPE_IMG
- en: So that's telling us about the gradient information。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_461.png)'
  prefs: []
  type: TYPE_IMG
- en: And we can use this gradient information to update the weights of this neural
    network。 So let's not do the update。 It's going to be very similar to what we
    had in micro grad。 We need no loop over all the parameters because we only have
    one parameter tensor and that is W。 So we simply do W。data plus equals。 We can
    actually copy this almost exactly。 Negative 0。1 times W。
  prefs: []
  type: TYPE_NORMAL
- en: grad。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_463.png)'
  prefs: []
  type: TYPE_IMG
- en: And that would be the update to the tensor。 So that updates the tensor。 And
    because the tensor is updated， we would expect that now the loss should decrease。
    So here if I print loss。item， it was 3。76。 So we've updated the W here。 So if
    I recalculate forward pass， loss now should be slightly lower。 So 3。76 goes to
    3。74。
  prefs: []
  type: TYPE_NORMAL
- en: And then we can again set to set grad to none and backward， update。 And now
    the parameters changed again。 So if we recalculate the forward pass。 we expect
    a lower loss again 3。72。 And this is again doing the， we're now doing reading
    the set。 And when we achieve a low loss， that will mean that the network is assigning
    high probabilities to the correct next characters。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_465.png)'
  prefs: []
  type: TYPE_IMG
- en: Okay， so I rearranged everything and I put it all together from scratch。 So
    here is where we construct our data set of by grams。 You see that we are still
    iterating only over the first word Emma。 I'm going to change that in a second。
    I added a number that counts the number of elements in Xs so that we explicitly
    see that number of examples is 5。
  prefs: []
  type: TYPE_NORMAL
- en: Because currently we were just working with Emma and there's 5 by grams there。
    And here I added a loop of exactly what we had before。 So we had 10 iterations
    of very new descent of forward pass， backward pass and update。 And so running
    these two cells initialization and creating descent gives us some improvement
    on the loss function。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_467.png)'
  prefs: []
  type: TYPE_IMG
- en: But now I want to use all the words。 And there's not 5 but 228，000 by grams
    now。 However。 this should require no modification whatsoever。 Everything should
    just run because all the code we wrote doesn't care if there's 5 by grams or 228。000
    by grams。 And with everything we should just work。 So you see that this will just
    run。 But now we are optimizing the entire training set of all the by grams。
  prefs: []
  type: TYPE_NORMAL
- en: And you see now that we are decreasing very slightly。 So actually we can probably
    afford a larger learning rate。 And probably afford even larger learning rate。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_469.png)'
  prefs: []
  type: TYPE_IMG
- en: Even 50 seems to work on this very， very simple example。 So let me reinitialize
    and let's run 100 iterations。 See what happens。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_471.png)'
  prefs: []
  type: TYPE_IMG
- en: Okay。 We seem to be coming up to some pretty good losses here。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_473.png)'
  prefs: []
  type: TYPE_IMG
- en: 2。47。 Let me run 100 more。 What is the number that we expect by the way in the
    loss？
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_475.png)'
  prefs: []
  type: TYPE_IMG
- en: We expect to get something around what we had originally actually。 So all the
    way back if you remember in the beginning of this video。 when we optimized just
    by counting， our loss was roughly 2。47 after we added smoothing。 But before smoothing
    we had roughly 2。45 likely at sorry loss。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_477.png)'
  prefs: []
  type: TYPE_IMG
- en: And so that's actually roughly the vicinity of what we expect to achieve。 But
    before we achieved it by counting and here we are achieving roughly the same result。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_479.png)'
  prefs: []
  type: TYPE_IMG
- en: but with gradient based optimization。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_481.png)'
  prefs: []
  type: TYPE_IMG
- en: So we come to about 2。46， 2。45， et cetera。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_483.png)'
  prefs: []
  type: TYPE_IMG
- en: And that makes sense because fundamentally we're not taking in any additional
    information。 We're still just taking in the previous character and trying to predict
    the next one。 But instead of doing it explicitly by counting and normalizing。
    we are doing it with gradient based learning。 And it just so happens that the
    explicit approach happens to very well optimize the loss function。
  prefs: []
  type: TYPE_NORMAL
- en: without any need for a gradient based optimization。 Because the setup for Bagram
    language models is so straightforward， it's so simple。 We can just afford to estimate
    those probabilities directly and maintain them in a table。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_485.png)'
  prefs: []
  type: TYPE_IMG
- en: But the gradient based approach is significantly more flexible。 So we've actually
    gained a lot because what we can do now is we can expand this approach。 and complexify
    the neural net。 So currently we're just taking a single character and feeding
    into a neural net。 and the neural net is extremely simple。 But we're about to
    iterate on this substantially。
  prefs: []
  type: TYPE_NORMAL
- en: We're going to be taking multiple previous characters and we're going to be
    feeding them。 into increasingly more complex neural nets。 But fundamentally。 the
    output of the neural net will always just be logits。 And those logits will go
    through the exact same transformation。
  prefs: []
  type: TYPE_NORMAL
- en: We are going to take them through a softmax， calculate the loss function and
    the negative log likelihood。 and do gradient based optimization。 And so actually
    as we complexify the neural nets and work all the way up to transformers。 none
    of this will really fundamentally change。 None of this will fundamentally change。
    The only thing that will change is the way we do the forward pass。
  prefs: []
  type: TYPE_NORMAL
- en: where we take in some previous characters and calculate the logits for the next
    character in a sequence。 That will become more complex and I will use the same
    machinery to optimize it。 And it's not obvious how we would have extended this
    by-gram approach into the case。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_487.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_488.png)'
  prefs: []
  type: TYPE_IMG
- en: where there are many more characters at the input。 Because eventually these
    tables would get way too large because there's way too many combinations。 of what
    previous characters could be。 If you only have one previous character。 we can
    just keep everything in a table， the counts。
  prefs: []
  type: TYPE_NORMAL
- en: But if you have the last 10 characters that are input。 we can't actually keep
    everything in a table anymore。 So this is fundamentally an unscalable approach。
    And the neural network approach is significantly more scalable。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_490.png)'
  prefs: []
  type: TYPE_IMG
- en: And it's something that actually we can improve on over time。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_492.png)'
  prefs: []
  type: TYPE_IMG
- en: So that's where we will be digging next。 I wanted to point out two more things。
    Number one。 I want you to notice that this x-ank here， this is made up of one-hot
    vectors。 And then those one-hot vectors are multiplied by this w matrix。 And we
    think of this as multiple neurons being forwarded in a fully connected manner。
  prefs: []
  type: TYPE_NORMAL
- en: But actually what's happening here is that， for example。 if you have a one-hot
    vector here that has a one， let's say the fifth dimension。 then because of the
    way the matrix multiplication works。 multiplying that one-hot vector with w actually
    ends up plucking out the fifth row of w。
  prefs: []
  type: TYPE_NORMAL
- en: Logits would become just the fifth row of w。 And that's because of the way the
    matrix multiplication works。 So that's actually what ends up happening。 But that's
    actually exactly what happened before。 Because remember all the way up here， we
    have a by-gram， we took the first character。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_494.png)'
  prefs: []
  type: TYPE_IMG
- en: and then that first character indexed into a row of this array here。 And that
    row gave us the probability distribution for the next character。 So the first
    character was used as a lookup into a matrix here to get the probability distribution。
    Well， that's actually exactly what's happening here。 So we're taking the index。
  prefs: []
  type: TYPE_NORMAL
- en: we're encoding it as one-hot and multiplying it by w。 So logits literally becomes
    the appropriate row of w。 And that gets just as before。 exponentiated to create
    the counts， and then normalized and becomes probability。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_496.png)'
  prefs: []
  type: TYPE_IMG
- en: So this w here is literally the same as this array here。 But w， remember， is
    the log counts。 not the counts。 So it's more precise to say that w exponentiated
    w。x is this array。 But this array was filled in by counting and by basically populating
    the counts of by-grams。 Whereas in the gradient-based framework， we initialize
    it randomly。
  prefs: []
  type: TYPE_NORMAL
- en: and then we let the loss guide us to arrive at the exact same array。 So this
    array exactly here is basically the array w at the end of optimization。 except
    we arrived at it piece by piece by following the loss。 And that's why we also
    obtain the same loss function at the end。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_498.png)'
  prefs: []
  type: TYPE_IMG
- en: And the second note is if I come here， remember the smoothing where we added
    fake counts to our counts。 in order to smooth out and make more uniform the distributions
    of these probabilities。 And that prevented us from assigning zero probability
    to any one by-gram。 Now。 if I increase the count here， what's happening to the
    probability？ As I increase the count。
  prefs: []
  type: TYPE_NORMAL
- en: probability becomes more and more uniform。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_500.png)'
  prefs: []
  type: TYPE_IMG
- en: Because these counts go only up to like 900 or whatever。 So if I'm adding plus
    a million to every single number here。 you can see how the row and its probability
    then when you divide。 it is just going to become more and more close to exactly
    even probability in the form distribution。
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that the gradient-based framework has an equivalent to smoothing。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_502.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_503.png)'
  prefs: []
  type: TYPE_IMG
- en: In particular， think through these w's here， which we initialize randomly。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_505.png)'
  prefs: []
  type: TYPE_IMG
- en: We could also think about initializing w's to be zero。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_507.png)'
  prefs: []
  type: TYPE_IMG
- en: If all the entries of w are zero， then you'll see that logits will become all
    zero。 And then exponentiating those logits becomes all one and then the probabilities
    turn out to be exactly uniform。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_509.png)'
  prefs: []
  type: TYPE_IMG
- en: So basically when w's are all equal to each other or say especially zero。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_511.png)'
  prefs: []
  type: TYPE_IMG
- en: then the probability is not completely uniform。 So trying to incentivize w to
    be near zero is basically equivalent to label smoothing。 And the more you incentivize
    that in a loss function。 the more smooth distribution you're going to achieve。
    So this brings us to something that's called regularization。
  prefs: []
  type: TYPE_NORMAL
- en: where we can actually augment the loss function to have a small component that
    we call a regularization loss。 In particular， what we're going to do is we can
    take w and we can， for example。 square all of its entries。 And then we can， whoops，
    sorry about that。 we can take all the entries of w and we can sum them。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_513.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_514.png)'
  prefs: []
  type: TYPE_IMG
- en: And because we're squaring， there will be no signs anymore。 Natives and positives
    all get squashed because of numbers。 And then the way this works is you achieve
    zero loss if w is exactly or zero。 But if w has nonzero numbers， you accumulate
    loss。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_516.png)'
  prefs: []
  type: TYPE_IMG
- en: And so we can actually take this and we can add it on here。 So we can do something
    like loss plus w square dot sum。 Or let's actually instead of sum。 let's take
    a mean because otherwise the sum gets too large。 So mean is like a little bit
    more manageable。 And then we have a regularization loss here。
  prefs: []
  type: TYPE_NORMAL
- en: like say 0。01 times or something like that。 You can choose the regularization
    strength。 And then we can just optimize this。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_518.png)'
  prefs: []
  type: TYPE_IMG
- en: And now this optimization actually has two components。 Not only is it trying
    to make all the probabilities work out， but in addition to that。 there's an additional
    component that simultaneously tries to make all w's be zero。 Because if w's are
    nonzero， you feel a loss。 And so minimizing this。
  prefs: []
  type: TYPE_NORMAL
- en: the only way to achieve that is for w to be zero。 And so you can think of this
    as adding like a spring force or like a gravity force that pushes w to be zero。
    So w wants to be zero and the probabilities want to be uniform。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_520.png)'
  prefs: []
  type: TYPE_IMG
- en: But they also simultaneously want to match up your probabilities as indicated
    by the data。 And so the strength of this regularization is exactly controlling
    the amount of counts that you add here。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_522.png)'
  prefs: []
  type: TYPE_IMG
- en: Adding a lot more counts here corresponds to increasing this number。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_524.png)'
  prefs: []
  type: TYPE_IMG
- en: Because the more you increase it， the more this part of the loss function dominates
    this part。 And the more these weights will be unable to grow。 Because as they
    grow。 they accumulate way too much loss。 And so if this is strong enough。 then
    we are not able to overcome the force of this loss。 And we will never。
  prefs: []
  type: TYPE_NORMAL
- en: and basically everything will be uniform predictions。 So I thought that's kind
    of cool。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_526.png)'
  prefs: []
  type: TYPE_IMG
- en: Okay， and lastly， before we wrap up， I wanted to show you how you would sample
    from this neural net model。 And I copy pasted the sampling code from before。 Where
    remember that we sampled five times and all we did is we started zero。 we grabbed
    the current iX row of P。 And that was our probability row from which we sampled
    the next index and just accumulated that and break when zero。 And running this
    gave us these results。 I still have the P in memory， so this is fine。 Now。
  prefs: []
  type: TYPE_NORMAL
- en: this P doesn't come from the row of P。 Instead it comes from this neural net。
    First we take iX and we encode it into a one-hot row of X-ink。 This X-ink multiplies
    our W。 which really just plucks out the row of W corresponding to iX。 Really that's
    what's happening。 And that gets our logits and then we normalize those logits。
  prefs: []
  type: TYPE_NORMAL
- en: Exponentiate to get counts and then normalize to get the distribution and then
    we can sample from the distribution。 So if I run this kind of anti-climatic or
    climatic， depending how you look at it。 but we get the exact same result。 And
    that's because this is in the identical model。 Not only does it achieve the same
    loss， but as I mentioned these are identical models and this W is the log counts
    of what we've estimated before。
  prefs: []
  type: TYPE_NORMAL
- en: But we came to this answer in a very different way and it's got a very different
    interpretation。 But fundamentally this is basically the same model and gives the
    same samples here。 And so that's kind of cool。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_528.png)'
  prefs: []
  type: TYPE_IMG
- en: Okay， so we've actually covered a lot of ground。 We introduced the bygram character
    level language model。 We saw how we can train the model， how we can sample from
    the model and how we can evaluate the quality of the model using the negative
    log likelihood loss。
  prefs: []
  type: TYPE_NORMAL
- en: And then we actually trained the model in two completely different ways that
    actually get the same result and the same model。 In the first way we just counted
    up the frequency of all the bygrams and normalized。 In the second way we used
    the negative log likelihood loss as a guide to optimizing the counts matrix or
    the counts array so that the loss is minimized in the gradient based framework。
    And we saw that both of them give the same result and that's it。
  prefs: []
  type: TYPE_NORMAL
- en: Now the second one of these the gradient based framework is much more flexible。
    And right now our neural net part is super simple。 We're taking a single previous
    character and we're taking it through a single linear layer to calculate the logits。
    This is about to complexify。 So in the follow up videos we're going to be taking
    more and more of these characters and we're going to be feeding them into a neural
    net。
  prefs: []
  type: TYPE_NORMAL
- en: But this neural net will still output the exact same thing。 The neural net will
    output logits。 And these logits will still be normalized in the exact same way
    and all the loss and everything else and the gradient based framework。 everything
    stays identical。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_530.png)'
  prefs: []
  type: TYPE_IMG
- en: It's just that this neural net will now complexify all the way to transformers。
    So that's going to be pretty awesome and I'm looking forward to it。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0f3ae8b78024dba93e80534d70c09b_532.png)'
  prefs: []
  type: TYPE_IMG
- en: For now。 Bye。
  prefs: []
  type: TYPE_NORMAL
