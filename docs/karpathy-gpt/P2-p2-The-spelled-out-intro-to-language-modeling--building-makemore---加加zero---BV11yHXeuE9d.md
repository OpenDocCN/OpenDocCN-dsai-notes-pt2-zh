# P2：p2 语言模型的详细介绍：构建 makemore - 加加 zero - BV11yHXeuE9d

大家好，希望你们都很好。接下来我想做的是构建 MakeMore。像之前的 Micrograd 一样，MakeMore 是我在 GitHub 网页上的一个仓库。你可以查看它。但就像 Micrograd 一样，我将一步一步地构建它。

![](img/9b0f3ae8b78024dba93e80534d70c09b_1.png)

我将把所有内容详细说明。所以我们将慢慢一起构建它。现在，什么是 MakeMore？MakeMore，如其名所示，可以生成你给它的事物。

![](img/9b0f3ae8b78024dba93e80534d70c09b_3.png)

![](img/9b0f3ae8b78024dba93e80534d70c09b_4.png)

这里是一个示例。names.txt 是一个给 MakeMore 的示例数据集。

![](img/9b0f3ae8b78024dba93e80534d70c09b_6.png)

当你查看 names.txt 时，你会发现这是一个非常大的名字数据集。所以这里有很多不同类型的名字。事实上，我相信有 32,000 个名字。

![](img/9b0f3ae8b78024dba93e80534d70c09b_8.png)

我在政府网站上随机找到的一些东西。

![](img/9b0f3ae8b78024dba93e80534d70c09b_10.png)

如果你在这个数据集上训练 MakeMore，它将学习生成更多这样的事物。特别是在这种情况下，这将意味着生成更多听起来像名字的东西，但实际上是独特的名字。如果你有个宝宝并试图给他起个名字，也许你在寻找一个很酷的新颖独特的名字，MakeMore 可能会帮助你。

所以这里是一些从神经网络生成的示例结果，一旦我们在我们的数据集上训练它。这里是一些它将生成的独特名字示例。不要告诉。I rot。Zen-di。等等。这些名字听起来像名字，但当然不是名字。所以在底层，MakeMore 是一个字符级语言模型。这意味着它将这里的每一行视为一个示例。

在每个示例中，它将所有内容视为单个字符的序列。所以 R-E-E-S-E 是这个示例。这是字符的序列。这是我们构建 MakeMore 的层次。而作为字符级语言模型的意义在于，它只是对这些字符序列进行建模。

它知道如何预测序列中的下一个字符。

![](img/9b0f3ae8b78024dba93e80534d70c09b_12.png)

现在，我们实际上要实现大量的字符级语言模型。

![](img/9b0f3ae8b78024dba93e80534d70c09b_14.png)

关于参与预测序列中下一个字符的神经网络。所以非常简单的 bi-gram 和 bag-of-word 模型，多层感知器，递归神经网络，一直到现代的 transformers。实际上，我们将构建的 transformer 基本上是 GPT-2 的等效 transformer。如果你听说过 GPT。这个确实是个大事情。这是一个现代网络。

到了系列的最后，你将真正理解这一切在字符层面的运作。现在，为了给你一个关于这里扩展的感觉，在字符之后。我们可能会花一些时间在下一个层面。我们可能会花一些时间在单词层面，以便能够生成单词的文档。

不仅仅是字符的小片段，我们可以生成整个更大、规模更大的文档。然后，我们可能会进入图像和图像文本网络，比如 Dali、Stable Diffusion 等等。但现在，我们必须从这里开始，字符级语言建模。开始吧。

![](img/9b0f3ae8b78024dba93e80534d70c09b_16.png)

和之前一样，我们从一张完全空白的重复页面开始。首先，我想基本上加载名为.txt 的数据集。所以我们将打开 names.txt 进行读取。然后我们会把所有内容读入一个大型字符串中。由于这是一个大型字符串，我们只想要单独的单词，并将它们放入列表中。

所以让我们在那个字符串上调用 split lines，以获得我们所有的单词作为 Python 字符串列表。因此，我们可以查看，例如，前 10 个单词。我们发现这是一个包括 Emma、Olivia、Eva 等的列表。

![](img/9b0f3ae8b78024dba93e80534d70c09b_18.png)

如果我们看看页面的顶部，确实就是我们所看到的。

![](img/9b0f3ae8b78024dba93e80534d70c09b_20.png)

这很好。这个列表实际上让我觉得它可能是按频率排序的。

![](img/9b0f3ae8b78024dba93e80534d70c09b_22.png)

不过，好吧，这些就是单词。现在，我们实际上想多了解一些这个数据集。

![](img/9b0f3ae8b78024dba93e80534d70c09b_24.png)

让我们来看一下总词数。我们预计大约为 32,000。那么，最短的单词是什么呢？例如，计算每个单词的最小长度。最短的单词将是两个字母。最长的单词将是 15 个字符。现在让我们思考一下我们的第一个语言模型。

正如我提到的，字符级语言模型是在预测序列中的下一个字符，给定之前的一些具体字符序列。现在，我们必须意识到这里的每一个单词，例如 Zabella，实际上包含了几个示例。

那么，像 Zabella 这样的单词在数据集中存在，实际上告诉我们什么呢？

这表明字符 i 在一个名字的序列中是非常可能首先出现的字符。字符 S 可能会在 i 后面出现。字符 A 可能会在 iS 后面出现。字符 B 非常可能会在 iSA 后面出现。以此类推，直到 A 在 Zabella 之后出现。实际上，这里还有一个例子，就是在 Zabella 之后。

这个单词很可能结束。所以这是我们在这里有的另外一个明确的信息点。我们必须小心。因此，在这些字符序列中，单个单词中包含了大量的统计结构，以预测哪些字符可能会跟随。并且当然，我们不仅仅有一个单独的单词。实际上我们有 32,000 个这样的单词。

因此，这里有很多结构需要建模。现在一开始，我想从构建一个 MIGRAM 语言模型开始。

![](img/9b0f3ae8b78024dba93e80534d70c09b_26.png)

现在在一个 MIGRAM 语言模型中，我们始终一次处理两个字符。

![](img/9b0f3ae8b78024dba93e80534d70c09b_28.png)

所以我们只关注一个给定的字符，并试图预测序列中的下一个字符。哪些字符可能会跟随 A 等等。我们仅仅是在建模这种局部结构。我们忽略了可能还有很多更多信息的事实。

我们总是只看前一个字符来预测下一个字符。所以这是一个非常简单且弱的语言模型，但我认为这是一个很好的起点。

![](img/9b0f3ae8b78024dba93e80534d70c09b_30.png)

现在让我们开始查看数据集中的这些 BIGRAMS，以及它们的样子。这些 BIGRAMS 再次只是连续两个字符。所以对于 W 和单词。这里的每个 W 都是一个单独的单词字符串。我们想要用连续字符迭代这个单词。所以每次滑动两个字符。现在有一种有趣的好方法。

在 Python 中，有一种有趣的做法，就是这样做。对于 character one，character two，在 zip 的 W 和 W 的一列中。打印，character one，character two。我们不做所有单词。我们只做前三个单词。我将稍后向你展示这是如何工作的。但现在，基本上作为一个示例。

让我们单独处理第一个单词，Emma。你看到我们有 Emma 吗？

这将打印出 EM，MMA。之所以这样有效，是因为 W 是字符串 Emma。W 在第一列是字符串 MMA。zip 接受两个迭代器。它将它们配对，然后创建一个迭代器，返回它们连续条目的元组。如果其中任何一个列表比另一个短，它将停止并返回。

所以基本上这就是我们为什么返回 EM，MMA，MMA。可是因为这个迭代器，这里的第二个，元素耗尽，zip 就结束了。这就是为什么我们只得到这些元组。所以相当可爱。这些是第一个单词中的连续元素。现在我们必须小心，因为我们实际上这里有比这三个示例更多的信息。

正如我提到的，我们知道 E 很可能是第一个出现的，而在这种情况下 A 是最后一个。因此，做这件事的一种方法是基本上在这里创建一个特殊的数组，我们的字符。我们将在这里幻想一个特殊的开始标记，我称之为特殊开始。因此，这是一个包含一个元素加 W 和一个特殊结束字符的列表。

我将 W 的列表包裹起来是因为 W 是一个字符串 MMA。W 的列表将仅包含列表中的单个字符。然后再次执行这个，但不是迭代 W，而是迭代字符，将给我们像这样。因此 E 是可能的。所以这是开始字符和 E 的字对。

这是字对 A 和特殊结束字符。现在我们可以查看，例如，对于 Olivia 或 Eva 这看起来像什么。实际上，我们可以对整个数据集进行这样的处理。但我们不会打印，这会太多。但这些是单个字符的字对，我们可以打印它们。

现在，为了了解哪些字符可能跟随其他字符的统计信息。在字对语言模型中，最简单的方法就是简单地通过计数来实现。因此，我们基本上只是在训练集中的这些单词里统计这些组合出现的频率。所以我们需要某种字典来维护每一个字对的计数。

所以我们来使用字典 B，它将映射这些字对。字对是字符元组（character on character）。然后 B 在字对中的值将是字对的计数，这基本上与 B 在字对中的值相同，但在字对不在字典 B 中的情况下。

我们想购买默认返回零加一。因此，这基本上会将所有的字对加起来并计算它们出现的频率。我们来去掉打印，或者说，我们保留打印，仅仅检查一下在这种情况下 B 是什么。我们发现许多字对只出现了一次。这一个据称出现了三次。

所以 A 是结束字符出现了三次，这对所有这些单词都是如此。所有的 Emma、Olivia 和 Eva，以及 A。所以这就是为什么它出现三次。现在让我们对所有单词做这个。哎呀，我不应该打印这个。我将把它删掉。让我们停止这个。让我们运行，现在 B 将拥有整个数据集的统计信息。

所以这些是各个字的单独字对的计数。我们可以，例如，查看一些最常见和最少见的字对。这在 Python 中是可以扩展的，但我喜欢的最简单方法是使用 B.dot items。

![](img/9b0f3ae8b78024dba93e80534d70c09b_32.png)

B.dot items 返回键值元组。

![](img/9b0f3ae8b78024dba93e80534d70c09b_34.png)

在这种情况下，键是字符组合，值是计数。因此，我们想要对这个进行排序。但是默认的排序是基于元组的第一个项目。但我们想按值排序，即元组的第二个元素，也就是键值。

所以我们想使用 key 等于 lambda，该函数接受键值并返回第一个值，而不是零，而是返回计数。因此，我们想按这些元素的计数进行排序。实际上，我们希望它反向排序。所以这里有 by gram Q 和 R 仅出现一次。DZ 也仅出现一次。当我们反向排序时。

我们将看到按字符组合的概率。因此，我们看到 N 很多时候是结束字符，许多次。而且显然 N 几乎总是跟随 A，这是一个非常可能的组合。因此，这是我们在整个 DZ 中获得的个别计数。

![](img/9b0f3ae8b78024dba93e80534d70c09b_36.png)

现在，将这些信息保存在二维数组中比保存在 Python 字典中要方便得多。因此，我们将把这些信息存储在一个 2D 数组中。行将是字符组合的第一个字符，列将是第二个字符。学生机器数组中的每个条目将告诉我们第一个字符在 DZ 中跟随第二个字符的频率。

特别是我们将使用的数组表示或库是 PyTorch。PyTorch 是一个深度学习神经网络框架。但其中的一部分也是这个 torch.tensor，它允许我们创建多维数组并高效地进行操作。因此，让我们导入 PyTorch，可以通过 import torch 来完成。然后我们可以创建数组。

所以让我们创建一个零数组，并给它一个大小。我们以 3x5 数组为例。这是一个 3x5 的零数组。默认情况下，你会注意到 D 类型的值为 8，代表数据类型为 float 32。因此，这些是单精度浮点数。

因为我们将表示计数，所以实际上使用 D 类型为 torch.in32。因此，这些是 32 位整数。因此，现在你会看到这个张量中有整数数据。张量允许我们高效地操作所有个别条目。例如，如果我们想改变这个位，我们必须对张量进行索引。

特别是在这里，这是第一行，因为它是从零开始索引的。所以这是行索引 1 和列索引 0、1、2、3。因此，我们可以将 A 在 1.3 位置设置为 1，然后 A 将在那有一个 1。我们当然也可以这样做。因此现在 A 在那里是 2 或 3。

我们还可以例如说 A00 是 5，那么 A 在这里将有一个 5。这就是我们如何索引数组。现在当然我们感兴趣的数组要大得多。

![](img/9b0f3ae8b78024dba93e80534d70c09b_38.png)

就我们的目的而言，我们有 26 个字母的字母表，然后有两个特殊字符 S 和 E。因此我们想要一个 28 乘 28 的数组。我们称之为大写 N，因为它将表示计数。让我擦掉这些东西。所以这是一个从 0 开始的 28 乘 28 的数组。现在让我们把这个复制粘贴到这里。但是我们现在没有字典 B，我们将擦掉它，而是有一个 N。

这里的问题是我们有这些字符，它们是字符串，但我们现在必须。

![](img/9b0f3ae8b78024dba93e80534d70c09b_40.png)

基本上是对数组进行索引，我们必须使用整数进行索引。因此我们需要某种从字符到整数的查找表。让我们构建这样一个字符数组。我们将采取所有单词，这是一系列字符串。我们将把它们连接成一个庞大的字符串。

所以这就是整个数据集作为一个单一字符串。我们将把它传递给集合构造函数，该函数接受这个庞大的字符串并去掉重复项，因为集合不允许重复。因此这个集合将是所有小写字符的集合。总共应该有 26 个。现在我们实际上不想要一个集合，我们想要一个列表。

但是我们不想要一个以某种奇怪的任意方式排序的列表，我们希望它从 A 排序到 Z。因此是一个排序列表。这就是我们的字符。现在我们想要的是这个查找表，正如我提到的。让我们创建一个特殊的 S 到 I，我将称之为 S 是字符串或字符。这将是这些字符的 S 到 I 映射。

所以 enumerate 基本上给我们提供了一个迭代器，遍历整数索引和列表的实际元素。然后我们将字符映射到整数。因此 S 到 I 是从 A 到零，B 到一，等等，一直到 Z 到 25 的映射。这在这里会很有用，但我们实际上还必须特别设置 S 为 26。

所以 S 到 I 的值将是 27，对吧，因为 Z 是 25。因此这些就是查找值。现在我们可以在这里将字符一和字符二映射到它们的整数值。所以这将是 S 到 I 字符一，而 I X 二将是 S 到 I 字符二。现在我们应该能够使用我们的数组来处理这一行。所以在 X 一，I X 二。

这是我之前向您展示的二维数组索引。老实说，只需加一，因为一切从零开始。所以这应该能工作，并给我们一个包含所有计数的大型 28 乘 20 数组。因此，如果我们打印 N，这就是数组，但当然看起来很丑。

![](img/9b0f3ae8b78024dba93e80534d70c09b_42.png)

让我们擦掉这堆丑陋的东西，试着把它可视化得更好一点。

![](img/9b0f3ae8b78024dba93e80534d70c09b_44.png)

为此，我们将使用一个名为 matplotlib 的库。matplotlib 允许我们创建图形。

![](img/9b0f3ae8b78024dba93e80534d70c09b_46.png)

因此我们可以做一些事情，比如 PLT，我会展示计数器 a。

![](img/9b0f3ae8b78024dba93e80534d70c09b_48.png)

这是一个 20 乘 20 的数组，这是结构。

![](img/9b0f3ae8b78024dba93e80534d70c09b_50.png)

但即便如此，我认为这仍然相当丑陋。因此我们将尝试创建一个更漂亮的可视化，我为此写了一些代码。我们首先需要做的是反转这个数组，这个字典。

![](img/9b0f3ae8b78024dba93e80534d70c09b_52.png)

S 到 I 是从 S 到 I 的映射，而在 I 到 S 中，我们将反转这个字典。所以它遍历所有项目并反转那个数组。

![](img/9b0f3ae8b78024dba93e80534d70c09b_54.png)

所以 I 到 S 的映射是从零到 A，1 到 B，等等。

![](img/9b0f3ae8b78024dba93e80534d70c09b_56.png)

所以我们需要那个。然后这是我想出的代码，试图让这个变得更好看。

![](img/9b0f3ae8b78024dba93e80534d70c09b_58.png)

我们创建一个图形，绘制 M，然后稍后可视化一些东西。

![](img/9b0f3ae8b78024dba93e80534d70c09b_60.png)

![](img/9b0f3ae8b78024dba93e80534d70c09b_61.png)

让我运行一下，这样你就能感受到它是什么。你会看到这里的数组是分开的，每一个基本上都是 B 跟随 G 零次。B 跟随 H 41 次。A 跟随 J 一次 75 次。因此你可以看到我在这里做的事情是，首先显示整个数组，然后迭代所有单独的单元。

我在这里创建了一个字符字符串，这是整数 I 和整数 J 的反向映射 I 到 S。所以这就是字符表示中的双字母组合。然后我仅绘制图表文本，并根据语法计数绘制出现次数。这里有一个点项的原因是当你索引这些数组时。

这些是 torch 张量。

![](img/9b0f3ae8b78024dba93e80534d70c09b_63.png)

你会看到我们仍然返回一个张量。所以你认为这东西的类型应该只是整数 149，但它实际上是一个 torch 张量。因此，如果你使用 dot item，然后它将弹出那个单独的整数。因此，它只是 149。这就是发生的事情。这些只是一些选项来让它看起来不错。

那么这个数组的结构是什么？我们有所有这些计数，我们看到其中一些经常出现，而一些则不常出现。仔细审视，你会注意到我们其实并没有很聪明。因为当你过来时，你会发现例如我们有一整行完全是零。这是因为结束字符不可能是双字母组合的第一个字符。

因为我们总是把这些结束标记放在双字组的末尾。同样，我们在这里有整列的零，因为 S 字符永远不可能是双字组的第二个元素。因为我们总是以 S 开始，以 E 结束，中间只有单词。因此，我们有一整列的零，一整行的零。

在这个小的 2x2 矩阵中，唯一可能发生的情况是 S 直接跟随 E。如果我们有一个没有字母的单词，这可以是非零的。所以在这种情况下，单词中没有字母。它是一个空单词，我们只有 S 跟随 E。但其他的情况是不可行的。因此，我们基本上是在浪费空间，不仅如此。

但是 S 和 E 在这里变得非常拥挤。我使用这些括号是因为在自然语言处理领域有使用这类括号的惯例，以表示特殊标记，但我们将使用其他东西。所以让我们修正这一切，让它变得更漂亮。我们实际上不会有两个特殊标记。

我们只会有一个特殊标记。因此，我们将有一个 27 乘以 27 的 n 乘以 n 数组。我们只会有一个，我称之为点。好吧。让我把这个移到这里。现在我想做的另一件事是，我实际上想把这个特殊字符的半位置设为零，并希望将所有其他字母偏移。我发现这样更令人愉悦。

所以我们需要在这里加一，这样第一个字符 a 就会从 1 开始。所以 S 到 I 现在 a 从 1 开始，点是 0。而 I 到 S，当然，我们不改变这个，因为 I 到 S 只是创建了一个反向映射，这将很好地工作。所以 1 是 a，2 是 B，0 是点。我们在这里反转它，我们得到了一个点和一个点。

这应该没问题。确保我从零开始计数，然后这里我们不去到 28，而是去到 27。这应该可以正常工作。

![](img/9b0f3ae8b78024dba93e80534d70c09b_65.png)

好吧，所以我们看到点点从未发生。这是 0，因为我们没有空单词。那么这一行现在只是所有首字母的简单计数。所以 J 开始一个单词。H 开始一个单词，I 开始一个单词，等等。然后这些都是结束字符。在之间，我们有字符彼此跟随的结构。

这是我们整个数据集的计数数组。这个数组实际上包含了我们从双字组字符级语言模型中进行采样所需的所有信息。大致而言，我们要做的就是开始遵循这些概率和这些计数。

我们将从模型开始抽样。因此，在一开始，当然，我们从点开始，开始标记点。为了抽样名称的第一个字符，我们要查看这一行。所以我们看到这些计数，它们外部告诉我们任何一个字符开始一个单词的频率。如果我们取这个 N 并抓取第一行，我们可以通过简单的索引和零来做到这一点。

然后使用这个符号列来获取该行的其余部分。因此，N 零列是在零行中进行索引，然后抓取所有列。这将给我们第一行的一维数组。所以零，四，四，十。你知道，零，四，四，十，一，三，哦，六，一，五，四，二，等等。

这只是第一行。它的形状是二十七。这只是二十七的行。你也可以这样做，你只需抓取零行。这是等效的。现在这些是计数，而我们想做的是基本上从中抽样。

由于这些是原始计数，我们实际上需要将其转换为概率。所以我们创建一个概率向量。我们将取 N 的零，并先将其转换为浮点数。好的，这些整数被转换为浮点数。我们创建浮点数的原因是因为我们即将对这些计数进行归一化。

为了在这里创建概率分布，我们想要进行除法。我们基本上想要做 p。p 除以 p 的总和。现在我们得到了一个较小数字的向量，这些现在是概率。因此，当然因为我们是通过总和除以的，所以 p 的总和现在是 1。这是一个良好的概率分布。

它的总和为 1，这为任何单个字符成为单词的第一个字符提供了概率。因此，现在我们可以尝试从这个分布中抽样。

![](img/9b0f3ae8b78024dba93e80534d70c09b_67.png)

为了从这些分布中抽样，我们将使用 Torre-Tut 多项分布，我在这里已经打开了。Torre-Tut 多项分布从多项式概率分布中返回样本，这是一种复杂的说法，意思是你给我概率，我就给你根据概率分布抽样的整数。这就是该方法的特征。

为了使一切确定性，我们将使用 PyTorch 中的生成器对象。

![](img/9b0f3ae8b78024dba93e80534d70c09b_69.png)

这样做使一切都是确定性的。所以当你在你的计算机上运行这个时，你会得到与我在我的计算机上获得的完全相同的结果。

![](img/9b0f3ae8b78024dba93e80534d70c09b_71.png)

让我给你展示一下这是如何工作的。这是创建 Torre-Tut 生成器对象的确定性方法。

![](img/9b0f3ae8b78024dba93e80534d70c09b_73.png)

用我们可以达成一致的某个数字来设置种子。这样就种下了一个生成器。它给我们一个对象 G。然后我们可以将 G 传递给一个创建随机数的函数。Torre-Tut 随机数生成三个随机数，并且它使用这个生成器对象作为随机性来源。所以不进行归一化。

我可以直接打印。这有点像是根据这个东西生成的随机数在零和一之间。而每次我再次运行时，我总会得到相同的结果，因为我一直在使用相同的生成器对象，这里我正在设置种子。如果我进行归一化，我会得到三个元素的良好概率分布。

![](img/9b0f3ae8b78024dba93e80534d70c09b_75.png)

然后我们可以使用 Torre-Tut 多项式从中抽取样本。

![](img/9b0f3ae8b78024dba93e80534d70c09b_77.png)

所以这看起来就是这样。 Torre-Tut 多项式将会获取概率分布的 Torre-Tensor。然后我们可以请求多个样本，比如说 20。替换为 true 意味着当我们抽取一个元素时，可以将其放回合格索引的列表中再次抽取。

我们必须将替换指定为 true，因为默认情况下由于某种原因是 false。

![](img/9b0f3ae8b78024dba93e80534d70c09b_79.png)

![](img/9b0f3ae8b78024dba93e80534d70c09b_80.png)

我认为这只是需要小心的事情。生成器在这里传入。所以我们总会得到确定的结果。

![](img/9b0f3ae8b78024dba93e80534d70c09b_82.png)

所以如果我运行这两个，我们将从这个分布中获得一堆样本。现在你会注意到，这个张量中第一个元素的概率是 60%。所以在这 20 个样本中，我们预计 60% 会是零。我们预计 30% 会是 1。由于元素索引 2 只有 10% 的概率，这些样本中几乎没有应该是二。

确实，我们只有少量的二。我们可以根据需要抽取任意数量。

![](img/9b0f3ae8b78024dba93e80534d70c09b_84.png)

我们抽取的样本越多，这些数字就越应该大致符合这里的分布。因此我们应该有很多零，数量是少量一的两倍。

![](img/9b0f3ae8b78024dba93e80534d70c09b_86.png)

我们应该有三倍 S 的少量一和三倍 S 的少量二。

![](img/9b0f3ae8b78024dba93e80534d70c09b_88.png)

所以你看到我们几乎没有二。我们有一些一，大多数都是零。这就是 Torx-Dynote-Nal 的作用。

![](img/9b0f3ae8b78024dba93e80534d70c09b_90.png)

对我们来说，我们不仅仅在这一行，我们在这里创建了这个 P，现在可以从中抽取样本。

![](img/9b0f3ae8b78024dba93e80534d70c09b_92.png)

所以如果我们使用相同的种子，然后从这个分布中抽样，我们只获取一个样本。然后我们看到样本是 13。所以这将是索引。你看到它是一个包裹 13 的张量吗？我们再次需要使用 .item 来取出那个整数。现在索引就是数字 13。我们当然可以将 I2S 的 Ix 映射，准确找出我们在这里抽样的字符。

我们正在抽样 M。也就是说，我们的生成中的第一个字符是 M。

![](img/9b0f3ae8b78024dba93e80534d70c09b_94.png)

看着这一行，M 被抽样。我们可以看到 M 实际上开始了大量的单词。M 开头的单词有 2,500 个，占 32,000 个单词的数量。因此，几乎有不到 10% 的单词是以 M 开头的。

![](img/9b0f3ae8b78024dba93e80534d70c09b_96.png)

所以这个字符实际上是一个相当可能被抽样的字符。这将是我们单词的第一个字符。现在我们可以继续抽样更多字符。因为我们现在知道是以 M 开头。

![](img/9b0f3ae8b78024dba93e80534d70c09b_98.png)

M 已经被抽样了。现在为了绘制下一个字符，我们将返回这里，查找以 M 开头的行。所以你看 M，并且我们这里有一行。所以我们看到 M 的计数是 516，M.A 的计数是这个，M.B 的计数是这个，等等。这些是下一行的计数。这就是我们现在要生成的下一个字符。

所以我想我们实际上已经准备好写出循环。因为我想你开始理解这是怎么进行的。我们总是从索引 0 开始，因为那是开始标记。然后在循环中，我们将抓取与当前索引对应的行。那就是 P。

所以这就是 N，数组在 Ix。浮动的方法是 R P。然后我们将这个 P 归一化，使其总和为 1。

![](img/9b0f3ae8b78024dba93e80534d70c09b_100.png)

意外的随机无限循环。我们将 P 归一化，使其总和为 1。

![](img/9b0f3ae8b78024dba93e80534d70c09b_102.png)

然后我们需要这个生成器对象。我们将在这里初始化。

![](img/9b0f3ae8b78024dba93e80534d70c09b_104.png)

我们将从这个分布中抽取一个样本。

![](img/9b0f3ae8b78024dba93e80534d70c09b_106.png)

然后这将告诉我们下一个索引是什么。如果抽样的索引是 0，那么这就是结束标记。所以我们将中断。否则，我们将打印 S2I 的 Ix。

![](img/9b0f3ae8b78024dba93e80534d70c09b_108.png)

I2S 的 Ix。这差不多就是全部内容。

![](img/9b0f3ae8b78024dba93e80534d70c09b_110.png)

我们只是，这应该可以工作。好的，更多。所以这是我们抽样的名字。我们以 M 开头，下一步是 O，然后是 R，最后是点。我们也在这里打印这个点。所以现在我们来做几次。

![](img/9b0f3ae8b78024dba93e80534d70c09b_112.png)

所以我们实际上在这里创建一个输出列表。

![](img/9b0f3ae8b78024dba93e80534d70c09b_114.png)

而不是打印，我们将附加。因此输出将附加这个字符。然后在这里。让我们在最后打印出来。

![](img/9b0f3ae8b78024dba93e80534d70c09b_116.png)

所以我们只需将所有的输出连接起来。我们将会打印更多。好吧。现在我们总是得到相同的结果，因为生成器的原因。如果我们想多做几次，我们可以设置为高和范围为 10。我们可以抽样 10 个名字。

![](img/9b0f3ae8b78024dba93e80534d70c09b_118.png)

我们可以这样做 10 次。这些是我们得到的名字。我们再做 20 次。老实告诉你，这看起来不太对。所以我会盯着它几分钟，以说服自己这确实是正确的。这些样本如此糟糕的原因是 n-gram 语言模型真的。就像是非常糟糕。我们可以再生成几个。

你可以看到这些名字有点像，比如 Yannu、Erile 等。但它们真的完全搞错了。我的意思是，这么糟糕的原因是，我们生成了 H 作为名字。但你必须从模型的视角来看待这个问题。它并不知道这个 H 是第一个 H。它只知道之前有 H。

那么 H 作为最后一个字符的可能性有多大呢？嗯，它是有些可能的。所以它就把它作为最后一个字符。它不知道之前是否有其他东西。因此这就是为什么我生成了这些无意义的名字。另一种做法是让自己相信它实际上在做一些合理的事情。

尽管这很糟糕，但这里的这一小部分是 27，对吗？像 27。那么如果我们这样做呢？而不是拥有任何结构。如果 P 只是 torch 的全 1 矩阵，大小为 27 呢？默认情况下，这是 float32。所以这没问题。除以 27。所以我在这里做的是均匀分布。

这会让所有结果同样可能。我们可以从中抽样。

![](img/9b0f3ae8b78024dba93e80534d70c09b_120.png)

所以让我们看看这样是否能更好。好吗？这是一个完全未训练的模型的结果，其中每个结果都是同样可能的。

![](img/9b0f3ae8b78024dba93e80534d70c09b_122.png)

所以显然这是垃圾。

![](img/9b0f3ae8b78024dba93e80534d70c09b_124.png)

然后如果我们有一个训练好的模型，仅仅通过 n-gram 训练，这就是我们得到的结果。

![](img/9b0f3ae8b78024dba93e80534d70c09b_126.png)

所以你可以看到这更像是名字，它实际上在工作。

![](img/9b0f3ae8b78024dba93e80534d70c09b_128.png)

仅仅使用 n-gram 是如此糟糕，我们必须做得更好。

![](img/9b0f3ae8b78024dba93e80534d70c09b_130.png)

现在接下来，我想修复我们在这里的低效。因为我们在这里做的是总是从计数矩阵中提取一行 N。

![](img/9b0f3ae8b78024dba93e80534d70c09b_132.png)

然后我们总是在做相同的事情。我们在进行浮点转换并进行除法。我们在这个循环的每一个迭代中都在做这件事。我们不断地对这些行进行重新归一化，这极其低效且浪费。因此，我想要做的是准备一个包含概率的矩阵 P。

![](img/9b0f3ae8b78024dba93e80534d70c09b_134.png)

换句话说，它将与这里的计数矩阵 N 相同。但每一行都会有一行概率，其总和归一化为 1，表示给定前一个字符的下一个字符的概率分布。由我们所处的行来定义。

![](img/9b0f3ae8b78024dba93e80534d70c09b_136.png)

基本上我们想要做的是在这里直接进行操作。然后我们想要直接使用这一行。因此在这里我们希望将 P 设为 P 的 I X，而不是其他方式。好的。我想这样做的另一个原因不仅仅是为了熟练，还希望我们能够练习这些多维张量。

我们要练习它们的操作，特别是有一个叫做广播的概念，我们马上会涉及到。实际上，我们需要非常擅长这些张量操作，因为我们要构建到变换器。

我们将进行一些相当复杂的数组操作以提高效率。我们需要真正理解这一点并且要非常擅长。因此，直观上我们想要做的首先是获取 N 的浮点副本。我在这里基本上是在模拟这一行。然后我们希望将所有行除以它们的总和，使其总和为 1。

所以我们想要做一些类似 P 除以 P 的总和。但现在我们必须小心，因为 P 的总和实际上会产生一个总和。

![](img/9b0f3ae8b78024dba93e80534d70c09b_138.png)

抱歉，P 等于并且是这个浮点副本。P 的总和会将整个矩阵 N 的所有计数相加，并给出一个只包含所有内容的总和的单一数字。

![](img/9b0f3ae8b78024dba93e80534d70c09b_140.png)

所以这不是我们想要的除法方式。我们想要同时并且逐行地将所有行除以各自的总和。

![](img/9b0f3ae8b78024dba93e80534d70c09b_142.png)

所以我们现在需要做的是查看 Torx 的文档，了解 dot sum。

![](img/9b0f3ae8b78024dba93e80534d70c09b_144.png)

我们可以向下滚动到与我们相关的定义。在这里，我们不仅提供一个我们想要求和的输入数组，还提供我们希望沿着哪个维度进行求和。特别是我们想对行进行求和。还有一个我希望你注意的参数是 keep them as false。

如果`keepdims`为真，那么输出张量的大小与输入相同，当然，沿着求和的维度将变为 1。但如果你将`keepdims`传入为假，那么这个维度就被挤压了。因此，`torch.sum`不仅进行了求和，还将维度压缩到大小为 1。

但除此之外，它还会进行一个叫做挤压的操作，挤出那个维度。

![](img/9b0f3ae8b78024dba93e80534d70c09b_146.png)

基本上我们想要的是进行`p`的求和操作，而不是简单的相加。特别地，注意`p`的形状是 27 乘 27。因此，当我们在轴零上求和时，我们将取零维度并对其进行求和。因此，当保持`keepdims`为真时，这个操作不仅会给我们沿列的计数。

但注意，这里的形状基本上是 1 乘 27。我们得到一个行向量。之所以得到行向量，是因为我们传入了零维度。因此，这个零维度变为 1，我们进行了求和，得到了一个行向量。基本上我们以这种方式竖直求和，得到了一个单一的 1 乘 27 的计数向量。

当你去掉`keepdims`时，我们只得到 27。因此，它压缩了那个维度，我们只得到大小为 27 的一维向量。现在我们实际上并不想要 1 乘 27 的行向量，因为那会给我们列上的计数或求和。我们实际上想要沿维度 1 进行反向求和。

你会看到它的形状是 27 乘 1。因此，这是一个列向量。它是一个 27 乘 1 的计数向量。这是因为这里发生的情况是，我们在水平方向上进行，而这个 27 乘 27 的矩阵变成了 27 乘 1 的数组。

顺便提一下，你会注意到这些计数的实际数字是相同的。这是因为这个特殊的计数数组来自于双语法。实际上，由于这个数组的构造方式，沿列或沿行的计数是相同的。但在这种情况下，我们想要的是沿着行水平求和。

我们想要的是保持`keepdims`为真的情况下，得到一个 27 乘 1 的列向量。现在我们想要做的是对其进行除法。我们在这里需要小心。这里的`p`形状是 27 乘 27，是否可以将一个 27 乘 27 的数组除以一个 27 乘 1 的数组。

这是一个可以执行的操作吗？是否能够执行这个操作取决于所谓的广播规则。

![](img/9b0f3ae8b78024dba93e80534d70c09b_148.png)

所以如果你只搜索`torch`中的广播语义。

![](img/9b0f3ae8b78024dba93e80534d70c09b_150.png)

你会注意到有一个特别的定义，关于所谓的广播，即这两个数组是否可以在像除法这样的二元操作中结合。第一个条件是每个张量至少有一个维度，这对我们来说是成立的。

然后在迭代维度大小时，从最后一个维度开始。维度大小必须相等，或者其中一个是 1，或者其中一个不存在。

![](img/9b0f3ae8b78024dba93e80534d70c09b_152.png)

好的，让我们这样做，我们需要对齐这两个数组及其形状，这非常简单，因为这两个形状都有两个元素，因此它们是对齐的。然后我们从右往左进行迭代。每个维度必须相等，或者其中一个是 1，或者其中一个不存在。因此在这种情况下，它们不相等，但其中一个是 1，所以这没有问题。

然后这个维度它们都是相等的，所以这没问题。因此所有维度都正常，因此这个操作是可广播的。这意味着这个操作是允许的，那么这些数组在你将 27 除以 27 再除以 27 再除以 1 时会发生什么呢？它会将这个维度 1 拉伸，并将其复制以匹配这里的 27。

在我们的案例中，它接受这个 27 行 1 列的列向量，并将其复制 27 次，使得这两者在内部都变为 27 行 27 列。你可以这样理解。因此，它复制这些计数，然后进行逐元素除法，这正是我们想要的，因为我们希望在这个矩阵的每一列上都对这些计数进行除法。

因此，我们实际上期望会对每一行进行归一化。我们可以通过拿第一行来检查这一点，比如说，取它的总和。我们期望这应该是 1，因为它没有被归一化。然后我们现在期望这个，因为如果我们确实正确归一化了所有行，我们期望在这里得到完全相同的结果。

所以让我们来运行这个。这是完全相同的结果。所以这是正确的。现在我想吓唬你一点。实际上，我强烈鼓励你仔细阅读广播语义。

![](img/9b0f3ae8b78024dba93e80534d70c09b_154.png)

我鼓励你尊重这一点。

![](img/9b0f3ae8b78024dba93e80534d70c09b_156.png)

这并不是可以随便处理的事情，而是需要真正尊重、理解，并查找一些广播的教程，进行练习，并小心处理，因为你很快就会遇到麻烦。

![](img/9b0f3ae8b78024dba93e80534d70c09b_158.png)

让我给你展示我所说的。你看这里我们有 Pete 在维度一的 Pete，这是真的。它的形状是二十七乘一。让我去掉这一行，这样我们就有 N，然后我们可以看到计数。我们可以看到这是所有行的计数，它是一个二十七乘一的列向量，对吧？现在假设我试着这样做，但我把保留它的 true 删掉了。

如果保持它们不为真而为假会有什么影响呢？然后根据文档记住，它会去掉这个维度，将其挤压出去。所以基本上我们得到的都是相同的计数，结果相同，只是它的形状不是二十七乘一，而只是二十七，那个一消失了。但所有的计数都是相同的。所以你会认为这个除法应该可以工作。

首先，我们能否写出这个，它是否期望能够运行，是不是可广播的？让我们确定这个结果是否可广播。Pete 在维度一的形状是二十七。这是二十七乘二十七。所以二十七乘二十七广播到二十七。因此广播的规则之一是将所有维度右对齐，完成。

现在从右到左对所有维度进行迭代。所有维度的数学关系要么相等，要么其中一个必须是 1，或者一个不存在。所以在这里它们相等，这里维度不存在。因此，内部广播会在这里创建一个 1，然后我们看到其中一个是 1，这将被复制，这将运行并进行广播。

好的，你会期望这个可以工作，因为我们正在广播，而这个我们可以进行除法。现在如果我运行这个，你会期望它能够工作，但实际上并不能。你会得到垃圾数据，结果是错误的，因为这实际上是一个 bug。这个保留它等于 true 使得它能够工作。这是一个 bug。

在这两种情况下，我们都在正确地计算，我们在行间求和，但保留它们使得它能够工作。所以在这种情况下，我希望你能暂停这个视频，思考一下为什么这个是有 bug 的，为什么在这里需要保留它。

好的，这样做的原因是我在试图暗示它，这样可以帮助你理解它是如何工作的。这个二十七的向量在广播内部变成了一个一乘二十七，而一乘二十七是一个行向量。

现在我们是在将二十七除以二十七，再除以一再除以二十七，而 torch 会复制这个维度。因此，它会将这个行向量垂直复制，重复二十七次。所以这个二十七乘二十七的相应元素进行逐元素除法。因此这里发生的情况是我们实际上是在对列进行归一化，而不是对行进行归一化。

所以你可以检查这里发生了什么，P 在零处，即 P 的第一行。它的总和不是 1，而是 7。作为一个例子，第一列的总和是 1。那么总结一下，问题出在哪里？问题来自于这里的维度默默添加，因为在广播规则中，你是从右到左对齐，如果维度不存在，就会创建它。所以问题发生在这里，我们仍然正确地进行了计数，我们在行上进行了计数，并且得到了作为列向量的正确计数。

但由于关键因素，这个维度被丢弃了，现在我们只剩下一个二十七的向量。由于广播的工作方式，这个二十七的向量突然变成了一个行向量，然后这个行向量被垂直复制，而在每一个点我们都是在用相反方向的计数进行除法。

所以这个东西就是不工作，需要保持简单。在这种情况下，P 在零处是归一化的，而第一列你可能会期望它不被归一化。这就是它能够工作的原因。

![](img/9b0f3ae8b78024dba93e80534d70c09b_160.png)

这很微妙，希望这能让你意识到你应该对广播保持尊重，要小心，检查你的工作，理解它是如何在底层工作的，并确保它的广播方向是你想要的。否则，你将引入非常微妙且难以找到的错误，要小心。

![](img/9b0f3ae8b78024dba93e80534d70c09b_162.png)

关于效率还有一点要注意，我们不想在这里做这个，因为这会创建一个完全新的张量并存储到 P 中，如果可能的话，我们更倾向于使用就地操作。因此，这将是一个就地操作，有潜力更快，它不会在底层创建新的内存，然后我们把这个擦掉，我们不需要它，同时也只做更少的操作，以免浪费空间。

![](img/9b0f3ae8b78024dba93e80534d70c09b_164.png)

好的，所以我们现在实际上处于一个相当不错的状态，我们训练了一个双元语言模型，主要是通过计算任意配对出现的频率，然后进行归一化，以获得良好的概率分布。因此，这个数组 P 的元素实际上是我们双元语言模型的参数，总结了这些双元的统计信息。

所以我们训练模型，然后我们知道如何从模型中抽样，我们每次迭代地抽取下一个字符并将其输入，从而得到下一个字符。现在我想做的是评估这个模型的质量。我们希望将这个模型的质量总结成一个单一的数字，以便评估它在训练集上的预测能力。作为例子，在训练集中我们现在可以评估训练损失，这个训练损失告诉我们这个模型的质量，用一个单一的数字表示，就像我们在微梯度中看到的那样。

让我们试着考虑模型的质量以及如何评估它。基本上我们要做的是复制之前用于计数的代码。好的，让我先打印这些二元组，我们将使用 F 字符串打印字符一后跟字符二，这就是二元组。然后我不想为所有单词做，只需做前三个单词。

所以这里我们有 Emma、Olivia 和 Eva 的二元组。现在我们希望基本上查看模型为每个二元组分配的概率。换句话说，我们可以查看在矩阵 B 中的概率，Ix1，Ix2。

然后我们可以在这里打印出概率。因为这些概率实在太大了，让我用百分比或者调用点 4f 来缩短一下。那么我们现在看到的是什么呢？我们在查看模型在数据集中为这些二元组分配的概率。

因此我们可以看到一些概率为 4%、3%等。顺便提一下，我们有 27 个可能的字符或符号，如果一切都是同样可能的，那么你会期望这些概率大约为 4%。所以任何超过 4%的概率意味着我们从这些二元组统计中学到了有用的东西。你会看到大致上有一些是 4%，但有一些高达 40%、35%等。

因此，你会看到模型实际上给训练集中的任何内容分配了相当高的概率。这是好事。基本上，如果你有一个非常好的模型，你会期望这些概率接近 1，因为这意味着你的模型正确预测了接下来会发生什么。

特别是在训练集上，你训练模型的地方。因此，现在我们希望思考如何将这些概率总结成一个数字来衡量模型的质量。

![](img/9b0f3ae8b78024dba93e80534d70c09b_166.png)

现在当你查看最大似然估计和统计建模等文献时。

![](img/9b0f3ae8b78024dba93e80534d70c09b_168.png)

你会看到，这里通常使用的是一种叫做似然的东西。似然是所有这些概率的乘积。因此，所有这些概率的乘积就是似然，它实际上告诉我们模型所训练的整个数据集的概率。这是质量的衡量标准。因此，这些的乘积应该尽可能高。

当你训练模型并且有一个好的模型时，这些概率的乘积应该非常高。由于这些概率的乘积是一个难以处理的东西，你可以看到它们都在 0 到 1 之间。因此，这些概率的乘积将是一个非常小的数字。

所以为了方便，人们通常处理的不是似然，而是称为对数似然的东西。因此，这些的乘积是似然。要得到对数似然，我们只需对概率取对数。

![](img/9b0f3ae8b78024dba93e80534d70c09b_170.png)

所以这里概率的对数，从 0 到 1 的对数，你会看到这是一个概率的单调变换，当你传入 1 时，你得到 0。所以概率 1 会给你对数概率 0。然后，随着概率越来越低，对数将变得越来越负，直到在 0 时达到负无穷大。

![](img/9b0f3ae8b78024dba93e80534d70c09b_172.png)

所以这里我们有一个对数问题，这实际上只是概率的 torch dot log。让我们打印出来，看看那是什么样子。对数问题也是 0.4f。好的，正如你所看到的，当我们输入非常接近的数字时，一些较大的数字，我们会越来越接近 0。如果我们输入非常糟糕的概率。

我们得到越来越多的负数。这不好。所以，我们之所以处理这个，主要是出于方便，对吧？

因为我们在数学上知道，如果你有一些概率的乘积 a 乘以 b 乘以 c，或者说似然是所有这些概率的乘积，那么这些的对数就是 a 的对数加上 b 的对数加上 c 的对数。如果你还记得高中或本科时学过的对数。

所以基本上，概率的乘积的似然，对数似然就是各个概率的对数之和。所以对数似然从 0 开始。然后对数似然在这里，我们可以简单地累积。最后，我们可以打印出来。打印对数似然。拥有字符串。

也许你对此比较熟悉。所以对数似然是负 38。好的，现在我们实际上想知道，对数似然能达到多高？它可以达到零。所以当所有概率都是 1 时，对数似然将为零。当所有概率更低时，这个值将变得越来越负。现在，我们其实不喜欢这样，因为我们希望有一个损失函数。

损失函数的语义是低是好的，因为我们试图最小化损失。所以我们实际上需要对其进行反转。这就是我们得到所谓负对数似然的原因。负对数似然只是对数似然的负值。顺便提一下，这些是 F 字符串，如果你想查一下。负对数似然等于。

所以，负对数似然现在只是它的负值。因此，负对数似然是一个非常好的损失函数，因为它能达到的最低值是零。而且它越高，你所做的预测就越糟糕。然后，有一个人们有时会做的修改，为了方便。

他们实际上喜欢进行归一化，他们更喜欢将其变为平均值而不是总和。因此在这里，我们也可以保持一些计数。所以 n 加等于一，从零开始。然后在这里，我们可以得到类似于归一化对数似然的东西。如果我们只是通过计数来归一化，那么我们将得到平均对数似然。

所以这通常就是我们的损失函数。这就是我们要使用的。因此，模型为训练集分配的损失函数是 2.4。这是该模型的质量。它越低，我们的效果就越好；而它越高，我们的效果就越差。我们训练的任务是找到能够最小化负对数似然损失的参数。

这将像是一个高质量的模型。好吧，简单总结一下，我实际上在这里写下来了。

![](img/9b0f3ae8b78024dba93e80534d70c09b_174.png)

所以我们的目标是最大化似然，即模型分配的所有概率的乘积。我们希望在模型参数方面最大化这个似然。在我们的例子中，这里的模型参数在表中定义。

![](img/9b0f3ae8b78024dba93e80534d70c09b_176.png)

这些数字，即概率，是到目前为止我们 Bagram 语言模型中的模型参数。

![](img/9b0f3ae8b78024dba93e80534d70c09b_178.png)

但你必须记住，这里我们将所有内容存储在表格格式中，即概率。但是，接下来会简要预览的是，这些数字不会被显式保留，而是由神经网络计算得出。所以这是即将发生的。我们想要改变和调整这些神经网络的参数。

我们希望改变这些参数以最大化似然，即概率的乘积。

![](img/9b0f3ae8b78024dba93e80534d70c09b_180.png)

现在，最大化似然等同于最大化对数似然，因为对数是单调函数。

![](img/9b0f3ae8b78024dba93e80534d70c09b_182.png)

这是对数的图形。基本上，它所做的就是缩放你的，你可以将其视为对损失函数的缩放。

![](img/9b0f3ae8b78024dba93e80534d70c09b_184.png)

因此，这里的优化问题实际上是等价的，因为这只是一个缩放。你可以这么看。所以这两个优化问题是相同的。最大化对数似然等同于最小化负对数似然。

![](img/9b0f3ae8b78024dba93e80534d70c09b_186.png)

在实际操作中，人们实际上最小化平均负对数似然，以得到像 2.4 这样的数字。然后这总结了你模型的质量。我们希望最小化它，并尽可能小。它的最低值可以是零。数值越低，模型越好，因为它给你的数据分配了高概率。

![](img/9b0f3ae8b78024dba93e80534d70c09b_188.png)

现在让我们估计整个训练集上的概率，以确保我们得到大约 2.4 的结果。

![](img/9b0f3ae8b78024dba93e80534d70c09b_190.png)

让我们在整个训练集上运行这个，哦，先把打印语句去掉。好的，2.45 或整个训练集。现在我想给你展示的是，你实际上可以评估你想要的任何单词的概率。比如说，如果我们只测试一个单词，Andre，并恢复打印语句。那么你会看到 Andre 实际上是一个不太可能的单词。平均而言。

我们取三个对数概率来表示它。大致上是因为 EJ 显然是一个非常不常见的例子。现在思考一下，当我把 Andre 和 Q 拼接在一起并测试它的概率，Andre Q。我们实际上得到的是无限大。这是因为根据我们的模型，JQ 的概率是 0%。因此，对数似然，零的对数将是负无穷大。我们得到无限损失。

所以这有点不理想，对吧？因为我们输入了一个可能是合理名字的字符串。但基本上，这表示该模型对预测这个名字的可能性为 0%。而在这个例子中，我们的损失是无限大的。真正的原因是 J 后面跟着 Q 的次数为 0。Q 在哪里？JQ 为 0。因此，JQ 的可能性是 0%。这实际上是有点恶心，人们对此并不太喜欢。

要解决这个问题，有一个非常简单的解决方案，大家喜欢用它来稍微平滑一下你的模型。

![](img/9b0f3ae8b78024dba93e80534d70c09b_192.png)

这被称为模型平滑。大致上发生的事情是我们会增加一些虚假的计数。

![](img/9b0f3ae8b78024dba93e80534d70c09b_194.png)

![](img/9b0f3ae8b78024dba93e80534d70c09b_195.png)

所以想象一下，对所有内容加上一个计数。所以我们像这样加上一个计数。然后我们重新计算概率。

![](img/9b0f3ae8b78024dba93e80534d70c09b_197.png)

这就是模型平滑。你可以根据需要添加任意多的内容。你可以添加五个，它会给你一个更平滑的模型。而你添加得越多，模型就会越均匀。当然，添加得越少，模型就会越尖锐。所以，一个适度的计数是相当不错的选择。

这将确保我们的概率矩阵 P 中没有零。因此，这当然会稍微改变生成结果。在这种情况下没有，但原则上它可能会。

![](img/9b0f3ae8b78024dba93e80534d70c09b_199.png)

但这将意味着没有什么是无穷小可能的。因此，现在我们的模型将预测其他概率。我们看到 JQ 现在的概率非常小。因此，模型仍然发现这个单词或字节是非常令人惊讶的。

但我们不会得到负无穷大。

![](img/9b0f3ae8b78024dba93e80534d70c09b_201.png)

所以这是一种人们有时喜欢应用的不错修复，称为模型平滑。

![](img/9b0f3ae8b78024dba93e80534d70c09b_203.png)

好的，我们现在已经训练了一个相当不错的字节级字符语言模型。

![](img/9b0f3ae8b78024dba93e80534d70c09b_205.png)

我们看到，我们都通过查看所有的字节计数并归一化行来训练模型，以获得概率分布。我们还看到，我们可以使用这个模型的参数来对新单词进行采样。

![](img/9b0f3ae8b78024dba93e80534d70c09b_207.png)

因此，我们根据这些分布来采样新的名称。

![](img/9b0f3ae8b78024dba93e80534d70c09b_209.png)

我们还看到，我们可以评估这个模型的质量。这个模型的质量用一个数字来总结，即负对数似然。这个数字越低，模型越好，因为它对训练集中所有字节的实际下一个字符赋予了高概率。所以这一切都很好，但我们通过做一些合理的事情显式地得到了这个模型。

![](img/9b0f3ae8b78024dba93e80534d70c09b_211.png)

我们只是进行了计数，然后对这些计数进行了归一化。

![](img/9b0f3ae8b78024dba93e80534d70c09b_213.png)

现在我想做的是采取另一种方法。我们将达到一个非常相似的位置，但方法看起来会非常不同。因为我想将字节级字符语言建模的问题转化为神经网络框架。在神经网络框架中，我们将略有不同地处理这些问题。

但最后又会陷入非常相似的境地。我稍后会讲到这一点。现在我们的神经网络仍然是一个字节级字符语言模型。因此，它接收一个字符作为输入。接着有一个带有一些权重或参数 W 的神经网络。它将输出序列中下一个字符的概率分布。

它将对输入到模型中的字符后面可能出现的字符进行猜测。此外，我们还将能够评估神经网络参数的任何设置，因为我们有一个损失函数。负对数似然。所以我们将查看其概率分布，并使用这些标签。

这些标签基本上只是该双字组中下一个字符的身份，即第二个字符。因此，知道双字组中实际接下来的第二个字符，允许我们查看模型对该字符分配的高概率。

然后我们当然希望概率非常高。这也是说损失低的另一种方式。所以我们将使用基于梯度的优化来调整这个网络的参数。因为我们有损失函数，并将其最小化。

所以我们要调整权重，使得神经网络能够正确预测下一个字符的概率。我们开始吧。首先我想做的是编译这个神经网络的训练集。所以创建所有的双字组的训练集。

![](img/9b0f3ae8b78024dba93e80534d70c09b_215.png)

好的。这里我将复制粘贴这段代码，因为这段代码遍历所有的双字组。

![](img/9b0f3ae8b78024dba93e80534d70c09b_217.png)

所以这里我们从单词开始，我们遍历所有的双字组，之前你还记得我们进行了计数。但现在我们不再计数。我们只是创建一个训练集。这个训练集将由两个列表组成。我们有输入和目标，标签。这些双字组将表示 x，y。它们就是字符，对吧？

所以我们得到了双字组的第一个字符，然后我们试图预测下一个字符。这两个都将是整数。所以这里我们将 x 的追加就是 x1。为什么追加 ix2？然后这里我们实际上不想要整数列表。我们将从这些中创建张量。所以 x 是 torch.tensor 的 x，y 是 torch.tensor 的 y。

然后我们实际上不想现在就取所有的单词，因为我希望一切都能管理。所以我们先做第一个单词 Emma。然后很明显这些 x 和 y 会是什么。让我打印一下字符一字符二，让你看到这里发生了什么。所以这些字符的双字组是.dot E E M M M A.dot。

所以这个单词，正如我提到的，有一个，两个，三个，四个，五个例子供我们的神经网络使用。Emma 中有五个独立的例子。这里是这些例子。当神经网络的输入为整数零时，期望的标签是整数五，对应于 E。

当神经网络的输入为五时，我们希望其权重安排得当，以便 13 获得非常高的概率。当 13 被输入时，我们希望 13 具有高概率。当输入 13 时，我们也希望 1 具有高概率。当输入 1 时，我们希望 0 具有非常高的概率。所以在这个数据集中，神经网络有五个独立的输入示例。

我想添加一个警告，提醒对某些框架的 API 要小心。你看到我默默地使用小写 t 的 torch 张量，输出看起来正确。

![](img/9b0f3ae8b78024dba93e80534d70c09b_219.png)

但是你应该知道其实有两种构建张量的方法。

![](img/9b0f3ae8b78024dba93e80534d70c09b_221.png)

有一个 torch 的小写张量，还有一个 torch 的大写张量类，你也可以构建。

![](img/9b0f3ae8b78024dba93e80534d70c09b_223.png)

所以你实际上可以调用两者。你也可以使用 torch 的大写张量，你同样会得到 Xs 和 Ys。因此，这一点并不混淆。

![](img/9b0f3ae8b78024dba93e80534d70c09b_225.png)

有关这两者之间区别的讨论很多。不幸的是，文档对此并不清晰。

![](img/9b0f3ae8b78024dba93e80534d70c09b_227.png)

当你查看小写张量的文档时，构建张量时没有签名历史是通过复制数据。

![](img/9b0f3ae8b78024dba93e80534d70c09b_229.png)

这听起来完全没有意义。

![](img/9b0f3ae8b78024dba93e80534d70c09b_231.png)

所以据我所知，实际区别最终在这个随机的讨论中有解释，你可以谷歌一下。真的，我认为这主要是，嗯，这是什么？

![](img/9b0f3ae8b78024dba93e80534d70c09b_233.png)

torch 张量首先自动识别数据类型。好吧，torch 张量只返回 float 张量。我建议你坚持使用 torch 的小写张量。

![](img/9b0f3ae8b78024dba93e80534d70c09b_235.png)

所以确实，当我用大写 T 构建时，Xs 的数据类型是 float 32。但是 torch 的小写张量，你可以看到现在 Xs 的数据类型是整数。因此建议你使用小写 T，如果你想了解更多，可以在一些讨论中阅读。但基本上，我提到这些是因为我想提醒你，并希望你习惯于阅读大量文档，以及像这样的问答和讨论。

而且，你知道，有些东西确实不容易，而且文档并不充分，你必须谨慎处理。我们在这里需要整数，因为这才有意义。所以我们使用小写张量。

![](img/9b0f3ae8b78024dba93e80534d70c09b_237.png)

好的，现在我们想要考虑如何将这些示例输入到神经网络中。实际上，这并不像直接插入那么简单，因为这些示例现在是整数。所以像零、五或十三这样的数字。这给出了字符的索引，而你不能仅仅将一个整数索引插入到神经网络中。这些神经网络是由这些神经元组成的，这些神经元有权重。

正如你在微观梯度中看到的，这些权重对输入 W X 加 B 是乘法作用的。有 10 个 H 等等。因此，让输入神经元接受整数值并与权重相乘并没有多大意义。因此，一种常见的编码整数的方法是所谓的独热编码。在独热编码中，我们取一个整数，比如十三，然后创建一个向量，除了第十三维外，其他都是零。

然后该向量可以输入到神经网络中。

![](img/9b0f3ae8b78024dba93e80534d70c09b_239.png)

现在，方便的是，PyTorch 实际上在 Torch 和 functional 中有一个名为 one-hot 的函数。它接受由整数长构成的张量。它还接受类别数量，即你希望张量向量的大小。

![](img/9b0f3ae8b78024dba93e80534d70c09b_241.png)

所以在这里，让我们导入 Torch。在这个功能 SF 中，这是一个常见的导入方式。然后让我们做 F.dot.one-hot，输入我们想要编码的整数。因此，我们实际上可以输入整个 Xs 数组。我们可以告诉它 num class 是 27。这样它就不必去猜测了。它可能会猜测只有 13，从而给出错误的结果。

所以这就是独热编码。我们称之为 X 输入，表示 X 编码。然后我们看到 X 编码的形状是 5 乘 27。我们还可以用 PLT.IMS 可视化 X 输入，使其更清晰，因为这有点混乱。

![](img/9b0f3ae8b78024dba93e80534d70c09b_243.png)

所以我们看到我们已经将所有五个示例编码为向量。我们有五个示例，所以我们有五行，每一行现在都是一个输入到神经网络的示例。我们看到相应的比特被打开为 1，其他都是零。因此，比如这里，第零位被打开，第五位被打开。

第十三位对于这两个示例都是开启的。

![](img/9b0f3ae8b78024dba93e80534d70c09b_245.png)

这里的第一个比特被打开。这就是我们如何将整数编码为向量。然后这些向量可以输入到神经网络中。

![](img/9b0f3ae8b78024dba93e80534d70c09b_247.png)

另外一个需要注意的问题是，让我们看一下编码的数据类型。我们总是要小心数据类型。你期望 X 编码的数据类型是什么？

当我们将数字输入到神经网络时，我们不希望它们是整数。我们希望它们是可以取不同值的浮点数。但这里的 dtype 实际上是 64 位整数。

![](img/9b0f3ae8b78024dba93e80534d70c09b_249.png)

我怀疑这是因为 one-hot 接收了一个 64 位整数，并返回了相同的数据类型。

![](img/9b0f3ae8b78024dba93e80534d70c09b_251.png)

当你查看 one-hot 的签名时，它甚至不需要 dtype。所需的输出张量的数据类型。

![](img/9b0f3ae8b78024dba93e80534d70c09b_253.png)

因此，在 Torter 的许多函数中，我们能够做类似 dtype 等于 Torx。flow32 的事情，这正是我们想要的，但 one-hot 不支持这个。所以我们将把它转换为浮点数，像这样。

![](img/9b0f3ae8b78024dba93e80534d70c09b_255.png)

因此，这样一来，所有的内容都是一样的。

![](img/9b0f3ae8b78024dba93e80534d70c09b_257.png)

一切看起来都一样，但 dtype 是 float32。浮点数可以输入到神经网络中。因此现在让我们构建第一个神经元。这个神经元将查看这些输入向量。如你所记得的，来自 Micrograd，这些神经元基本上执行一个非常简单的函数。WX 加 B，其中 WX 是点积。对吧？所以我们可以在这里实现同样的功能。

![](img/9b0f3ae8b78024dba93e80534d70c09b_259.png)

首先让我们定义这个神经元的权重，基本上。这个神经元在初始化时的初始权重在哪里？

让我们用 torch dot random 初始化它们。

![](img/9b0f3ae8b78024dba93e80534d70c09b_261.png)

Torx dot random 是用随机数填充张量。

![](img/9b0f3ae8b78024dba93e80534d70c09b_263.png)

从正态分布中抽取。正态分布的概率密度函数是这样的。因此，从这个分布中抽取的大多数数字将围绕零，但其中一些将高达接近三等等。绝大多数数字的绝对值将高于三的情况非常少。

![](img/9b0f3ae8b78024dba93e80534d70c09b_265.png)

所以我们需要在这里将 size 作为输入。我将使用 size 为 27 乘 1。因此是 27 乘 1，然后让我们可视化 W。

![](img/9b0f3ae8b78024dba93e80534d70c09b_267.png)

所以 W 是一个包含 27 个数字的列向量。

![](img/9b0f3ae8b78024dba93e80534d70c09b_269.png)

然后这些权重与输入相乘。

![](img/9b0f3ae8b78024dba93e80534d70c09b_271.png)

现在为了执行这个乘法，我们可以将 X 编码，并且可以将其与 W 相乘。这是 PyTorch 中的矩阵乘法操作符。

![](img/9b0f3ae8b78024dba93e80534d70c09b_273.png)

此操作的输出是 5 乘 1。

![](img/9b0f3ae8b78024dba93e80534d70c09b_275.png)

五乘以五的原因如下。我们取 X 编码，五乘以 27，然后将其乘以 27 乘以 1。在矩阵乘法中，你会看到输出变成五乘以 1，因为这 27 将进行乘法和加法。因此，基本上我们在这个操作中看到的是五个。

我们并行评估了所有这些。因此，我们可以只向单个神经元输入一个输入。我们同时将所有五个输入输入到同一个神经元中。并行的，PyTorch 已经评估了 WX 加 B，但在这里它只是 WX。没有偏差。

它对所有的 W 乘以 X 进行了独立计算。现在，我想要 27 个神经元，而不是单个神经元。我稍后会告诉你为什么我有 27 个神经元。所以不是只有一个在这里，表示一个单一神经元的存在，而是我们可以使用 27 个。当 W 是 27 乘以 27 时，这将并行评估所有 27 个神经元在所有五个输入上的激活。

五个输入，给我们一个更好，更大，更大的结果。

![](img/9b0f3ae8b78024dba93e80534d70c09b_277.png)

现在我们所做的是五乘以 27，再乘以 27，输出现在是五乘以 27。所以我们可以看到这个形状是五乘以 27。

![](img/9b0f3ae8b78024dba93e80534d70c09b_279.png)

每个元素告诉我们什么？它告诉我们，对于我们创建的 27 个神经元，每个神经元在这五个示例中的发射率是什么？例如，3，13 这个元素给我们的是第 13 个神经元在第三个输入下的发射率。这是通过第三个输入和第 13 个神经元之间的点积实现的。

![](img/9b0f3ae8b78024dba93e80534d70c09b_281.png)

这个 W 矩阵的列。

![](img/9b0f3ae8b78024dba93e80534d70c09b_283.png)

所以使用矩阵乘法，我们可以非常高效地评估批量中大量输入示例与许多神经元之间的点积，所有这些神经元的权重都在这些 W 的列中。在矩阵乘法中，我们只是并行地进行这些点积。为了向你展示确实如此。

我们可以取 X，并取第三行。

![](img/9b0f3ae8b78024dba93e80534d70c09b_285.png)

![](img/9b0f3ae8b78024dba93e80534d70c09b_286.png)

我们可以取 W 并获取它的第 13 列。

![](img/9b0f3ae8b78024dba93e80534d70c09b_288.png)

然后我们可以做 X，并用第 13 个 W 进行元素逐个相乘并求和。这是 WX 加 B。实际上，没有加 B。只是 WX 点积。这就是这个数字。所以你可以看到，这是通过矩阵乘法操作高效完成的。

![](img/9b0f3ae8b78024dba93e80534d70c09b_290.png)

输入示例以及这一层所有输出神经元的情况。好吧。所以我们将 27 维输入送入具有 27 个神经元的神经网络的第一层。

![](img/9b0f3ae8b78024dba93e80534d70c09b_292.png)

神经元。因此，我们有 27 个输入，现在我们有 27 个神经元。这些神经元执行 W 乘以 X。它们没有偏置，也没有像 10H 那样的非线性激活函数。我们将它们设为线性层。除此之外，我们不会有其他层。这将是它。这将是最简单的。

最小、最简单的神经网络，仅仅是一个线性层。

![](img/9b0f3ae8b78024dba93e80534d70c09b_294.png)

现在我想解释我希望这 27 个输出是什么。

![](img/9b0f3ae8b78024dba93e80534d70c09b_296.png)

从直观上看，我们要为每一个输入示例生成某种概率。

![](img/9b0f3ae8b78024dba93e80534d70c09b_298.png)

用于序列中下一个字符的分布。总共有 27 个这样的字符。

![](img/9b0f3ae8b78024dba93e80534d70c09b_300.png)

但我们必须准确地定义如何解释这 27 个数字的语义。

![](img/9b0f3ae8b78024dba93e80534d70c09b_302.png)

我们将使这两个神经元占据整体。

![](img/9b0f3ae8b78024dba93e80534d70c09b_304.png)

从直观上看，你会发现这些数字有的为负数，有的为正数，等等。

![](img/9b0f3ae8b78024dba93e80534d70c09b_306.png)

这是因为这些是来自一个用这些初始化的神经网络层。

![](img/9b0f3ae8b78024dba93e80534d70c09b_308.png)

正态分布参数。但我们想要的是像这里的结果一样。每一行告诉我们计数。

![](img/9b0f3ae8b78024dba93e80534d70c09b_310.png)

然后我们对计数进行归一化以获得概率。我们希望从神经网络中得到类似的东西。但我们现在所拥有的只是一些负数和正数。现在我们希望这些数字能以某种方式表示下一个字符的概率。

但你会看到概率有一个特殊结构。它们是正数，并且它们的总和为一。因此，这不仅仅来自神经网络。而且它们不能是计数，因为这些计数是正数，而计数是整数。所以，计数实际上并不是神经网络输出的一个好的选择。

所以神经网络将输出的内容，以及我们如何解释这 27 个数字，是这些数字给我们提供了对数计数。因此，它们并不是直接给我们计数，像这个表格那样，而是给我们对数计数。

![](img/9b0f3ae8b78024dba93e80534d70c09b_312.png)

为了获得计数，我们将对对数计数进行指数运算。指数运算的形式如下。它可以处理负数或正数，涵盖整个实数范围。如果你输入负数，将得到 e 的 x 次方，这总是小于一。因此，你得到的数字小于一。

如果你输入大于零的数字，你会得到大于一的数字，一直增长到无穷大。

![](img/9b0f3ae8b78024dba93e80534d70c09b_314.png)

而这里增长到零。所以基本上我们将采用这些数字。

![](img/9b0f3ae8b78024dba93e80534d70c09b_316.png)

而不是它们正负混杂。

![](img/9b0f3ae8b78024dba93e80534d70c09b_318.png)

我们将把它们解释为对数计数。然后我们将对这些数字进行两次指数处理。

![](img/9b0f3ae8b78024dba93e80534d70c09b_320.png)

指数化它们现在给我们一些类似的东西。你会看到这些数字，因为经过指数处理，所有的负数变成了小于一的数字，比如 0.338。而所有的正数，最初变成了更大的正数，所以我们大于一。

![](img/9b0f3ae8b78024dba93e80534d70c09b_322.png)

例如，七是这里的一个正数。这大于零。但是这里的指数输出基本上给我们一些可以用来解释为计数的等价物。所以你会看到这些计数，121，7，51，1 等等。

![](img/9b0f3ae8b78024dba93e80534d70c09b_324.png)

神经网络现在预测的是计数。这些计数是正数。它们永远不会小于零，所以这很有意义。

![](img/9b0f3ae8b78024dba93e80534d70c09b_326.png)

它们现在可以根据 W 的设置取不同的值。

![](img/9b0f3ae8b78024dba93e80534d70c09b_328.png)

让我来分解一下。我们将把这些解释为对数计数。换句话说，对于这个，常用的被称为对数值。这些就是对数计数。

![](img/9b0f3ae8b78024dba93e80534d70c09b_330.png)

那么这些将是某种计数。对数值的指数化。这相当于 n 矩阵，类似于我们之前使用的 n 数组。记得这个是 n 吗？这是计数的数组。这里的每一行都是下一个字符的计数，差不多。所以这些就是计数。现在的概率只是计数的归一化。因此我不会找到相同的。

但基本上我不会到处滚动。

![](img/9b0f3ae8b78024dba93e80534d70c09b_332.png)

我们已经做过这个。我们希望计数沿第一个维度求和。我们希望保持它们的真实性。我们已经讨论过了。这就是我们如何归一化计数矩阵的行，以获得我们的概率。所以现在这些是概率，而这些是我们目前拥有的计数。

![](img/9b0f3ae8b78024dba93e80534d70c09b_334.png)

现在，当我展示概率时，你会看到这里的每一行当然会加起来等于 1，因为它们是归一化的。它的形状是 5 乘 27。

![](img/9b0f3ae8b78024dba93e80534d70c09b_336.png)

所以实际上我们所取得的成果是，对于我们的每一个五个示例，我们现在有一行来自神经网络的输出。由于这里的变换，我们确保这个神经网络的输出现在是概率，或者我们可以将其解释为概率。

![](img/9b0f3ae8b78024dba93e80534d70c09b_338.png)

所以我们的 WX 在这里给了我们 logits，然后我们将其解释为日志计数。我们通过指数运算得到看起来像计数的东西，然后我们对这些计数进行归一化，以获得概率分布。而这些都是可微分的操作。所以我们现在所做的是接受输入。

我们有可微分的操作，可以进行反向传播，并且我们得到了概率分布。

![](img/9b0f3ae8b78024dba93e80534d70c09b_340.png)

所以，例如，对于第 0 个输入示例，它的第 0 个示例是一个值为 0 的 1/2 向量。

![](img/9b0f3ae8b78024dba93e80534d70c09b_342.png)

基本上对应于将这个示例输入到这里。

![](img/9b0f3ae8b78024dba93e80534d70c09b_344.png)

所以我们将一个点输入到神经网络中。我们将点输入到神经网络的方法是首先获取其索引，然后进行独热编码。

![](img/9b0f3ae8b78024dba93e80534d70c09b_346.png)

然后它进入神经网络，输出了这个概率分布。它的形状是 27。有 27 个数字。我们将其解释为神经网络对每个字符的可能性分配。

![](img/9b0f3ae8b78024dba93e80534d70c09b_348.png)

接下来是 27 个字符。当我们调整权重 W 时，当然会为你输入的任何字符输出不同的概率。

![](img/9b0f3ae8b78024dba93e80534d70c09b_350.png)

所以现在的问题是，我们能否优化并找到一个好的 W，使得输出的概率相当不错？

我们衡量良好的方式是通过损失函数。好的。所以我将所有内容整理成一个单一的摘要，希望它能更清晰一点。它从这里开始。我们有一个输入数据集。我们有一些输入到神经网络中。我们还有一些关于序列中下一个正确字符的标签。这些都是整数。

![](img/9b0f3ae8b78024dba93e80534d70c09b_352.png)

在这里我现在使用 Torx 生成器，这样你就能看到我所看到的相同数字。我正在生成 27 个神经元的权重。每个神经元在这里接收 27 个输入。

![](img/9b0f3ae8b78024dba93e80534d70c09b_354.png)

然后在这里我们将所有输入示例 X 插入到神经网络中。所以这里是一个前向传递。首先我们必须将所有输入编码为独热表示。我们有 27 个类别。我们传入这些整数。X，Inc。变成一个 5 乘 27 的数组。除了几个 1，其他都是 0。然后我们在神经网络的第一层中乘以这个以得到 logits。

将 logits 进行指数运算以获得伪计数。并对这些计数进行归一化以得到概率。顺便提一下，这最后两行被称为 softmax。这里我提到的就是这个。

![](img/9b0f3ae8b78024dba93e80534d70c09b_356.png)

Softmax 是神经网络中一个非常常用的层，它接受这些 z 值，也就是 logits。对它们进行指数运算并进行归一化。它是处理神经网络层输出的一种方式。这些输出可以是正数或负数。它输出概率分布。它的输出总是加和为 1，且为正数。就像概率一样。

所以这有点像一个归一化函数，如果你想这么理解的话。你可以把它放在神经网络内部的任何其他线性层上。它基本上使神经网络输出概率。这种用法非常常见。我们在这里也使用了它。

![](img/9b0f3ae8b78024dba93e80534d70c09b_358.png)

所以这是前向传播，这就是我们让神经网络输出概率的方式。

![](img/9b0f3ae8b78024dba93e80534d70c09b_360.png)

现在你会注意到，所有这些，整个前向传播都是由可微分层组成的。这里的一切我们都可以进行反向传播。我们在 micrograd 中看到了一些反向传播。这只是乘法和加法。这里发生的所有事情只是乘法和加法。我们知道如何对它们进行反向传播。对于指数运算，我们也知道如何进行反向传播。

然后这里我们在求和，和也是很容易进行反向传播的。除法也是如此。所以这里的一切都是可微分操作。我们可以进行反向传播。

![](img/9b0f3ae8b78024dba93e80534d70c09b_362.png)

现在我们得到了这些概率，大小为 5 乘 27。

![](img/9b0f3ae8b78024dba93e80534d70c09b_364.png)

对于每一个示例，我们有一个概率向量，映射到一个。

![](img/9b0f3ae8b78024dba93e80534d70c09b_366.png)

然后在这里我写了一堆东西，试图分解这些示例。

![](img/9b0f3ae8b78024dba93e80534d70c09b_368.png)

所以我们有 5 个示例构成 Emma。Emma 中有 5 个字母组。因此，字母组的示例一是 E 是紧跟在点后面的第一个字符。

![](img/9b0f3ae8b78024dba93e80534d70c09b_370.png)

![](img/9b0f3ae8b78024dba93e80534d70c09b_371.png)

这些的索引是 0 和 5。因此我们输入 0。这是神经网络的输入。我们从神经网络得到 27 个数字的概率。然后标签是 5，因为 E 实际上是在点后面。

![](img/9b0f3ae8b78024dba93e80534d70c09b_373.png)

所以这就是标签。然后我们使用这个标签 5 在概率分布中索引。因此这里的索引 5 是 0，1，2，3，4，5。就是这个数字。它在这里。所以这基本上是神经网络分配给实际正确字符的概率。

![](img/9b0f3ae8b78024dba93e80534d70c09b_375.png)

你会看到网络目前认为这个字符 E 在点之后的可能性只有 1%。这当然不是很好，因为这实际上是一个训练示例。而且网络认为它目前非常不可能。

![](img/9b0f3ae8b78024dba93e80534d70c09b_377.png)

但这只是因为我们在生成一个好的 W 设置时没有运气。

![](img/9b0f3ae8b78024dba93e80534d70c09b_379.png)

现在这个网络认为不太可能，0.01 不是一个好的结果。

![](img/9b0f3ae8b78024dba93e80534d70c09b_381.png)

所以对数似然非常负。

![](img/9b0f3ae8b78024dba93e80534d70c09b_383.png)

负对数似然非常积极。所以 4 是一个非常高的负对数似然。这意味着我们会有很高的损失。因为损失是什么？

损失就是平均负对数似然。

![](img/9b0f3ae8b78024dba93e80534d70c09b_385.png)

所以第二个字符是 EM。你会看到网络认为 E 之后的 M 也很不可能，只有 1%。

![](img/9b0f3ae8b78024dba93e80534d70c09b_387.png)

![](img/9b0f3ae8b78024dba93e80534d70c09b_388.png)

对于 M 之后的 M，它认为是 2%。对于 A 之后的 M，它实际上认为是 7%的可能性。

![](img/9b0f3ae8b78024dba93e80534d70c09b_390.png)

所以恰好这个实际上有一个相当好的概率，因此负对数似然也相当低。

![](img/9b0f3ae8b78024dba93e80534d70c09b_392.png)

最后这里它认为这个可能性是 1%。所以总体来说，我们的平均负对数似然就是损失。总结基本上是这个网络当前工作效果的总损失。至少在这个单词上，而不是在整个数据集上，仅这个单词是 3.76。这个损失实际上相当高。这不是一个很好设置的 W。

![](img/9b0f3ae8b78024dba93e80534d70c09b_394.png)

现在我们可以做什么。我们当前得到的是 3.76。

![](img/9b0f3ae8b78024dba93e80534d70c09b_396.png)

我们实际上可以来到这里，改变我们的 W。我们可以重新采样它。所以让我加一来有一个不同的种子。然后我们得到了一个不同的 W。然后我们可以重新运行这个。

![](img/9b0f3ae8b78024dba93e80534d70c09b_398.png)

以及这个不同的 C 和这个不同的 W 设置。

![](img/9b0f3ae8b78024dba93e80534d70c09b_400.png)

我们现在得到了 3.37。所以这是一个更好的 W。它更好是因为这个概率恰好对下一个字符更高。

![](img/9b0f3ae8b78024dba93e80534d70c09b_402.png)

![](img/9b0f3ae8b78024dba93e80534d70c09b_403.png)

所以你可以想象重新采样这个。

![](img/9b0f3ae8b78024dba93e80534d70c09b_405.png)

我们可以尝试两个。所以，好的，这感觉不好。

![](img/9b0f3ae8b78024dba93e80534d70c09b_407.png)

我们再试一次。我们可以尝试三次。好的，这个设置非常糟糕，因为我们的损失非常高。

![](img/9b0f3ae8b78024dba93e80534d70c09b_409.png)

不管怎样，我要把这个擦掉。

![](img/9b0f3ae8b78024dba93e80534d70c09b_411.png)

我在这里所做的就是随机分配参数并检查网络的好坏。

![](img/9b0f3ae8b78024dba93e80534d70c09b_413.png)

这就像是业余时光。这不是优化神经网络的方式。优化神经网络的方法是你从某个随机猜测开始，我们会坚持这个猜测，即使它并不好。但现在关键是我们有一个损失函数。

![](img/9b0f3ae8b78024dba93e80534d70c09b_415.png)

所以这个损失仅由可微操作构成。

![](img/9b0f3ae8b78024dba93e80534d70c09b_417.png)

我们可以通过计算损失相对于这些 W 矩阵的梯度来最小化损失，从而调整 W。因此我们可以调整 W 以最小化损失，并使用基于梯度的优化找到 W 的良好设置。

![](img/9b0f3ae8b78024dba93e80534d70c09b_419.png)

现在让我们看看这将如何运作。现在的东西实际上看起来几乎与 micrograd 中的一样。

![](img/9b0f3ae8b78024dba93e80534d70c09b_421.png)

所以我这里调出了来自 micrograd 的讲座，那个笔记本。

![](img/9b0f3ae8b78024dba93e80534d70c09b_423.png)

这是来自这个代码库。当我向下滚动到我们与 micrograd 停下的地方时，我们有一些非常非常相似的东西。我们有多个输入示例。在这种情况下，我们在`X`中有四个输入示例。我们还有它们的目标。

![](img/9b0f3ae8b78024dba93e80534d70c09b_425.png)

这些是我们的目标。就像这里我们有`X`一样，但现在我们有五个。它们现在是整数，而不是向量。但我们会将我们的整数转换为向量。

![](img/9b0f3ae8b78024dba93e80534d70c09b_427.png)

除了我们的向量将是 27 维，而不是 3 维。然后在这里我们首先进行了前向传播，在所有输入上运行神经网络以获取预测。那时我们的神经网络，`N of X`是一个多层感知器。我们的神经网络将看起来不同，因为我们的神经网络只是一个单层。

![](img/9b0f3ae8b78024dba93e80534d70c09b_429.png)

单个线性层后跟一个 softmax。

![](img/9b0f3ae8b78024dba93e80534d70c09b_431.png)

所以这就是我们的神经网络。这里的损失是均方误差。我们简单地将预测值从真实值中减去，然后平方并将其全部相加。这就是损失。损失是总结神经网络质量的单一数字。当损失很低，几乎为零时，意味着神经网络的预测是正确的。

所以我们有一个数字来总结神经网络的性能。这里的一切都是可微的，并且存储在一个庞大的计算图中。然后我们遍历所有参数。我们确保梯度被设置为零。然后我们调用`loss.backward`。而`loss.backward`在损失的最终输出节点启动了反向传播。

对。是的，记得这些表达式吗？我们在最后有损失。我们开始反向传播，然后一路向后。并确保填充了所有参数的 dot grad。于是 grad 从零开始，但反向传播填充了它。然后在更新中，我们遍历了所有参数。

我们简单地进行了参数更新，其中我们参数的每个元素都朝着梯度的相反方向微调。所以我们将在这里做完全相同的事情。

![](img/9b0f3ae8b78024dba93e80534d70c09b_433.png)

所以我将把它拉到旁边。

![](img/9b0f3ae8b78024dba93e80534d70c09b_435.png)

所以我们有了它，我们实际上会做完全相同的事情。这是前向传播。在这里我们做了这个。并且 props 是我们的白色线程。现在我们需要评估损失，但我们不使用均方误差。我们使用负对数似然，因为我们正在进行分类。

我们并没有进行所谓的回归。所以在这里，我们想计算损失。

![](img/9b0f3ae8b78024dba93e80534d70c09b_437.png)

现在我们计算的方式就是这个平均负对数似然。

![](img/9b0f3ae8b78024dba93e80534d70c09b_439.png)

现在这个 props 的形状是五乘以二十七。

![](img/9b0f3ae8b78024dba93e80534d70c09b_441.png)

所以为了获取所有的概率，我们基本上想要提取出正确索引处的概率。特别是因为标签是以数组的形式存储的。基本上我们关注的是第一个示例中索引五处的概率。在第二个示例中是在第二行或行索引一。

我们对索引十三分配的概率感兴趣。在第二个示例中，我们也有十三。在第三行，我们想要一个。在最后一行，即四，我们想要零。

![](img/9b0f3ae8b78024dba93e80534d70c09b_443.png)

所以这些是我们感兴趣的概率。你可以看到它们并不是很优秀，正如我们之前看到的那样。所以这些是我们想要的概率。但我们想要更高效的方法来访问这些概率，而不是像这样列出它们。所以结果是，在 PyTorch 中做这件事的方式是。

至少其中一种方法是我们可以基本上传入所有这些整数，以一个向量的形式。所以你看到的这些就是 0，1，2，3，4。我们实际上可以使用 empty 创建它。不是 empty，抱歉，是 torch.arrange 的五个数字：0，1，2，3，4。所以我们可以用 torch.arrange 的五个数字进行索引。在这里我们按数组索引。你看到这正好给我们这些数字。

![](img/9b0f3ae8b78024dba93e80534d70c09b_445.png)

所以这提取出神经网络分配给正确下一个字符的概率。

![](img/9b0f3ae8b78024dba93e80534d70c09b_447.png)

现在我们取这些概率，我们不直接使用，而是查看对数概率。所以我们想要计算对数。然后我们想要简单地平均这些值。所以取所有的均值。然后是负的平均对数似然。这就是损失。因此这里的损失是 3.7 左右。

![](img/9b0f3ae8b78024dba93e80534d70c09b_449.png)

你会看到这个损失 3.76，3.76 正是我们之前获得的。但是这是该表达式的向量化形式。因此我们得到了相同的损失。相同的损失我们可以视为前向传播的一部分，我们现在实现了损失。

![](img/9b0f3ae8b78024dba93e80534d70c09b_451.png)

好的，所以我们一路走到了损失。我们定义了前向传播。我们推进了网络和损失。现在我们准备进行反向传播。对于反向传播，我们首先要确保所有的梯度都被重置为零。在 PyTorch 中，你可以将梯度设置为零，但也可以直接设置为 none。

设置为 none 更高效。PyTorch 会将 none 解释为缺乏梯度，这与零相同。所以这是一种将梯度设置为零的方法。现在我们进行 loss.backward。在执行 loss.backward 之前，我们需要再确认一件事。如果你还记得在微梯度中，PyTorch 实际上要求我们传入.requires_grad 为 true。这样我们就告诉 PyTorch 我们希望计算这个叶子张量的梯度。

默认情况下这是假。让我用这个重新计算一下。

![](img/9b0f3ae8b78024dba93e80534d70c09b_453.png)

然后设置为 none 并调用损失的.backward。现在在运行 loss.backward 时发生了奇妙的事情。因为 PyTorch 和微梯度一样，当我们在这里进行前向传播时，它会跟踪所有底层的操作。

![](img/9b0f3ae8b78024dba93e80534d70c09b_455.png)

它构建了一个完整的计算图。就像我们在微梯度中生成的图。这些图存在于 PyTorch 内部。因此，它知道所有的依赖关系和所有的数学运算。当你计算损失时，我们可以对其调用.backward。这时反向传播会填充所有中间变量的梯度，一直回溯到 W。

这些是我们神经网络的参数。现在我们可以查看 WL.grad，看到它具有结构。里面有东西。这些梯度，这里的每个元素。所以 W 的形状是 27 乘 27。W.grad 的形状也是一样。27 乘 27。

![](img/9b0f3ae8b78024dba93e80534d70c09b_457.png)

W.grad 的每个元素都告诉我们该权重对损失函数的影响。例如，这个数字。如果这是 W 的零零元素。因为梯度是正的，它告诉我们这对损失有正面影响。稍微推动 W。稍微调整 W 的零零元素，加一个小的 h 会增加损失。

略微。因为这个梯度是正的。一些梯度也是负的。

![](img/9b0f3ae8b78024dba93e80534d70c09b_459.png)

这告诉我们关于梯度的信息。

![](img/9b0f3ae8b78024dba93e80534d70c09b_461.png)

我们可以使用这个梯度信息来更新这个神经网络的权重。所以我们不要进行更新。这将与我们在 micro grad 中拥有的非常相似。我们不需要遍历所有参数，因为我们只有一个参数张量，那就是 W。因此我们简单地做`W.data +=`。我们实际上可以几乎完全复制这个。负的 0.1 乘以 W。

grad。

![](img/9b0f3ae8b78024dba93e80534d70c09b_463.png)

这将是张量的更新。所以更新了张量。由于张量被更新，我们预计现在损失应该降低。所以这里如果我打印`loss.item`，它是 3.76。因此我们在这里更新了 W。如果我重新计算前向传播，损失现在应该稍微低一些。所以 3.76 变成 3.74。

然后我们可以再次设置梯度为零并进行反向传播，更新。现在参数再次改变。如果我们重新计算前向传播，我们再次期待损失降低到 3.72。这又是在进行读取集。当我们实现低损失时，这意味着网络正在将高概率分配给正确的下一个字符。

![](img/9b0f3ae8b78024dba93e80534d70c09b_465.png)

好吧，我重新整理了一切，从头开始把所有内容组合在一起。这是我们构建按字组的数据集的地方。你会看到我们仍然只对第一个词“Emma”进行迭代。我马上会改变这一点。我添加了一个数字来计算 Xs 中的元素数量，这样我们就可以明确看到示例数量是 5。

因为目前我们只在使用“Emma”，那里有 5 个字组。在这里我添加了一个循环，正是我们之前所拥有的。所以我们进行了 10 次迭代的非常新下降，前向传播，反向传播和更新。因此运行这两个单元的初始化和创建下降给我们在损失函数上带来了一些改善。

![](img/9b0f3ae8b78024dba93e80534d70c09b_467.png)

但现在我想使用所有单词。而不是 5，而是 228,000 的字组。然而，这不应该需要任何修改。一切应该都能运行，因为我们编写的所有代码都不在乎字组是 5 还是 228,000。而且所有内容应该都能正常工作。所以你会看到这将会运行。但现在我们正在优化所有字组的整个训练集。

你现在会看到我们正在非常轻微地减少。因此我们实际上可能可以承受更大的学习率。并且可能承受更大的学习率。

![](img/9b0f3ae8b78024dba93e80534d70c09b_469.png)

即使是 50 在这个非常简单的示例中似乎也能工作。所以让我重新初始化，进行 100 次迭代。看看会发生什么。

![](img/9b0f3ae8b78024dba93e80534d70c09b_471.png)

好吧。我们似乎在这里得到了相当不错的损失。

![](img/9b0f3ae8b78024dba93e80534d70c09b_473.png)

2.47。让我再运行 100 次。顺便问一下，我们预计损失应该是多少？

![](img/9b0f3ae8b78024dba93e80534d70c09b_475.png)

我们期望得到的结果大约和我们最初的一样。如果你还记得在视频开始时，我们通过计数优化，损失大约是 2.47，在添加平滑后。但在没有平滑之前，我们的损失大约是 2.45。

![](img/9b0f3ae8b78024dba93e80534d70c09b_477.png)

所以，这实际上大致是我们期望实现的目标。但是在我们通过计数实现之前，这里我们大致上达到了相同的结果。

![](img/9b0f3ae8b78024dba93e80534d70c09b_479.png)

但使用基于梯度的优化。

![](img/9b0f3ae8b78024dba93e80534d70c09b_481.png)

所以我们达到了大约 2.46、2.45 等等。

![](img/9b0f3ae8b78024dba93e80534d70c09b_483.png)

这很有道理，因为从根本上讲，我们并没有获取任何额外的信息。我们仍然只是获取前一个字符并尝试预测下一个字符。但我们不是通过计数和归一化明确进行，而是通过基于梯度的学习进行。而且恰好是显式方法非常好地优化了损失函数。

而不需要任何基于梯度的优化。因为 Bagram 语言模型的设置非常简单。我们可以直接估计这些概率并将它们保存在表格中。

![](img/9b0f3ae8b78024dba93e80534d70c09b_485.png)

但基于梯度的方法灵活性显著更高。因此，我们实际上获得了很多，因为现在我们可以扩展这种方法，并复杂化神经网络。所以目前我们只是在输入一个字符并送入一个非常简单的神经网络。但我们即将大幅迭代这个过程。

我们将会处理多个之前的字符，并将它们输入到越来越复杂的神经网络中。但从根本上讲，神经网络的输出将始终只是 logits。这些 logits 将经过完全相同的转换。

我们将对它们进行 softmax 计算，计算损失函数和负对数似然，并进行基于梯度的优化。因此，实际上随着我们复杂化神经网络并逐步提高到变换器，这些基本上不会改变。唯一会改变的是我们进行前向传播的方式。

我们将获取一些之前的字符，并计算下一个字符在序列中的 logits。这将变得更加复杂，我将使用相同的机制来优化它。而且不明显的是，我们将如何将这种 by-gram 方法扩展到这种情况。

![](img/9b0f3ae8b78024dba93e80534d70c09b_487.png)

![](img/9b0f3ae8b78024dba93e80534d70c09b_488.png)

输入中有许多更多的字符。因为最终这些表会变得太大，因为有太多组合。前一个字符可能是什么。如果你只有一个前一个字符，我们可以把所有内容都保存在一个表中，计数。

但如果你有最后 10 个输入字符，我们实际上不能再把所有东西保存在一个表中。因此，这从根本上来说是一种不可扩展的方法。而神经网络的方法则显著更具可扩展性。

![](img/9b0f3ae8b78024dba93e80534d70c09b_490.png)

而且这实际上是我们可以随着时间的推移改进的东西。

![](img/9b0f3ae8b78024dba93e80534d70c09b_492.png)

所以这就是我们接下来要深入探讨的地方。我想指出两件事。第一，我希望你注意到这个 x-ank，这由一个个热编码向量构成。然后这些个热编码向量与这个 w 矩阵相乘。我们把这看作多个神经元以完全连接的方式进行前向传播。

但实际上这里发生的是，例如，如果你在这里有一个个热编码向量，其中有一个，假设在第五维。那么由于矩阵乘法的方式，与 w 相乘的个热编码向量实际上会提取出 w 的第五行。

Logits 将变成 w 的第五行。这是由于矩阵乘法的方式。所以实际上发生的就是这样。但这实际上和之前完全一样。因为记得在这里，我们有一个 by-gram，我们取了第一个字符。

![](img/9b0f3ae8b78024dba93e80534d70c09b_494.png)

然后第一个字符索引到了这个数组的一行。那一行给了我们下一个字符的概率分布。因此，第一个字符被用作查找这里的一个矩阵以获得概率分布。嗯，这实际上正是这里发生的事情。因此，我们在获取索引。

我们将其编码为个热编码并与 w 相乘。所以 logits 实际上变成了 w 的适当行。然后就像之前一样，指数化以创建计数，然后进行归一化，成为概率。

![](img/9b0f3ae8b78024dba93e80534d70c09b_496.png)

所以这里的 w 实际上和这个数组是一样的。但记住，w 是日志计数，而不是计数。因此，更准确地说，w 的指数化 w.x 就是这个数组。但这个数组是通过计数和基本上填充 by-grams 的计数而生成的。而在基于梯度的框架中，我们是随机初始化的。

然后我们让损失引导我们到达完全相同的数组。所以这个数组实际上就是在优化结束时的数组 w。只不过我们是通过逐步跟随损失得出的。这也是为什么我们在最后得到相同的损失函数。

![](img/9b0f3ae8b78024dba93e80534d70c09b_498.png)

第二个注意事项是如果我来到这里，记住我们添加虚假计数以平滑和均匀这些概率分布的平滑。这样可以防止我们将零概率分配给任何一个字对。现在，如果我增加这里的计数，概率会发生什么变化？随着计数的增加。

概率变得越来越均匀。

![](img/9b0f3ae8b78024dba93e80534d70c09b_500.png)

因为这些计数最多大约为 900。所以如果我在这里每个数字上加上百万，你可以看到这一行及其概率在除法后将会变得越来越接近完全均匀的概率分布。

事实证明，基于梯度的框架与平滑是等价的。

![](img/9b0f3ae8b78024dba93e80534d70c09b_502.png)

![](img/9b0f3ae8b78024dba93e80534d70c09b_503.png)

特别是，思考这些 w，我们是随机初始化的。

![](img/9b0f3ae8b78024dba93e80534d70c09b_505.png)

我们还可以考虑将 w 初始化为零。

![](img/9b0f3ae8b78024dba93e80534d70c09b_507.png)

如果 w 的所有条目都是零，那么你会看到 logits 都变为零。然后对这些 logits 进行指数运算会变成全一，然后概率正好是均匀的。

![](img/9b0f3ae8b78024dba93e80534d70c09b_509.png)

所以基本上当 w 都相等，或者特别是等于零时。

![](img/9b0f3ae8b78024dba93e80534d70c09b_511.png)

那么概率就不是完全均匀的。所以尝试鼓励 w 接近零基本上等同于标签平滑。而在损失函数中越是鼓励这一点，就会获得越平滑的分布。这引出了一个叫做正则化的概念。

我们实际上可以增强损失函数，使其包含一个小组件，我们称之为正则化损失。具体而言，我们要做的是可以取 w，并且我们可以，例如，平方它的所有条目。然后我们可以，哦，抱歉，我们可以取 w 的所有条目并将其求和。

![](img/9b0f3ae8b78024dba93e80534d70c09b_513.png)

![](img/9b0f3ae8b78024dba93e80534d70c09b_514.png)

因为我们是在平方，所以将不再有符号。负数和正数都被数字压缩了。然后这样做的方式是如果 w 正好为零或零，损失为零。但如果 w 有非零数值，你就会累积损失。

![](img/9b0f3ae8b78024dba93e80534d70c09b_516.png)

所以我们实际上可以将其添加到这里。我们可以做一些比如损失加上 w 的平方总和。或者我们实际上可以改为取均值，因为否则总和会变得太大。均值更易于管理。然后我们有一个正则化损失。

比如说 0.01 倍或类似的东西。你可以选择正则化强度。然后我们可以优化这个。

![](img/9b0f3ae8b78024dba93e80534d70c09b_518.png)

现在，这个优化实际上有两个组成部分。不仅试图使所有概率正常工作，而且还有一个附加组成部分，试图同时使所有 w 为零。因为如果 w 不为零，你会感受到损失。因此，要最小化这个。

唯一实现这一点的方法是让 w 等于零。因此，你可以将其视为增加一个像弹簧力或重力的力，使 w 推向零。因此，w 想要等于零，而概率希望是均匀的。

![](img/9b0f3ae8b78024dba93e80534d70c09b_520.png)

但他们也希望与你的数据所指示的概率相匹配。因此，这种正则化的强度正好控制你在这里添加的计数量。

![](img/9b0f3ae8b78024dba93e80534d70c09b_522.png)

在这里添加更多计数相当于增加这个数字。

![](img/9b0f3ae8b78024dba93e80534d70c09b_524.png)

因为你越是增加它，这个损失函数的这一部分就会主导这一部分。而且这些权重将无法增长。因为当它们增长时，它们会累积过多的损失。因此，如果这个强度足够大，那么我们将无法克服这个损失的力量。我们将永远无法做到。

基本上所有的预测都是均匀的。所以我觉得这有点酷。

![](img/9b0f3ae8b78024dba93e80534d70c09b_526.png)

好的，最后，在我们结束之前，我想向你展示如何从这个神经网络模型中进行采样。我复制了之前的采样代码。记得我们采样了五次，所有做的就是从零开始。我们抓取了当前 iX 行的 P。这就是我们的概率行，从中采样下一个索引，并累加，直到遇到零。运行这个给我们带来了这些结果。我仍然在内存中有 P，所以这没问题。现在。

这个 P 不是来自 P 的行，而是来自这个神经网络。首先我们取 iX，将其编码为一个 X-ink 的独热行。这个 X-ink 乘以我们的 W。实际上，它只是提取出与 iX 对应的 W 的行。实际上就是这样。然后我们得到 logits，并对这些 logits 进行归一化。

进行指数运算以获得计数，然后归一化以获得分布，然后我们可以从该分布中采样。所以如果我运行这个，取决于你的看法，它有点反高潮或高潮，但我们得到了完全相同的结果。这是因为这在同一模型中。不仅实现了相同的损失，正如我提到的，这些是相同的模型，这个 W 是我们之前估计的对数计数。

但是我们以非常不同的方式得出了这个答案，并且它有非常不同的解释。但从根本上讲，这基本上是同一个模型，并且在这里给出了相同的样本。所以这有点酷。

![](img/9b0f3ae8b78024dba93e80534d70c09b_528.png)

好的，我们实际上已经覆盖了很多内容。我们介绍了基于字符的二元组语言模型。我们看到了如何训练模型，如何从模型中采样，以及如何使用负对数似然损失来评估模型的质量。

然后我们实际上用两种完全不同的方式训练了模型，结果却得到了相同的结果和相同的模型。第一种方法我们只是计算所有二元组的频率并进行归一化。第二种方法我们使用负对数似然损失作为优化计数矩阵或计数数组的指导，以便在基于梯度的框架中最小化损失。我们发现两者都得到了相同的结果，仅此而已。

现在，第二种基于梯度的框架更加灵活。而目前我们的神经网络部分非常简单。我们只使用一个之前的字符，并通过一个线性层来计算对数值。这个过程即将变得复杂。因此，在后续视频中，我们将会引入越来越多的字符，并将它们输入到神经网络中。

但这个神经网络仍然会输出完全相同的内容。神经网络将输出对数值（logits）。这些对数值仍然会以完全相同的方式进行归一化，所有的损失及其他一切，以及基于梯度的框架，所有内容保持不变。

![](img/9b0f3ae8b78024dba93e80534d70c09b_530.png)

只是这个神经网络现在会复杂化到变换器（transformers）。这将会非常令人兴奋，我对此充满期待。

![](img/9b0f3ae8b78024dba93e80534d70c09b_532.png)

现在先这样。再见。
