# P10：p10 让我们重现GPT-2 (124M) - 加加zero - BV11yHXeuE9d

大家好。今天我们将继续我们的Zero2Hero系列，特别是今天我们要重现GPT-2模型，即124百万版本。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_1.png)

所以当我发布GPT-2时，那是2019年，他们通过这篇博客发布了它。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_3.png)

除此之外，他们还发布了这篇论文，并在此基础上将代码发布到了GitHub上。所以打开I/GPT-2。现在，当我们谈论重现GPT-2时，我们必须小心，因为在这个视频中，我们将重现124百万参数的模型。

所以要明白的是，每当发布这些内容时，总会有一个迷你系列。因此，GPT-2迷你系列由不同大小的模型组成，通常最大的模型被称为GPT-2。但基本上，我们这样做的原因是你可以将模型大小放在图的x轴上。在y轴上，你可以放置很多你感兴趣的下游指标，比如翻译。

摘要、问答等等。你可以绘制缩放损失。因此，基本上随着模型大小的增加，你在下游指标上表现得越来越好。因此，特别是对于GPT-2，如果我们在论文中向下滚动，可以看到GPT-2迷你系列中有四个模型，从1.24亿参数到15.58亿参数。现在，我的数字与这个表格不一致的原因是这个表格是错误的。

如果你实际访问GPT-2的GitHub库，他们会说在计算参数时出现了错误。但基本上这就是124百万参数的模型等等。所以124百万参数的模型在变压器中有12层，并且在变压器中有768个通道，768个维度。我将假设你对这些术语有一定的熟悉度，因为我在之前的视频中已经涵盖了所有这些内容。

让我们构建GPT-2。让我们从头开始构建GPT。我在这个播放列表的前一个视频中覆盖了这个内容。现在，如果我们一切都正确，所有事情都顺利进行，到视频结束时，我们将看到类似这样的结果。我们将查看验证损失，这基本上衡量我们在一些模型在训练期间未见过的验证数据中预测下一个token的能力。

我们看到，我们从一开始不太好地完成这个任务，因为我们是从头开始初始化，到训练结束时，这个任务的完成情况相当不错。希望我们能够超越GPT-2 124M模型。现在之前他们在处理这个时，已经是五年前的事了。因此，这可能是当时相当复杂的优化，而当时的GPU和计算能力都比较小。

今天你可以在大约一个小时或更短的时间内重现这个模型，如果你想在云计算上进行，这大约需要十美元，所有这些计算机都可以租用。如果你为那台计算机支付十美元，你大约等一个小时或更少。

你实际上可以获得一个与OpenAI发布的模型一样好的模型。还有一件事要提到的是，与许多其他模型不同，OpenAI确实发布了GPT-2的权重。这些权重在这个存储库中都可以获得。但GPT-2论文在训练的所有细节上并不总是那么出色。

除了GPT-2论文，我们还将参考GPT-3论文，它在许多高参数和优化设置上更加具体。并且在架构上与GPT-2版本的模型没有太大偏离。

所以我们将在尝试重现GPT-2，124M时参考GPT-2和GPT-3。让我们开始吧。首先，我想从最后开始，或者说从目标开始。换句话说，让我们加载OpenAI发布的GPT-2，124M模型，并进行一些试验。我们来从中采样一些标记。现在的问题是，当你去到GPT-2的代码库，进入源代码并点击Pi模型时。

你会意识到实际上这是在使用TensorFlow。因此，原始的GPT-2代码是用TensorFlow编写的，嗯，不用说，现在用得不多。所以我们喜欢使用PyTorch，因为它更友好，更容易，我个人更喜欢它。问题在于初始代码是在TensorFlow中。

我们喜欢使用PyTorch。因此，为了获得目标，我们将使用hugging face的transformers代码，我个人更喜欢这个。进入transformers源代码中的transformers models GPT-2 modeling GPT-2.py文件，你会看到他们在这个文件中实现的GPT-2变换器。这是中等可读的，但并不完全可读。

它所做的是将所有这些权重从TensorFlow转换为PyTorch友好的格式，因此加载和使用起来要容易得多。因此，我们可以在这里查看GPT-2模型，并使用hugging face transformers加载它。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_5.png)

这就是它的样子。从transformers导入GPT-2 L&M头模型，然后从预训练的GPT-2。现在有一件尴尬的事情是，当你将GPT-2作为我们要加载的模型时，这实际上是124百万参数的模型。

如果你想要实际的 GPT-2，即 15 亿参数版本，那么你实际上要使用 dash-accel。这是 124M R 目标。现在我们做的是，当我们实际获取这个时，我们正在初始化 PyTorch 和在这个类中定义的 N 模块。我们想要获取的就是状态字典，它只是原始张量。因此，我们只有该文件的张量。顺便说一下，这里是一个 Jupyter Notebook，但这是在 VS Code 内部运行的 Jupyter Notebook。

我喜欢在单一的界面中进行操作，因此我喜欢使用 VS Code，这里是 VS Code 中的 Jupyter Notebook 扩展。我们可以获取状态字典，这只是一个字典，因此我们可以打印出键和值，即张量，让我们来看一下形状。

这些是 GPT-2 模型内部的不同参数及其形状。因此，标记嵌入的 W 权重大小为 50257 x 768。这来自于我们在 GPT-2 词汇中有 50257 个标记。顺便提一下，这些正是我们在之前的标记化系列视频中提到的标记。

在之前的视频中，我详细讲解了标记化。GPT-2 的标记器恰好有这么多标记。对于每个标记，我们有一个 768 维的嵌入，作为该标记的分布式表示。

每个标记都是一个小字符串片段，然后 P768 数字是表示该标记的向量。因此，这只是我们用于标记的查找表，然后这里我们有位置的查找表。由于 GPT-2 的最大序列为 1024，我们最多可以有 1024 个位置，每个标记可以在过去的这些位置上进行关注。GPT-2 中的每个位置都有一个通过优化学习到的固定 768 维向量。

这就是位置嵌入和标记嵌入。这里的所有内容只是 Transformer 的其他权重和偏置等等。因此，当你拿取例如位置嵌入并将其扁平化，然后仅取 20 个元素时，你可以看到这些仅仅是参数。这些是权重，浮点数，我们可以拿取并绘制它们。

这就是位置嵌入。我们得到的结果像这样，你可以看到它有结构。它有结构是因为在这个可视化中，每一行代表一个不同的位置，范围从 0 到 1024 的固定绝对位置。这里的每一行是该位置的表示。

它具有结构，因为这些位置嵌入最终学习了这些正弦和余弦，类似于表示每个位置。这里的每一行代表该位置，并由 Transformer 处理以恢复所有相对位置，并根据它们的位置来识别和关注相应的标记。

不仅仅是它们的内容。因此，当我们实际查看这些中的一个单独列时，我随便选了三列。你会看到，例如，我们专注于每个通道。我们在观察从0到1023的位置上这个通道的表现。

所以这个绿色通道对于200到800之间的所有内容非常敏感，但低于这个范围则响应较少，并且在零附近有一个急剧下降。因此，谁知道这些嵌入在做什么，为什么会是这样的。你可以看出，例如，由于它们有些锯齿状且有些嘈杂，你可以判断这个模型并没有完全训练好。而这个模型训练得越多。

你会更期望将其平滑化。因此，这告诉你这个模型有点训练不足。但原则上，这些曲线甚至不必是平滑的。这应该完全是随机噪声。实际上，在优化的开始阶段，它确实是完全的随机噪声，因为这个位置嵌入表完全是随机初始化的。因此，最开始你会有锯齿状，而你最终得到的光滑结果已经是有些令人印象深刻，因为原则上你甚至不应该从中得到任何有意义的图形。

不过我们实际上得到的东西看起来有点嘈杂，但大部分看起来是正弦波的。在原始的变换器论文《注意力即所有你需要的》中，位置嵌入实际上是初始化并固定的，并且在优化过程中呈现正弦波特征。

我们也可以查看这里的其他矩阵，所以我这里选取了变换器的第一层，观察其一个权重，大小为300乘300的第一个块。你会看到一些结构，但再次强调，谁知道这些是什么。

如果你对机械互操作性感兴趣，你可能会很高兴尝试弄清楚到底发生了什么，这个结构是什么，这一切意味着什么，但在这个视频中我们不会这样做。不过，我们确实看到一些有趣的结构，这很酷。

但我们最感兴趣的是，我们加载了OpenAI发布的这个模型的权重，现在使用Hugging Face的变换器，我们不仅可以获取所有原始权重，还可以获取他们所称的管道并从中采样。

这是前缀“你好，我是一个语言模型”，然后我们采样30个标记，并生成五个序列。我运行了这个，这是它产生的结果。“你好，我是一个语言模型。”但我真正做的是生成一个人类可读的文档。

还有其他语言，但这些是我们认为的想法。如果你愿意，可以阅读这些，但基本上这是来自该模型的同一前缀的五个不同补全，范围从2到124M。现在如果我在这里，我从这里取了这个例子。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_7.png)

遗憾的是，即使我们固定了种子，我们从片段中得到的生成结果也与他们的不同。因此可以推测代码发生了变化，但在这个阶段重要的是，我们得到了连贯的文本。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_9.png)

我们成功加载了模型，可以查看所有参数，键告诉我们这些参数来自模型的哪个部分。我们实际上想要编写自己的GPT-2类，以便完全理解其中发生的事情。我们不想使用像modeling GPT-2.py这样的东西，因为太复杂了，我们希望从头开始自己编写。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_11.png)

![](img/fffe638cf84a8d1e020ce63d0efeee6e_12.png)

我们将在这里并行实现GPT模型。作为我们的第一项任务，让我们将24M的GPT-2加载到我们将从头开始开发的类中。这将给我们信心，因为我们可以加载开源模型，因此有一个权重设置正好是124模型。但当然，我们将从头开始初始化模型，尝试在我们将获得的一些文档上自行训练。

我们将尝试超越这个模型。因此，我们将获取不同的权重，希望一切看起来有所不同，甚至更好。但我们会对这一点充满信心，因为我们可以加载开源模型，我们处于同一模型系列和类别，只需从头开始重新发现权重的良好设置。

现在让我们编写GPT-2模型，并加载权重，确保我们也能够生成看起来连贯的文本。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_14.png)

好的，现在让我们转到关注任何开始一切的论文，并浏览原始变压器的模型架构。请记住，GPT-2相较于原始变压器有所修改，特别是我们没有编码器，GPT-2是我们所称的仅解码器变压器。

所以这个完整的编码器在这里缺失。此外，使用这个编码器的交叉注意力也缺失，因此我们删除了整个部分。其他一切几乎保持不变，但我们将在这里查看一些差异。

所以主要有两个区别。接下来我们去看 GPT 二的论文中的二点三模型。我们注意到，首先层归一化的顺序被重新排列了，其次，在最终的自注意力模块中添加了一个额外的层归一化。因此，所有的层归一化现在是在 MLP 或注意力之前。

它们在之前进行，并且在最终分类器之前添加了一个额外的层名称。所以现在让我们在我们的 GPT 和模块中实现一些初步的框架和模块。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_16.png)

特别是，我们将尝试匹配 hugging face transformers 使用的此架构，因为这样会更容易从这个状态加载这些权重。因此，我们想要一些反映这个架构的东西。这是我想到的。

基本上，我们看到这里的主要容器称为 transformer。因此，我在一个 end 模块字典中反映了这一点。这基本上是一个模块，允许你使用键索引子模块，就像字典字符串一样。

在其中，我们有 token embeddings 的权重 WTE，这也是一个嵌入。还有位置嵌入的权重，这也只是一个嵌入。如果你记得，嵌入实际上只是围绕一组数字的华丽小包装模块。就像这样，它是一个单一的张量。

嵌入是一个华丽的包装器，围绕一个张量，使你可以通过索引行来访问其元素。现在除了这一点，我们看到这里有一个 dot H，并且这个是用数字进行索引，而不是用字符串进行索引。所以有 dot H dot zero、one、two 等等，一直到 dot H dot eleven。这是因为这个变换器中有十二层。

所以为了反映这一点，我也创建了一个 H，我想这可能代表隐藏层。并且这不是一个模块字典，而是一个模型列表，这样我们可以使用整数进行索引，正如我们在这里看到的那样。我们可以使用 dot zero、dot one、two 等进行索引。模块列表有 n 层块，而这些块尚待定义，稍后将定义模块。

此外，根据 GPT 二的论文，我们需要一个额外的最终层归一化，我们将把它放在那里。然后我们有最终分类器，语言模型头，它从 768 这个 GPT 中嵌入维度的数量进行投影。

一直到词汇大小，大约是 50,257，而 GPT 二在这个最终的投影中不使用偏置。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_18.png)

所以这是框架，你可以看到它反映了这一点，所以WTE是令牌嵌入。这里称为输出嵌入，但它实际上是令牌嵌入。PE是位置编码，这两部分信息正如我们之前看到的，将被相加，然后进入变压器。dot H是旧的灰色块，LNF是GPT二模型在这里添加的新层。

L和head是这里的线性部分。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_20.png)

所以这就是GPT二的框架。我们现在需要实现这个块。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_22.png)

好吧，现在让我们递归到块本身，我们要定义这个块。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_24.png)

所以我开始把它们放在这里。这个块我喜欢这样写。这些是一些初始化，然后这是这个块计算的实际前向过程。注意这里有一个变化，这在GPT二论文中提到的变压器。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_26.png)

所以在这里，层归一化是在应用注意力或前馈之后。除此之外，请注意归一化是在残差流内。你可以看到前馈是如何应用的，这个箭头穿过归一化。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_28.png)

这意味着你的残差路径内部有归一化。这并不是很好或理想的。你实际上更希望有一条从监督到令牌输入的干净的单一残差流。这是非常理想和好的，因为如果你还记得微分梯度的流动，梯度在反向阶段会均匀分配到它的两个分支。

所以加法是梯度中的一个分支。这意味着来自顶部的梯度直接流向输入。令牌通过残差路径不变地流动。但除此之外，梯度也通过块流动。块随着时间的推移贡献自己的部分，并介入并改变优化。

但从优化的角度来看，干净的残差路径是理想的。然后这是预归一化版本，你可以看到我们的X首先通过层归一化，然后是注意力，然后返回去进入第二层归一化和多层感知器。

有时也称为前馈网络或FM，然后它又进入残差流。在这里值得注意的是，回想一下注意力是一种通信操作。这是所有令牌的地方，有一千零二十四个令牌排成一个序列。这是令牌进行交流的地方，这是它们交换信息的地方。

因此，注意力是一种聚合函数。它是一个池化函数。它是加权求和函数。它是一个减少操作。而MLP，这里的MLP在每个单独的标记上发生。标记之间没有信息被收集或交换。因此，注意力是减少，而MLP是映射。

最终你得到的是变换器最终只是重复应用map reduce。如果你想这样考虑。所以这是它们交流的地方，也是它们对收集到的信息进行独立思考的地方。

每一个块确实细化了残差流中的表示。这是我们的块。它可能是从这张图片中修改而来。好的，现在让我们继续讨论MLP。所以我实现的MLP块如下。它相对简单。我们基本上有两个线性投影，它们夹在GALU非线性之间。

所以，在那个GALU近似中是10H。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_30.png)

现在，当我们切换到文档补丁时。这是GALU，并且有这种格式。它有两个版本。GALU的原始版本我们稍后会深入探讨，还有可以请求的10H近似版本。所以正如你所看到的，GALU基本上就像一个relu。

除了在零处没有完全平坦的尾部之外，但除此之外，它看起来非常像稍微平滑的relu。它来自这篇论文。天哪，线性误差单位。你可以逐步阅读这篇论文，其中有一些数学推理，至少引导到具体公式的解释。这与随机真实升降机及其修改的期望有关。

如果你愿意，可以浏览这些内容。还有一点历史可以解释为什么会有GALU的近似版本。根据我的了解，这来自于这个问题。在这个问题中，Daniel Hendrix提到在他们开发这种非线性时的情况。

评估精确GALU的IRF函数在tensor flow中非常慢。因此他们最终开发了这个近似。这个近似随后被BERT和GPT-2等模型采用。但今天没有真正的理由使用近似版本。

你可能更倾向于使用精确版本。因为我认为没有太大差异了。这有点像历史上的一种怪癖。但我们正试图完全再现GPT-2。GPT-2使用的是10H近似版本。所以我们更愿意坚持这一点。还有一个原因是直观上使用GALU而不是VARLU，因为以前的视频中有提到。

我们讨论过死 ReLU 神经元问题，在 ReLU 的尾部，如果它在零点是完全平坦的，任何落在那里的激活都会得到完全为零的梯度。这没有变化。这没有适应性。如果任何激活结束于这个平坦区域，网络就不会有发展。但 GALU 总是会贡献一个局部梯度。因此，始终会有变化。

始终会有适应性。而且将其平滑化在实践中经验上证明效果更好，正如这篇论文所示。并且它也被 BERT 论文、GPT-2 论文等所采用。因此，出于这个原因，我们在 GPT-2 的再现中采用了这种非线性。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_32.png)

现在在更现代的网络中，比如 LAMA3 等，这种非线性进一步变化为 SWIGWOO 和其他类似的变体。但对于 GPT-2，它们使用这种近似的 GALU。好吧，最后我们有了注意力操作。让我粘贴我的注意力。所以我知道这很多，我会稍微快一点地讲解，但又不会太慢，因为我们在之前的视频中已经覆盖过这个内容，我只是提醒你一下。

所以这就是注意力操作。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_34.png)

现在在之前的视频中，你会记得这不仅仅是注意力。这是多头注意力。在之前的视频中，我们有这个多头注意力模块。这种实现使得这些头实际上并没有那么复杂。它们基本上是并行存在于每个注意力块内。

有多个头，它们都并行工作，输出被串联。然后这成为多头注意力的输出。因此，头就像并行流，它们的输出被串联在一起。因此，这非常简单，使得头的实现相对直接。

这里发生的事情是，与其有两个独立的模块，实际上还有更多的模块被串联，所有这些都被放入一个单一的自注意力模块中。相反，我非常小心地进行了大量的转置、分割张量的操作，以使这个在 PyTorch 中非常高效。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_36.png)

但从根本上讲，算法上与我们之前在这个给定的仓库中看到的实现没有什么不同。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_38.png)

![](img/fffe638cf84a8d1e020ce63d0efeee6e_39.png)

所以我想简单提醒一下，不想花太多时间在这个上面。我们有这些令牌排成一个序列，总共有一千二十个。在这个阶段，每个令牌的注意力会发出三个向量，查询、键和值。首先发生的事情是，查询和键必须相互相乘，以获得注意力的数量。

就像他们对彼此的有趣程度。因此他们必须以乘法的方式进行交互。在这里我们在计算 qkv 时进行拆分，然后正如我在这里提到的，有很多技巧。这种工作方式是我们基本上将头数 H 作为批次维度。所以它是一个批次维度，就像 B 一样，以便在后续的操作中。

PyTorch 将 B 和 H 视为批次，并在批次和头部上并行应用所有操作。可以应用的操作有，首先，查询和键进行交互，以提供我们的注意力。这是自回归掩码，确保标记仅关注它们之前的标记，而不会关注未来的标记。这里的 softmax 正常化注意力，使其始终总和为一。

然后回想一下之前的视频，做注意力矩阵与值相乘基本上是一种对我们找到的标记值的加权求和方法。最后的转置连续视图只是将所有内容重新组装在一起。

而这实际上执行了连接操作。如果你愿意，可以慢慢查看这一点。但在数学上它等同于我们之前的实现。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_41.png)

在 PyTorch 中这更高效。这就是我选择这个实现的原因。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_43.png)

现在除了这一点，我对变量命名非常谨慎。例如，CATEN 与 CATEN 是相同的。因此实际上我们的键应该完全遵循 hugging face transformers 代码的模式。这将使我们现在轻松地将所有权重移植过来，因为我们所有变量的命名都是相同的。

但在这一点上，我们已经完成了 dgpt2 的实现。这使我们无需使用来自 hugging face 的这个文件，而这个文件相当长。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_45.png)

这是 2000 行代码。相反，我们只需要不到 100 行代码。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_47.png)

这是完整的 gpt2 实现。因此在这个阶段，我们应该能够接管所有权重。设置它们，然后进行生成。那么让我们看看这是什么样子。好的，我在这里也修改了 gpt 配置，以便这里的数字，超参数与 gpt2、124 和模型一致。所以最大序列长度。

我称之为块大小，这里是 124。标记数量是 5257。如果你看过我的标记视频，知道这是 50,000 个合并，Bp 合并。256 字节标记，bp3 的叶子，以及一个特殊的结束文本标记，用于区分不同文档，并可以开始生成。共有 12 层，注意力中有 12 个头，变换器的维度是 768。

现在我们可以将参数从 Hugging Face 加载到我们的代码中，并用这些参数初始化 GPT 类。让我复制粘贴一堆代码。我不会太快或太慢地讲解这段代码，因为说实话，它并不太有趣，不太激动人心。我们只是加载权重，所以这有点乏味。但正如我提到的。

在这个 GPT2 的迷你系列中有四个模型。这是我们在右侧的 Jupyter 代码，我只是将其移植过来。这些是 GPT2 模型的超参数。我们正在创建配置对象并创建我们自己的模型。

接下来这里发生的事情是，我们为我们的模型和 Hugging Face 模型创建状态字典。然后我们遍历 Hugging Face 模型的键，并复制那些张量。在这个过程中，我们忽略了一些缓冲区，它们不是参数。

它们是缓冲区。例如，注意力偏差仅用于 R aggressive mask。因此我们忽略了其中一些掩码。就是这样。还有一个额外的烦恼是这来自 TensorFlow 仓库，我不太确定这有点烦人，但一些权重的转置与 PyTorch 想要的不同。因此，我们手动处理。

我硬编码了应该转置的权重，然后如果需要就转置它们。然后我们返回这个模型。因此，firm pre-trained 是一个构造函数或 Python 中的类方法，如果我们只给定模型类型，它将返回 GPT 对象。

在我们的案例中是 GPT2，这是我们感兴趣的最小模型。这是代码，这就是你如何使用它。我们可以在 VS Code 中打开一个终端，运行 Python train GPT2.py，希望一切顺利。好的，我们没有崩溃。因此，我们可以加载权重、偏差和其他所有内容到我们的模块中。

但现在让我们进一步确认这个模型的有效性，并尝试实际生成内容。在我们能够从这个模型生成内容之前，我们必须能够向前传播。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_49.png)

我们实际上还没有编写那段代码。所以这是向前传播函数。向前传播的输入将是我们的索引、标记和标记索引。它们的形状始终为 B x T。因此，我们有批次维度 B，然后我们有最多 T 的时间维度。T 可以大于块大小。

块大小是最大序列长度。因此，B 和 T 的索引排列成一种类似二维布局的形式。请记住，基本上每一行的大小都不超过块大小。这是序列中的 T 个标记。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_51.png)

然后我们有 B 个独立序列堆叠在一个批次中，这样是高效的。现在这里我们正在前向传播位置嵌入和 token 嵌入。这段代码应该与之前的讲座非常相似。所以我们基本上使用一个范围，它有点像 PyTorch 的一个版本。

我们从零迭代到 T，创建这些位置的索引。然后确保它们是不可见的设备 IDX，因为我们不会仅在 CPU 上训练。这将太低效。我们希望在 GPU 上进行训练，稍后会实现。然后我们有位置嵌入和 token 嵌入，以及这两者的加法操作。

现在注意，位置嵌入在输入的每一行都是相同的。因此，在这个加法中隐藏着广播，我们必须在这里创建一个额外的维度。然后这两个相加，因为相同的位置嵌入适用于我们在批次中堆叠的每一行示例。接着我们通过 transformer 块，最后是最后的层归一化和 LMD。

所以在前向传播后得到的是 logits。如果输入是 B 乘 T 的索引，那么在每个 B 乘 T 的位置我们都会计算下一个序列中哪个 token 会出现。因此，token B T 加 1 是这个 token 右侧的那个。而这里的半个书的大小是可能的 token 数量。

因此，这就是我们将获得的张量。这些 logits 只需通过 softmax 转换即可得出概率。这是网络的前向传播，现在我们可以得到 logits，所以我们能够从模型中生成内容。好的，现在我们将尝试在左侧设置与右侧 hugging face 匹配的相同内容。

在这里，我们从管道中抽样，最多抽样五次以获得 30 个 token，并在语言模型中以 hello 作为前缀。这是我们得到的完成内容。所以我们将在左侧尝试复制这一点。因此，序列的数量是五，最大长度是 30。因此我们首先做的当然是初始化我们的模型。然后我们将其置于评估模式。

现在，把模型置于评估模式是一个好习惯，当你不打算训练时，只是要使用它。我实际上不知道这是否在起作用，原因是我们上面提到的模型不包含在训练或评估时行为不同的模块或层。例如，dropout、batch norm 和其他一些层具有这种行为。

但是我们在这里使用的所有层在训练和评估时应该是相同的。因此，评估模型潜在上并不是什么特殊情况，我不确定是否确实如此，也许 pytorch 内部在这里根据评估模式做了一些聪明的处理。

我们接下来要做的是将整个模型移动到 CUDA。也就是说，我们将所有张量移动到 GPU。我在这里通过 SSH 连接到一个云服务器，这个服务器上有很多 GPU。在这里，我正在移动整个模型及其所有成员和所有张量，所有这些都被发送到一个基本上独立的计算机，它坐落于 GPU 上。

GPU 连接到 CPU，它们可以通信，但基本上是一个独立的计算机，拥有自己的计算机架构。它非常适合并行处理任务，比如运行神经网络。所以我这样做是为了让模型在 GPU 上运行，整个独立的计算机，这会使我们的代码效率更高，因为所有这些任务在 GPU 上运行得更高效。

这就是模型本身。接下来，我们想要做的是在生成时将这个作为前缀。所以让我们创建这些前缀代币。这是我写的代码。我们将从 OpenAI 导入 tick token 库，并获取 GPT-2 编码。也就是 GPT-2 的分词器。然后我们将编码这个字符串，获取一个整数列表，这些就是代币。

现在，这些整数应该相当简单，因为我们可以直接复制粘贴这个字符串，并检查它在 tick tokenizer 中是什么。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_53.png)

所以稍微安排一下，这些是将要输出的代币。这个整数列表就是我们期望代币变成的样子。如你所记，如果你看过我的视频，当然，所有的代币都是小字符串块。对吧？

所以这些是将这个字符串切分为 GPT-2 代币。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_55.png)

一旦我们有了这些代币，它们是一个整数列表，我们可以创建一个 torch 张量。在这个案例中，有八个代币。然后我们将这些八个代币复制五次，以获得五行八个代币。这就是我所称的初始输入 X。它也运行在 GPU 上。所以现在的 X 是这个 IDX，我们可以将其放入前向传播中，以获取我们的 logits，从而知道第六个代币是什么。

抱歉，作为这五行中每一行的第九个代币。好的，现在我们准备生成了。所以让我再粘贴一个代码块。这个代码块中发生的事情是我们有这些 X，大小为 B 乘 T，对吧？所以是批量与时间。在这个循环的每一次迭代中，我们将向每一行添加一列新的索引，对吧？

所以这些是新的索引，我们在采样时将它们附加到序列中。随着每次循环迭代，我们向 X 中添加一列。在 Torx 的上下文管理器中执行所有操作，.no.grad 这只是告诉 PyTorch，我们不会在任何这些操作上调用反向传播。

因此，它不需要缓存所有的中间张量。也不需要为后续的潜在反向传播做任何准备。这节省了很多空间，也可能节省一些时间。所以我们得到 logits。我们只在最后的位置获取 logits。我们丢弃所有其他 logits。我们不需要它们。

我们只关心最后一列的 logits。因此这有些浪费，但这只是一种低效的采样实现。虽然是正确的，但效率不高。所以我们获取最后一列的 logits，通过 softmax 得到我们的概率。然后在这里我进行 50 的 top K 采样。

我这样做是因为这是 Hugging Face 的默认设置。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_57.png)

所以看看这里 Hugging Face 的管道。有一堆小粒子进入 Hugging Face。老实说，这有点多。但我想我注意到的重要一点是，他们默认使用 top K，数量是 50。这在这里也在使用。

这样做的基本目的是我们想保留我们的概率，并且只想保留前 50 个概率。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_59.png)

任何低于第 50 个概率的标记，我们都将其限制为零并重新归一化。这样一来，我们就不会采样非常罕见的标记。我们要采样的标记总是在最可能标记的前 50 个中。这有助于保持模型在轨道上，不会喋喋不休，也不容易迷失方向或偏离主题。

它在可能的标记附近的表现要好得多。所以这是在 PyTorch 中的做法，如果你愿意，可以逐步了解。我认为这不是特别有见地，所以我会快速过一遍。但大致来说，我们得到了这一列新的标记。

我们将它们附加到 X 上，基本上 X 的列增长，直到这个 while 循环被触发。最终，我们得到了一个大小为 5x30 的完整 X。在这个例子中。我们基本上可以打印出所有这些单独的行。所以我得到了所有的行。我得到了所有被采样的标记，并使用 tick tokness 的解码函数将其转换回字符串，以便我们可以打印。

所以终端，新的终端。让我用 Python 训练 GPT。好的。这就是我们得到的生成结果。你好，我是一个语言模型，不是一个程序。新行，新行，等等。你好，我是一个语言模型，当他们创造语言时，令我困扰的主要事情之一是创造某种东西是多么容易。我是说，这样一来，它就会在所有这些情况下喋喋不休。

你会注意到，这些生成并不是hugging face的生成。说实话，我找不到这个差异，我没有完全检查所有这些选项，但可能还有其他东西在top P之外隐藏。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_61.png)

![](img/fffe638cf84a8d1e020ce63d0efeee6e_62.png)

我无法将其匹配，但为了确保正确性，在Jupyter notebook中使用hugging face模型。所以这里是hugging face模型。我复制了代码，如果我这样做并运行它。那么我得到的结果是相同的。因此，基本上模型的间隔并没有错。只是我不完全确定hugging face的管道做了什么，这就是为什么我们无法将它们匹配。

但除此之外，代码是正确的，我们已经正确加载了所有的张量。所以我们正确地初始化了模型，这里的一切都在正常工作。因此，一切都很简短。我们已经移植了所有权重，初始化了GPT。这里是GPT的确切启动，它能够生成看起来合理的序列。

现在这里当然是用GPT的模型权重进行初始化。但现在我们想从随机数开始初始化，实际上训练模型，以生成质量与这些序列一样好或更好的序列。因此，这就是我们接下来要做的。事实证明，使用随机模型实际上相当简单，因为PyTorch已经默认随机初始化我们的模型。因此，当我们在构造函数中创建GPT模型时。

所有这些层和模块都有默认的随机初始化器。因此，当这些线性层被创建等时，会有默认构造器。例如，使用我们之前看到的哈维尔初始化来构建这些层的权重。因此，创建一个随机模型而不是GPT模型实际上相当简单，我们只需在这里创建模型等于GPT。

然后我们想要使用默认的GPT配置，默认配置使用124m参数。因此，这是随机模型初始化，我们可以运行它。我们应该能够得到结果。当然，这里的结果完全是垃圾——这是因为它是一个随机模型。所以我们只是随机地得到了这些随机的标记字符串片段。

这就是我们现在拥有的。顺便说一句，我想指出的是，如果你没有CUDA可用，因为你没有GPU。你仍然可以在某种程度上跟随我们正在做的事情。可能到最后不能完全跟上，因为到最后我们将使用多个GPU进行实际的训练。但是现在你实际上可以相当好地跟随。

我在PyTorch中喜欢做的一件事是自动检测可用的设备。因此特别是你可以这样做。在这里我们尝试检测可运行的设备，以便具有最高的计算能力。你可以这样考虑。所以默认情况下，我们从CPU开始，当然这在任何地方都是可用的，因为每台计算机都会有CPU。但然后我们可以尝试检测强大的GPU，如果有的话，就使用CUDA。

如果你没有CUDA，至少你有MPS吗？MPS是Apple Silicon的后端。如果你有一台相对较新的MacBook，你可能内部有Apple Silicon。这种设备的GPU实际上相当强大，具体取决于你拥有的MacBook。因此你可以使用MPS，这可能比CPU更快。

所以我们可以在这里打印设备。现在一旦我们有了设备，就可以在CUDA的位置上使用它。因此我们只需替换，并注意当我们在X上调用模型时，如果这里的X是在CPU而不是GPU上，那么它也能正常工作，因为在前向传播中PyTorch会这样处理。

当我们创建Pause时，我们谨慎地使用IDX设备来创建这个张量。因此不会出现一个张量在CPU上而另一个在GPU上的不匹配，导致无法结合的情况。但在这里，我们小心地根据输入模型的指示在正确的设备上初始化。因此，这将自动检测设备。对我来说，这当然是GPU。所以使用设备CUDA。

但正如我提到的，你也可以使用另一个设备。这样不会慢太多。所以如果我在这里覆盖设备。如果我将设备重写为CPU，那么我们当然会交换打印CUDA，但现在我们实际上是在使用CPU。一个，二，三，四，五，六。好吧，大约六秒。实际上我们没有使用torch compile等，这样可以大大加快速度。

但即使在CPU上，你也不能完全跟上，我认为这在一定程度上是个问题。所以这是一个说明。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_64.png)

好的，我确实想最终回到在PyTorch中拥有不同设备意味着什么。当你执行类似module.to device或将torch张量用.to device时，PyTorch在后台为你做了什么，以及这到底发生了什么，如何运作。但现在我想直接进入训练，开始训练模型。

目前我们先说这个设备可以让代码运行得很快。接下来我们来讨论如何实际训练模型。为了训练模型，我们需要一些数据集。对我来说，我喜欢使用的最简单的调试数据集是小型的莎士比亚数据集。

它可以在这个URL获取，所以你可以W获取它，或者直接搜索tiny Shakespeare数据集。因此，我在我的文件系统中将其命名为input.txt。我已经下载好了。在这里我读取数据集，获取前1000个字符并打印前100个。请记住，GPT2大约有一个压缩比。

该tokenizer的压缩比大约为3比1。因此，1000个字符大约会产生300个token，这些token将来自我们目前正在获取的幻灯片。这是前几个字符。如果你想获取更多统计信息，我们可以对input.txt进行字数统计。因此，我们可以看到这个数据集有40000行，约200000个单词。

该文件大约有1百万字节，并且知道这个文件只有ASCII字符。到目前为止我知道这里没有疯狂的Unicode。因此，每个ASCII字符用一个字节编码。这与这个数据集中的大约一百万字符相同。所以这是默认的数据集大小，一个非常小且最小的数据集，用于调试，以便我们起步。

为了对这个数据集进行标记化，我们将获取tiktoken编码以供GPT2使用，编码前1000个字符，然后我只会打印前24个token。这些token是整数列表。如果你能阅读GPT2的token，你会看到198，你会认识到这是换行符。因此这里有两个换行符，所以这里198出现了两次。

这只是前24个token的标记化。所以我们现在想要做的是实际上处理这些token序列并将其输入到transformer中。特别是我们希望将这些token重新排列成我们将输入到transformer中的IDX变量。因此，我们不想要一个非常长的一维序列。

我们希望整个批次中的每个序列都能达到。基本上是t个token，而t不能大于最大序列长度。然后我们有这些t长的token序列，还有b个独立的序列示例。那么我们如何创建一个b乘t的tensor，以便将这些一维序列输入到前向计算中呢？

这是我最喜欢的实现方式。如果我们使用torch，然后从这个整数列表创建一个tensor对象，并且只取前24个token。我最喜欢的做法是基本上执行一个点视图，例如4乘6，结果为24。因此，这只是这些token的二维重新排列。当你将这个一维序列视为二维4乘6时，你会注意到。

这里的前6个标记最终成为第一行。接下来的6个标记成为第二行，依此类推。因此，基本上在这种情况下，每6个标记将堆叠为独立的行，并创建一个标记批次。因此，例如，如果我们在转换器中是标记25，当我们输入时这将成为IDX。

这个标记将看到这三个标记，并将尝试预测接下来是198。因此，我们能够创建这个相当不错的二维批次。至于我们需要的标签以计算损失函数，我们该如何获取呢？我们可以在前向传播中写一些代码，因为我们知道序列中的下一个标记即标签就在我们右侧。

你会注意到，实际上对于这个最后的标记13，我们并没有加载下一个正确的标记，因为我们没有加载它。因此我们实际上没有获取到足够的信息。接下来我会展示我最喜欢的获取这些批次的方式，我个人喜欢的不仅是输入到转换器的部分，我称之为X。同时我也喜欢创建一个与X大小完全相同的标签张量，但在每个位置都包含目标。

这是我喜欢的做法。我会确保获取+1个标记，因为我们需要最后一个标记13的真实值。然后在创建输入时，我们获取所有直到最后一个标记（不包括它），并以6的形式展示。

在创建目标时，我们使用缓冲区，但从索引1开始而不是索引0，所以我们跳过第一个元素，并以完全相同的大小展示。当我打印这个时，发生的情况是，比如对于这个标记25，它的目标是198，现在正好存储在目标张量中的相同位置，即198。

这个最后的标记13现在有它的标签，即198，这仅仅是因为我们在这里加载了+1。因此，基本上这是我喜欢的做法，你可以把长序列以二维的方式展示，这样就能获得时间的批次。然后我们确保加载一个额外的标记，所以我们基本上加载了b乘以t加1的标记缓冲区，然后对其进行偏移和展示。我们有两个张量，一个是输入到转换器的，另一个正好是标签。

现在让我们重新组织这段代码，创建一个非常简单的数据加载器对象，基本上加载这些标记并将它们输入到转换器中以计算损失。好的，我在这里相应地重组了代码，正如你所看到的，我暂时覆盖以在CPU上运行。

导入到token，所有这些应该很熟悉，我们正在加载一千个字符。我现在将bt设置为b4和32，仅仅是因为我们在调试，我们只想要一个非常小的单批次。所有这些现在应该看起来很熟悉，并遵循我们在右侧所做的。然后在这里我们创建模型并获取logits。

所以在这里，如你所见，我已经运行过，这仅仅在几秒钟内完成，但因为我们有一个4乘以32的批次。我们的logits现在的大小是4乘以32乘以50,257。这些是每个位置上接下来会发生的logits。现在我们有存储在Y中的标签。因此，现在是计算损失并进行反向传播和优化的时间。那么我们先计算损失。好的，为了计算损失，我们将调整模型中的这个forward函数。

特别是我们不仅返回logits，还会返回损失。我们不仅将输入传入cease，还将目标传入Y。现在我们不再打印logits的形状，而是打印损失函数。然后是`syst.exit(0)`，以便跳过一些样本逻辑。

现在让我们回到forward函数，它在那里被调用。因为现在我们也有这些可选目标。当我们得到目标时，我们也可以计算损失。记住，我们基本上希望返回logits的损失，而默认情况下损失为none，但让我们把它放在这里。如果目标不是none，那么我们想要计算损失。

而助手在这里已经开始兴奋地计算，看起来是正确的损失。它正在使用交叉熵损失，具体文档在这里。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_66.png)

所以这是PyTorch中功能下的一个函数。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_68.png)

现在这里实际发生了什么，因为看起来有点吓人。基本上，F。这个交叉熵不喜欢多维输入。它不能接受B乘以T乘以词汇大小。因此，这里发生的事情是我们将这个三维张量展平为两个维度。第一个维度将自动计算，并且将是B乘以T。

然后最后一个维度是词汇大小。因此，基本上这将三维的logits张量展平为二维的B乘以T。所有单个示例和词汇大小对应于每一行的长度。然后，它也将目标展平，此时目标也是二维的。但我们将把它们展平为单个B乘以T的张量。

然后这可以传入交叉熵以计算返回的损失。因此此时基本上应该运行，因为它并不太复杂。让我们运行它，看看是否应该打印损失。在这里我们看到我们大约打印了11。所以。

注意这是一个单元素的张量，就是这个数字11。现在我们还希望能够为随机线性网络计算一个合理的起始点。我们在之前的视频中覆盖了这一点，但我们的词汇大小是50,257。在网络初始化时，你希望每个词汇元素得到大致均匀的概率。

这样我们在初始化时不会过于偏向任何标记。我们在初始化时并不会自信地错误。因此我们希望任何任意标记的概率大致为1/50,257。现在我们可以检查损失。因为记住交叉熵损失基本上就是负对数似然。因此如果我们现在将这个概率取自然对数然后取负。

这是我们在初始化时期望的损失，我们在之前的视频中讨论过这个。所以我预计大约在10.82左右，而我们看到的也是这个水平。所以并没有太大的偏差。这大致是我在初始化时所期望的概率。这告诉我初始化或概率分布大致是分散的。

这是一个好的起始点，我们现在可以进行优化，并告诉网络哪些元素应该按正确顺序跟随。所以在这个时候，我们可以进行loss.backward，计算梯度并进行优化。那么我们来开始吧。好的，现在进行优化。所以这里是损失。这是我们获得损失的方式。但现在基本上我们想要一个加载循环。所以对于i的范围。

我们做50步之类的。让我们在PyTorch中创建一个优化器对象。所以我们使用的是atom优化器，这是我们使用的随机梯度下降优化器S.G.D.的替代方案。S.G.T.简单得多。Atom稍微复杂一些。实际上特别是像atom W的变体，因为在我看来，它就像是修复了一个bug。

所以我认为atom W是atom的一个bug修复。当我们查看atom W的文档时。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_70.png)

哦，我的天啊。我们看到它需要一堆参数，且比之前讨论的S.G.D.要复杂一些。因为除了基本上用学习率缩放的梯度来更新参数外，它还保留了一些缓冲区，并且保留了两个缓冲区，M和V，称之为第一和第二动量。所以这看起来有点像动量，也有点像你熟悉的RMS prop。

但你并不一定需要这样做。这只是在每个梯度元素上单独发生的归一化，加快了优化，尤其是对于语言模型。但我不会在这里详细讨论。我们会把它视为一个黑箱，它优化目标的速度比S.G.D.快。

D，这就是我们在前面的讲座中看到的。因此，让我们在这种情况下将其视为一个黑箱。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_72.png)

创建优化器对象，然后进行优化。首先始终确保副驾驶没有忘记将梯度归零。因此，请始终记住，你必须从零梯度开始。然后当你得到损失并进行dot backward时，dot backward会累加梯度。

所以它会存储梯度。它总是对梯度进行加法运算，这就是为什么你必须将它们设为零。因此，这会累积来自损失的梯度，然后我们调用优化器上的步骤函数来更新参数并减少损失。

然后我们打印步骤，loss的dot item在这里使用，因为损失是一个包含单个元素的张量。dot item实际上会将其转换为单个浮点数，而这个浮点数将存在于CPU上。因此，这再次涉及到设备的一些内部机制，但损失是一个包含单个元素的张量，并且对我来说它存在于GPU上，因为我正在使用GPU。当你调用dot item时，PyTorch在后台将取出这个一维张量。

将其发送回CPU内存并转换为我们可以打印的浮点数。所以这是优化，这应该能正常工作。让我们看看会发生什么。实际上，抱歉，让我删除CPU覆盖。这样对我来说快一点，并且在CUDA上运行。哦。

期望所有张量在同一设备上，但发现至少有两个设备：CUDA零和CPU。所以CUDA零是零GPU，因为我实际上在这个设备上有一个GPU。因此，我的箱子的第0个GPU和CPU。我们将模型移动到设备上，但当我编写此代码时，实际上引入了一个错误，因为缓冲区从未移动到设备上。你必须小心，因为你不能只是将缓冲区移动到设备。这不是有状态的。

它不会将其转换为设备。相反，它返回指向设备上新内存的指针。因此，你会看到我们如何将模型移动到设备，但这并不适用于张量。你必须做缓冲区等于。将缓冲区移动到设备，然后这应该有效。那么我们期望看到什么？我们期望在开始时看到合理的损失，然后继续优化单个批次。

因此，我们希望看到我们可以对这个单个批次进行过拟合。我们可以压缩这个小批次，并且我们可以完美预测这个小批次的C。这大致就是我们在这里看到的。因此，我们开始时大约是10.82。 在这种情况下是11。然后随着我们在这个单个批次上继续优化，而不加载新的示例。

我们确保可以对单个批次进行过拟合。而且我们得到了非常非常低的损失。因此，变压器正在记忆这个单独的批次。我还没提到的另一件事是这里的学习率是3e负四，对于大多数早期调试阶段的优化来说，这是一个相当不错的默认值。

所以这是我们简单的内部循环，我们正在对单个批次进行过拟合，这看起来不错。接下来我们想要的不仅仅是对单个批次进行过拟合。我们实际上想要进行优化。因此，我们需要迭代这些xy批次，并创建一个小数据加载器，以确保我们始终获得一个新鲜的批次，并且我们实际上是在优化一个合理的目标。接下来我们来做这个。好的，这就是我想出的，并写了一个小数据加载器。

这个数据加载器的作用是我们在这里导入标记，从这个单一的impa.txt读取整个文本文件，对其进行标记化，然后打印总标记数，以及在遍历此数据集的单个时期中的批次数量。

在循环回文档开头并再次开始读取之前，我们输出了多少个唯一批次。因此，我们从位置零开始，然后简单地以b乘以t的批次遍历文档。因此，我们取b乘以t的块，然后总是向前推进b乘以t。值得注意的是，我们始终将位置推进恰好是b乘以t。

但是当我们获取标记时，我们实际上是从当前位置提取到b乘以t加一。我们需要这个加一，因为记住我们需要当前批次最后一个标记的目标标记。因此，我们可以像之前那样精确地进行x，y。如果数据用完，我们就会循环回零。这是编写非常简单的数据加载器的一种方式。

这只是将文件分块处理。这对于我们当前的目的来说不可行。我们稍后会使其复杂化。现在我们想回到这里，实际上使用我们的数据加载器。所以导入的tic to可以已经上升了。实际上，这一切现在都是无用的。因此，我们只想要一个用于训练数据的训练加载器。

我们希望为四个使用相同的超参数。所以批量大小是四，时间是32。然后在这里我们需要获取当前批次的x，y。所以让我们看看CoPALD是否能够做到，因为这简单到足以。我们调用下一个批次，然后确保将我们的张量从CPU移动到设备上。

在这里，当我转换这些标记时，请注意我并没有将这些标记移动到GPU上。我把它们留在CPU上，这是默认设置。这只是因为我不想在GPU上浪费太多内存。在这种情况下，这是一个微小的数据集，能够适应。但为了我们的目的，现在将其发送到GPU也是可以的。所以我们获取下一个批次。

我们保持数据加载器简单为CPU类。然后我们实际上将其传送到GPU上进行所有计算。让我们看看这个是否能运行。所以用Python运行train dpt to dot pi。在这实际发生之前，我们期待看到什么？

我们期待看到的是，现在我们实际上正在获取下一个批次。所以我们希望不对单个批次过拟合。因此，我期望我们的损失会下降，但不会下降太多。这是因为我仍然期望下降，因为在50,257个令牌中，许多令牌从未出现在我们的数据集中。因此，在优化中这里有一些非常简单的增益可以获得。

例如，通过将所有从未出现的logits的偏置推向负无穷大。这基本上意味着所有这些疯狂的unicode或不同语言，这些令牌从未出现。所以它们的概率应该非常低。因此，我们应该看到的增益大致上是删除从未出现的令牌的使用。

这大概是我们现在在这个规模上看到的大部分损失增益。但我们不应该归零，因为我们只进行了50次迭代。我认为现在不需要进行一个完整的epoch。所以让我们看看我们有什么。我们有338,000个令牌，这与我们的三比一压缩比是合理的，因为有一百万个字符。

因此，使用当前的B和T设置，一个epoch将需要2600个批次。而我们在这里只进行50个批次的优化。所以我们像预期的那样从熟悉的领域开始，然后似乎降到了大约6.6。因此，基本上我认为在我们期望的范围内，现在的情况似乎工作得还不错。

所以这很好。好的，接下来我想修复我们代码中的一个bug。这个bug并不重大，但与GPT-2的训练方式有关。这个bug是这样的。我们在从Hugging Face加载权重时不够小心，实际上错过了一个小细节。所以如果我们来看这里，注意这两个张量的形状是相同的。

这里的这个是变换器底部的令牌嵌入。而这里的这个是变换器顶部的语言建模头。这两个基本上都是二维张量，它们的形状是相同的。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_74.png)

所以这里，第一个是输出嵌入，即令牌嵌入。第二个是在分类器层顶部的这个线性层。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_76.png)

它们的形状都是 50,000 到 57 乘以 768。这里的这个给出了我们底部的标记嵌入。这里的这个则是将变换器的 768 个通道试图上升到 50,257，以获得下一个标记的逻辑。因此它们的形状相同，但更重要的是，如果你在 PyTorch 中比较它们的元素，这是元素双重相等性。然后我们使用 .all，并且我们看到每一个元素都是相同的。更重要的是。

我们看到如果实际查看数据指针，这是在 PyTorch 中获取实际数据和存储的指针。我们看到实际上指针是相同的。因此，这两个是分开的张量，恰好形状和元素相同，但它们实际上指向的是相同的张量。

所以这里发生的事情是，这是一个常见的权重时间方案，实际上来自于最初的注意力中的所有内容的论文，实际上甚至在它之前的引用。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_78.png)

所以如果我们查看嵌入和注意力中的 softmax，论文中提到，在我们的模型中，我们在两个嵌入层和预先付款之间共享相同的权重矩阵。预 softmax 线性变换类似于 30。所以用这种方式来表达这两个是共享的且绑定在同一矩阵中的确有些笨拙。

而 30 的引用是这篇论文。这篇论文于 2017 年发表。你可以阅读完整的论文，但基本上它主张这种权重时间方案。我认为直观上，为什么你可能想这样做的想法来自于这一段。基本上你可以观察到，你实际上希望这两个矩阵在以下集合中表现相似。

如果两个标记在语义上非常相似，比如一个全小写，另一个全大写，或者是同一个标记在不同语言中的表示等。如果你在两个标记之间存在相似性，推测你会期待它们在标记嵌入空间中是相近的。

但以同样的方式，你会期待如果有两个在语义上相似的标记，它们在变换器的输出中应该获得相同的概率，因为它们在语义上是相似的。因此，在变换器的底部和顶部的两个位置都有这种特性，即相似的标记应该具有相似的嵌入或相似的权重。这就是促使他们在这里进行探索的原因。

我不想逐篇讨论整个论文，你可以去阅读它。但这是他们观察到的。他们还观察到，如果你查看输出嵌入，它们的行为也像词嵌入。如果你尝试将这些权重用作词嵌入，他们发现了这种相似性。他们试图将它们绑定，并观察到这种方式能获得更好的性能。

所以这是被采纳的，注意力在论文上。然后它也在 GPT-2 中再次使用。所以我找不到它在变压器实现中的位置。我不确定他们是如何关联这些嵌入的，但我在 OpenAI 提出的原始 GPT-2 代码中找不到它。所以这是 OpenAI GPT-2 源模型。在这里他们正在转发这个模型。

这是一个张量流，但没关系。我们看到他们获取了 WTE 令牌嵌入。然后这里是令牌嵌入和位置的编码器。然后在底部，他们再次使用 WTE 来计算 logits。所以当他们获得 logits 时，它是来自变压器的输出的矩阵模型，而 WTE 张量被重复使用。

所以 WTE 张量基本上在变压器的底部和顶部使用了两次。在反向传播中，我们将从两个分支获得梯度贡献，对吧？

这些梯度将在 WTE 张量上累加。因此我们将获得来自分类器层的贡献。然后在变压器的最底部，我们将再次获得流入 WTE 张量的贡献。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_80.png)

所以我们目前在代码中并没有共享 WTE，但我们想这样做。所以稍等，共享，方案。而实现这一点的一种方法是，让我们看看 Quapal 是否能理解。哦，能理解。好的。这是实现这一点的一种方式。基本上，相对简单。我们在这里做的是，我们正在获取 WTE。稍等。我们只是简单地重定向它以指向该元素。所以这基本上是复制数据指针。

对吧？它复制了引用。现在 WTE。稍等，变得孤立，它的旧值。而 pytorch 会清理它。所以我们只锁定了一个张量。它将在前向传播中使用两次。根据我的了解，这就是所需的全部。因此我们应该能够使用这个。这应该也能训练。

我们基本上只会使用这个完全相同的传感器两次。我们没有小心地跟踪可能性。但根据论文和结果，你实际上会期望这样做能获得稍微更好的结果。除此之外。

还有一个原因让我们觉得这非常好，那就是这有大量参数，对吧？

这里的大小是多少？是 768 乘以 50,257。所以这是 4000 万个参数。这是一个 1.24 亿参数的模型。所以 40 除以 124。因此大约 30% 的参数通过这种等待时间方案得以节省。所以这可能是它稍微工作得更好的原因之一。

如果你没有足够长时间地训练模型，因为等待时间，你不需要训练那么多参数。因此，你在训练过程中变得更加高效。因为你有更少的参数，并且你引入了这种归纳偏置，使得这两个嵌入在标记之间共享相似性。所以这是等待时间方案。而且我们节省了大量参数。

我们期待我们的模型因为这个方案而稍微表现得更好。好的，接下来。我希望我们在初始化时更加小心，并尽量遵循GPT-2初始化模型的方式。现在，不幸的是，GPT-2论文和GPT-3论文对初始化并没有非常明确的说明。因此，我们必须略微解读一下。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_82.png)

而不是去查阅论文（它相当模糊），在开放的应用程序发布的代码中有一些信息。因此，当我们查看model.py时，我们会发现他们在初始化权重时使用标准差为0.02。这是权重的正态分布，标准差为0。

对于偏置，他们将其初始化为零。当我们向下滚动时，为什么不滚动？令牌嵌入初始化为0.02，而位置嵌入初始化为0.01，出于某种原因。这些是初始化，我们希望在这里的GPT-2模块中镜像它。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_84.png)

这里有一个我很快想到的代码片段。这里发生的事情是在我们GPT模块的初始化器结束时，我们调用一个终止模块的apply函数，它会遍历该模块的所有子模块，并在其上应用权重函数。

所以这里发生的事情是我们在迭代所有模块。如果它们是线性模块，我们会确保使用标准差为0.02的正态分布来初始化权重。因此，在该层的偏置中，我们确保将其初始化为零。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_86.png)

请注意，偏置的零初始化实际上不是pie-torch的默认值。默认情况下，这里的偏置是用均匀分布初始化的。这很有趣，所以我们确保使用零。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_88.png)

对于嵌入，我们将使用0.02并保持不变。因此，我们不会将其更改为0.01用于位置嵌入，因为差不多。而且如果你查看我们的模型，唯一需要初始化且具有参数的层是层归一化。pie-torch的默认初始化将层归一化中的比例设置为1，偏移量设置为0。

所以这正是我们想要的。因此，我们将保持这种状态。如果我们遵循默认初始化。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_90.png)

它在哪里？他们发布的GPT-2源代码。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_92.png)

顺便指出，通常在这种初始化下的标准差，如果你遵循更重的初始化，将是进入该层的特征数量的平方根的倒数。但如果你注意到，0.02实际上与此一致，因为这些变压器内部的D模型大小大致为768，1,600等。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_94.png)

例如，768的平方根的倒数为0.03。如果我们代入1,600，得到0.02。如果我们代入三倍的值，得到0.014，等等。所以基本上，0.02大致上是这些初始化的合理值。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_96.png)

所以在这里硬编码0.02并不是完全疯狂，但你通常会希望有一些与模型大小增长的内容。但我们将保留这个，因为这是根据其源代码的GPT-2初始化。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_98.png)

不过我们在初始化上还没有完全完成，因为这里还有一个 caveat。因此，这里使用了修改过的初始化，考虑到模型深度下的残差路径的积累。我们通过因子1除以平方事件缩放了残差层初始化的权重，其中n是残差层的数量。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_100.png)

这就是GPT-2论文所说的。因此，我们还没有实现这一点，现在可以进行实现。现在，我想稍微激励一下他们在这里的意思。我认为大致是这样的。如果你在残差流中从0开始，记住每个残差流都是这种形式，我们会继续向其添加内容。

X是x加上某种贡献。因此，残差网络的每一个块都贡献一定的量并被添加。最终发生的情况是残差流中的激活方差增加。所以这是一个小例子。如果我们从0开始，然后进行100次，我们就有一个768个零的残差流。然后100次我们添加随机数。

这是一个正态分布，均值为0，标准差为1。如果我们向其中添加东西，那么最终残差流的标准差将增长到10。这仅仅是因为我们总是在添加这些数字。因此，他们在这里使用的缩放因子正好补偿了这种增长。

如果我们取n，并基本上将每个对残差流的贡献缩小到n平方根的倒数。因此，n平方根的倒数是n的负0.5，对吗？因为n的0.5是平方根，然后平方根的倒数是n的负0.5。如果我们以这种方式缩放，那么我们实际上得到1。这是一种控制前向传递中残差流内激活增长的方法。

因此，我们希望以相同的方式初始化，这些在每个块末尾的权重。GPT 论文建议按残差层数的平方根缩小这些权重。因此，有一种粗略的方法实现这一点。我不知道这是否被 PyTorch 官方认可，但对我来说是有效的。我们在初始化时都这样做。看，这并不特别。

NanogPT 的规模是 1。所以我们为这个模块设置了一个标志。在 PyTorch 中一定有更好的方法，对吧？但我不知道。好吧。所以我们基本上是在附加这个标志，并试图确保它不与之前的任何东西冲突。然后当我们到这里时，这个 STD 默认应该是 0.02。

但是如果它有这个东西的真实模块，那么 STD 乘以等于……什么？Copal 没有正确处理。所以我们希望是残差层数的平方根的倒数。因此这里的残差层数是 saltout 冲突层的两倍。然后这个乘以负 0.5。所以我们想缩小那个标准差。

这应该是正确的并实现了这一点。我应该澄清一下，层数的两倍来自于我们每个 transformer 层实际上有两个块添加到残差路径，对吧？

我们有注意力机制和 MLP。所以这就是两倍的来源。还有一点要提到的是，这有点尴尬，但我们不会去修复，因为我们在 WTE 和 LMA 之间共享权重。在我们旧的子模块的这个迭代中，我们实际上会对那个张量进行两次操作。

所以我们将首先将其初始化为嵌入，值为 0.02。然后我们会在全连接层中再次使用 0.02 初始化它。因为 LMA 当然没有缩放，所以它不会到这里。基本上会使用相同的初始化进行两次初始化。

但这没关系。然后在这里滚动，我添加了一些代码，以便我们可以设置种子，确保可复现性。现在我们应该能够使用 Python 训练 `GPT2.py` 并让它运行。就我所知，这是我们目前实现的 GPT2 初始化。因此，这看起来合理。好吧，目前我们有了 GPT2 模型。

我们有一定的信心它已正确实现。我们已正确初始化，并且有一个数据加载器正在迭代数据批次，我们可以训练。因此，接下来就是有趣的部分。我希望我们能大大加快训练速度。我们希望充分利用我们在这里使用的硬件。

我们将加快训练速度。现在你总是想先了解你有什么硬件？它提供了什么？

你是否充分利用它？所以就我而言，如果我们去看NVIDIA SMI。我们可以看到我有八个GPU。每个GPU都是8100 SXM 80GB。这就是我在这个盒子中可用的GPU。顺便说一下，当我用来启动这些类型的盒子时。

我最喜欢的地方是Lambda Labs。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_102.png)

他们确实资助我的开发和我的项目。但这是我最喜欢的地方。而且在这里，你可以启动这些机器，按小时付费。而且这非常简单。所以我喜欢启动它们，然后将S代码连接上去。这就是我的开发方式。现在我们来看看这里可用的8100。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_104.png)

8100 80GB SXM是我这里的GPU。我们这里有很多数字，说明你可以从这个GPU上期望得到多少计算。所以当我过来这里并且在这里打断时，所以是Python、True和Jupy。这样我就可以在计算logits和损失之后打断。我要你注意的有趣之处是，当我执行logits.detive时，这会打印一个torch。

float32。因此，在PyTorch中，当你创建张量时，默认情况下，所有的激活和网络参数等都是float32。也就是说，每一个数字、激活或权重等都使用32位的浮点表示。

这实际上占用了一些内存。实证结果表明，对于深度学习作为计算工作负载来说，这个是太多了。深度学习和这些网络的训练可以容忍显著较低的精度。并不是所有的计算工作负载都不能容忍小精度。例如，如果我们回到数据表。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_106.png)

你会看到这些GPU实际上支持高达FP64。这对于很多科学计算应用是非常有用的。我理解他们确实需要这个。但我们在深度学习训练中并不需要那么高的精度。所以目前我们在这里，FP32。根据目前的代码，我们预计最多能得到19.5 teraflops的性能。

这意味着我们正在进行19.5万亿次操作，浮点操作。所以这很可能是浮点乘法。这就是浮点操作。现在，注意，如果我们愿意降低精度。TF32是一种较低精度格式。我们稍后会看到。你实际上可以在这里获得8倍的提升。

如果你愿意降低到float 16或B float 16，你实际上可以得到16倍的性能，甚至达到312 teraflops。你会看到NVIDIA喜欢引用带有星号的数字。这个星号表示带稀疏性。但我们不会在代码中使用稀疏性。

我不知道目前这在行业中是否被广泛使用。因此，大多数人看到这个数字时没有稀疏性。你会注意到我们本可以获得更多的结果。但这是int 8。而int 8用于推理，而不是训练。因为int 8有一个。

它基本上具有均匀的间隔。我们实际上需要浮点数，以便更好地匹配在神经网络训练期间出现的正态分布，其中激活和权重分布为正态分布。因此，浮点数对于匹配这种表示是非常重要的。因此，我们通常不使用int 8进行训练，但用于推理。

如果我们降低精度，就可以从张量中获得更多的teraflabs。课程可在GPU中获得。我们稍后会谈到这一点。但除此之外，如果所有这些数字的表示位数更少，那么移动它们将会容易得多。这就是我们开始涉及内存带宽和模型内存的地方。

我们不仅有有限的位数容量供我们的GPU存储，而且还有访问这块内存的速度。你有一定的内存带宽。这是一种非常宝贵的资源。实际上，许多深度学习的训练工作负载是受内存限制的。

这实际上意味着进行所有这些极快的乘法运算的张量核心，大部分时间都在等待，它们处于空闲状态。因为我们无法快速为它们提供数据。我们无法快速从内存加载数据。因此，如果你的硬件利用率达到60%。

你实际上做得非常好。因此，在一个调优良好的应用中，张量核心的一半时间并不在进行乘法运算，因为数据不可用。因此，这里的内存带宽也极其重要。如果我们对所有浮点数、所有数字、权重和激活降低精度。

突然间需要更少的内存。因此我们可以存储更多，访问更快。一切都加速了，真是惊人。现在让我们享受其带来的好处。首先看一下张量浮点32格式。好吧，首先，张量核心是什么？

好的，张量核心只是A100架构中的一条指令，对吧？

它的作用基本上是执行一个小的4x4矩阵乘法。这只是4x4矩阵的矩阵乘法。关于这些矩阵的精度有多种配置。所以内部累加发生的精度是什么，输出精度又是什么。

还有精度等。这有几个开关，但基本上是一个4x4的乘法。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_108.png)

每当我们有需要矩阵乘法的操作时，它们就会被拆分为这条4x4乘法指令。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_110.png)

因此，所有这些都被分解为这条指令，因为这是乘法矩阵的最快方式。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_112.png)

结果发现我们上面做的大部分计算工作实际上都是矩阵乘法。大多数计算工作发生在线性层中。线性，线性，等等。中间夹杂着一些其他操作。有一些在残差中的加法，一些非线性，和一些层归一化。

等等。但如果你计时，你会发现这些几乎是没有的。基本上，内部变压器只是大量的矩阵乘法，特别是在这个小规模，1.24亿参数的模型中，实际上最大矩阵乘法无疑是顶部的分类器层。

这是一个巨大的矩阵乘法，从768到50,257。大致来说，这个矩阵乘法主导了网络中发生的其他所有操作。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_114.png)

所以矩阵乘法变得更快，这些操作隐藏在我们的线性层中。它们通过张量核心加速。至于张量核心的最佳参考，我会建议查看8100架构的白皮书。内容相当详细，但我认为如果你想了解发生了什么，阅读起来相对容易。

所以图九的张量浮点32。这基本上是TF32的解释，以及这里发生了什么。你会看到这里有许多可用的配置选项。那么输入操作数是什么精度呢？

累加器以及当你进行这个矩阵乘法的累加时，指令内部的基本表示。因此，这里中间的小向量乘法的中间加法都发生在FV32中。正如我提到的，这相较于我们获得的操作有8倍的提升。因此我们专门关注TF32这一行。

这个工作原理是，通常FV32有32位。TF32是完全相同的位数。我们有一个符号位，8个指数位。只不过尾数位在浮点数中被裁剪。因此，我们最终只有19位，而不是32位。因为最后的13位被截断，丢弃了。

所有这些都是指令内部的内容。因此，在我们的PyTorch中没有任何可见的东西。我们的PyTorch代码不会改变。所有数字看起来都是相同的。只是在硬件内部调用张量核心指令时，它会裁剪出这13位。这使得它能够显著更快地计算这个小矩阵乘法。

速度提高了8倍。当然，这种加速是有代价的。代价就是我们降低了精度。我们的累加器仍然是FV32。我们的输出是FV32。我们的输入是FV32。但是在内部，为了更快地执行操作，操作数会被截断。因此，我们的结果开始变得更加近似。

但实际上，当你进行训练时，你基本上无法分辨出差异。所以我喜欢TF32的原因是，如果你能容忍一点精度的妥协，那么这是可以接受的。你的代码并不会看到这个。它完全是操作内部的事情。对于你来说，这个操作就是快8倍，而且结果略微近似。

所以在优化方面，我会说这是一个相当不错的平衡点。让我们先看看这是什么样子。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_116.png)

所以我设置了我们的代码来计时迭代。导入时间。我改变了超参数，以便我们有一些更能反映我们想要运行的工作负载的东西。因为我们希望在最后进行一次相当大的运行。所以让我们使用批处理大小16。现在使用实际的GPT-2最大序列长度为1024个标记。

这是配置。在50次迭代中，我在这里做一些非常简单的操作。我在计时，获取当前时间。然后这是优化循环。现在我想计时这需要多长时间。与GPU合作时的一个问题是，当你的CPU运行时，它只是安排GPU上的工作。它在排序一些工作。

它发出请求，然后继续运行。有时会发生这种情况，我们快速通过这个过程，并且我们在GPU上排队了很多内核来运行。然后CPU似乎到了这里，并在那个时间花费了时间。但实际上，GPU仍在运行，因为它需要时间来处理计划运行的工作。因此，你只是为GPU建立了一个队列。如果需要，你可能想要等待开始去同步，这将等待GPU完成所有上述计划要运行的工作。然后我们实际上可以计时。所以基本上我们在等待GPU停止这一轮。

记录时间，然后我们将打印出来。所以在这里，我将运行训练循环。在右侧，我在监视NVIDIA SMI。所以我们从零开始。我们不使用GPU。然后我默认的PyTorch将使用GPU零。因此我们看到它逐渐填满。我们使用了80GB可用空间中的35GB。

然后在左侧，我们看到因为我们提高了批处理大小。现在在我们的小型莎士比亚数据集上，只需20个批次就能完成一个纪元。我们看到每次迭代大约需要1000毫秒。对吗？

所以第一次迭代有时会比较慢。这是因为PyTorch可能在第一次迭代时进行大量初始化。因此，它可能正在初始化所有这些张量和缓冲区来保存所有梯度。我不确定这里发生的所有工作。但这可能会导致较慢的迭代。当你在计时你的逻辑时，你总是要对此保持谨慎。

但基本上我们看到每次迭代大约需要1000毫秒。因此，根据现在的设置，这大约会运行50秒。这是我们在浮点32中的基线。我还想提到的是，如果这不适合你的GPU并且出现内存不足错误，那么请开始减少你的批量大小，直到适合为止。因此，而不是16。

尝试8、4或你需要的任何数字，以便将批量适配到你的GPU。如果你有更大的GPU，你实际上可以使用32等。默认情况下，你基本上想要最大化适合你GPU的批量大小。你想保持在合理的数字范围内。因此，使用包含大量2的幂的数字。

所以16是一个不错的数字，8、24、32、48。这些都是不错的选择。但不要使用像17这样的数字，因为这在GPU上会运行得非常低效。稍后我们也会看到这一点。所以现在，让我们坚持使用16和1024。我还添加了一件事，并再次运行，我是在训练过程中计算每秒的标记数。

因为随着时间的推移，我们可能会更改批量大小。但每秒的标记数是我们实际关心的客观指标。我们正在训练多少个数据标记？在优化中，我们得到的标记吞吐量是多少？

现在我们大约以每秒处理163,000个标记的速度进行训练。这是一个更客观的指标。好的，现在我们启用TF32。幸运的是，PyTorch为我们提供了相对简单的方式来启用TF32。你只需要写一行代码。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_118.png)

这就是重点。当我们查看PyTorch文档中的这个函数时，基本上这告诉PyTorch应该运行什么类型的内核。我相信默认情况下是最高的。最高精度用于矩阵乘法。这意味着一切都在Flow32中进行，就像之前一样。但如果我们设置得太高，就像现在这样。

矩阵乘法现在会在可用时使用TensorFlow32。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_120.png)

我的GPU是A100，所以是安培系列，因此TF32是可用的。如果你使用的是旧GPU，这可能对你来说不可用。但对我来说是可用的。因此，我期望PyTorch在我们看到的每个end-out线性地方，里面都会进行矩阵乘法。我期望这个矩阵乘法现在能够在TensorCore上运行。

利用TF32精度。这是我认为必要的单行代码修改。让我们重新运行这个。现在我们看到，根据承诺的吞吐量，我们应该大约获得8倍的提升。那么让我们看看会发生什么。这8倍来自于这里，对吧？

![](img/fffe638cf84a8d1e020ce63d0efeee6e_122.png)

8倍。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_124.png)

这也来自于这里的观察。156 T-flops，而不是19.5。好的，那么实际上发生了什么？所以我们看到我们的吞吐量大约是3倍，而不是8倍。我们从1000毫秒降至300毫秒。现在我们的吞吐量大约是每秒50,000个令牌。所以我们大约是3倍，而不是8倍。

那么发生了什么？基本上这里发生的是，很多工作负载都受限于内存。因此，尽管TF32原则上提供了更快的吞吐量，但所有这些数字仍然是float 32s。并且通过内存系统传输的都是float 32数字。

我们在移动所有这些数据时耗费了太多时间。因此，尽管我们使乘法本身变得更快，但我们受限于内存。我们并没有真正看到来自这个简化映射的全部好处。也就是说，我们的吞吐量快了3倍。这是免费的。在PyTorch中的一行代码。

你所有的变量仍然是float 32。它只是运行得更快。它稍微更近似，但基本上我们不会注意到。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_126.png)

所以这是TF32。好的，现在我们继续。我们已经对这一行进行了操作，并且看到我们可以在操作内部裁剪出一些精度。但是我们发现我们仍然受限于内存。我们仍然在移动这些浮点数，对吧？否则的话。我们因为这个付出了代价。

所以现在让我们减少要移动的数据量。我们将通过降至Bflow 16来实现。因此，我们将只保持每个浮点16位。我们将使用Bflow 16，我会稍后解释B16的差异，我们将处于这一行。

当我们回到A100的文档时，我们看到可用的精度。这是原始的F3.2。TF32裁剪掉了精度。然后在Bf16中，你会看到它与TF32非常相似，但在裁剪浮点数的尾数方面更为激进。

Bflow 16的重要之处在于指数位和符号位当然保持不变。所以如果你熟悉浮点数，我认为这可能应该是一个完整的视频。指数决定了你可以表示的数字范围。

精度是你对数字的精确度。因此，数字的范围是相同的，但我们在该范围内的可能性更少，因为我们截断了尾数，所以在该范围内的精度较低。这意味着事情实际上相当不错，因为我们拥有可以在浮点数中表示的原始数字范围。

但是我们只是对它的精度较低。与 Fb16 的不同之处在于，他们实际上触及并改变了范围。因此，Fb16 不能表示 Fb32 的全部范围。它的范围被缩小了。这就是你开始遇到问题的地方，因为现在你需要这些梯度缩放器等等。我不会在这个视频中详细讲解这些，因为那是一个完整的视频。

但是 Fb16 实际上历史上是第一个。它在安培架构之前就已经在 Volta 系列中可用。因此，Fb16 首先出现，每个人都开始训练 Fb16。但每个人都必须使用所有这些梯度缩放操作，这些操作有点烦人，并且是额外的状态和复杂性来源。

这样做的原因是因为 Fb16 中的指数范围被缩小了。这是 IEEE Fb16 规范。然后他们推出了 Bf16 和安培架构。他们让事情变得简单得多，因为我们只是截断了尾数。我们拥有完全相同的范围，并且不需要梯度缩放器。所以一切都简单得多。

现在，当我们使用 Bf16 时，我们实际上会影响在 PyTorch 代码中可能看到的数字。这一变化不仅限于操作本身。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_128.png)

所以让我们看看这是如何工作的。这里有一些文档，我认为这可能是解释如何在 PyTorch 中使用混合精度的最佳页面。因为还有许多其他教程，甚至在 PyTorch 文档中，这些都更加混乱。因此，我特别推荐这一篇，因为还有五个我不推荐的副本。

然后当我们来到这里时，忽略关于所有事情的一切。忽略关于梯度缩放器的一切，仅查看 Torx.autocast。基本上，这在最后变成了一行代码。因此，这是我们想要的上下文管理器。我们希望在我们的网络中使用它。

当你点击 Torx.autocasting 时，它会给你更多的指导。它告诉你不要在任何张量上调用 Bf16。只需使用 autocast，并仅围绕模型的前向传播和损失计算。这是你应该围绕的唯一两个部分。

不要碰回传和离线测量步骤。因此，这是 PyTorch 团队给出的指导。我们将遵循这一指导。对于我们来说，因为损失计算在模型的前向传播内部，所以我们将这样做。然后我们不想使用 Torx。

自动混合精度，因为如果我们这样做，我们需要开始使用梯度缩放器。因此我们将使用Bf16。这只有在Ampere架构上才能实现。但这意味着变化非常微小。基本上就是这一行代码。让我先插入一下，然后再实际运行。在logits之后。

我想向你展示，与我们看到的TF32不同，这实际上会影响我们的张量。因此这个logit张量，如果我们现在看看它的D类型，我们突然发现它现在是Bf16。它不再是flow 32了。因此我们的激活值已经改变。激活张量现在是Bf16。

但并不是所有的东西都改变了。模型。transformer。WTE。这是权重令牌嵌入表。它里面有一个点权重。这个权重的D类型，这个参数，仍然是torchflow 32。因此我们的参数似乎仍然在flow 32中。但我们的激活值，logits现在是Bf16。因此，这显然是我们获得混合精度的原因。有些东西PyTorch仍然保留在flow 32中。

有些东西PyTorch正在转换为低精度。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_130.png)

什么被转换，在哪个时点，并不是特别清楚。我记得往下滚动。是这里吗？

![](img/fffe638cf84a8d1e020ce63d0efeee6e_132.png)

好吧，我找不到它。我以为在这里。好吧，有一些文档说明你在使用这个AutoCast时，什么会被转换为Bf16，以及何时转换？

例如，只有这些类似矩阵乘法的操作会被转换为Bf16。但许多操作仍然在flow 32中。因此，特别是很多标准化操作，如层归一化等。并不是所有这些层都可能被转换。因此，只有某些层会选择性地运行Bf16。

但像softmax、层归一化、log softmax以及损失函数计算这样的东西。许多这些东西可能仍然在flow 32中，因为它们更容易受到精度变化的影响。主要的乘法对精度变化相对鲁棒。因此，网络的某些部分受到精度变化的影响程度不同。基本上。

只有模型的某些部分在减少精度下运行。让我们试试看，实际上看看我们在这里取得了什么样的改进。好的，我们以前是333毫秒。现在是300毫秒。我们以前每秒大约50,000个标记。现在是55,000。因此我们肯定运行得更快，但可能并没有快很多。

这因为RGP2中仍然有许多瓶颈。我们才刚刚开始。但我们已尽可能降低精度，使用的是A100 GPU。我们正在使用PyTorch AutoCast。不幸的是，我实际上并不知道PyTorch AutoCast到底做了什么。我也不确切知道Bflow 16中包含了什么，flow 32中又有什么。

我们可以深入探讨，但这是 PyTorch 内部的一些规则。不幸的是，他们没有很好地记录它。因此我们不会深入细节。但现在，我们在 Bflow 16 中训练。我们不需要梯度缩放器。

事情运行得更快的原因是我们现在能够在 Bflow 16 中运行张量课程。这意味着我们在这一行中。但我们也因此付出了精度的代价。因此，我们预期与原始 IP32 相比，结果会略微不准确。但从经验上来看，在很多情况下，这种权衡是值得的。因为它允许你运行得更快。

比如说，你可以训练更长时间，以弥补精度下降。所以目前的 Bflow 16。好的，正如我们所见，我们当前每次迭代大约需要 300 毫秒。现在我们将使用 PyTorch 工具箱中的一些强大武器。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_134.png)

特别是，我们将介绍 `Torch.compile`。所以 `Torch.compile` 实际上是 PyTorch 团队提供的非常棒的基础设施。它基本上是一个神经网络的编译器。就像是 C 和 C++ 代码的 GCC。这就是神经网络的 GCC。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_136.png)

这个工具不久前推出，使用起来非常简单。使用 `Torch.compile` 的方式就是这样。编译你的模型并返回它只需要一行代码。现在，这行代码会花费你一些编译时间。但正如你可能猜到的，这会使代码变得更快。所以我们实际运行一下。

因为这会花费一些时间来运行。但目前请记住，我们是 300 毫秒。我们将看看发生了什么。现在在运行时，我想稍微解释一下 `Torch.compile` 在幕后做了什么。所以随时可以阅读这页 PyTorch 的内容。但基本上没有什么好的理由让你不使用 `Torch`。

在你的 PyTorch 中使用 `compile`。我觉得如果你不这样做，你几乎应该默认使用它。除非你在调试并希望你的代码运行得非常快。这里有一行在 `Torch.compile` 中，我发现它实际上有助于解释为什么这更快。速度提升来自于减少 Python 的开销和 GPU 的读写。因此让我详细解释一下。

好的，我们到了。好的，从 300 毫秒开始。现在我们运行在 129 毫秒。所以这是 300 除以 129，约 2.3 倍。从 PyTorch 中的一行代码获得的改进，真是不可思议。那么到底发生了什么？在幕后发生了什么？当你将模型传递给 `Torch.compile` 时。

在这个结束模块中，我们所拥有的实际上是我们希望在网络中发生的算法描述。`Torch.compile` 将分析整个过程，并查看你希望使用的操作。凭借确切知道将会发生什么的优势，它不必在所谓的急切模式下运行。

它不必逐层进行，就像Python解释器通常从正向开始一样。Python解释器会说，好的，我们先执行这个操作，然后再执行那个操作。它在进行时逐步实现所有操作。

这些计算被调度并按顺序运行。而Python解释器和这段代码不知道后续将会发生什么样的操作。但Torch.compile能够同时查看你的整个代码，并能够知道你打算执行哪些操作，它会对这个过程进行优化。

我们首先要做的是完全移除正向传递中的Python解释器。它会将整个神经网络编译为一个单一对象，而不涉及Python解释器。因此它确切知道将要运行的内容，我们将直接运行它。所有内容都将以高效的代码运行。

第二件事是他们提到的读/写，这里简要提到过。所以我认为一个很好的例子是我们一直在研究的Galou非线性。在这里，我们使用NN Galou。现在这里是我基本上只是拆分NN Galou，你还记得这个公式。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_138.png)

![](img/fffe638cf84a8d1e020ce63d0efeee6e_139.png)

这里是与Galou内部发生的事情相等效的实现。在算法上，它是相同的。现在如果我们只是使用这个而不是NN Galou，会发生什么情况而没有Torch.compile呢？那么Python解释器会来到这里。然后它会说，好的，有一个输入。那么让我先将这个输入提升到三次方。

它将调度一个内核，该内核接收你的输入并提升到三次方。然后这个内核将运行。当这个内核运行时，输入被存储在GPU的内存中。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_141.png)

这里是一个有用的示例，展示了正在发生的事情。如果你的CPU，这是每台计算机都有的，有几个核心在里面，你有你的RAM和内存。CPU可以与内存通信，这一切都是众所周知的。但现在我们增加了GPU。GPU的架构稍微不同，当然它们可以通信。

它的不同之处在于它的核心数量比CPU多得多。所有这些核心在各自的层面上也都简单得多。但它也有内存，对，这种高带宽内存。抱歉，如果我说错了。HBM我甚至不知道那代表什么。我现在才意识到。

但这就是内存，基本上等同于计算机中的RAM。发生的事情是输入存储在内存中。当你执行输入的三次方时，这个数据必须传输到GPU的核心，以及这个GPU芯片上的所有缓存和寄存器。它必须将所有元素计算到三次方，然后将结果保存回内存中。

实际上，这段旅行时间导致了很多问题。所以这里记住这个内存带宽？我们可以以每秒大约两个太字节的速度进行通信，这非常多。但我们也必须穿越这条链接，而这非常慢。因此在GPU上，我们处于一个芯片上，一切都在芯片内部超级快速。

但是去内存是非常昂贵的，耗时极长。因此我们加载输入，进行计算并加载输出。这次往返需要很多时间。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_143.png)

现在在我们做完之后，我们会乘以这个常量。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_145.png)

所以接下来发生的是，我们调度另一个内核，然后结果回传，所有元素都与常量相乘。然后结果传回内存。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_147.png)

然后我们取结果并加回输入。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_149.png)

所以这一切又作为输入传送到GPU，并写回去。因此我们正在进行许多往返，从内存到实际进行计算的地方。因为所有的张量核心和算术逻辑单元（ALU）等都存储在GPU芯片上。所以我们进行大量的往返。PyTorch在不使用Torch Compile的情况下不知道如何优化这一点，因为它不知道你之后要执行的操作类型。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_151.png)

你只需告诉它将指数提升到三次，然后进行这个，再进行那个。它会按照顺序执行这些操作。但Torch Compile可以看到你的整个代码。它会来到这里，意识到所有这些都是逐元素操作。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_153.png)

其实我将要做的是，我会将输入单次传送到GPU。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_155.png)

然后对于每个元素，我会在内存仍在GPU上的时候进行所有这些操作。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_157.png)

或者说是它的一部分。然后我将只写回一次。因此我们不会有这些来回。这就是所谓的内核融合的一个例子，是加速的主要方式之一。因此基本上，如果你对概念有了明确的认识，并且知道你要计算的内容，你可以优化与内存的往返次数。你不会为内存带宽成本而付出代价。

这就是根本上让某些操作变得更快的原因。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_159.png)

那么他们这里所说的读写是什么意思呢？让我把这个擦掉，因为我们不再使用它。是的，我们应该使用Torch Compile，现在我们的代码显著更快了。我们每秒处理大约125,000个标记。但我们仍然有很长的路要走。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_161.png)

在我们继续之前，我想用几个数字补充一下讨论。因为这是一个复杂的话题，但值得高层理解这里发生了什么。我可能会用两个小时的整个视频来讨论这个，但只是简单预览一下。因此，这个芯片就是 GPU。这颗芯片主要进行旧计算。

但这颗芯片内部确实有一些内存。但绝大多数内存仍在高带宽内存 HBM 中，并且是相互连接的。它们基本上是两个独立的芯片。现在这里是 GPU 卡通图的放大图。我们在这里看到的是，第一，你可以看到这个 HBM。

我意识到对你来说可能非常小。但这里的侧面标记着 HBM。所以那是 HBM 的长度。现在 HBM 还是在芯片外。在芯片上有大量的流式多处理器。每一个都是一个 SM，总共有 120 个。

这就是大量计算发生的地方。这里是一个单个 SM 的放大图。它有这四个象限和 C，比如张量核心。这是矩阵乘法的主要部分。但还有其他单元执行不同类型的计算，比如 FV64 和 FV32。

以及整数等等。现在，我们在这里有所有逻辑进行计算。此外，芯片上还有分散的内存。因此，L2 缓存是芯片上的某种内存。而在 SM 本身上有 L1 缓存。我意识到对你来说可能非常小，但这个蓝条就是 L1。

还有寄存器。因此，这里存储了内存。但这种内存的存储方式与 HBM 中存储内存的方式截然不同。这是一种非常不同的实现，仅在硅的外观上来看。这是一个非常不同的实现。所以这里会使用晶体管和电容器。

这里的实现非常不同，使用 SRAM，外观也不同。但长话短说，芯片内部有内存，但并不多。这是关键点。因此，这是一个稍微不同的 GPU 示例图，就像这里一样。它显示了典型的 CPU D 或 M 内存的典型数值，即这里的东西。

你可能有一太字节的磁盘，但访问成本极高。特别是对于 GPU，你必须通过 CPU 来访问。接下来是 HBM。因此，典型的 GPU 上有数十 GB 的 HBM 内存。但正如我提到的，访问成本非常高。然后在芯片上，内部一切都是极快的。

但我们在整个芯片上只有几MB的内存。因此，空间非常有限，因为芯片上的内存非常昂贵。所以内存不多，但在相对条件下访问速度非常快。基本上，每当我们有这些内核时，实际发生的情况是我们获取这些默认存在于全局内存中的输入。

现在我们需要进行一些计算。因此，我们开始将数据从全局内存流向芯片。我们在芯片上进行计算，然后再将其流回并存储到全局内存中。如果没有torch compile，我们将在计算期间通过芯片流动数据并保存到内存。这种往返操作会进行很多次。

但如果是torch compiled，那么我们会像之前一样开始流动内存。但在芯片上，我们有一部分数据正在处理。因此，这部分数据现在驻留在芯片上。驻留在芯片上时，它的操作速度极快。如果我们有内核融合，我们可以在这里逐元素进行所有操作。

这些操作非常便宜。然后我们只需进行一次往返到全局内存。因此，操作融合基本上允许你将数据块保留在芯片上，并在写回之前对其进行大量计算。这带来了巨大的节省，这也是torch compile速度更快的主要原因之一。再次简单介绍一下内存层次结构以及torch compile为你做的大致工作。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_163.png)

现在，torch compile很惊人，但有些操作torch compile无法找到。一个惊人的例子就是flash attention，我们接下来会讨论它。flash attention源自2022年斯坦福大学的一篇论文。这是一个令人难以置信的算法，用于执行注意力机制，并且运行速度更快。

因此，flash attention会在这里来，我们将提取这四行。flash attention非常快速地实现这四行。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_165.png)

它是如何做到的呢？flash attention是一个内核融合操作。你可以在这个图中看到PyTorch和这四个操作。它们包括dropout，但我们这里不使用dropout。所以我们只有这四行代码。我们将其融合为一个单一的flash attention融合内核。

这是一个内核融合算法，但它是torch compile无法找到的内核融合。它无法找到的原因在于需要对这里实际实现的注意力机制进行算法重写。令人惊讶的是，如果你只计算flops，flash attention的flops数量比这里的注意力还要多。

但flash attention实际上要快得多。事实上，他们的速度可能快7.6倍。这是因为它非常关注内存层次结构，就像我刚才描述的那样。因此，它非常注意高带宽内存中有什么，共享内存中有什么。它在协调计算时非常小心，从而减少对高带宽内存的读取和写入。

因此，尽管我们执行了更多的浮点运算，但昂贵的部分是加载和存储到HBM中，而这正是他们所避免的。尤其是，他们从未实现这个端到端的注意力矩阵。这种ADT设计使得这个矩阵在任何时候都不会被实现，也不会被读写到HBM中。这是一个非常大的矩阵，因为这是所有查询和键交互的地方，我们在为每个头部获取信息。

对于每个批次元素，我们获得一个T乘T的注意力矩阵，即使对于单个头在单个批次索引中也是一百万个数字。因此，这基本上占用了大量内存，而且这个矩阵从未被实现。实现这一点的方法是基本上依赖于之前提出的在线softmax技巧，我稍后会向你展示那篇论文。这个来自前一篇论文的在线softmax技巧展示了如何增量地评估softmax，而不需要实现所有输入到softmax归一化的过程。

通过有这些中间变量M和L，你可以进行更新，使你能够以在线方式评估softmax。最近flash attention也发布了，所以我在这里也有那篇论文，里面有关于它如何计算flash attention的额外收获。

基于此的原始论文基本上是关于软max的在线归一化计算，值得注意的是它出自Nvidia，并且是在2018年初期提出的。这比flash attention早了四年。这篇论文提出了一种方法，可以减少内存访问次数来计算经典的softmax，并假设这种内存访问的减少应该能提高实际硬件上softmax的性能。

因此，他们在这个假设上是极其正确的，但对我来说很有趣的是他们来自Nvidia，并且他们有这种认识，但实际上并没有将其应用到真正的flash attention上，这要等到四年后才由斯坦福大学提出。

我并不完全理解这在历史上是如何发生的，但他们基本上在这里提出了对softmax的在线更新。这是他们在流式计算中重用的基本内容。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_167.png)

然后他们意识到他们实际上可以将所有其他操作与在线softmax计算融合成一个单一的融合内核闪存注意力，这就是我们即将使用的。因此，我认为这是一个很好的例子，意识到内存层次结构，flobs并不重要。

整个内存访问模式很重要，torch compile很棒，但仍然有许多优化可供我们使用，这些优化可能torch compile无法找到。也许有一天可以，但现在看来有点要求过高。所以我们要做的就是这样。

我们将使用闪存注意力，基本上在PyTorch中的方法是我们将注释掉这四行，并用一行替换它们。在这里，我们调用这个在PyTorch中的复合操作，称为scale.productattention。

当你以这种方式使用它时，PyTorch将调用闪存注意力。我实际上不太确定为什么torch compile不意识到这四行应该以这种确切方式调用闪存注意力。我们必须再次为此操作，这在我看来有点奇怪。不过我们在这里。所以你必须使用这个复合操作，让我们等几秒钟再等torch compile处理它。

然后让我们记住，我们达到了6.05661，我在这里有。这是我们期望看到的损失。在这次更改之前，我们花了130毫秒。因此，我们期望在第49次迭代时看到完全相同的结果，但我们预计运行时间会更快。因为闪存注意力只是算法重写，是一个更快的内核，但它实际上并没有改变任何计算，我们应该拥有完全相同的优化。

所以好的，我们快得多。我们大约在95毫秒左右，达到了6.058。所以它们基本上在浮点获取因子上是相同的。因此，计算是相同的，但从130毫秒缩短到大约96毫秒，速度显著更快。这是96除以130，因此这可能是27%的提升。

这真的很有趣，那就是闪存注意力。好的，我们现在要谈到我最喜欢的优化之一。这是同时最愚蠢和最聪明的优化，这总是让我有点惊讶。无论如何，基本上我几分钟前提到了一些数字，有些数字很好，有些数字很丑。

所以64是一个美丽的好数字。128更好。256也很美。使这些数字美丽的原因是其中包含了许多二的幂。你可以多次除以二。丑陋数字的例子是像13和17这样的质数，不是偶数的数字等等。

因此，你几乎总是想在所有与神经网络或CUDA相关的代码中使用好数字。因为CUDA中的一切都是以2的幂的方式运作，许多内核是以2的幂为基础编写的。还有许多大小为16和64的块等等。因此，一切都是以这些术语编写的，你总是需要处理所有输入不是由好数字组成的各种逻辑的特殊情况。

所以让我们看看这是什么样子。基本上扫描你的代码，寻找难看的数字是大致的启发式方法。所以三倍有点难看。我不是100%确定，也许这可以改进，但这是难看的，不理想。四倍是好的。所以这很好。1024是非常好的。这是2的幂。12有点可疑。2的幂不多。768很好。50,000到57确实是一个很。

真的很难看的数字。首先，它是奇数。而且里面的2的幂不多。因此，这是一个非常难看的数字，而且非常可疑。当我们向下滚动时，所有这些数字都很好，然后在这里我们有大多数不错的数字，除了25。因此在这个GPT-2 XL配置中，头的数量是25。这真是一个非常难看的数字。

这是一个奇怪的数字，实际上当我们最近试图优化一些内核以快速运行时，这确实给我们带来了很多麻烦。这需要处理一堆特殊情况。因此，基本上这些数字是……我们有一些难看的数字，其中一些比其他的更容易修复。特别是词汇量为50,000到57，这是一个非常难看的数字。

非常可疑，我们想要修复它。现在，当你修复这些事情时，简单的方法之一是基本上增加这个数字，直到它成为你喜欢的最接近的2的幂。因此，这里有一个更好的数字。它是50,000，304。为什么呢？因为50,000。

304可以被8、16、32和64整除。我想它甚至可以被128整除。因此，这是一个非常好的数字。那么我们在这里要做的是，这个是GPT配置，你会看到我们将词汇量初始化为50,000，257。让我们把那个元素覆盖为50,304。好的，其他的一切保持不变。我们只是增加我们的词汇量。

这几乎就像我们在添加虚假的令牌。因此，词汇量内部有2的幂。顺便说一下，我在这里所做的事情是我在增加我们的网络将要进行的计算量。如果你只是计算一下像是做数学的操作，我们将进行更多的flops。而我们仍然需要考虑这是否会破坏任何东西。

但是如果我只是运行这个，看看我们能得到什么。目前，这大概每步运行在96.5毫秒。我只是大致估算一下。让我们看看我们会得到什么结果。在这个编译的过程中，让我们思考一下我们的代码实际上是否有效。好的。

当我们以这种方式增加词汇量时。让我们看看词汇量实际上是在哪里使用的。因此，我们转到初始化，当然看到它在嵌入表中使用。所以在变换器的底部，并且它在变换器顶部的分类器层中使用。所以在两个地方。让我们看看，我们运行在 93。因此是 93 毫秒，而不是 96.5。

因此，通过进行更多计算，我们看到这里有大约 1% 的提升。原因是我们修复了，将一个丑陋的数字变成了一个好数字。让我再稍微解释一下。但现在，让我们首先相信，当我们这样做时不会破坏任何东西。所以首先。

我们已经制作了 WTE，即用于令牌的嵌入表。我们将其做得更大。几乎就像我们在底部引入了更多的令牌。这些令牌从未被使用，因为 GPT 的分词器仅包含 50,000 到 56 的令牌。因此，我们永远不会索引到我们添加的行。

因此，通过创建永远不会被访问、永远不会被使用等的内存，我们在这里浪费了一些空间。虽然这并不完全正确，因为这个 WTE 权重最终被共享，并在最后的分类器中使用。

那么这对分类器有什么影响？我带来了。好吧。这的确是，我们现在正在预测分类器的额外维度。我们正在预测那些当然永远不会出现在训练集中的令牌的概率。因此，网络必须学习这些概率必须被驱动到零。

因此，网络产生的 logits 必须将输出的这些维度驱动到负无穷大。但这与我们数据集中已经存在的其他令牌并无不同，或者说是数据集中不存在的令牌。所以莎士比亚可能只使用了大约 1,000 个令牌中的 50,000 到 57 个令牌。因此，大多数令牌已经通过优化被驱动到零概率。

我们刚刚引入了一些更多的令牌，它们以类似的方式将永远不会被使用，并且必须在概率上被驱动到零。因此，从功能上来说，尽管没有什么破坏。我们使用了稍微多一点的内存。但就我所知，这是一项无害的操作。不过，我们增加了计算，但它运行得更快。

它运行得更快，因为正如我在 CUDA 中提到的，许多内核使用块大小。这些块大小通常是好数字，即 2 的幂。因此，计算是以 64 或 32 的块进行的。当你想要的计算不能完美地适应这些块大小时。

有各种边界内核可以启动来处理最后一部分。因此在很多内核中，它们会截断你的输入，并首先处理漂亮的部分。然后它们有一个完整的第二阶段，回来处理剩余的部分。用于处理剩余部分的内核可能非常低效。

所以你基本上是在利用额外的计算资源，这样非常低效。因此你不如填充输入，让它看起来更合适。通常从经验上来看，这样的做法实际上运行得更快。这是我们增加的另一个4%改进的例子。

这也是Torch Compile没有为我们找到的东西。你会希望Torch Compile在某个时候能够找出这样的优化。但目前就只有这些。同时我也要指出，我们正在使用PyTorch的夜间版本。所以这就是为什么我们只看到了4%的提升。如果你使用的是PyTorch 2.3。

如果是1或更早的时候，你实际上会看到大约30%的提升。仅仅是因为从50,000改到57再到53.04。所以，这也是我最喜欢的例子之一，理解底层是如何运作的，以及知道应该调整哪些内容来推动代码的性能。好的，到目前为止，我们的性能提高了大约11倍。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_169.png)

因为我们最初每一步约为1000毫秒，而现在降到了93毫秒。所以这是相当不错的。我们在利用GPU资源方面做得更好了。现在我将转向更多的算法改进和优化本身。我们希望遵循在GPT2或GPT3论文中提到的超参数。

现在遗憾的是，GPT2并没有详细说明太多内容。他们发布模型权重和代码是非常不错的。但论文本身在优化细节方面非常模糊。发布的代码也一样。代码主要关注的是推理部分，并没有训练代码，超参数也非常少。

因此这并没有告诉我们太多。因此我们需要参考GPT3的论文。在GPT3论文的附录中，他们为我们提供了更多的超参数。而且GPT3论文在模型训练的所有小细节方面要详细得多。但GPT3模型从未发布。因此我们有GPT2的权重，但没有细节。

在GPT3中，我们有很多细节，但没有权重。大致而言，GPT2和GPT3的架构非常相似。基本上变化很少。上下文长度从1024扩展到2048，这大致就是主要的变化。

变压器的一些超参数发生了变化。但否则它们基本上是同一模型。只是 GPT-3 在更大的数据集上训练了更长时间，并进行了更全面的评估。GPT-3 模型的参数为1750亿，而 GPT-2 只有16亿。简而言之，我们将查看 GPT-3 的论文，以跟踪一些超参数。

为了训练所有版本的 GPT-3，我们使用的 Adam 优化器的 beta 1 和 beta 2 分别为0。9和0。95。接下来我们确保 beta 参数，这里默认值为0。9，而0。999实际设置为0。9和0。95。然后可以看到，epsilon 参数的默认值为1 -8，这也是1 -8。我们明确设置一下。接下来他们说我们将梯度的全局范数裁剪到1。

0。这里提到的是，在我们计算完损失的梯度后执行`loss.backward()`，我们基本上就得到了所有参数张量的梯度。人们通常会将这些梯度裁剪到某种最大范数。因此在 PyTorch 中，这个操作相对简单。我们只需在计算梯度后插入一行代码。

这个工具函数的作用是计算参数的全局范数。对所有参数的每一个梯度进行平方，然后将其全部相加，最后取平方根。这基本上就是参数向量的范数。如果你愿意，也可以将其视为长度。

我们基本上确保它的长度不超过1。0，并且我们将对其进行裁剪。人们喜欢使用这个方法的原因是，有时在优化过程中可能会出现不幸情况。也许是一个坏的数据批次或其他问题。如果在某个批次中非常不幸，可能会得到非常高的损失，而高损失会导致非常高的梯度。

这可能会冲击你的模型并冲击优化过程。因此，人们喜欢使用梯度范数裁剪来防止模型在梯度大小和上界方面受到过大的冲击。这有点像一种应急解决方案，类似于在更深层次问题上打补丁，但人们仍然相当频繁地使用它。现在，`clip_grad_norm`返回的是梯度的范数，我总是喜欢将其可视化，因为这是一条有用的信息，有时你可以查看梯度的范数。

如果情况良好，那就没问题。如果出现波动，那就糟糕了，并且在训练过程中会不稳定。有时范数会出现尖峰，这意味着存在某种问题或不稳定性。因此这里的范数将是一个范数，我们将设定为0。4或其他值。我相信这只是一个浮点数。因此我们应该能够打印出来。

这就是全局梯度裁剪。现在他们开始详细介绍学习率调度器。因此，他们并不像我们在这里使用的三负四那样仅使用固定学习率，而是基本上使用了余弦衰减学习率调度。

它有预热，并且在某个范围内以10%的比例进行余弦衰减。因此我们很快就会实现这个。我只想看到这里打印的范数。好的，来了。所以这里发生的事情是，开始的30个阶段范数实际上非常高。你会看到随着训练的继续，它会在低于1的值附近稳定下来。

在最初的几个阶段，范数高并不是那么罕见。基本上这里发生的事情是模型完全随机。因此在网络的早期有大量的学习发生。但这种学习主要是学习输出token的偏差。

这段时间有点不稳定，但网络通常在很少的迭代中会稳定下来。所以这对我来说看起来相对合理，尽管通常我会觉得它有点奇怪。我们从28降到6再到2，然后又升到10。这并不是完全疯狂，但就是有点奇怪。好吧，让我们现在来看学习率调度器。

在GPT-3中使用的学习率调度被称为余弦衰减学习调度，带有预热。这个过程的方式是，学习率基本上从零开始，线性上升一段时间，然后以余弦的形式下降，最终达到一个最低学习率，这个最低学习率由你来决定。

在这里，最低学习率是零，但在论文中他们提到使用了余弦衰减。在前2600亿个token中，学习率下降到其值的10%，然后训练继续10%之后。并且在前375百万个token中有线性预热。这就是关于学习率的情况，现在我们来实现这个。

我已经在这里实现了这个功能，首先让我向下滚动一下。我稍微修改了我们的训练循环，这个循环在最大步骤中是4i。我将其改为step，现在我们有了单次优化步骤的概念。

然后在PyTorch中设置学习率，我认为这是设置学习率的方法。这个过程有点复杂，因为优化器中可能存在不同的参数组。尽管我们目前只有一个参数组，但实际上我们需要遍历它们。而且你必须以这种4循环的方式来设置学习率，这是我目前的印象。

所以我们有这个本地的学习率，我们设置学习率，然后底部也打印它。这就是我对这个循环所做的所有更改，当然get LR是我的调度器。值得指出的是，PyTorch实际上有学习率调度器，你可以使用它们，我相信PyTorch中有余弦学习率调度。我只是并不特别喜欢使用那段代码，因为老实说，它只有五行代码，我完全理解这些代码内部发生了什么。

所以我不喜欢使用那些几乎无法治愈的抽象概念，然后我不知道他们在做什么。这是个人风格。因此这里的最大学习率可以说是三负四，但我们将会在GPT-3中看到。他们有一个表格，列出了每种模型规模的最大学习率。对于这个模型，基本上是12层768的GPT-3。

所以GPT-3小模型大致相当于GPT-2的1.24亿参数。我们在这里看到他们使用的学习率是六E负四。所以我们实际上可以设定更高。事实上，我们可能希望尝试遵循这一点，并将最大学习率设置为六。然后根据论文的描述，最大学习率的中间学习率是其10%。

一些步骤将会在此升温，然后是优化的最大步骤，我现在在下面的for循环中也使用了。然后如果你喜欢，可以查看这段代码，它并不是特别有趣。我只是根据迭代次数调节应该使用的学习率。这是升温区域。这是优化后的区域，然后这是介于两者之间的区域，这是我计算余弦学习率调度的地方。

如果你愿意，可以详细了解这一点，但这基本上是在实现这个曲线。我已经运行过这个，这是它的效果。所以当我们现在运行时，从一个非常低的数字开始。请注意，我们并不是从零开始，因为用学习率为零进行更新是没有用的。

这就是为什么有一个加一项，这样在零次迭代时我们并不是使用完全的零，而是使用一个非常非常小的值。然后我们线性地升温到最大学习率，在我的运行中是三负四，但现在是十六负四。

然后它开始衰减，直到三负五，当时是原始学习率的10%。现在我们没有完全遵循他们所提到的。我看看能否再找到一次。他们提到他们的训练范围是3000亿个token。并且在2600亿时降低到初始学习率的10%，然后在2600亿之后以10%进行训练。

基本上，他们的衰减时间小于最大步数的时间，而对我们来说它们是完全相等的。所以这并不是完全忠实，但对我们来说还可以。这对我们目前的目的来说是可以接受的，我们就打算这样使用。我老实说不认为这有什么太大的区别。我应该指出，使用什么学习率调度完全取决于你自己。

所以我们有两种不同类型。余弦学习率在GPT 200和GPT 3中被广泛宣传，但人们提出了各种各样的其他学习率调度。这实际上是一个活跃的研究领域，研究哪种方法在训练这些网络时最有效。好的，下一个。论文讨论了逐渐增加批量大小。

所以批量大小的提升是线性的，你从非常小的批量大小开始，随着时间的推移逐渐增加到较大的批量大小。我们实际上会跳过这个步骤，不会使用它。我不喜欢使用它的原因是，它使许多算术变得复杂，因为你在优化的每一步都在改变处理的令牌数量。我喜欢保持这些数学非常简单。此外，我的理解是，这并不是一个重大的改进。

我同样理解，这并不是一种算法优化改进。它更像是系统和速度的提升。我实际上这么说是因为在优化的早期阶段。再次强调，模型处于一个非常不典型的环境中，你大部分学到的只是学会忽略那些在训练集中出现频率很低的令牌。你学到的是非常简单的偏差和类似的东西。

因此，你放入网络的每个示例基本上只是告诉你使用这些令牌，而不要使用那些令牌。因此，来自每个示例的梯度实际上是高度相关的。它们在优化的最初阶段看起来大致相同，因为它们都在告诉你这些令牌不出现，而这些令牌会出现。所以因为这些梯度非常相似且高度相关，为什么你要使用像几百万这样的批量大小，而如果你使用32K的批量大小，你基本上在训练早期获得的梯度是完全相同的呢？

然后在优化的后期，一旦你学会了所有简单的内容，实际的工作才开始，这时每个示例的梯度变得更加去相关。那时候它们在某种意义上确实提供了统计能力。

所以我们将跳过这一部分，因为它有点复杂。我们将讨论在训练过程中数据是无替换抽样的。直到达到一个训练周期的边界。无替换意味着它们不是从某个固定的池中抽样然后训练一个序列，而是消耗掉这个池。当它们抽取一个序列时，直到下一个训练周期，它就消失了。

我们已经在这样做，因为我们的数据加载器遍历数据块。因此没有替换。它们在下一个周期之前不会再次被抽取。所以我们实际上已经在这样做了。所有模型使用0.1的加权衰减来提供少量正则化。因此让我们实现加权衰减。

你可以看到我已经进行了更改。特别是，我没有在这里直接创建优化器，而是在模型内部创建一个新的配置优化器函数，并传入一些超参数。那么让我们看看配置优化器开关应该返回优化器对象。好的。

看起来复杂，但其实非常简单，我们只是非常小心，并且这里有一些设置需要通过。与这一行相关的最重要的事情是，你会看到这里有一个加权衰减参数，我将其传递进去。

我将其传递给一个叫做优化组的东西，最终会进入加法和W优化器。这里默认使用的加权衰减是0.01，因此比GPT 3论文中使用的低10倍。因此，加权衰减最终会传递到加法和W 3优化器组中。

现在在这个函数中还有什么其他的事情发生？这里发生的两个重要事情是我将参数分为应该进行加权衰减和不应该进行加权衰减的参数。特别是，通常不对偏置和其他类型的一维张量进行加权衰减。

一维张量在节点衰减参数中，这些还包括层归一化、尺度和偏置。对这些进行加权衰减没有太大意义。你主要希望对参与矩阵乘法的权重进行加权衰减，并且可能希望对嵌入进行加权衰减。在之前的视频中，我们已经讨论了为什么进行权重衰减是有意义的，因为你可以将其视为一种正则化，因为当你降低所有权重时，你迫使优化。

使用更多的权重，而不允许任何一个权重单独变得过大。你迫使网络在多个通道之间分配权重，因为在权重本身上有一种引力的池。

这就是我们在这些方面进行区分的原因。我们只对嵌入和参与权重进行衰减。我们打印出正在衰减和未衰减的参数数量。大多数参数将会衰减。然后我们在这里做的另一件事情是我正在进行另一个优化。

之前的Adam W没有这个选项，但PyTorch后来的版本引入了它。这就是我用一个检查来保证的原因，这个检查基本上是在确认Adam W中是否存在融合选项。如果存在，我会使用它并在这里传递它。因为一些早期版本没有fused equals。所以这里是Adam W的fused equals。

它以前是不存在的，后来才添加的。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_171.png)

这里有一些文档解释发生了什么。基本上，他们表示默认情况下不使用融合，因为它相对较新，我们希望给它足够的高峰时间。因此默认情况下不使用融合，但在可用时融合会快得多，尤其是在CUDA上运行时。

这段内容的作用是替代在循环中迭代所有参数张量并更新它们。这样会启动很多内核。所以融合的意思是，它将所有这些内核融合成一个单一的内核。你消除了很多开销，只需一次调用内核来更新所有的参数。因此，这基本上是对Adam W更新的内核融合，而不是对所有张量进行迭代。

这就是我喜欢使用的配置优化器函数。我们可以重新运行。我们不会看到与之前的显著差异，但我们会看到一些来自这里的打印信息。让我们看看它们的样子。我们看到衰减张量的数量为50，且大部分是参数。非衰减张量的数量为98，主要是层归一化参数中的偏差。

这些参数总共有100,000个，所以大部分已经衰减。我们正在使用Adam W的融合实现，这会更快。因此，如果你可以使用它，我建议你使用。我并不完全确定为什么它们不默认为此，似乎相对无害。

而且因为我们使用了融合实现，我认为这就是我们减少运行时间的原因。注意到每步的运行时间为93毫秒，现在因为使用融合的Adam W优化器降低到了90毫秒。因此，在这里我们引入融合Adam，以提高时间效率，并添加或更改权重衰减。但我们仅对二维参数进行权重衰减。

嵌入和参与线性运算的矩阵。这就是这一点，我们可以将其拿出来，是的，这就是这一行的内容。在继续之前，我想快速指出权重衰减、学习率、批量大小、Adam参数、beta 1、beta 2、epsilon等之间的关系。

这些是在优化文献中非常复杂的数学关系。在本视频中，我只是尝试复制OpenAI使用的设置。但这是一个复杂的主题，深入且复杂，因此在本视频中我只是想复制这些参数，因为详细讨论这些内容是完全不同的视频，而不是仅仅提供高层次的直觉。接下来我想讨论的是这段文字，顺便提一下，当我们改进数据加载时，我们将回到这里。

目前我想回到这个表格，您会注意到对于不同的模型，我们当然有不同的变换器参数，决定变换器网络的大小。我们也有不同的学习率，因此我们看到更大网络的训练使用稍低的学习率。

我们还看到这个批量大小，在小型网络中使用较小的批量大小，而在大型网络中使用较大的批量大小。现在对我们来说的问题是，我们不能仅仅使用0.5百万的批量大小，因为如果我试图进入这里，设置这个B，B在哪里？

B等于我该如何称之？好的，B等于16。如果我尝试设置。我们必须小心，不能是0.5百万，因为这是以标记数为单位的批量大小。我们的每一行是1024个标记，所以0.5 e6 1百万除以1024。这大约需要488的批量大小，所以问题是我不能将其设置为488，因为我的GPU会崩溃。

这肯定无法适应。但是我们仍然想使用这个批量大小，因为正如我提到的，批量大小与所有其他优化高参数和学习率等相关。因此，我们希望准确表示所有超参数，因此需要使用0的批量大小。

大约5百万。但是问题是，如果我们只有一个小GPU，如何使用0.5百万？

为此，我们需要使用所谓的梯度累积，因此我们将转向这一点，它允许我们以串行方式模拟任何任意的批量大小。因此，我们可以使用0.5百万的批量大小，这需要运行更长时间，我们必须处理多个序列，并基本上将它们的所有梯度相加以模拟0的批量大小。

500万。所以让我们接着这个话题。好的，我在这里开始实现，正是通过添加这些代码行。基本上我做的是首先设置我们期望的总批量大小。因此，这正好是0.5百万，我使用了一个好的二的幂的数字，因为2的19次方是52428。所以大约是0.5百万的一个漂亮数字。现在我们的微批量大小，如我们所称的，现在是16。

所以这将是，我们仍然有B乘以T在输入到变换器中进行前向反向传播，但我们不会进行更新，对吧？我们将进行多次前向和反向传播。所有这些梯度都会在参数梯度上进行累加。

因此，我们将进行前向反向传播的梯度累积步骤次数，然后在所有累积完成后进行一次更新。因此，特别是我们的微批量大小现在只是控制我们在一次前向反向传播中处理多少tokens和多少行。

所以我们在进行16乘以24的计算。我们正在处理16。每次前向和反向传播有384个tokens，而我们应该处理2的19次方。哎呀，我总共是在处理2的19次方吗？所以梯度累积的次数将是32。因此，这里的梯度累积将计算为32，我们需要进行32次前向反向传播，然后进行一次更新。

现在我们看到单次前向反向传播大约需要100毫秒。因此，进行32次将使每个步骤大约需要三秒钟，这只是个粗略估算。所以这是梯度累积步骤，但现在我们确实想要实现这一点。我们将转到我们的训练循环，因为现在这部分和这部分的前向和反向传播。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_173.png)

现在我们需要在进行接下来的所有操作之前重复这一过程32次。让我们看看如何实现这个。我们每次确实需要加载一个新的batch。让我把这个移过来，现在这是我们进行内部循环的地方。所以对于微步（micro step）在梯度累积步骤范围内，我们进行这个操作，并记住反向传播的损失总是会累积梯度。

因此，我们在反向传播的损失中。梯度总是会进行累加。因此，在每一次反向传播的损失中，梯度会在梯度累积器上相加。所以我们进行反向传播，然后获取所有的梯度，然后进行归一化，其他一切都应该跟着进行。所以我们非常接近，但实际上这里存在一个微妙而深刻的问题，这实际上是错误的。

因此我邀请你思考一下为什么这还不足够，然后让我来解决这个问题。好的，我带回了Jupyter笔记本，这样我们可以在一个简单的玩具环境中仔细考虑这个问题，看看发生了什么。让我们创建一个非常简单的神经网络，它接受一个16个数字的16维向量，并返回一个单一的数字。然后在这里，我创建一些随机示例X和一些目标Y，然后我们使用均方损失来计算损失。

基本上这就是四个独立的示例，我们仅对这四个示例执行简单的回归，使用均方损失。现在当我们计算损失并对其反向执行并查看梯度时，这就是我们获得的梯度。

现在损失目标注意到，MSC损失的默认设置是均值作为损失函数的减少方式。所以我们在计算这四个示例的平均损失。这样就是确切的损失目标，这里是四个示例的平均值，因为这里有四个独立的示例。然后我们有这四个示例及其均方误差，平方误差，这样就得到了均方误差。

因此我们计算平方误差，然后将其标准化以使其成为示例的均值，这里有四个示例。所以现在当我们进入梯度累积版本时，这里是它的梯度累积版本，我们有四个活动步骤，并且我重置了梯度。

我们有四个步骤，现在我在单独评估所有示例，并多次对它们进行反向调用损失。然后我们查看由此获得的梯度。因此基本上现在我们前向计算我们的函数，计算完全相同的损失。反向执行，并且我们这样做了四次。当我们查看梯度时，你会注意到梯度不匹配。

所以在这里我们做了一个四个的单一批次，而这里我们进行了四个批大小为一的梯度累积步骤，梯度并不相同。基本上，它们不相同的原因正是因为这个均方误差在这四分之一中丢失了，而这个损失也丢失了，因为在这里每个循环的损失目标仅是均方误差。

然后在这种情况下，由于只有一个示例，所以就只有这一项。因此这是零次迭代的损失，在第一次、第三次等中都是一样的。然后当你进行反向损失时，我们在积累梯度。而积累的梯度基本上等同于损失的求和。

因此我们这里的损失实际上是没有外部四分之一因子的。这意味着我们缺少标准化因子，因此我们的梯度都是。所以解决这个问题的方法，或者其中一种方法是我们可以来到这里并说损失等于损失除以四。现在发生的事情是我们在缩放我们的损失，我们在所有这些地方前面引入了四分之一。

所以所有的单个损失没有按四分之一缩放。然后当我们反向传播这些损失时，它们会累积成一个总和，但现在每个组件内部都有一个四分之一。因此我们的损失将是等价的。当我运行这个时，你会看到梯度现在是相同的。长话短说，通过这个简单的例子，当你逐步查看时，你可以看到基本上这不正确的原因是因为与MSC损失在这里的情况相同。

我们在模型中计算的损失也使用均值归约。那么，损失在哪里呢？所以我们有交叉熵，默认情况下交叉熵的归约我不知道为什么他们不显示，但它是所有B和T元素的均值损失。

因此这里有一个通过均值的归约，如果我们只是进行梯度累积，我们就会遗漏这一点。因此，解决这个问题的方法是简单地补偿梯度累积步骤的数量，我们可以同样地将这个损失除以它。

特别是这里我们进行的步骤数量是损失等于损失除以梯度累积步骤。因此即使是助手也会得到这个修改。但是同样，我们确实在缩小损失，以便当我们进行反向传播时，这基本上对应于目标中的一个总和。

我们正在汇总已经标准化的损失，因此当我们将损失除以梯度步骤时，我们恢复了额外的标准化因子。因此现在这两个将等价于原来的优化，因为梯度的结果是相同的。

好的，我需要再进行几次调整，并且我在这里启动了优化。特别是我们想做的一件事是因为我们想要优雅地打印内容，所以首先我们需要创建一个损失的累加器。我们不能只打印损失，因为我们只会在最后一个微步打印最终的损失。因此我们有一个初始为零的损失，然后我将损失累加到它上面。

我正在使用`detached`，这样我就可以将张量从图中分离出来，并且我只是试图跟踪这些值。所以当我添加它们时，我在创建这些叶节点。这就是损失的来源，我们在这里打印，而不是打印损失。此外，我还必须考虑梯度，因此这看起来相当不错。如果你想验证你的优化和这里的实现是正确的，而你在旁边工作。

现在由于我们有总的后备大小和梯度累积步骤，我们的 B 设置纯粹是性能优化的设置。所以如果你有一个大 GPU，你实际上可以将这个值增加到 32，这样可能会快一点。如果你有一个非常小的 GPU，你可以尝试八或四，但无论如何，你都应该获得完全相同的优化和相同的答案，直到浮点误差，因为梯度累积启动并可以按需处理所有内容。

所以关于梯度累积就这些。我想可以了。好了，现在是时候拿出重武器了。你可能注意到，到目前为止，我们只使用了一个 GPU 进行训练，但实际上我在这里支付了 AGPs。所以我们应该让它们全部投入工作。特别是它们将同时协作并优化标记，并进行通信，以便它们都在优化上进行协作。为此，我们将使用 PyTorch 的分布式数据并行。

还有一个遗留的数据并行，我建议你不要使用，那种有点像遗留技术。分布式数据并行以非常简单的方式工作。我们有 AGPs，因此我们将启动八个进程，每个进程将被分配一个 GPU。对于每个进程，训练循环和我们到目前为止工作的所有内容看起来几乎都是相同的。

每个 GPU 只是在处理我们到目前为止构建的内容。但现在秘密地有八个，它们都会处理数据的略微不同部分。我们将再添加一个部分，一旦它们都计算出梯度，还有一个部分是我们对这些梯度进行平均。因此，这就是它们在这里协作处理计算工作负载的方式。

因此，为了使用所有八个，我们不再仅仅用 PyTorch train GPT2.py 启动我们的脚本。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_175.png)

我们将通过一个名为 Torchrun 的特殊命令在 PyTorch 中运行它。稍后我们会看到它。Torchrun 在运行我们的 Python 脚本时，实际上会确保并行运行八个进程。它创建这些环境变量，使每个进程都可以查找基本上是哪个进程。因此，例如，Torchrun 将设置排名、局部排名和世界大小的环境变量。

这是一种不好的检测 DDP 是否正在运行的方法。所以如果我们使用 Torchrun。如果 DDP 正在运行，那么我们必须确保 good 是可用的，因为我不知道你是否还能在 CPU 上运行这个，或者这样做是否有意义。这是一些设置代码。重要的部分是有一个世界大小，对我们来说是八。这是正在运行的进程总数。还有一个排名，每个进程基本上会在大致相同的时间运行完全相同的代码。

但是所有进程之间的唯一区别是它们都有不同的DDP等级。因此，GPU零的DDP等级为零，GPU一的等级为一，依此类推。否则，它们都在运行完全相同的脚本。只是在DDP等级上会有一个略微不同的整数，这样我们才能协调它们不在同一数据上运行。

我们希望它们在不同的数据部分上运行等。现在，local rank仅在多节点设置中使用。我们只有一个带GPU的单节点，因此local rank是单节点上GPU的等级。例如，从零到七。但对我们来说，我们主要会在单个节点上运行，所以我们关心的是等级和世界大小。

这就是八个，而这将取决于这个特定脚本实例运行的GPU。现在，在这里我们确保根据local rank设置设备为CUDA列，列指示如果有多个GPU时使用哪个GPU。

所以根据这个进程的local rank，它将使用适当的GPU，以避免哪些GPU被哪个进程使用的冲突。最后，我喜欢创建一个布尔变量，即DDP等级等于零。

主进程是任意的进程编号零，它负责大部分打印、日志记录、检查点等。其他进程主要被视为辅助计算进程。因此，主进程零会有一些额外的工作要做。所有其他进程主要只做前向和后向传播。

如果我们不使用DDP，并且这些变量都未设置，我们将退回到单GPU训练。因此，这意味着我们只有rank零，世界大小仅为一，我们是主进程，我们尝试自动检测设备，这一切都是正常的。

到目前为止，我们所做的就是初始化DDP，在我们稍后看到的与Torchrun一起运行的情况下，将会有八个副本并行运行，每个副本都有不同的等级。现在我们必须确保之后的一切都正确进行。运行多个进程的棘手之处在于，你总是要想象将有八个进程并行运行。

所以，当你阅读代码时，你需要想象有八个Python解释器在这些代码行中运行。它们之间唯一的区别是它们有不同的DDP等级。它们都在这里，选择完全相同的种子。它们进行所有这些计算时，完全不知道其他副本的存在。

所以它们都进行相同的计算，现在我们必须调整这些计算，以考虑到实际上有一个特定的世界大小和某些等级。因此，特别是这些微批次和序列链接，这些都是针对每个GPU的。

现在将会有非进程并行运行。所以我们必须调整这个，因为梯度步长现在将是总大小除以时间T乘以DDP角色大小。因为每个进程将执行B乘以T，而它们的数量正是这样。因此，除此之外，我们还希望确保这很好地适配到总批量大小中。

对我们来说，情况是这样的，因为16乘以124乘以8 GPU。是131 K和524288。这意味着我们的梯度将为当前设置。对。因此，每个GPU中将会有16乘以124个进程，然后有一个GPU，所以我们将会在HPUs上进行131,000个标记的单次前向反向计算。

所以我们希望确保这适配得很好，以便我们可以得出一个良好的梯度累积步骤。是的，让我们在这里调整一下注释。TDP角色大小。好的。所以每个GPU计算这个。现在这里就是我们开始遇到问题的地方，每个进程都会打印，它们都会打印。因此，我们将会有这八个打印的副本。

处理这个问题的一种方法正是我们所拥有的这个主进程变量，因此如果是主进程，就要进行保护。这只是为了我们只打印一次，因为否则所有进程都计算了相同的变量，没有必要打印八次。

在进入数据加载器之前，我们显然需要重构它。也许在这一点上我们应该做一些打印，简单地运行一下并在此退出，所以导入sys。然后在打印中使用sys.exit。我是GPU DDP等级。我是GPU DDP等级，并且按打印。所以现在让我们试着运行一下，看看这怎么运作。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_177.png)

所以让我们运行一下，看看它看起来是什么样的。因此，通常我们习惯于启动Python。像这样更改D2.py。现在我们将使用tort run运行，这就是它的样子。所以tort run独立进程数，例如对我们来说是八，因为我们有一个GPU，然后更改D2.py。所以这就是命令的样子，而tort run将再次运行八个这样的进程。

那么我们来看看会发生什么。所以首先有点忙，所以这里发生了很多事情。首先，分布式系统有一些警告，我实际上不知道这些是否意味着什么。我认为这只是代码正在设置中，进程正在上线，我们看到在进程启动时初步收集失败。我不确定这一点。但我们开始看到实际的打印。

所有进程都停止了，然后第一个打印实际上来自进程五，纯粹是偶然。然后它打印了，所以进程五基本上最先到达这里。它说我是 GPU five 的进程，然后这些打印来自主进程。所以进程五无论出于什么原因首先完成，这完全取决于操作系统如何调度进程运行。

然后 GPU zero 结束，接着是 GPU three 和 two。然后可能是进程五或类似的东西退出了。DDP 实际上不喜欢这样，因为我们没有正确处理多 GPU 设置。因此，进程组在我们销毁之前并没有被销毁。所以它对此不太满意，在实际应用中我们希望调用进程组。

这样我们就可以正确清理 DDP。因此，它对此也不太满意。然后最后一个 GPU 完成了。这就结束了。所以基本上我们无法保证这些进程何时运行，这完全是任意的，但它们是并行运行的。我们不希望这打印出来。接下来让我们把这擦掉。

接下来我们希望确保在创建数据时，我们需要让它意识到多进程设置。因为我们不希望所有进程都加载完全相同的数据。我们希望每个进程获取自己的一部分数据，以便它们都在不同的数据集部分上工作。因此，让我们进行调整。实现这一点的一种特别简单而又深刻的方法是确保我们将 rank 和 size 传递给数据加载器。

然后我们来到这里，我们看到现在获取了 rank 和进程并保存它们。现在当前的位置不会是零，因为我们想要遍历所有进程。实现这一点的一种方法是基本上取 cell that B 乘以 cell that T，然后再乘以进程 rank。所以进程 rank zero 将从零开始，但进程 rank one 现在从 B 乘以 D 开始。

进程 rank two 从两个 B 乘以 D 开始。这就是初始化。现在我们仍然以相同的方式进行，但是现在当我们推进时，我们不再按 B 乘以 T 进行推进。我们按 B 乘以 T 乘以进程数量进行推进。所以我们消耗的总令牌数是 B 乘以 T 乘以进程数量，它们都会发送到不同的 rank。

位置必须按整个块推进。然后在 B 乘以 T 乘以 cell that number of processes 加一处将超过令牌数量。然后我们将循环。当我们循环时，当然希望以完全相同的方式循环。所以我们有点像是重置回去。这是我能找到的最简单的更改，适用于一种非常简单的分布式数据加载器。

你会注意到如果进程等级是零，然后进程是一个，那么整个过程将与我们之前的一模一样。但现在我们可以实际运行多个进程，这应该运行良好。这就是数据加载器。好的，接下来一旦它们都初始化了数据加载器。它们就会来这里，并且都创建一个GPT模型。因此我们在八个进程上创建了八个GPT模型。

但是因为这里的种子是固定的，它们都创建了相同的模型。它们都将模型移动到各自的设备上，并且都编译模型。因为模型是相同的，所以会并行发生八次相同的编译，但这没关系。现在这一切都不会改变，因为这是逐步进行的，我们目前在步骤内工作，因为我们所做的所有更改都像是步骤内的更改。

现在这里重要的是，当我们构建模型时，我们实际上需要做一些工作。获取logits已经被弃用，因此需要创建模型。我们实际上需要将模型包装到分布式数据并行容器中。这就是我们如何将模型包装到DDP容器中。这些是DDP的文档，而且相当详细。

这里有很多注意事项和需要小心的地方，因为一旦涉及多个进程，一切都会复杂化十倍。但大致来说，这个设备ID我认为必须传入。现在不幸的是，关于设备ID的文档是极其不明确的。因此，当你实际上来这里时，关于设备ID的注释大致上是无意义的。

但我很确定这里应该是DDP的本地等级，而不是DDP等级，而是本地等级。所以这是你在这里传入的内容。这会包装模型，特别是DDP为你做的就是在前向传播中它实际上表现得一模一样。因此，我的理解是前向传播中不应该有任何更改。但在反向传播中，当你在最简单的设置中进行反向传播时，一旦每个独立GPU的反向传播结束。

每个独立的GPU都有所有引导词的梯度。而DDP为你做的就是，一旦反向传播经过它，就会调用所谓的“全局归约”。它基本上是在所有梯度的等级上进行平均，然后将这个平均值存储到每个等级上。

所以每个等级最终会得到一个平均值。因此，基本上这就是通信，它只是同步并平均梯度，这就是DDP为你提供的。现在DDP实际上稍微复杂一点，因为在你进行反向传播时，它实际上可以在反向传播仍在进行时调度梯度的通信。因此，梯度的通信与它们的同步以及反向传播之间有重叠。

这样做更有效率。所以这就是 DDP 为你做的。前向传播没有改变，反向传播大部分没有改变，而我们将添加这个平均值，稍后会看到。好的，现在让我们来看看优化。这里没有变化。让我们来看看优化的内部循环，并思考在 DDP 中这些梯度的同步。

所以基本上默认情况下，正如我提到的，当你在这里做反向传播时，它将进行反向传播，然后同步梯度。这里的问题是由于梯度累积步骤的循环。我们实际上不想在每一次反向传播后进行同步，因为我们只是逐步累积梯度。我们希望它们累加，不想每次都同步。

这将是极其浪费的。因此，基本上我们希望将它们相加，然后在最后一步当微步骤变为 gradicum steps-1 时，仅在最后一步我们希望实际上进行梯度平均的过度操作。为了做到这一点，我们来到这里，官方推荐的方式是使用这个不进行同步的上下文管理器。

所以 PyTorch 说这是一个上下文管理器，用于禁用 DDP 进程之间的梯度同步。在这个上下文中，梯度将被累积，基本上，当你不进行同步时将没有通信。因此，他们告诉我们在 DDP 中不进行同步，进行梯度累积。累积梯度，然后他们要求我们再次使用另一个输入进行 DDP 并进行反向传播。

我真的不喜欢这样。我真的不喜欢。你必须在这里复制粘贴你的代码并使用上下文管理器，这太难看了。所以当我查看这个源代码时，你可以看到当你进入时，简单地切换这个变量。这个所需的反向梯度同步正在被切换和更改。

这是一个变量，基本上如果你逐步执行它，会被切换以确定梯度是否会被同步。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_179.png)

所以我实际上就是喜欢直接使用它。因此，我喜欢做的是以下操作。在最后的反向传播之前，如果我们正在使用 DDP，那么基本上我们只想同步。我们只希望这个变量在最后一次迭代时为真，而在微步骤内部的所有其他迭代中希望为假。所以我这样切换它。所需的反向梯度同步应仅在微步骤是最后一步时打开。

所以我直接切换这个变量，希望这会影响`lost.backward`。这是件麻烦事，因为他们可能会改变DDP，这个变量就会消失。但现在我相信这样做有效，并且可以避免使用上下文管理器和代码重复。我只是切换变量，然后`lost.backward`不会同步大多数步骤，而是同步最后一步。

一旦结束，每个进程会突然神奇地拥有存储在所有进程上的所有梯度的平均值。现在我们必须考虑这是否是我们想要的，是否足够，以及它如何与损失和`lossacoom`一起工作。

让我们思考一下。我的问题是我们已经对梯度进行了平均，这很好，但`lossacoom`尚未受到影响。这在DDP容器外，所以没有被平均。因此在这里打印`lossacoom`时，显然我们只会在主进程（rank 0）上打印。这只是打印它在该进程中看到的损失。

我们想要打印所有进程的损失和该损失的平均值，因为我们已经对梯度进行了平均，所以我们也想要损失的平均值。因此在这里，这段代码是我过去使用的。我们需要使用`lossacoom`而不是`lossac`。如果是DDP，那么这是一个PyTorch分布式，我导入了什么呢？哦天哪，这个文件开始变得难以控制。

所以如果这样导入`torch.distributed`，那么调用`all_reduce`。我们对`lossacoom`进行平均，因此这个`lossacoom`张量在所有进程中都存在，当我们调用`all_reduce`进行平均时，它会创建这些数字的平均值，并将该平均值存储在所有进程中。

所以在此调用后，所有进程现在将包含平均的`lossacoom`。当我们在主进程中打印时，`lossacoom`在所有其他进程中也是相同的。因此在主进程中，我们想要像这样打印。好吧，最后我们必须小心，因为我们还没有处理更多的标记，所以是DDP世界大小的倍数。

这是我们处理的标记数量。其他一切应该没问题。唯一需要注意的是，如我提到的，你想要销毁进程组，以便我们对`nickel`友好，当我们退出时DDP不会抱怨。

这应该就是了。让我们试试吧。好吧，我启动了脚本，它应该会马上打印。我们现在使用8个GPU同时训练。因此梯度累积步骤不是32，而是8的除数，现在是4。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_181.png)

否则，优化的样子就是这样。哇，我们的速度真快，现在每秒处理150万个token。这是一些严肃的数字。而且小莎士比亚数据集是如此之小，以至于我们可能会进行很多轮次。但这大致就是它的样子。顺便说一下，我需要修复的一件事是，这是一个配置优化器的模型，现在不再有效，因为模型现在是DDP模型。

所以这必须成为配置优化器的原始模型，而原始模型是在这里创建的东西。因此，在我将模型包装成DDP之后，我必须创建原始模型，在DDP的情况下，它是存储原始模型的模块，以及我们所拥有的GPT2模块。

这个包含我们想要调用的配置优化器函数。因此，这是我需要修复的一件事。否则，这似乎运行正常。现在你会注意到，当你实际比较这个运行及其数字与单个GPU的运行时，你会发现这是一个32 radacum的单个GPU运行。数字并不会完全匹配。

这发生的原因有些无聊。原因是，在数据加载器中，我们基本上是以略微不同的方式迭代批次。因为现在我们在寻找一整页数据。如果对于所有GPU来说，该块超出了token的数量，我们就会循环。

实际上，单个GPU和GPU处理将以略微不同的方式重置。因此，我们的批量略有不同，因此我们得到略微不同的数字。但说服自己这是可以的一个方法是，将总批量大小减小得更小，B和A T。

然后，我想我使用了4倍的1.24乘以8。所以我使用了32.768作为总批量大小。然后我确保单个GPU可以进行8个出色的仿真步骤，然后是多GPU。这样可以减少数据加载器的边界效应，你会看到数字是匹配的。简而言之，我们现在的速度非常快。

优化大致与GPT 2和3的混合参数一致。我们已经超越了我们的小莎士比亚文件，并希望对其进行升级。那么让我们接下来看看GPT 2和GPT 3使用了什么数据集。GPT 2使用了这个从未发布的webtext数据集。

有一个试图重现它的项目叫做open webtext。简单来说，他们在论文中提到，这创建了来自Reddit的所有外部链接，并且至少有三次karma。这大致是他们的起点，他们收集了所有的网页及其文本。因此，这有4500万个链接，最终形成了40GB的文本。

大致上，GPT-2关于其数据集的描述是这样的。基本上是来自Reddit的外部链接。现在当我们转到GPT-3时，有一个训练数据集的部分，这时他们开始讨论常见的网络爬虫数据。实际上，常见网络爬虫使用得更多。其实我认为GPT-2也谈到了常见的网络爬虫。但基本上，这个数据集本身并不是一个很高质量的数据集，因为它非常嘈杂。

这是互联网的一个完全随机的子集，情况远比你想象的糟糕。因此，人们非常努力地过滤常见的网络爬虫数据，因为其中有好的内容。但大部分内容就像是广告垃圾、随机表格和数字以及股票行情。整个数据非常混乱。因此，人们喜欢在这些经过精心策划的数据混合物上进行训练。

这些数据混合物中的一大部分通常会是常见的网络爬虫数据。例如，50%的标记将来自常见的网络爬虫数据。但在GPT-3中，他们还使用了之前的网页文本。他们没有使用网页文本来获取外部链接，但他们还添加了例如书籍和维基百科的内容。

你可以决定添加很多其他的东西。现在这个GPT-3的数据集也从未发布。所以今天我所熟悉的一些相当好的数据集将会在这些方面具有代表性。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_183.png)

我们的第一数据集是红色睡衣数据集，或者更具体地说是红色睡衣数据集的精简子集。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_185.png)

这是一个干净且去重的版本。在另一种意义上，它又是一堆常见的网络爬虫数据。C4也如我所知，是更多的常见网络爬虫数据，但处理方式不同。然后我们还有GitHub、书籍、档案、维基百科、堆栈变化。这些就是会进入这些数据混合物的数据集类型。

我特别喜欢最近发布的一个数据集，叫做精细网络数据集。这是一个基本上收集高质量常见网络爬虫数据并过滤到15万亿标记的尝试。此外，最近Hugging Face发布了这个精细网络教育子集，其中包含1.3万亿的教育内容和5.4万亿的高质量教育内容。

基本上，他们试图对常见的网络爬虫数据进行过滤，以获得高质量的教育子集。这就是我们将使用的内容。这里有一篇关于精细网络的长网页，他们详细讲述了如何处理这些数据，这非常值得一读。

如果你对数据混合物以及这些规模下的数据如何处理感兴趣，我绝对推荐你查看这个页面。更具体地说，我们将使用精细网络教育子集，我认为它基本上是来自互联网的教育内容。他们展示了在教育内容上进行训练的效果非常好。

我们将使用这个样本的10亿个标记子样本，因为我们不会在万亿个标记上进行训练。我们只会在10亿个样本上训练，来自fine web edu，因为在我之前的几个实验中经验表明。

这实际上足以让性能接近GPT二，并且操作起来相对简单。所以我们来处理样本的10亿个标记。我们的目标是下载、处理，并确保我们的数据加载器可以使用它。让我们开始吧。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_187.png)

好吧，我在这里引入了另一个文件，基本上会从hugging face数据集中下载fine web edu。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_189.png)

它会对所有数据进行预处理和预标记化，并将数据片段保存到本地磁盘的一个文件夹中。因此，在此运行时，我想简要提到你可以通过数据集查看器大致了解这里有什么内容。

这还挺有趣的。它看起来运行得相当不错。比如它在讨论法国的核能，它在讨论墨西哥美国，还有一些Mac，Pi。J的等等。因此，实际上看起来他们的过滤器运作得相当好。顺便提一下，这里的过滤器是自动应用的，使用的是llama 370 B，我相信。因此基本上。

LLM正在判断哪些内容是教育性的，这最终会通过过滤器。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_191.png)

这真不错。关于脚本本身，我不会详细讲解完整的脚本，因为它没有那么有趣，也不那么以LLM为中心。但是当你运行这个时，首先，我们会加载数据集，这都是hugging face的代码。

然后我们需要用`pip install datasets`来安装数据集。所以它正在下载数据集。然后它正在对这个数据集中的所有文档进行标记化。现在，当我们对文档进行标记化时，你会注意到，要标记一个单独的文档，我们首先用文本结束标记开始标记。正如你所知道的，这在GPT2标记器中是一个特殊标记，所以是50。

256是文本结束的ID。这是一个文档开始的标志。尽管它被称为文本结束，但这是开始文档的第一个标记。然后我们扩展所有该文档的标记。接着我们用这些标记创建一个NumPy数组。我们确保所有的标记都在之间。哦。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_193.png)

好吧，让我调试一下。好的，抱歉。这与我在Python中使用浮点除法有关，必须是整数除法，这样这就是一个整数，一切都很好。好的，但基本上这里的标记化相对简单。返回MP.UN16格式的标记。我们使用UN.16来节省一些空间，因为2的16次方减1是65000。

因此，GPT2 的最大代币 ID 远低于此。而且这里有一堆多进程代码，老实说，这并不是那么令人兴奋，所以我不打算逐步讲解。但是我们在加载数据集，我们在对其进行标记，并将所有内容保存到分片中，分片是 NumPy 文件。

所以只需存储一个 NumPy 数组，它与 Torx 张量非常相似。第一个分片，0 0 0 是验证分片，所有其他分片都是训练分片。正如我提到的，它们都有 1 亿个代币，完全一样。这使得与我们合作时处理分片文件更容易，因为如果我们只有一个巨大的文件，有时在磁盘上处理会比较困难。

从这个角度来看，分片是相当庞大的。是的，我们就让它运行吧。这可能需要大约 30 分钟，然后我们将回到实际在这些数据上进行训练。我们实际上将进行一些合法的预训练。这是一个很好的数据集。我们每秒处理大量的代币。我们有 HEPUs。

代码已经准备好了，所以我们实际上将进行一次严肃的训练运行。我们等一会儿回来。好的，我们回来了。所以如果我们 LSE 查找 web，我们会看到现在里面有 100 个分片。这是合理的，因为每个分片有 1 亿个代币。所以 100 个分片总共有 100 亿个代币。现在切换回主文件。

我再次对我们的数据加载器进行了调整。这是因为我们不再使用莎士比亚。我们想使用查找 web 的分片。所以你会看到一些代码，基本上可以加载这些分片。我们加载 un16 的 NumPy 文件，将其转换为 torch.long 张量，这正是上面许多层默认期望的格式。

然后这里我们只是列举所有的分片。我还为数据加载器添加了一个拆分功能，因此我们可以加载拆分的训练数据，但也可以加载拆分的验证数据，零拆分。然后我们可以加载分片，这里我们不仅有当前的位置，还有当前的分片。所以我们在一个分片内有一个位置，当我们在单个分片中耗尽代币时。

我们首先推进分片，并在需要时循环。然后我们获取代币并重新调整位置。所以这个数据加载器现在也会遍历所有分片。所以我做了这些更改，然后在数据处理的同时，我们的训练加载器现在当然有拆分训练。

在这里，我设定了一些数字。所以我们每步处理 2 的 19 次方个代币。我们想大约处理 100 亿个代币，因为这就是我们拥有的唯一代币数量。所以如果我们处理 100 亿个代币，再除以 2 的 19 次方，我们会发现这是 19,073 步。这就是来源。然后 GPT 三论文提到他们在 3.75 亿个代币上预热学习率。

所以我来到这里，375 e6 代币除以 2 的 19 次方是 715 步。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_195.png)

所以这就是为什么热身步骤设置为 715。所以这将完全匹配 GPT-3 使用的热身计划。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_197.png)

我觉得 715 顺便说一下是非常温和的，这可以显著变得更具攻击性。可能像 100 就足够了。但没关系。我们就先这样，以便我们有 GPT-3 的确切超参数。所以我修复了这个，然后差不多就这样。我们可以运行。

所以我们这里有我们的脚本，我们可以启动。实际上抱歉。让我再做一件事。抱歉。对于我的 GPU，我实际上可以适应更多的批量大小，我相信我可以在我的 GPU 上适应 64 作为微批量大小。所以让我尝试一下。我可能记错了。但这意味着每个 GPU 64 乘以 24，然后我们有一个 GPU。所以这意味着我们在这个适配的累积中甚至不会表现得很好，因为这只是乘以了完整的总批量大小。

所以没有更好的累积。如果适合的话，那将运行得非常快。我是说如果这有效，那么这基本上就是一个严肃的交易运行。我们没有记录。我们没有评估验证集。我们还没有进行任何评估。所以并不是说我们已经划好我们的“t”，点好我们的“i”。

但如果我们让它运行一段时间，我们实际上会得到一个相当不错的模型。而且这个模型可能与或优于 GPT-2 在 24 M 上。好的。所以看起来一切都在良好增长。我们每秒处理 150 万个标记。这里一切看起来不错。我们每次迭代花费 330 毫秒，而我们必须做的总数是。

我们在哪里打印 1973？所以 19.073 乘以 0.33 是这么多秒。这么多分钟。因此这将运行 1.7 小时。像这样的一个半小时的运行。而且我们甚至不需要使用较好的累积，这很好。而且你可能没有在你的 GPU 上享受这种奢侈。在那种情况下，只需开始减小批量大小，直到适合为止。

但保持为好的数字。所以这非常令人兴奋。我们目前正在加热学习率，所以你看到它仍然非常低。负 4 的 1。因此这将在接下来的几步中逐渐升高，达到这里的 6E 负 4。非常酷。所以现在我想做的是让我们划好“t”，点好“i”。

让我们在验证集上进行评估。让我们尝试弄清楚我们如何运行 E valves。我们如何进行记录，如何可视化我们的损失和所有好的东西。所以在我们实际进行运行之前，让我们先处理这个。好的。所以我调整了代码，以便我们在验证分片上进行评估。

所以通过传入 split 等于 val 来创建验证器。这基本上会为验证分片创建一个数据加载器。我在数据加载器中做的另一件事是引入了一个新函数 reset，它被称为 init，基本上重置数据加载器。

这非常有用，因为当我们进入主要训练时。这是我添加的代码。基本上每进行100次迭代，包括第0次迭代，我们将模型置于评估模式。我们重置验证器，然后不涉及梯度。我们基本上将在大约20个步骤中累积梯度，然后将其平均并打印出验证损失。

基本上，这和训练的逻辑是完全一样的，但没有反向传播的损失。这里只是推理。我们只是测量损失。我们将其累加。其他一切都是一样的，和我们之前看到的一样。所以每100次迭代会打印验证损失。

包括第一次迭代。这很好，这将告诉我们一些关于我们过拟合程度的信息。不过，考虑到我们大致拥有无限的数据，我们基本上预期训练和验证损失会大致相同。但我对这个感兴趣的另一个原因是，我们可以使用GPT-2。

OpenAI发布的124M。我们可以从中初始化，基本上可以看到它在验证损失上达到什么样的损失。这为我们提供了一种指示，说明该模型在124M上的泛化能力。但这不是一个非常公平的比较，因为GPT-2是在完全不同的数据分布上训练的。

但这仍然算是一个有趣的数据点。无论如何，你总是希望在这样的训练过程中有一个验证集，以确保你没有过拟合。尤其是在我们进行更多迭代时，这点尤其需要关注。

例如，现在我们只进行一次迭代。但如果我们想训练10次迭代或类似的情况，我们会非常小心，可能我们会过度记忆那些数据。如果我们的模型足够大，而我们的验证集是判断这种情况的一种方式。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_199.png)

好的，此外，如果你记得在我们脚本的底部，我们有一些从很久以前留下来的孤立代码进行采样。所以我删除了那段代码并将其移动到这里。因此，我们偶尔评估验证。偶尔我们进行采样，生成样本。然后我们只在每100个步骤中进行一次训练。

这就是我目前的结构。我已经运行了1000次迭代。这里是关于1000次迭代的一些样本。你好，我是一个语言模型，我无法变得更具创意。从一个语言模型来看，你在这里学习的语言是计算机的开始。

好的，这一切都还是有点混乱。但我们现在只有 1000 的比率，我们刚刚达到最大学习率。所以这仍然是一个学习阶段。我们将在 100 次之后获得更多样本。好的。这是，模型仍然还是个小宝宝。好的。

基本上，我在这里放的所有采样代码应该对你来说都很熟悉，并且来自之前的内容。我所做的唯一事情是我在 PyTorch 中创建了一个生成器对象，这样我就可以直接控制随机数的采样。因为我不想影响用于训练的全局随机数生成器的 RNG 状态。

我希望这完全在训练循环之外。因此，我使用了一个特殊的采样 RNG，并确保为每个单独的 rank 设置了不同的种子。然后我在这里传入我们在多项式中消费随机数的地方，采样在这里发生。我确保在这里传入生成器对象。否则，这与之前是完全相同的。

现在你会注意到我们运行得稍微慢一些。这是因为我实际上不得不禁用 torch.compile 才能进行采样。因此我们运行得稍微慢一些。所以出于某种原因，它在没有 torch compile 的情况下可以工作，但当我使用 torch compile 我的模型时，我收到了 PyTorch 的一个非常可怕的错误，而我现在不知道如何解决。因此，可能等你看到这个代码发布或类似的情况时，也许它已经修复了。

但现在我只是打算将其设置为 false。我将重新启用 torch compile，你不会得到样本。我想我稍后会修复这个。顺便说一下，我会发布所有这些代码。实际上，我在每次添加内容时都非常小心地进行 Git 提交。因此，我将发布从零开始到现在的整个代码库，包括之后的部分。

所以一切都应该在 Git 提交历史中准确记录。因此，我觉得这样很好。希望等你去 GitHub 时，这个问题会被解决，并且它可以正常工作，我会修复这个 bug。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_201.png)

好的，所以我在这里运行优化。它正在进行，我们大约在第 6,000 步，所以训练大约进行到 30%。现在在训练期间，我想介绍一个评估方法，我们将用它来补充验证集。

这是 Hellesfag 评估。Hellesfag 来源于 2019 年的一篇论文，因此它现在是一个 5 年前的评估。Hellesfag 的工作方式是，它基本上是一个句子完成数据集。它是一个多项选择题。对于这些问题中的每一个，我们都有一个共享的上下文，比如一个女人在外面带着一个桶和一只狗。狗在跑来跑去，试图避免洗澡。她，A。

用肥皂提起水桶并用吹风机吹干狗的头。B。用水管保持它不变得肥皂泡沫。C，弄湿狗，它又跑掉了。或者D。和狗一起进入浴缸。所以基本上这个想法是这些多个选项构造得使得其中一个是句子的自然延续，而其他的则不是。其他选项可能没有意义，比如使用水管保持它不变得肥皂泡沫。

这毫无意义。因此，训练不好的模型无法区分这些，但具有丰富世界知识的模型能够创建这些完成。

这些句子来源于活动网和Wookieow。在论文底部有一个很酷的图表，展示了Wookieow中的各种领域。因此有很多关于计算机、电子、家庭和花园的句子。它涵盖了你需要了解世界的广泛知识，以便找到最可能的完成及其身份。

关于最后一项工作的一个有趣之处是，其构造方式是错误选项是故意来源于对抗性。因此它们不仅仅是随机句子。实际上它们是由语言模型生成的句子。

而且它们的生成方式使得语言模型基本上觉得它们很难，但人类觉得很简单。他们提到人类在这个集合上的准确率为95%，但当时最先进的语言模型只有48%。因此那时这是一个很好的基准。现在你可以阅读这篇论文的细节来了解更多。不过要指出的是，这已经是五年前的事了，自那以后Halaswag完全被解决了。

现在这里的语言模型准确率为96%，所以最后的4%可能是数据集中的错误，或者问题真的很难。因此这个数据集在语言模型方面有点不足，但当时最好的语言模型只有大约50%。

但这就是事情发展的程度。不过，喜欢Halaswag的人喜欢它，而它并未在GPT-2中使用，但在GPT-3中有Halaswag evil。很多人使用Halaswag。因此对于GPT-3，我们这里有被引用的结果。我们知道GPT-3在所有这些不同模型检查点的Halaswag evil的准确率。

所以“早期信号”意味着即使是小型语言模型也会从25%的随机机会开始，但它们会慢慢提高，你会看到25，26，27等。即使模型非常小，而且还是很早的时候，你也可以看到缓慢的改进。

所以它是顺畅的，具有“早期信号”，并且存在很长时间了。这就是为什么人们喜欢这种方法。现在我们评估它的方式如下。如我所提到的，我们有一个共享的上下文，这有点像多项选择任务。但不是给模型一个多项选择问题并询问它选择A、B、C或D。

我们无法做到这一点，因为这些模型在如此小的情况下，如我们所见。这些模型实际上无法进行多项选择。他们不理解将标签与多项选择中的选项关联的概念。他们不明白这一点。因此，我们必须以原生形式提供给它们，而原生形式是令牌完成。所以在这里，我们构造了一个四行和T个令牌的批次，无论这个T是什么。

然后共享的上下文基本上是四个选择的上下文。这些令牌在所有行之间共享。接着我们有四个选项，所以我们把它们摆放在一起。只有一个选项是正确的，在这种情况下是标签三，选项三。

因此，这就是正确的选项，选项一和二是我们的并行选项。现在这些选项的长度可能不同。所以我们所做的是取最长的长度，这就是批次B乘以T的大小。然后这些中的一些将是填充维度，因此它们将未使用。因此，我们需要令牌，我们需要正确的标签，以及一个掩码，告诉我们哪些令牌是有效的。

掩码对于这些填充区域为零。这就是我们构造这些批次的方式。然后，为了让语言模型预测a、b、c或d，基本上我们将查看这些令牌。它们的概率。我们将选择令牌集的最低或最高平均概率的选项。

因为这是根据语言模型最有可能的完成方式。所以我们将查看这里的概率，并在选项中进行平均，选择概率最高的一个。大致来说，这就是我们将如何进行“超酷”。我相信这也是GPT-3的工作方式。根据我所知，这就是GPT-3的运作方式。但你应该注意，其他一些可能会看到“超酷”的邮件可能不是这样做的。

他们可能采用多项选择的格式，你只需给出一次上下文，然后是四个完成选项。因此，模型能够在选择最佳选项之前看到所有四个选项。这实际上是模型更容易完成的任务，因为在选择时你可以看到其他选项。但不幸的是，我们这种规模的模型无法做到这一点。

只有更大规模的模型才能做到这一点。因此，我们的模型在这一点上稍微受限，它们只能一次看到一个选项，必须为每个选项分配概率，正确的选项必须在这个指标中获胜。

好的，让我们现在简要实现这一点并将其融入到我们的脚本中。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_203.png)

好的，我在这里做的是引入了一个新的文件叫做hella swag.py，你可以查看一下。我不会逐行讲解，因为这并不是完全深入的代码。实际上，这有点乏味，因为发生的事情是我正在从GitHub下载hella spark，并渲染它的所有示例，总共有10个。

000个示例。我将它们渲染成这种格式。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_205.png)

在这个渲染示例函数的末尾，你可以看到我返回了标记。这个四行T数组的标记，掩码指示哪些部分是选项，其他的都是零。还有标签，即正确标签。这使得我们能够迭代示例并渲染它们。

我这里有一个评估函数，可以从一个面加载GPT，并在这里运行评估。基本上就是如我所描述的那样，它预测具有最低或最高概率的选项。实际上做到这一点的方法是我们基本上可以评估交叉熵损失。

所以我们基本上是在评估预测序列中下一个标记的损失。然后我们查看具有最低平均损失的行。我们选择的预测就是这个选项。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_207.png)

然后我们做一些统计和打印之类的。这是一种评估当前损失的方法。如果你查看上面，我显示的是GPT到124M。如果你运行这个脚本，你会看到损失为29.55%。所以这是我们在这里获得的表现。现在记住，随机机会是25%，所以我们还没有走得太远。

GPT到XL，这个最大的V GPT大约能达到49%的准确率。所以考虑到今天的最先进技术大约是95%，这些数值相对较低。显然，这些模型现在已经过时了。还有一个叫做Luther harness的东西，这是运行语言模型电子邮件的非常常见基础设施。

它们得到的数值稍有不同。我对这些差异并不完全确定。可能是它们实际上进行了多项选择，而不仅仅是补全。这可能就是差异所在。但我对此并不完全确定。我需要看看。不过现在，我们的脚本报告29.55。所以这是我们希望超越的数字，如果我们从零开始训练一个GP到124M的话。

现在我将实际将这个评估集成到我们的主要训练脚本中。基本上，因为我们想以周期性的方式进行评估，以便跟踪 Haloswag 及其随时间的演变。并查看我们是否跨越了这个 29.55 的区域。现在让我们逐步了解一下训练 GPT 到管道的一些更改。

我在这里做的第一件事是使 compile 成为可选的，并默认禁用。问题是 compile 使我们的代码更快，但它确实破坏了评估代码和采样代码。它给了我一个非常复杂的错误信息，我不知道为什么。

所以希望当你在我把代码库放到 GitHub 时，你能看到它。到那时我们将修复这个问题。但目前我在没有 torch compile 的情况下运行，这就是你看到运行速度有点慢的原因。因此我们在没有 torch compile 的情况下运行。我还创建了一个日志目录 log，我们可以在其中放置 log.txt，记录训练损失、验证损失和 Haloswag 准确性。

我们将打开一个非常简单的文本文件以进行写入，这样它就会从空开始，然后我们将向其中附加内容。我创建了一个简单的变量，帮助我们判断何时是最后一步。然后基本上在这个循环中，每进行 250 次迭代或在最后一步时，我们将评估验证损失。每 250 次迭代，我们将评估 Haloswag，但只有在不使用 compile 的情况下，因为 compile 会导致问题。

我将稍后再回到用于评估 Haloswag 的代码。然后每进行 250 次迭代，我们也会从模型中采样。所以你应该认出这是我们在视频开始时的古老代码，我们正在从模型中进行采样。最后，这里是，如果我们在验证样本和评估 Haloswag 后，我们实际上会在这里执行一个训练步骤。

这是训练的一步，你应该对它的所有功能非常熟悉。在这里，当我们获得训练损失时，我们将其写入文件。所以我真正添加的唯一更改是整个 Haloswag 评估部分。它的工作方式是我试图让所有 GPU 在 Haloswag 上协作。

我们正在迭代所有示例，每个进程只选择分配给它的示例。因此我们取 i 对世界大小取模，并使其等于 rank，否则我们继续。然后我们渲染一个示例，放在 GPU 上，获取 logits。接着我创建了一个辅助函数，帮助我们基本上预测损失最低的选项。

这就是预测，如果正确我们就记录下来。如果多个过程在协同工作，我们需要同步它们的统计数据。一个方法是将我们的统计数据打包到张量中，然后调用这个.alverduson并进行求和。然后在这里我们将它们从张量中解开，只得到整数。

然后这里主进程将打印并记录Haloswag的准确性。这就是我在这里运行的内容。所以你看到这里的优化，我们刚刚进行了生成。这是从大约20,000步中的第10,000步，所以我们完成了一半。

这些是在这个阶段我们获得的样本，让我们来看看。你好，我是一个语言模型，我想用它生成一些输出。我是一个语言模型，也是许多公司的开发者。我是一个语言模型。让我们看看我能否找到有趣的内容。我不知道你可以自己浏览，但显然预测变得越来越不随机。

模型似乎变得更自我意识，使用的语言更贴合作为语言模型的特点。你好，我是一个语言模型，语言是用来交流的，我会说英语和德语。

好吧，我不知道。那么我们就等到这个优化完成，再看看我们得到什么样的样本。我们还将查看训练、验证和hellosware的准确性，看看相对于GPT2的表现如何。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_209.png)

好的，早上好。所以稍微关注一下右边的Jupyter Notebook，我创建了一个新的单元，基本上允许我们可视化训练、验证和helloscore。你可以逐步查看，这基本上解析了我们正在写的日志文件。

这大部分就像无聊的matplotlib代码，但基本上这就是我们的优化过程。所以我们运行了19,073步，大约是100亿个标记，哎呀，天哪。这是找到BDO的样本10b的一个周期。在左侧我们有损失，在蓝色部分是训练损失。

橙色部分是验证损失，红色是水平线。我们有GPT2 124M模型检查点的开头，当它刚在这个微调的BDO验证集上评估时。所以你可以看到，橙色部分低于红色，意味着我们超过了这个数据集的验证集。正如我提到的，数据集的分布与GPT2训练时的非常不同。

这并不完全是公平的比较，但这是一个好的交叉检查。我们理想中希望有一些被保留且可比较的标准。所以对我们来说是helloscoreg。在这里我们看到从25%开始的helloscoreg进展。红色部分是GPT2 124M模型的开头。

所以它在这里达到了HelloScoreG，而训练了3000亿标记的GPT-3模型124M达到了绿色。所以在这里你可以看到，我们基本上超过了GPT-2的120程序模型，这真的很不错。有趣的是，我们仅用100亿标记进行训练就能做到这一点，而GPT-2是用1000亿标记进行训练的。因此，出于某种原因，我们能够在训练时使用显著更少的标记。

有很多可能性解释为什么我们能够以仅100亿的训练数据匹配或超过这个准确度。首先，可能是因为GPT-2是在更广泛的数据分布上进行训练的。因此，尤其是Fine Web EDU是全英文的，并不是多语言的，代码中的数学内容也不多。因此，数学和代码及多语言可能从原始GPT-2模型中抽取了能力，这可能是部分原因为什么这并没有奏效。

还有许多其他原因。例如，HelloScoreG评估相当旧，可能有五年左右。某些HelloScoreG的方面可能以某种方式，甚至完全相同地，进入了Fine Web的训练集。我们不能确定，但如果真是这样，那么我们基本上是在看训练曲线而不是验证曲线。长话短说，这并不是一个完美的评估，有一些警告，但至少我们对自己并没有做错什么有一定信心。

而且，当人们尝试创建这些数据集时，他们可能会确保非常常见的测试集不包含在训练集中。例如，当Hugging Face创建Fine Web EDU时，他们将HelloSwag作为评估数据，所以我希望他们能确保去重，并且训练集中没有HelloSwag，但我们不能确定。

我想简单提一下的是，看看这个LOSCER，这看起来真的很不对。我并不完全知道这是什么，我怀疑是因为Fine Web EDU的100亿样本没有被正确打乱。而且这里的数据存在一些我还不完全理解的问题，还有一些奇怪的周期性。因为我们以非常懒惰的方式对所有标记进行序列化，并且从头开始对它们进行迭代，而没有进行任何置换或随机抽样。

我认为我们正在继承数据集中他们的一些排序。因此这并不理想，但希望到你查看这个仓库时，其中一些问题能得到解决。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_211.png)

我会发布这个构建Nano GPT的仓库，目前看起来有点丑陋和初步，所以希望你到达这里时它会更好。但在这里我将展示Arata，并且我将谈论视频后发生的一些事情，我预计我们会修复这个小问题。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_213.png)

但目前为止，这表明我们的训练并不完全错误，并且显示出我们能够在仅使用10倍的令牌预算的情况下超越准确率。可能数据集也有所改进。原始的GPT-2数据集是webtext，可能在数据集上没有投入太多的关注和精力。这是在LMS早期，而现在对去重的好做法有了更多的审查。

过滤、质量过滤等等。而且可能我们正在训练的数据集在每个令牌上的质量更高，这也可能给我们带来提升。因此，有许多需要考虑的注意事项，但目前为止我们对此相当满意。是的。

接下来我感兴趣的是，现在是早晨，所以经历了一夜的训练，我基本上想看看我能把结果推得多远。为了进行过夜训练，我基本上做了四个周期，而不是一个周期，大约花了两个小时。

我把它做了四倍，这样在我睡觉的时候大约会花八个小时。所以我们进行了四个周期的训练，大约40亿个令牌，我在试图看看能达到什么程度。因此，这是唯一的变化，我重新运行了脚本，当我查看40B的日志文件时，这就是曲线的样子。好的，所以来叙述一下，首先我们看到不同周期间存在周期性的问题，以及FindWeb EDU数据集的一些奇怪现象，这还需要进一步确定。

但除此之外，我们看到heliswag的结果实际上大幅上升，我们几乎达到了GPT-3-124M的准确率。可惜的是，我没有多睡一会儿，我认为如果这是五个周期的训练，我们可能会达到这个结果。

现在需要指出的一点是，如果你正在进行多轮周期的训练，我们在数据加载器方面并没有非常仔细。这个数据加载器以完全相同的格式和顺序处理数据，这实际上是次优的，你应该考虑使用扩展功能来随机打乱数据。

然后你在每个新周期的每个分片中打乱文档，甚至可能还会打乱分片。这将大大减少预先相似性，同时对于优化也是更好的，这样你不会以完全相同的格式看到内容，同时引入一些文档之间的随机性。

因为你必须记住，在每一行中，这些文档是相互跟随的，然后是文本结束标记，接着是下一个文档。因此，文档目前是以完全相同的方式粘在一起的，但我们实际上想要打乱这些文档，因为文档的顺序不应该重要。

基本上，我们想打破这种依赖关系，因为它是一种虚假的相关性，因此我们的数据当前并没有做到这一点，这是一项你可以考虑改进的地方。另一个需要指出的地方是，我们几乎用仅40亿个令牌达到了与GPT-3相匹配的准确性，而GPT-3是在300亿个令牌上训练的。

所以我们再次看到学习效率在这里大约提高了10倍。我想提到的另一件事是，我实际上并不知道具体该归因于什么，除了我之前提到的一些因素。

我想简要提到的另一件事是这里的最大学习率。我看到一些人已经在之前的相关仓库中对此进行了一些尝试，结果发现你实际上可以几乎将其提高3倍，因此最大学习率可能会高得多。

由于某种原因，我们继承的GPT-3超参数实际上是非常保守的，你实际上可以使用更高的学习率，它会训练得更快。因此，这些超参数都是相当可调的，可以自由尝试，它们可能并没有被精确设置，你可能会发现这样做是可行的。

如果你想严格遵循GPT-3，你还需要做以下修改。你需要来这里，将GPT-3的序列长度改为2倍，即2048，而不是1024。所以你需要把T改为2048，然后如果你想要每次迭代或每一步的确切令牌数量为50万，你需要将其减少到32。这样仍然乘积到半个L。因此，这将使你的模型序列长度等于GPT-3，在这种情况下，模型在我所知的范围内基本上是相同的，因为GPT-2和GPT-3是非常相似的模型。

现在我们也可以看看这个过夜训练的模型的一些样本。这是优化的结果，你可以看到我们一路走到了7600到90左右，这些是Hellersmag，它是33.24，这些是模型的一些样本。如果你稍微暂停一下视频，你会看到这些样本更加连贯。

实际上，他们正在解决这是一个语言模型的问题。所以你好，我是一个语言模型，我尽量做到尽可能准确。我是一个语言模型，而不是编程语言。我知道如何进行沟通。我使用Python。如果你暂停一下，看看它，然后将其与仅训练了100亿的模型进行比较。

你会发现这些更加连贯，你可以自己试试。另外，我在代码中添加的一个东西是这段代码。所以基本上在我们评估验证损失后，如果我们是主进程，除了每5000步记录验证损失，我们还将保存检查点。实际上只是模型的状态字典。

检查点的好处在于你可以保存模型，之后可以以某种方式使用它。如果你想恢复优化，那么除了保存模型，我们还需要保存优化器状态字典。因为要记住，优化器有一些额外的缓冲区，因为Adam算法。所以你需要正确地恢复优化器。

所以你需要小心随机数生成器的种子等。如果你想准确地恢复优化过程，就必须考虑训练过程的状态。但如果你只想保存模型，这就是你要做的。而且，你可能想这样做的一个好理由是，你可能想更仔细地评估模型。

所以在这里我们只是随便使用L.S.Y.G.值。不过你可能想用更好的方法，比如说路德评价的难度。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_215.png)

这也是评估语言模型的一种方式。因此，可能你会想使用不同的基础设施，更全面地评估模型在不同评估上的表现。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_217.png)

并将其与许多其他任务的开源RGP-T2模型进行比较，比如数学代码或不同语言等。因此，这也是一个不错的功能。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_219.png)

还有我想提到的另一点是，我们在这里构建的所有内容仅仅是预训练步骤。因此，这里的GPT是一个梦想文档。它只是预测下一个开放项。你无法像与聊天GPT对话那样与它交谈。如果你想与模型对话，我们必须将其微调为聊天格式。

实际上并没有那么复杂。如果你在看监督微调或SFT。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_221.png)

其实这意味着我们只是将一个数据集换成一个更加对话化的数据集，并且有一种用户助手的结构。我们只是在这个基础上进行微调，然后基本上填充用户的标记，采样助手的标记。其实没那么复杂，但基本上我们更换数据集并继续训练。

但现在我们将停留在预训练阶段。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_223.png)

我想简要展示的另一件事是，当然我们今天构建的内容是为了NANDA GPT而搭建的，这是之前提到的这个代码库。但实际上还有另一个NANDA GPT实现，它隐藏在我最近正在进行的一个名为LLAN的项目中。

C。LLAN。C是一个纯C CUDA实现的GPT2或GPT3训练。它直接使用CUDA并以C CUDA形式编写。现在这里的NANDA GPT作为PyTorch中的参考代码，针对C实现。因此，我们尝试完全匹配这两者，但我们希望C CUDA能更快。

当然，目前看起来确实如此，因为这是一个直接优化的实现。所以在LLAN。C中的traingpt2。py基本上就是NANDA GPT。当你浏览这个文件时，你会发现很多看起来非常像我们在这次讲座中构建的内容。然后当你查看traingpt2。cu时，这是C CUDA实现。

因此，有很多MPI调用GPU CUDA CC++，你需要对这些有所了解。但是当这个构建完成后，我们实际上可以将两个并排运行。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_225.png)

它们将产生完全相同的结果，但LLAN。C实际上运行得更快。让我们看看这一点。所以在左侧我有PyTorch，NANDA GPT看起来的东西。在右侧我有LLAN C调用，现在我要启动这两个。它们都会在单个GPU上运行。这里我放置了LLAN。

C在GPU1上，而这个默认会抓取GPU0。然后我们可以看到这里LLAN。C已编译并分配了空间，而且正在进行中。因此，基本上同时PyTorch仍在编译，因为Torp编译在这里比LLAN。C和BCCC C CUDA编译稍慢。因此，这个程序已经开始运行，而我们仍在这里等待Torp编译。

当然，这个实现是非常特定于GPT 2和3的。PyTorch是一个非常通用的神经网络框架，因此它们并不完全可比。但是如果你只对训练GPT 2和3感兴趣，LLAN。C非常快。它占用的空间更小，启动更快，每步的速度也更快。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_227.png)

所以PyTorch开始在这里进行步进，如你所见，我们的运行速度约为每秒223,000个标记，而这里大约为每秒185,000个标记。因此，速度确实慢了一些，但我并不完全确定我是否完全挤出了PyTorch实现的所有性能。但这里重要的是，如果我对齐这些步骤，你会看到这两者之间打印的损失和范数是相同的。

所以左边是PyTorch，右边是C CUDA实现，它们是相同的，只是这个运行得更快。所以我也想简要介绍一下LLAN.C，这是一个并行实现，你可能想要尝试一下或者看看，它还挺有趣的。好吧，此时我可能应该开始结束这个视频，因为我觉得它已经比预期的长得多了。

但我们确实涵盖了很多内容，并且一切都是从零开始构建的。所以简单总结一下，我们在研究GPT2和GPT3的论文。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_229.png)

我们在研究如何设置这些训练运行以及涉及的所有考虑事项。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_231.png)

我们从头开始编写了一切，然后我们发现，在两小时的训练运行或一夜的运行期间，我们实际上能够在很大程度上匹配GPT2和GPT3的1.24亿参数检查点。原则上，我们编写的代码能够训练更大的模型，只要你有耐心或计算资源。因此，你可以考虑训练一些更大的检查点。

还有一些待解决的问题。这里的损失情况我怀疑与最终的网络EDU数据采样有关。为什么我们不能启用Torch Compile？这目前会破坏生成效果，真是奇怪。数据加载器中，当我们达到时代边界时，可能应该对数据进行随机打乱。因此还有一些类似的问题，我希望能在管理GPT的构建过程中记录这些。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_233.png)

我将与这个视频一起发布它。如果你有任何问题或者想讨论我们涵盖的内容，请前往讨论标签，以便我们可以在这里交流。或者请根据你想要贡献的内容去查看问题或拉取请求。

或者看看零到英雄的Discord，我将会在管理GPT上待一会儿。总的来说，我对我们所取得的进展感到很满意，希望你喜欢这个视频，我们稍后再见。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_235.png)

我们稍后再见。

![](img/fffe638cf84a8d1e020ce63d0efeee6e_237.png)
