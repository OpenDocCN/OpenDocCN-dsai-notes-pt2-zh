- en: P5：p5 Building makemore Part 4： Becoming a Backprop Ninja - 加加zero - BV11yHXeuE9d
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P5：p5 建造 makemore 第四部分：成为一个反向传播忍者 - 加加zero - BV11yHXeuE9d
- en: Hi everyone。 So today we are once again continuing our implementation of make
    more。 Now so far we've come up to here multilial perceptions and our neural net
    looked like this。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 大家好。今天我们再次继续实现 makemore。到目前为止，我们已经到了这里的多层感知器，我们的神经网络看起来是这样的。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_1.png)'
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_2.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_2.png)'
- en: and we were implementing this over the last few lectures。 Now I'm sure everyone
    is very excited to。 go into recurring neural networks and all of their variants
    and how they work and the diagrams look。 cool and it's very exciting and interesting
    and we're going to get a better result but unfortunately。 I think we have to remain
    here for one more lecture and the reason for that is we've already。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在过去的几节课中实现了这个。现在我相信每个人都非常兴奋要深入研究循环神经网络及其所有变种，以及它们是如何工作的，图表看起来也很酷，这非常令人兴奋和有趣，我们将获得更好的结果，但不幸的是，我认为我们还需要在这里待一节课，原因是我们已经。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_4.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_4.png)'
- en: trained this multilial perception right and we are getting pretty good loss
    and I think we have a。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 训练了这个多层感知器，我们得到了相当不错的损失，我认为我们有。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_6.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_6.png)'
- en: pretty decent understanding of the architecture and how it works but the line
    of code here that I。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对架构及其工作原理有了相当不错的理解，但这里的代码行。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_8.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_8.png)'
- en: take an issue with is here lost up backward that is we are taking a pytorch
    autograph and using it。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的一个问题是丢失了向后传播，也就是我们正在使用一个 PyTorch 的自动微分。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_10.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_10.png)'
- en: to calculate all of our gradients along the way and I would like to remove the
    use of lost up backward。 and I would like us to write our backward pass manually
    on the level of tensors and I think that。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算我们所有的梯度，我希望去掉使用向后传播，并希望我们手动在张量级别上编写我们的反向传递，我认为。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_12.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_12.png)'
- en: this is a very useful exercise for the following reasons。 I actually have an
    entire blog post on。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常有用的练习，原因如下。我实际上有一整篇关于这个的博客文章。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_14.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_14.png)'
- en: this topic but I like to call backpropation a leaky abstraction and what I mean
    by that is。 backpropagation doesn't just make your neural networks just work magically
    it's not the case。 that you can just stack up arbitrary Lego blocks of the furnishable
    functions and just。 cross your fingers and back propagate and everything is great
    things don't just work automatically。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这个主题，我喜欢称反向传播为一个漏水的抽象，我的意思是，反向传播并不仅仅使你的神经网络神奇地工作，并不是说你可以随便堆叠任意的乐高块和可微函数，只需交叉你的手指并进行反向传播，一切就会很好，事情不会自动运行。
- en: it is a leaky abstraction in the sense that you can shoot yourself in a foot
    if you do not。 understanding its internals it will magically not work or not work
    optimally and you will need。 to understand how it works under the hood if you're
    hoping to debug it and if you are hoping to。 address it in your neural nut so
    this block post here from a while ago goes into some of those。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个漏水的抽象，因为如果你不理解其内部结构，你可能会自损，神奇的是，它可能不会工作或者不能最优工作，因此你需要理解它在内部是如何工作的，如果你希望调试它，如果你希望在你的神经网络中解决它，那么这里的这篇博客文章深入探讨了一些内容。
- en: examples so for example we've already covered them some of them already for
    example the flat。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一些例子，比如我们已经覆盖了一些例子，比如平面。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_16.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_16.png)'
- en: tails of these functions and how you do not want to saturate them too much because
    your gradients。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数的细节，以及你不想过度饱和它们，因为你的梯度。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_18.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_18.png)'
- en: will die the case of dead neurons which I've already covered as well the case
    of exploding or。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 将会死去的情况是死亡神经元，我已经覆盖过，还有爆炸的情况。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_20.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_20.png)'
- en: vanishing gradients in the case of a perinural networks which we are about to
    cover and then。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们即将讨论的前馈神经网络中，消失梯度的情况。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_22.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_22.png)'
- en: also you will often come across some examples in the wild this is a snippet
    that I found in a random。 code base on the internet where they actually have like
    a very subtle but pretty major bug in。 their implementation and the bug points
    at the fact that the author of this code does not actually。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '你也会经常在网上遇到一些实例，这是我在互联网上随机代码库中发现的一段代码，实际上它在实现中有一个非常微妙但相当严重的错误，这个错误表明作者并不真正了解。 '
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_24.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_24.png)'
- en: understand back propagation so what they're trying to do here is they're trying
    to clip the loss。 at a certain maximum value but actually what they're trying
    to do is they're trying to clip the。 gradients to have a maximum value instead
    of trying to clip the loss at a maximum value and。 indirectly they're basically
    causing some of the outliers to be actually ignored because when you。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 理解反向传播，他们试图在某个最大值处剪切损失，但实际上他们想剪切梯度，使其有最大值，而不是试图在最大值处剪切损失，间接导致一些异常值被忽略。
- en: clip a loss of an outlier you are setting its gradient to zero and so have a
    look through this。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 剪切一个异常值的损失，你将其梯度设置为零，看看这个。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_26.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_26.png)'
- en: and read through it but there's basically a bunch of subtle issues that you're
    going to avoid if you。 actually know what you're doing and that's why I don't
    think it's the case that because PyTorch or。 other frameworks offer autograd it
    is okay for us to ignore how it works now we've actually already。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通读它，但如果你知道自己在做什么，会避免一系列微妙的问题，因此我认为，正因为PyTorch或其他框架提供自动微分，我们就可以忽略其工作原理，这显然不是这样。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_28.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_28.png)'
- en: covered autograd and we wrote micrograd but micrograd was an autograd engine
    only on the level of。 individual scalars so the atoms were single individual numbers
    and you know I don't think it's enough。 and I'd like us to basically think about
    back propagation on level of tensors as well and so in。 a summary I think it's
    a good exercise I think it is very very valuable you're going to become。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了自动微分，并编写了微型自动微分引擎，但微型引擎仅限于单个标量，因此原子是单个数字。我认为这还不够。我们应该考虑在张量层面进行反向传播，总体来说，我认为这是一个很好的练习，非常有价值，你会变得更优秀。
- en: better at debugging neural networks and making sure that you understand what
    you're doing it is going。 to make everything fully explicit so you're not going
    to be nervous about what is hidden away from。 you and basically in general we're
    going to emerge stronger and so let's get into it。 A bit of a。 fun historical
    note here is that today writing your backward pass by hand and manually is not
    recommended。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在调试神经网络时更好，确保你理解自己的操作，这将使一切变得非常明确，因此你不会对隐藏的内容感到紧张。总体而言，我们将变得更强大，所以让我们开始吧。顺便提一下，如今手动编写反向传递并不推荐。
- en: and no one does it except for the purposes of exercise but about 10 years ago
    in deep learning。 this was fairly standard and in fact pervasive so at the time
    everyone used to write their backward。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在除了作为练习之外，没有人这样做，但大约10年前，在深度学习中，这非常标准，实际上是普遍的，因此当时每个人都习惯于手动编写反向传播。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_30.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_30.png)'
- en: pass by hand manually including myself and it's just what you would do so we
    used to write backward。 pass by hand and now everyone just calls lost backward
    we've lost something。 I wanted to give you。 a few examples of this so here's a
    2006 paper from Jeff Hinton and Russell Select Enough in science。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 手动进行反向传递，包括我自己，这就是你会做的事情。我们曾经手动编写反向传递，现在大家只需调用丢失的反向传递，我们失去了某些东西。我想给你几个例子，这里有一篇2006年Jeff
    Hinton和Russell Select Enough的论文。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_32.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_32.png)'
- en: that was influential at the time and this was training some architectures called
    restricted。 bulletin machines and basically it's an autoencoder trained here and
    this is from roughly 2010 I had。 a library for training restricted bulletin machines
    and this was at the time written in MATLAB so Python。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这在当时具有影响力，训练一些被称为限制玻尔兹曼机的架构，基本上这是一个训练的自编码器，来自大约2010年，我有一个用于训练限制玻尔兹曼机的库，那个时候是用MATLAB编写的，所以用Python。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_34.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_34.png)'
- en: was not used for deep learning pervasively it was all MATLAB and MATLAB was
    this scientific computing。 package that everyone would use so we would write MATLAB
    which is barely a programming language。 as well but it had a very convenient tensor
    class and it was this a computing environment and you。 would run here it would
    all run on the CPU of course but you would have very nice plots to go。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习并没有被广泛使用，所有的都是MATLAB，MATLAB是一个科学计算包，大家都会使用，所以我们会写MATLAB，这几乎不是一种编程语言，但它有一个非常方便的张量类，这是一个计算环境，你在这里运行，当然所有的都在CPU上运行，但你会得到非常好的图表。
- en: with it and a built-in debugger and it was pretty nice。 Now the code in this
    package in 2010 that I。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 使用它和一个内置调试器，这真是不错。现在，这个包中的代码是在2010年我。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_36.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_36.png)'
- en: wrote for fitting restricted bulletin machines to a large extent is recognizable
    but I wanted to。 show you how you would well I'm creating the data in the xy batches
    I'm initializing the neural nut。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我为适应受限布尔机所写的代码在很大程度上是可识别的，但我想展示你如何……我在创建xy批数据时初始化神经网络。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_38.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_38.png)'
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_39.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_39.png)'
- en: so it's got weights and biases just like we're used to and then this is the
    training loop where。 we actually do the forward pass and then here at this time
    didn't even necessarily use back。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 所以它有权重和偏置，就像我们习惯的那样，然后这是训练循环，我们实际上执行前向传播，然后在这里，此时甚至不一定使用反向传播。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_41.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_41.png)'
- en: propagation to train neural networks so this in particular implements contrastive
    divergence。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 传播用于训练神经网络，因此这特别实现了对比散度。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_43.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_43.png)'
- en: which estimates a gradient and then here we take that gradient and use it for
    a parameter update。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数估计一个梯度，然后在这里我们获取这个梯度并用于参数更新。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_45.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_45.png)'
- en: along lines that we're used to yeah here but you can see that basically people
    are meddling with。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 沿着我们习惯的方向，是的，你可以看到基本上人们在干扰。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_47.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_47.png)'
- en: these gradients directly and in line and themselves it wasn't that common to
    use an autograd engine。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 直接使用这些梯度，并且在行内，这并不常见于使用自动求导引擎。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_49.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_49.png)'
- en: Here's one more example from a paper of mine from 2014 called the fragment embeddings
    and here。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我2014年发表的一篇论文中的另一个例子，标题是“片段嵌入”。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_51.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_51.png)'
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_52.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_52.png)'
- en: what I was doing is I was aligning images and text and so it's kind of like
    a clip if you're。 familiar with it but instead of working on the level of entire
    images and entire sentences it。 was working on the level of individual objects
    and the little pieces of sentences and I was embedding。 them and then calculating
    a very much like a clip-like loss and I deck up the code from 2014 of how。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我所做的是对齐图像和文本，因此这有点像“Clip”，如果你熟悉的话，但它并不是在整个图像和整个句子的层面上工作，而是在单个对象和句子的片段层面上工作，我在嵌入它们，然后计算一种类似于“Clip”的损失，并且我拿到了2014年的代码。
- en: I implemented this and it was already in numpy and python and here I'm implementing
    the cost function。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我实现了这个，已经在numpy和python中，这里我在实现成本函数。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_54.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_54.png)'
- en: and it was standard to implement not just the cost but also the backward pass
    manually so here。 I'm calculating the image embeddings sentence embeddings the
    last function I calculate the scores this is。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 实现不仅仅是成本，而且还手动实现反向传播是标准做法，所以在这里，我在计算图像嵌入和句子嵌入，最后的函数我计算得分，这就是。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_56.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_56.png)'
- en: the loss function and then once I have the loss function I do the backward pass
    right here so I。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数，一旦我有了损失函数，我就在这里执行反向传播。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_58.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_58.png)'
- en: backward through the loss function and through the neural net and I append regularization
    so。 everything was done by hand manually and you were just right out the backward
    pass and then you would。 use a gradient checker to make sure that your numerical
    estimate of the gradient agrees with the。 one you calculated during back propagation
    so this was very standard for a long time but today of course。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 通过损失函数和神经网络进行反向传播，我添加了正则化。一切都是手动完成的，你只需直接进行反向传播，然后使用梯度检查器确保你对梯度的数值估计与在反向传播中计算的梯度一致。这在很长一段时间内都是非常标准的做法，但今天当然。
- en: it is standard to use an autograd engine but it was definitely useful and I
    think people sort of。 understood how these neural networks work on a very intuitive
    level and so I think it's a good。 exercise again and this is where we want to
    be okay so just as a reminder from our previous lecture。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 使用自动求导引擎是标准做法，但这确实很有用，我认为人们在直观上理解这些神经网络是如何工作的，所以我认为这是一个很好的练习，再次强调，这是我们想要达到的地方。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_60.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_60.png)'
- en: this is the jupyter notebook that we implemented at the time and we're going
    to keep everything the。 same so we're still going to have a two layer multi-layer
    perception with a batch normalization。 layer so the forward pass will be basically
    identical to this lecture but here we're going to get rid。 of loss that backward
    and instead we're going to write the backward pass manually now here's the。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们当时实现的Jupyter Notebook，我们将保持一切不变，因此我们仍将有一个具有批量归一化层的两层多层感知器，因此正向传播基本上与本次讲座相同，但这里我们将去掉损失的反向传播，而是将手动编写反向传播。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_62.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_62.png)'
- en: starter code for this lecture we are becoming a backprop ninja in this notebook
    and the first few。 cells here are identical to what we are used to so we are doing
    some imports loading the data set and。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 本次讲座的启动代码，我们在这个Notebook中成为反向传播的忍者，这里的前几个单元与我们熟悉的完全相同，因此我们正在进行一些导入，加载数据集。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_64.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_64.png)'
- en: processing the data set none of this changed now here i'm introducing a utility
    function that we're。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 处理数据集的过程没有改变，现在我引入一个工具函数。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_66.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_66.png)'
- en: going to use later to compare the gradients so in particular we are going to
    have the gradients。 that we estimate manually ourselves and we're going to have
    gradients that pytorch calculates。 and we're going to be checking for correctness
    assuming of course that pytorch is correct。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 稍后将用于比较梯度，具体来说，我们将有我们手动估计的梯度，以及PyTorch计算的梯度，我们将检查其正确性，假设当然PyTorch是正确的。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_68.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_68.png)'
- en: then here we have the initialization that we are quite used to so we have our
    embedding table for。 the characters the first layer second layer and a batch normalization
    in between and here's where。 we create all the parameters now you will note that
    i changed the initialization a little bit。 to be small numbers so normally you
    would set the biases to be all zero here i am setting them to。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然后这里是我们相当熟悉的初始化，因此我们有角色的嵌入表、第一层、第二层以及中间的批量归一化，并在这里创建所有参数。你会注意到我稍微改变了初始化，以使其成为小数字，因此通常你会将偏差初始化为零，而我将其设置为。
- en: be small random numbers and i'm doing this because if your variables are initialized
    to exactly zero。 sometimes what can happen is that can mask an incorrect implementation
    of a gradient because。 when everything is zero it sort of like simplifies and
    gives you a much simpler expression of the。 gradient and then you would otherwise
    get and so by making it small numbers i'm trying to unmask。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成小的随机数，我这样做是因为如果你的变量初始化为零，有时会掩盖梯度实现中的错误，因为当一切都为零时，它会简化并给出一个更简单的梯度表达式，而不是你本来会得到的。因此，通过生成小数字，我试图揭示。
- en: those potential errors in these calculations you also notice that i'm using
    b1 in the first layer。 i'm using a bias despite batch normalization right afterwards
    so this would typically not be。 what you do because we talked about the fact that
    you don't need a bias but i'm doing this here just。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些计算中你也注意到我在第一层使用了b1。我尽管在之后使用了批量归一化，但仍然使用了偏置，所以这通常不是你所做的事情，因为我们谈到过你不需要偏置，但我在这里这样做只是。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_70.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_70.png)'
- en: for fun because we're going to have a gradient with respect to it and we can
    check that we are。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了好玩，因为我们将有一个与之相关的梯度，并且我们可以检查我们是否。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_72.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_72.png)'
- en: still calculating it correctly even though this bias is spurious so here i'm
    calculating a single。 batch and then here i am doing a forward pass now you'll
    notice that the forward pass is significantly。 expanded from what we are used
    to here the forward pass was just here now the reason that the forward。 pass is
    longer is for two reasons number one here we just had an fdath cross entropy but
    here i am。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 即使这个偏置是多余的，仍然正确计算，因此在这里我计算一个单一的批次，然后我在这里进行前向传播。现在你会注意到，前向传播的长度显著扩展，与我们所习惯的相比，这里的前向传播仅在这里。前向传播之所以更长有两个原因，首先这里我们只是进行了一个fdath交叉熵，但在这里我。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_74.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_74.png)'
- en: bringing back a explicit implementation the loss function and number two i've
    broken up the。 implementation into manageable chunks so we have a lot a lot more
    intermediate tensors along the way。 in the forward pass and that's because we
    are about to go backwards and calculate the gradients。 in this back propagation
    from the bottom to the top so we're going to go upwards and just like we。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 带回一个明确的损失函数实现，同时我将实现分解成可管理的部分，所以我们在前向传播中有很多更多的中间张量。这是因为我们即将向后计算梯度，从底部到顶部，因此我们将向上移动，就像我们。
- en: have for example the lock props tensor in a forward pass in a backward pass
    we're going to have a d。 lock props which is going to store the derivative of
    the loss with respect to the lock props tensor。 and so we're going to be pretending
    d to every one of these tensors and calculating it along the way。 of this back
    propagation so as an example we have a b and raw here we're going to be calculating
    a d b。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，前向传播中的锁定属性张量，在反向传播中我们将有一个d。锁定属性，它将存储损失对锁定属性张量的导数。因此，我们将在这次反向传播中伪装每一个这些张量，并沿途计算它。例如，我们有a、b和raw，在这里我们将计算一个d
    b。
- en: and raw so here i'm telling pytorch that we want to retain the grad of all these
    intermediate values。 because here in exercise one we're going to calculate the
    backward pass so we're going to。 calculate all these d variables and use the cmp
    function i've introduced above to check our。 correctness with respect to what
    pytorch is telling us this is going to be exercise one where we。
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 和raw，所以在这里我告诉pytorch我们想保留所有这些中间值的梯度。因为在练习一中我们将计算反向传播，所以我们将计算所有这些d变量，并使用我上面介绍的cmp函数来检查我们相对于pytorch所告诉我们的正确性，这将是练习一。
- en: sort of back propagate through this entire graph now just to give you a very
    quick preview of what's。 going to happen in exercise two and below here we have
    fully broken up the loss and back propagated。 through it manually in all the little
    atomic pieces that make it up but here we're going to。 collapse the loss into
    a single cross entropy call and instead we're going to analytically derive。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播整个图形，现在快速预览一下第二个练习将要发生的事情，下面我们完全分解了损失，并手动反向传播通过所有构成它的小原子片段，但在这里我们将损失合并成一个交叉熵调用，而是将通过分析推导出。
- en: using math and paper and pencil the gradient of the loss with respect to the
    logits and instead of。 back propagating through all of its little chunks one at
    a time we're just going to analytically drive。 what that gradient is and we're
    going to implement that which is much more efficient as we'll see in。 a bit then
    we're going to do the exact same thing for batch normalization so instead of breaking
    up。
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 使用数学和纸笔，我们将损失对logits的梯度进行计算，而不是逐个小块进行反向传播，我们将分析性地推导出这个梯度，并且我们将实现它，这样效率更高，稍后我们会看到。然后我们将对批量归一化做同样的事情，因此不再拆分。
- en: batch room into all the little tiny components we're going to use pen and paper
    and mathematics and。 calculus to derive the gradient through the batch room layer
    so we're going to calculate the backward。 pass through batch room layer in a much
    more efficient expression instead of backward propagating。 through all of its
    little pieces independently so it's going to be exercise three and then。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 批量处理为所有的小组件，我们将使用笔和纸以及数学和微积分来推导通过批量处理层的梯度，因此我们将以更有效的方式计算反向传播，而不是独立地反向传播每个小部分，所以这将是练习三。
- en: exercise four we're going to put it all together and this is the full code of
    training this two-layer。 MLP and we're going to basically insert our manual backprop
    and we're going to take up loss。 the backward and you will basically see that
    you can get all the same results using fully your own。 code and the only thing
    we're using from PyTorch is the torch。
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在练习四中，我们将把所有内容结合在一起，这是训练这个两层MLP的完整代码，我们将基本上插入我们的手动反向传播，我们将获取损失的反向传播，你将基本上看到你可以完全使用自己的代码获得所有相同的结果，唯一从PyTorch中使用的是torch。
- en: tensor to make the calculations efficient， but otherwise you will understand
    fully what needs to forward and backward in your alert and train it。
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 张量使计算更高效，但除此之外，你将完全理解在你的警报中需要前向和反向传播的内容，并进行训练。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_76.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_76.png)'
- en: and I think that'll be awesome so let's get to it。 Okay so I ran all the cells
    of this notebook all。
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这将是非常棒的，所以让我们开始吧。 好的，我已经运行了这个笔记本的所有单元。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_78.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_78.png)'
- en: the way up to here and I'm going to erase this and I'm going to start implementing
    backward pass。 starting with delockprobs so we want to understand what should
    go here to calculate the gradient of。 the loss with respect to all the elements
    of the lockprobs tensor。 Now I'm going to give away the。 answer here but I wanted
    to put a quick note here that I think will be most pedagogically useful for。
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 到这里，我将擦除这个内容，并开始实现反向传播。 从delockprobs开始，我们想要了解这里应该放置什么，以计算损失相对于所有lockprobs张量元素的梯度。
    现在我要给出答案，但我想在这里做个简短的说明，我认为这在教学上会非常有用。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_80.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_80.png)'
- en: you is to actually go into the description of this video and find the link to
    this stupid notebook。 you can find it both on github but you can also find google
    collab with it so you don't have to。 install anything you'll just go to a website
    on google collab and you can try to implement these。 derivatives or gradients
    yourself and then if you are not able to come to my video and see me do it。
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 你实际上是要进入这个视频的描述中找到这个无聊笔记本的链接。 你可以在github上找到它，也可以在google colab上找到，因此你不必安装任何东西，你只需去google
    colab的网站，就可以尝试自己实现这些导数或梯度，如果你不能做到，欢迎来我的视频看看我怎么做。
- en: and so work in tandem and try it first yourself and then see me give away the
    answer and I think。 that would be most valuable to you and that's how I recommend
    you go through this lecture。 So we are。
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 所以请先自己尝试，然后再看看我如何给出答案，我认为这对你来说会是最有价值的，我推荐你这样进行这次讲座。 所以我们来。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_82.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_82.png)'
- en: starting here with delockprobs now delockprobs will hold the derivative of the
    loss with respect to。 all the elements of lockprobs。 What is inside lockprobs？
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 从delockprobs开始，delockprobs将保存损失相对于所有lockprobs元素的导数。 lockprobs中包含什么？
- en: The shape of this is 32 by 27 so it's not going to。 surprise you that delockprobs
    should also be an array of size 32 by 27 because we want the derivative。 loss
    with respect to all of its elements so the sizes of those are always going to
    be equal。 Now how does lockprobs influence the loss？ Okay， loss is negative lockprobs
    indexed with range of n。
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 形状是32乘27，因此delockprobs也应该是大小为32乘27的数组，这不会让你感到惊讶，因为我们想要损失相对于所有元素的导数，所以这些大小总是会相等。
    那么lockprobs如何影响损失呢？ 好吧，损失是负的lockprobs，索引范围为n。
- en: and yb and then the mean of that。 Now just as a reminder yb is just basically
    an array of all the。 correct indices。 So what we're doing here is we're taking
    the lockprops array of size 32 by 27。
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: yb的平均值。 现在提醒一下，yb基本上是一个包含所有正确索引的数组。 所以我们这里做的是取一个大小为32乘27的lockprops数组。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_84.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_84.png)'
- en: Right， and then we are going in every single row and in each row we are plugging，
    plugging out。 the index 8 and then 14 and 15 and so on。 So we're going down the
    rows that's the iterator range of n。 and then we are always plugging out the index
    at the column specified by this tensor yb。 So in the zero throw we are taking
    the eighth column， in the first row we're taking the 14th column。
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 对，然后我们在每一行中进行操作，在每一行中我们插入，插入出。索引8，然后14和15，依此类推。所以我们沿着行向下，这是迭代器的范围n。然后我们总是将指定列的索引从这个张量yb中插出。所以在零行我们取第八列，在第一行我们取第14列。
- en: etc。 And so lockprobs at this plucks out all those lock probabilities of the
    correct next。
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 等等。所以lockprobs会提取出所有正确下一个的锁定概率。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_86.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_86.png)'
- en: character in a sequence。 So that's what that does and the shape of this or the
    size of it is of course。
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 序列中的字符。所以这就是它的作用，当然，它的形状或大小是。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_88.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_88.png)'
- en: 32 because our batch size is 32。 So these elements get plucked out and then
    their mean and the。
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 32，因为我们的批量大小是32。所以这些元素被提取出来，然后它们的平均值和。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_90.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_90.png)'
- en: negative of that becomes loss。 So I always like to work with simpler examples
    to understand the。
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 负值变成损失。所以我总是喜欢用简单的例子来理解。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_92.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_92.png)'
- en: numerical form of derivative。 What's going on here is once we've plucked out
    these examples。
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 导数的数值形式。这里发生的事情是，一旦我们提取出这些例子。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_94.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_94.png)'
- en: we're taking the mean and then the negative。 So the loss basically， I can write
    it this way。 is the negative of say a plus b plus c and the mean of those three
    numbers would be say negative。 would divide three。 That would be how we achieve
    the mean of three numbers a， b， c， although we。 actually have 32 numbers here。
    And so what is basically the loss by say like dA， right？ Well。
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们取平均值，然后取负值。所以损失基本上可以这样写。是负的a加b加c的总和，而这三个数字的平均值可以说是负的。除以三。这就是我们如何实现三个数字a，b，c的平均，尽管我们这里实际上有32个数字。那么，损失对dA的影响是什么呢？对吧？那么。
- en: if we simplify this expression mathematically， this is negative one over three
    of a and negative。 plus negative one over three of b plus negative one over three
    of c。 And so what is the loss by dA？
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从数学上简化这个表达式，这就是负三分之一的a加上负三分之一的b加上负三分之一的c。那么，损失对dA的影响是什么？
- en: It's just negative one over three。 And so you can see that if we don't just
    have a， b and c， but we。 have 32 numbers， then d loss by d， you know， every one
    of those numbers is going to be one over n。 more generally， because n is the size
    of the batch 32 in this case。 So d loss by d lockprobs。
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是负的三分之一。所以你可以看到，如果我们不仅有a，b和c，而是有32个数字，那么损失对d的影响，你知道，每一个数字将是一个除以n。更一般地说，因为n是批量的大小，在这种情况下是32。所以损失对lockprobs的影响。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_96.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_96.png)'
- en: is negative one over n in all these places。 Now， what about the other elements
    inside lockprobs？
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些地方是负的三分之一。现在，lockprobs内部的其他元素呢？
- en: Because lockprobs is a large array。 You see that lockprobs are checked is 32
    by 27， but only 32 of。 them participate in the loss calculation。 So what's the
    derivative of all the other most of the。 elements that do not get plucked out
    here？ Well， their loss intuitively is zero。 So they're。 they're gradient intuitively
    zero。 And that's because they did not participate in the loss。
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 因为lockprobs是一个大型数组。你会看到lockprobs的检查是32乘27，但只有32个参与损失计算。那么其他没有被提取出的元素的导数是什么？好吧，它们的损失直观上是零。所以它们的梯度直观上也是零。这是因为它们没有参与损失。
- en: So most of these numbers inside this tensor does not feed into the loss。 And
    so if we were to change， these numbers， then the loss doesn't change。 which is
    the equivalent of us saying that the， rate of the loss with respect to them is
    zero。 they don't impact it。 So here's a way to implement。
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这个张量内部的大多数数字并未影响损失。因此，如果我们改变这些数字，损失不会改变。这相当于我们说，损失对它们的变化率为零。它们没有影响。所以这是一个实现的方法。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_98.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_98.png)'
- en: this derivative。 Then we start out with torched at zeros of shape 32 by 27，
    or let's just say。
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这个导数。然后我们从形状为32乘27的零开始，或者我们可以简单地说。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_100.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_100.png)'
- en: instead of doing this， because we don't want to hard code numbers， let's do
    torched at zeros。 like lockprobs。 So basically， this is going to create an array
    of zeros exactly in the shape of。 lockprobs。 And then we need to set the derivative
    negative one over n inside exactly these locations。 So here's what we can do。
    The lockprobs indexed in the identical way will be just set to negative。
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们不想硬编码数字，因此不这样做，让我们做 torched 的 zeros，像 lockprobs。因此，基本上这将创建一个完全形状与 lockprobs
    一样的零数组。然后我们需要在确切这些位置设置导数为负的 1/n。所以我们可以这样做。以相同方式索引的 lockprobs 将被设置为负值。
- en: one over zero， divide n， right， just like we derived here。 So now let me erase
    all these reasoning。
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 1 除以 0，除以 n，对吧，就像我们在这里推导的那样。那么现在让我擦掉这些推理。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_102.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_102.png)'
- en: And then this is the candidate derivative for D lockprobs。 Let's uncomment the
    first line and。
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 然后这是 D lockprobs 的候选导数。让我们取消注释第一行。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_104.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_104.png)'
- en: check that this is correct。 Okay， so CMP ran and let's go back to CMP。 And you
    see that what。
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 检查这是正确的。好的，所以 CMP 运行了，让我们回到 CMP。你看到什么。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_106.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_106.png)'
- en: is doing is it's calculating if the calculated value by us， which is dt， is
    exactly equal to t。 dot grad as calculated by pytorch。 And then this is making
    sure that all the elements are exactly。 equal。 And then converting this to a single
    Boolean value， because we don't want to Boolean tensor。 we just want to Boolean
    value。 And then here， we are making sure that， okay， if they're not。
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '它所做的是计算我们计算的值 dt 是否恰好等于 pytorch 计算的 t.dot.grad。然后确保所有元素完全相等。然后将其转换为单个布尔值，因为我们不想要布尔张量。我们只想要布尔值。然后在这里，我们确保，如果它们不是。 '
- en: exactly equal， maybe they are approximately equal because of some floating point
    issues。 but they're very， very close。 So here we are using torched at all close。
    which has a little bit of a， wiggle available， because sometimes you can get very，
    very close。 But if you use a slightly different， calculation， because of floating
    point arithmetic。
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 完全相等，可能因为某些浮点数问题它们大致相等，但它们非常，非常接近。所以在这里我们使用 torched 的 all close，这有一点点的波动，因为有时你可以得到非常，非常接近的结果。但是如果你使用略微不同的计算，因为浮点运算。
- en: you can get a slightly different result。 So this， is checking if you get an
    approximately close result。 And then here， we are checking the maximum， basically
    the value that has the highest difference。 And what is the difference and the
    absolute， value difference between those two。 And so we are printing whether we
    have an exact equality， an approximate equality。
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会得到一个略有不同的结果。所以这是在检查你是否得到一个大致接近的结果。然后这里我们检查的是最大值，基本上是具有最高差异的值。这个差异以及这两者之间的绝对值差异是什么。因此，我们在打印是否有确切的相等或近似相等。
- en: and what is the largest difference。 And so here， we see that we actually。
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 以及最大的差异是什么。所以在这里，我们看到我们实际上。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_108.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_108.png)'
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_109.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_109.png)'
- en: have exact equality。 And so therefore， of course， we also have an approximate
    equality。 And the。 maximum difference is exactly zero。 So basically， our delog
    props is exactly equal to what pytorch。
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 具有确切的相等。因此，当然，我们也有近似相等。最大差异恰好为零。所以基本上，我们的 delog props 和 pytorch 是完全相等的。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_111.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_111.png)'
- en: calculated to be log props dot grad in its back propagation。 So so far， we're
    doing pretty well。
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 被计算为 log props.dot.grad 在其反向传播中。因此到目前为止，我们做得很好。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_113.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_113.png)'
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_114.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_114.png)'
- en: Okay， so let's now continue our back propagation。 We have that log props depends
    on props through a。 log。 So all the elements of props are being element wise applied
    log two。 Now。 if we want deep props， then remember your micro graph training，
    we have like a log node。 it takes in props and creates， log props。 And deep props
    will be the local derivative of that individual operation log times the。
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，所以让我们继续我们的反向传播。我们有 log props 通过一个 log 依赖于 props。因此 props 的所有元素都逐元素应用了 log。现在，如果我们想要
    deep props，记得你的微图训练，我们有一个 log 节点。它接收 props 并创建 log props。而 deep props 将是那个单独操作
    log 的局部导数乘以。
- en: derivative loss with respect to its output， which in this case is D log props。
    So what is the local。 derivative of this operation？ Well， we are taking log element
    wise， and we can come here。 and we can。
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 关于其输出的导数损失，在这种情况下是 D log props。那么这个操作的局部导数是什么呢？嗯，我们是逐元素取 log，接下来我们可以在这里做。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_116.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_116.png)'
- en: see， well， from all five is your friend， that d by dx of log of x is just simply
    one over x。
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 看，嗯，五个中的任何一个是你的朋友，log x 的导数 d by dx 简单地就是 1/x。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_118.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_118.png)'
- en: So therefore， in this case， x is problems。 So we have d by dx is one over x，
    which is one over。 problems。 And then this is the local derivative。 And then times
    we want to train it。 So this is。
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在这种情况下，x 是问题。因此我们有 d by dx 是 1/x，也就是 1/问题。然后这是局部导数。然后我们希望训练它。因此这是。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_120.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_120.png)'
- en: chain rule， times do log props。 Then let me uncomment this and let me run the
    cell in place。
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 链式法则，乘以 do log props。接下来让我取消注释并在当前位置运行单元格。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_122.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_122.png)'
- en: And we see that the derivative of props as we calculated here is exactly correct。
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到我们在这里计算的 props 的导数是完全正确的。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_124.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_124.png)'
- en: And so notice here how this works。 Props that are props is going to be inverted
    and then。 element wise multiplied here。 So if your props is very， very close to
    one， that means your network。 is currently predicting the character correctly，
    then this will become one over one and V log。 props is just passed through。 But
    if your probabilities are incorrectly assigned， so if the correct。
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 所以注意这里是如何工作的。props 将被反转，然后在这里逐元素相乘。所以如果你的 props 非常接近 1，这意味着你的网络当前正确地预测了字符，那么这将变成
    1/1，V log props 就直接传递。但如果你的概率分配不正确，比如正确的。
- en: character here is getting a very low probability， then 1。0 dividing by it will
    boost this。 and then multiply by the problem。 So basically what this line is doing
    intuitively， is it's taking。 the examples that have a very low probability currently
    assigned， and it's boosting their gradient。 You can look at it that way。 Next
    up is count some imp。 So we want the derivative of this。
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的字符概率非常低，然后用 1.0 除以它会提升这个概率，然后乘以问题。所以基本上，这行的直观意义在于，它正在提升当前分配了非常低概率的样本的梯度。你可以这么看。接下来是计数一些
    imp。因此我们想要这个的导数。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_126.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_126.png)'
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_127.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_127.png)'
- en: Now let me just pause here and kind of introduce what's happening here in general，
    because I know。 it's a little bit confusing。 We have the logis that come out of
    the neural net。 Here what I'm doing。 is I'm finding the maximum in each row， and
    I'm subtracting it for the purpose of numerical。 stability。 And we talked about
    how if you do not do this， you run these numerical issues of some。
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我在这里暂停一下，介绍一下这里发生的一般情况，因为我知道这有点困惑。我们有来自神经网络的 logits。这里我正在做的是找出每一行的最大值，并为数值稳定性减去它。我们讨论过如果不这样做，你会遇到一些数值问题。
- en: of the logits take on two large values， because we end up exponentiating them。
    So this is done just。 for safety numerically。 Then here's the exponentiation of
    all the sort of logits to create our counts。 and then we want to take the sum
    of these counts and normalize so that all the probes sum to one。 Now here instead
    of using one over count sum， I use raised to the power of negative one。
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: logits 取了两个大的值，因为我们最终对它们进行指数运算。因此这样做仅仅是为了数值安全。然后这里对所有的 logits 进行指数运算以创建我们的计数。接着我们希望对这些计数求和并进行归一化，使得所有的
    probes 之和为 1。现在这里我没有使用 1/计数和，而是使用了负一的幂。
- en: Mathematically they are identical。 I just found that there's something wrong
    with the pytorch implementation。 of the backward pass of division， and it gives
    like a real result， but that doesn't happen for。 star star， negative one。 So I'm
    using this formula instead。 But basically all that's happening here。
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 数学上它们是相同的。我只是发现 pytorch 在除法的反向传播实现上有问题，虽然它会给出一个真实结果，但在 star star，负一 的情况下并不会发生。所以我使用这个公式。但基本上，这里发生的一切。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_129.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_129.png)'
- en: is we got the logits， we want to exponentiate all of them， and want to normalize
    the counts。 to create our probabilities。 It's just that it's happening across
    multiple lines。 So now。 here we want to first take the derivative， we want to
    back propagate into counts a minute。 and then into counts as well。 So what should
    be the count sum？ Now we actually have to be careful。
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了logits，我们想对它们进行指数运算，并对计数进行归一化，以创建我们的概率。只是这个过程发生在多行之间。所以现在，我们首先想要计算导数，我们想要回传到计数，过一会儿再回传到计数。那么，计数的总和应该是多少？现在我们实际上需要小心。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_131.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_131.png)'
- en: here because we have to scrutinize and be careful with the shapes。 So counts
    that shape。 and then counts some in that shape are different。 So in particular
    counts is 32 by 27。 but this count。
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，因为我们必须仔细检查形状。因此，计数的形状，以及计数和的形状是不同的。特别是计数是32乘27，但这个计数。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_133.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_133.png)'
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_134.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_134.png)'
- en: sum in is 32 by one。 And so in this multiplication here， we also have an implicit
    broadcasting that。
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 和的形状是32乘1。因此在这个乘法中，我们也有隐式广播。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_136.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_136.png)'
- en: pytorch will do， because it needs to take this column tensor of 32 numbers and
    replicate it。 horizontally 27 times to align these two tensors so it can do an
    element twice multiply。 So really。
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: pytorch需要这样做，因为它需要将32个数字的列张量水平复制27次，以对齐这两个张量，以便进行元素乘法。因此，实际上。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_138.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_138.png)'
- en: what this looks like is the following using a toy example again。 What we really
    have here is just。 props is counts times consumption。 So it's a equals a times
    b。 but a is three by three and b is just， three by one a column tensor。 And so
    pytorch internally replicated this elements of b， and it did that。
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来像是以下的玩具示例。我们真正拥有的是props是计数乘消耗。所以这是a等于a乘b，但a是三乘三的，b只是三乘一的列张量。因此，pytorch在内部复制了b的元素，进行了这个操作。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_140.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_140.png)'
- en: across all the columns。 So for example， b one， which is the first element of
    b would be replicated。
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有列之间。例如，b中的第一个元素b1将被复制。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_142.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_142.png)'
- en: here across all the columns in this multiplication。 And now we're trying to
    back propagate through。
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个乘法中，在所有列之间。现在我们试图进行回传。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_144.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_144.png)'
- en: this operation to count some in。 So when we are calculating this derivative，
    it's important to。
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这个操作用于计数。那么，当我们计算这个导数时，这一点很重要。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_146.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_146.png)'
- en: realize that these two， this looks like a single operation， but actually it's
    two operations。 applied sequentially。 The first operation that pytorch did is
    it took this column tensor and。 replicated it across all the， across all the columns
    basically 27 times。 So that's the first。 operation， it's a replication。 And then
    second operation is the multiplication。 So let's first。
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 认识到这两个看起来像是一个单一操作，但实际上是两个操作，顺序执行。pytorch进行的第一个操作是复制了这个列张量，基本上在所有列上复制了27次。这是第一个操作，复制。然后第二个操作是乘法。所以让我们先。
- en: backtrack through the multiplication。 If these two arrays were of the same size，
    and we just have。 a and b， both of them three by three， then how do we how do
    we back propagate through a multiplication？
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 回溯乘法。如果这两个数组大小相同，而我们只有a和b，它们都是三乘三的，那么我们该如何回传乘法？
- en: So if you just have scalars and not tensors， then if you have c equals a times
    b， then what is the。 order of the of c with respect to b？ Well， it's just a。 And
    so that's the local derivative。 So here in our case， I'm doing the multiplication
    and back propagate through just multiplication itself。
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只有标量而不是张量，那么如果c等于a乘b，那么c关于b的阶数是什么？嗯，它只是a。因此，这就是局部导数。在我们的案例中，我在进行乘法并回传通过乘法本身。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_148.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_148.png)'
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_149.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_149.png)'
- en: which is element wise， is going to be the local derivative， which in this case，
    is simply counts。 because counts is the a。 So this is the local derivative and
    then times because the chain rule。 deprops。 So this here is the derivative or
    the gradient， but with respect to replicated b。 But we don't have a replicated
    b， we just have a single b column。 So how do we now back propagate。
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这是逐元素的，将是局部导数，在这种情况下，仅仅是计数。因为计数就是 a。因此这是局部导数，然后乘以链式法则。deprops。因此这里是导数或梯度，但相对于复制的
    b。但是我们没有复制的 b，只有一个单一的 b 列。那么我们现在如何进行反向传播呢？
- en: through the replication？ And intuitively， this b one is the same variable and
    it's just reused。 multiple times。 And so you can look at it as being equivalent
    to a case with encountered in micrograd。 And so here， I'm just pulling out a random
    graph we used in micrograd。 We had an example where a。 single node has its output
    feeding into two branches of basically the graph until the last function。
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 通过复制？直观上，这个 b 一是同一个变量，只是被多次重用。因此，你可以将其视为在 micrograd 中遇到的情况。因此在这里，我只是拉出我们在 micrograd
    中使用的随机图形。我们有一个示例，其中一个单一节点的输出输入到基本上是图的两个分支，直到最后一个函数。
- en: And we're talking about how the correct thing to do in the backward pass is
    we need to sum all。 the gradients that arrive at any one node。 So across these
    different branches。 the gradients would sum。 So if a node is used multiple times。
    the gradients for all of its uses sum during back propagation。
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论的是在反向传递中正确的做法是需要对到达任何一个节点的所有梯度进行求和。因此，在这些不同的分支中，梯度将进行求和。如果一个节点被多次使用，则在反向传播期间对其所有用法的梯度求和。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_151.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_151.png)'
- en: So here， b one is used multiple times in all these columns。 And therefore。 the
    right thing to do here， is to sum horizontally across all the rows。 So to sum
    in dimension one。 but we want to retain this， dimension so that the so that counts
    them in and its gradient are going to be exactly the same shape。 So we want to
    make sure that we keep them as true。 So we don't lose this dimension。
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在这里，b 一在所有这些列中被多次使用。因此，在这里正确的做法是沿着所有行进行横向求和。因此在维度一中求和。但我们想保留这个维度，以便计数它们和它的梯度将完全保持相同的形状。因此我们想确保保持它们的真实。因此我们不会丢失这个维度。
- en: And this will make the counts on him be exactly shape 32 by one。 So revealing
    this comparison as。
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使计数的形状完全是 32 乘 1。因此揭示这个比较为。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_153.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_153.png)'
- en: well and running this， we see that we get an exact match。 So this derivative
    is exactly correct。
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，运行这个，我们看到我们得到了完全匹配。因此这个导数是完全正确的。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_155.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_155.png)'
- en: And let me erase this。 Now let's also back propagate into counts， which is the
    other variable here。
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 让我擦掉这个。现在让我们也反向传播到计数，这是这里的另一个变量。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_157.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_157.png)'
- en: to create props。 So from props to count some info， we just did that， let's go
    into counts as well。 So the counts will be the counts are a。 So dc by da is just
    b。 So therefore， it's count some， info。 And then times chain rule， d props。 Now，
    counts， I'm in is three， two by one。 D props is 32 by 27。 So those will broadcast
    fine and will give us decounts。 There's no。
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 用于创建道具。因此，从道具到某些信息的计数，我们刚刚做了这个，让我们深入了解计数。所以计数将是计数是一个。所以 dc 通过 da 仅仅是 b。因此，所以它是某些信息的计数。然后乘以链式法则，d
    道具。现在，计数，我在的是三，二乘一。D 道具是 32 乘 27。因此这些将广播良好，并将给我们 decounts。没有。
- en: additional summation required here。 There will be a broadcasting that happens
    in this。 multiply here， because counts am in needs to be replicated again to correctly
    multiply。
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要额外的求和。因为在这里进行相乘时，会发生广播，因为计数 am 需要再次复制以正确相乘。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_159.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_159.png)'
- en: d props。 But that's going to get the correct result。 So as far as the single
    operation is concerned。 so we've back propagate from props to counts， but we can't
    actually check the derivative of counts。
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: d 道具。但这将获得正确的结果。因此，就单一操作而言。我们已经从道具反向传播到计数，但我们实际上无法检查计数的导数。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_161.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_161.png)'
- en: I have it much later on。 And the reason for that is because counts am in depends
    on counts。 And so there's a second branch here that we have to finish because
    counts am in back propagates。 into count some and count some will back propagate
    into counts。 And so counts is a node that is being， used twice。
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我稍后会处理这个。原因是因为计数的一部分依赖于计数。因此这里有第二个分支，我们必须完成，因为计数的一部分在反向传播。到计数的一部分，而计数的一部分将反向传播到计数。因此计数是一个被使用了两次的节点。
- en: It's used right here into props and it goes through this other branch through
    count， some info。 So even though we've calculated the first contribution of it，
    we still have to calculate。
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 它在这里被用作道具，穿过这个其他分支经过计数的一些信息。所以即使我们已经计算了它的第一个贡献，我们仍然必须计算。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_163.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_163.png)'
- en: the second contribution of it later。 Okay， so we're continuing with this branch。
    We have the derivative。
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 之后它的第二个贡献。好的，我们继续这个分支。我们有导数。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_165.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_165.png)'
- en: for counts am in now we want the derivative counts some。 So decount some equals，
    what is the local。 derivative of this operation？ So this is basically an element
    twice one over counts some。 So counts。 some raise to the power of negative one
    is the same as one over counts some。 If we go to wall from。
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 对于计数的一部分，现在我们想要计数的一部分的导数。因此计数的一部分的导数等于，这个操作的局部导数是什么？所以这基本上是元素两次的1除以计数的一部分。所以计数的一部分的负一次方等同于1除以计数的一部分。如果我们从墙那里去。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_167.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_167.png)'
- en: alpha， we see that x is negative one d by d by d x of it is basically negative
    x to the negative。 two， right？ One negative one over square is the same as negative
    x to the negative two。
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: alpha，我们看到x是负的，d对d x的导数基本上是负x的负二次方，对吧？负一除以平方与负x的负二次方是相同的。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_169.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_169.png)'
- en: So decount some here will be local derivative is going to be negative counts
    some to the negative two。 That's the local derivative times chain rule， which
    is decount some info。 So that's decount some。
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，计数的一部分的局部导数将是负计数的一部分的负二次方。这是局部导数乘以链式法则，即计数的一些信息。因此就是计数的一部分。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_171.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_171.png)'
- en: Let's uncomment this and check that I am correct。 Okay， so we have perfect equality。
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们取消注释并检查我是否正确。好的，所以我们有完美的相等。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_173.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_173.png)'
- en: And there's no sketching is going on here with any shapes because these are
    of the same shape。
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这里没有任何形状的草图，因为这些是相同的形状。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_175.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_175.png)'
- en: Okay， next up we want to back propagate through this line。 We have that counts
    some is counts that。
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，接下来我们想要通过这条线进行反向传播。我们有的计数是计数的一部分。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_177.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_177.png)'
- en: some along the rows。 So I wrote out some help here。 We have to keep in mind
    that counts of。 course is 32 by 27 and counts some is 32 by one。 So in this back
    propagation， we need to take this。 column of the root is and transform it into
    a array of the roots to the machine learning。
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 沿着行来进行一些操作。因此我在这里写了一些帮助信息。我们必须记住计数的维度，当然是32乘27，而计数的一部分是32乘1。因此在这个反向传播中，我们需要将这一列的根转换为机器学习中的根数组。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_179.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_179.png)'
- en: So what is this operation doing？ We're taking in some kind of an input like
    say a three by。 two matrix A and we are summing up the rows into a column tensor
    B。 B1 B2 B3 that is basically this。
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 那么这个操作在做什么？我们正在输入某种输入，比如一个3乘2的矩阵A，并将行相加到列张量B中。B1 B2 B3基本上就是这样。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_181.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_181.png)'
- en: So now we have the derivatives of the loss with respect to B all the elements
    of B。
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有损失对B的导数，B的所有元素。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_183.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_183.png)'
- en: And now we want to deliver the loss with respect to all these little a's。 So
    how do the b's depend。 on the a's is basically what we're after what is the local
    derivative of this operation。
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们想要交付损失，相对于这些小的a。那么b是如何依赖于a的，这基本上就是我们要追求的，即这个操作的局部导数是什么。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_185.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_185.png)'
- en: Well， we can see here that B1 only depends on these elements here。 The derivative
    of B1 with。 respect to all of these elements down here is zero。 But for these
    elements here like a one one a。 one two etc。 the local derivative is one right。
    So db one by d a one one for example is one。 So it's one one and one。 So when
    we have the derivative of the loss with respect to B1。
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我们可以看到这里 B1 仅依赖于这些元素。B1 对这里所有元素的导数是零。但对于这些元素，比如 a one one、a one two 等，局部导数是一个。因此
    db one 对 d a one one 的导数就是一个。所以是一个、一个和一个。因此当我们计算损失对 B1 的导数时。
- en: the local derivative of B1 respect to these inputs is zeros here but it's one
    on these guys。 So in the chain rule， we have the local derivative times sort of
    the derivative of B1。 And so because， the local derivative is one on these three
    elements。 the local derivative of multiplying the derivative， of B1 will just
    be the derivative of B1。
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: B1 对这些输入的局部导数在这里是零，但在这些元素上是一个。因此，在链式法则中，我们有局部导数乘以 B1 的某种导数。由于局部导数在这三个元素上是一个，乘以
    B1 的导数的局部导数将只是 B1 的导数。
- en: And so you can look at it as a router。 Basically an addition， is a router of
    gradient。 Whatever gradient comes from above， it just gets routed equally to all。
    the elements that participate in that addition。 So in this case， the derivative
    of B1 will just。 flow equally to the derivative of a one one a one two and a one
    three。
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可以把它看作一个路由器。基本上，添加是梯度的路由器。无论来自上方的梯度是什么，它都会均等地路由到所有参与该加法的元素。因此在这种情况下，B1 的导数将均等地流向
    a one one、a one two 和 a one three 的导数。
- en: So if we have a derivative of， all the elements of B and in this column tensor。
    which is d counts sum that we've calculated just now。
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们对 B 的所有元素和这个列张量的导数，即我们刚刚计算的 d counts sum。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_187.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_187.png)'
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_188.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_188.png)'
- en: we basically see that what that amounts to is all of these are now flowing to
    all these elements of A。 and they're doing that horizontally。 So basically what
    we want is we want to take the。 decount sum of size 32 by one and we just want
    to replicate it 27 times horizontally to create 32。
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基本上看到这意味着这些现在流向 A 的所有元素，并且它们是水平流动的。因此，我们想要的是取大小为 32 乘 1 的计数总和，并希望将其水平复制 27
    次以创建 32。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_190.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_190.png)'
- en: by 27 array。 So there's many ways to implement this operation。 You could of
    course just replicate。
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 27 数组。因此，有很多方法来实现这个操作。当然，你可以简单地复制。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_192.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_192.png)'
- en: the tensor。 But I think maybe one clean one is that the counts is simply torch
    dot once like。 so just an two dimensional arrays of once in the shape of counts。
    So 32 by 27 times， the counts sum。 So this way we're letting the broadcasting
    here， basically implement the。
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 张量。但我认为一个简单的方式是计数仅仅是 torch dot once，就像是一个形状为计数的二维数组。因此 32 乘 27 的计数总和。这样，我们就让广播在这里基本上实现了。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_194.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_194.png)'
- en: replication。 You can look at it that way。 But then we have to also be careful
    because。
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 复制。你可以这样看。但我们也必须小心，因为。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_196.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_196.png)'
- en: decounts was all already calculated。 We calculated earlier here。 And that was
    just the first branch。 and we're now finishing the second branch。 So we need to
    make sure that these gradients add。
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 计数已经全部计算。我们在这里早些时候计算过。这只是第一分支，而我们现在完成第二分支。因此，我们需要确保这些梯度相加。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_198.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_198.png)'
- en: so plus equals。 And then here， let's comment out the comparison and let's make
    sure crossing fingers。
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 所以加等于。然后在这里，让我们注释掉比较，并确保万事如意。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_200.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_200.png)'
- en: that we have the correct result。 So pytorch agrees with us on this gradient
    as well。 Okay。 hopefully。
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了正确的结果。因此，pytorch 也同意我们的这个梯度。好的，希望如此。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_202.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_202.png)'
- en: we're getting a hang of this now。 Counts is an element why X of norm logits。
    So now we want D norm。 logits。 And because it's an element has operation everything
    is very simple。 What is the local。 derivative of e to the X？ It's famously just
    e to the X。 So this is the local derivative。 That is the local derivative。 Now
    we already calculated it and it's inside counts。 So we。
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在对此有了一些了解。Counts是norm logits的一个元素。因此现在我们想要D norm logits。因为这是一个元素操作，一切都很简单。e的X的局部导数是什么？它著名地就是e的X。因此这是局部导数。这是局部导数。我们已经计算过了，它在counts里面。所以我们。
- en: made as well potentially just reuse counts。 That is the local derivative times，
    uh， decounts。 Funny as that looks， constant decounts is iterative on the norm
    logits。 And now let's。
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以潜在地重用counts。那是局部导数乘以，呃，decounters。虽然看起来有点搞笑，但常量decounters是对norm logits的迭代。现在让我们。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_204.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_204.png)'
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_205.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_205.png)'
- en: erase this and let's verify and it looks good。 So that's a normal digits。 Okay，
    so we are here。
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 抹去这个，让我们验证一下，看起来很好。所以这是正常的digits。好的，我们在这里。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_207.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_207.png)'
- en: on this line now， the normal digits。 We have that and we're trying to calculate
    the logits and the。 logit maxes。 So back propagating through this line。 Now we
    have to be careful here because the。 shapes again are not the same。 And so there's
    an implicit broadcasting happening here。 So normal。
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一行，现在是正常的digits。我们有这个，我们正在尝试计算logits和logit maxes。因此在通过这一行进行反向传播时。我们必须在这里小心，因为形状再次不相同。因此这里发生了一个隐式广播。因此正常。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_209.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_209.png)'
- en: digits has this shape 32 by 27。 Logits does as well。 But logit maxes is only
    32 by one。 So there's。 a broadcasting here in the minus。 Now here I tried to sort
    of write out a two example again。 We basically have that this is our C equals
    A minus B。 And we see that because of the shape。 these are three by three， but
    this one is just a column。 And so for example， every element of C。
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: digits的形状是32乘27。Logits也是如此。但logit maxes仅为32乘1。因此这里在减法上有一个广播。现在我尝试再次写出一个例子。我们基本上有C等于A减去B。我们看到由于形状，这些是三乘三，但这一只是一个列向量。因此，例如，C的每个元素。
- en: we have to look at how it came to be。 And every element of C is just the corresponding
    element of A。 minus basically that associated B。 So it's very clear now that the
    derivatives of every one of。 these C's with respect to their inputs are one for
    the corresponding A。 And it's a negative one for。 the corresponding B。 And so
    therefore， the derivatives on the C will flow equally to the corresponding A's。
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须看看它是如何形成的。C的每个元素只是对应的A的元素，基本上减去那个相关的B。所以现在很清楚，所有这些C对其输入的导数对于对应的A都是1，而对应的B则是负1。因此，C上的导数会平等地流向对应的A。
- en: And then also to the corresponding B's。 But then in addition to that， the B's
    are broadcast。 So we'll， have to do the additional sum just like we did before。
    And of course。 derivatives for B's will， undergo A minus because the local derivative
    here is negative one。 So the C 32 by D B three is negative， one。 So let's just
    implement that。 Basically。
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 然后也到对应的B。但除此之外，B也被广播。因此我们将不得不做额外的求和，就像我们之前所做的那样。当然，B的导数将经历A减去，因为这里的局部导数是负1。所以C
    32乘D B 3是负1。因此让我们实现这一点。基本上。
- en: D logits will be exactly copying the derivative on。
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: D logits将精确复制导数。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_211.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_211.png)'
- en: normal digits。 So D logits equals D normal logits， and I'll do a dot clone for
    safety。 So we're just， making a copy。 And then we have that D logit Maxis will
    be the negative of D normal logits。 is of the negative sign。 And then we have
    to be careful because logit Maxis is a column。 And so just like we saw before，
    because we keep replicating the same elements across all the columns。
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 正常的digits。因此D logits等于D正常logits，我会做一个dot clone以确保安全。所以我们只是在做一个复制。然后我们有D logit
    Maxis将是D正常logits的负值，带有负号。然后我们必须小心，因为logit Maxis是一个列向量。正如我们之前所看到的，因为我们不断复制相同的元素到所有列。
- en: then in the backward pass， because we keep reusing this。 these are all just
    like separate branches of， use of that one variable。 And so therefore。 we have
    to do a sum along one would keep them equals true。
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在反向传播中，因为我们不断重用这个。这些都是就像是对那个变量的单独分支。因此，我们必须沿着一个进行求和，这样它们就会保持相等为真。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_213.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_213.png)'
- en: so that we don't destroy this dimension。 And then the logit Maxis will be the
    same shape。
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 以便我们不破坏这个维度。然后 logit 最大值将保持相同的形状。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_215.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_215.png)'
- en: Now we have to be careful because this D logits is not the final D logits。 And
    that's because。
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们必须小心，因为这个 D logits 不是最终的 D logits。这是因为。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_217.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_217.png)'
- en: not only do we get gradient signal into logits through here， but the logit Maxis
    is a function of。 logits， and that's the second branch into logits。 So this is
    not yet our final derivative for logits。
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅能通过这里获得梯度信号到 logits，而且 logit 最大值是 logit 的函数，这就是第二个分支到 logits。因此这还不是我们对 logits
    的最终导数。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_219.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_219.png)'
- en: We will come back later for the second branch。 For now， D logit Maxis is the
    final derivative。
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍后会回到第二个分支。现在，D logit 最大值是最终的导数。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_221.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_221.png)'
- en: So let me uncomment this CMP here， and let's just run this。 And logit Maxis。
    if PyTorch agrees with us。
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我在这里取消注释这个 CMP，然后我们来运行一下。看看 logit 最大值，如果 PyTorch 同意我们的看法。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_223.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_223.png)'
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_224.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_224.png)'
- en: So that was the derivative into through this line。 Now before we move on， I
    want to pause here。
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是通过这一行的导数。在我们继续之前，我想在这里暂停一下。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_226.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_226.png)'
- en: briefly， and I want to look at these logit Maxis and especially their gradients。
    We've talked。 previously in the previous lecture that the only reason we're doing
    this is for the numerical。 stability of the softmax that we are implementing here。
    And we talked about how if you take these。 logits for any one of these examples，
    so one row of this logit's tensor。 If you add or subtract any。
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我想看看这些 logit 最大值及其梯度。我们之前在上一讲中提到，我们这样做的唯一原因是为了我们在这里实现的 softmax 的数值稳定性。我们谈到如果你获取这些
    logit 的话，对于这些例子中的任何一个，也就是这个 logit 张量的一行。如果你加或减任何。
- en: value equally to all the elements， then the value of the probes will be unchanged。
    You're not changing， the softmax。 The only thing that this is doing is it's making
    sure that X doesn't overflow。 And the， reason we're using a max is because then
    we are guaranteed that each row of logits。 the highest number， is zero。 And so
    this will be safe。 And so basically what that has repercussions。
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所有元素的值相等，那么探针的值将保持不变。你没有改变 softmax。此操作唯一的作用是确保 X 不会溢出。我们使用最大值的原因是我们保证每一行的
    logit 最高的数字是零。因此这将是安全的。这基本上带来了后果。
- en: If it is the case that， changing logit Maxis does not change the props and therefore
    does not change the loss。 then the gradient on logit Maxis should be zero。 Because
    saying those two things is the same。 So indeed we hope that this is very， very
    small numbers。 Indeed we hope this is zero。 Now because of。
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 如果改变 logit 最大值不改变属性，因此不改变损失，那么 logit 最大值的梯度应该为零。因为这两件事是相同的。所以确实我们希望这个数值非常非常小。确实我们希望这是零。现在因为。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_228.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_228.png)'
- en: floating point sort of wonkiness， this doesn't count exactly zero only in some
    of the rows it does。 But we get extremely small values like one in negative nine
    or ten。 And so this is telling us。
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 浮点数的某种奇怪性，这并不完全等于零，只有在某些行中是这样。但是我们得到的极小值像是负九或十的指数级数值。所以这告诉我们。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_230.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_230.png)'
- en: that the values of logit Maxis are not impacting the loss as they shouldn't。
    It feels kind of。
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: logit 最大值的值并没有影响损失，正如它们不应该那样。这感觉有点。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_232.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_232.png)'
- en: weird to back propagate through this branch honestly。 Because if you have any
    implementation of like。
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 诚实地说，通过这个分支反向传播有点奇怪。因为如果你有任何实现，比如。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_234.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_234.png)'
- en: F dot cross entropy and pytorch and you you block together all these elements
    and you're not doing。 the back propagation piece by piece， then you would probably
    assume that the derivative through here。 is exactly zero。 So you would be sort
    of skipping this branch because it's only done for numerical。 stability。 But it's
    interesting to see that even if you break up everything into the full。
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: F dot 交叉熵和 pytorch，并且你将所有这些元素一起处理，而不是逐个进行反向传播，那么你可能会假设这里的导数是完全为零。因此，你会在某种程度上跳过这个分支，因为它只是为了数值稳定性。但有趣的是，即使你将所有内容拆分成完整的。
- en: atoms and you still do the computation as you'd like with respect to numerical
    stability。 the correct thing happens and you still get a very， very small gradients
    here。 Basically。 reflecting the fact that the values of these do not matter with
    respect to the final loss。
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 原子，你仍然可以按照你希望的方式进行计算，以确保数值稳定性，正确的事情会发生，你仍然会得到非常非常小的梯度。基本上，反映出这些值与最终损失无关。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_236.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_236.png)'
- en: Okay， so let's now continue back propagation through this line here。 We've just
    calculated the。 logit Maxis and now we want to back prop into logits through this
    second branch。 Now here， of。 course， we took logits and we took the max along
    all the rows and then we looked at its values here。 Now the way this works is
    that in pytorch this thing here， the max returns both the values and。
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在让我们继续通过这一行进行反向传播。我们刚刚计算了 logit Maxis，现在我们想要通过这个第二个分支向 logits 进行反向传播。在这里，当然，我们取了
    logits，并沿着所有行取了最大值，然后查看其值。现在，这在 pytorch 中的工作方式是，max 返回值和。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_238.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_238.png)'
- en: it returns the indices at which those values to column the maximum value。 Now
    in a forward pass。 we only used values because that's all we needed。 But in the
    backward pass。 it's extremely useful to， know about where those maximum values
    occurred and we have the indices at which they occurred。 And this will of course
    help us do help us do the back propagation。 Because what should the。
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 它返回这些值的索引，也就是列出最大值的位置。现在在前向传播中，我们只使用了值，因为那是我们所需要的。但在反向传播中，知道这些最大值发生的位置是极其有用的，我们有发生的位置的索引。这当然会帮助我们进行反向传播。因为应该是什么。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_240.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_240.png)'
- en: backward pass be here in this case？ We have the logis tensor which is 32 by
    27 and in each row。 we find the maximum value and then that value gets plucked
    out into logit Maxis。 And so intuitively。 basically the derivative flowing through
    here then should be one times the local。 derivative is one for the appropriate
    entry that was plucked out and then times the global derivative。
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，反向传播应该在这里吗？我们有一个 32x27 的 logis 张量，在每一行中，我们找到最大值，然后将该值提取到 logit Maxis
    中。因此，直观上来看，基本上通过这里流动的导数应该是 1 乘以局部导数，对于被提取的适当条目局部导数为 1，然后再乘以全局导数。
- en: of the logit Maxis。 So really what we're doing here， if you think through it，
    is we need to take。 the delogit Maxis and we need to scatter it to the correct
    positions in these logits from where the。 maximum values came。 And so I came up
    with one line of code that does that。 Let me just say very。
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 这是关于 logit Maxis 的内容。所以实际上我们在这里所做的，如果你仔细想想，就是我们需要取出 delogit Maxis，然后将其散布到这些
    logits 的正确位置，从哪里获取最大值。因此我想出了这一行代码来实现这一点。让我简单说一下。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_242.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_242.png)'
- en: much of stuff here。 So the line of， you could do it kind of very similar to
    what we've done here。
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有很多内容。因此，这一行代码，你可以做得与我们在这里做的非常相似。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_244.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_244.png)'
- en: where we create a zeros and then we populate the correct elements。 So we use
    the indices here and。
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们创建一个零数组，然后填充正确的元素。因此我们在这里使用索引。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_246.png)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_246.png)'
- en: we would set them to be one。 But you can also use one heart。 So at that one
    heart and then I'm taking， the logit Maxis over the first dimension that indices
    and I'm telling PyTorch that the。 dimension of every one of these tensors should
    be 27。 And so what this is going to do。
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会将它们设置为 1。但你也可以使用一个心跳。因此，在这个心跳下，我取了 logit Maxis 的第一个维度索引，并告诉 PyTorch，每一个这些张量的维度应该是
    27。因此，这将会发生什么。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_248.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_248.png)'
- en: is， okay， I apologize， this is crazy。 Beau Thieau times show of this。 It's really
    just an array of。
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，好的，我道歉，这太疯狂了。Beau Thieau的时间展示了这一点。它实际上只是一个数组。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_250.png)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_250.png)'
- en: where the Maxis came from in each row and that element is one and the all the
    other elements are。 zero。 So it's a one-hot vector in each row and these indices
    are now populating a single one。
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 每行的Maxis来自于哪里，该元素为1，其他所有元素均为零。所以每行都是一个one-hot向量，这些索引现在填充了一个单一的1。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_252.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_252.png)'
- en: in the proper place。 And then what I'm doing here is I'm multiplying by the
    logit Maxis。 And keep in。
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在适当的位置。然后我在这里做的就是与logit Maxis相乘。请记住。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_254.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_254.png)'
- en: mind that this is a column of 32 by one。 And so when I'm doing this times the
    logit Maxis。
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，这是一个32乘1的列。因此，当我将其与logit Maxis相乘时。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_256.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_256.png)'
- en: the logit Maxis will broadcast and that column will get replicated and then
    the element wise。 multiply will ensure that each of these just gets routed to
    whichever one of these bits is turned。 on。 And so that's another way to implement
    this kind of an operation。 And both of these can be。
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: logit Maxis会广播，这一列将被复制，然后逐元素相乘将确保这些仅路由到其中一个被打开的位。因此，这是一种实现这种操作的另一种方式。这两种方式都可以。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_258.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_258.png)'
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_259.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_259.png)'
- en: used。 I just thought I would show an equivalent way to do it。 And I'm using
    plus equals because。 we already calculated the logits here。 And this is now the
    second branch。 So let's。
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 使用。只是想展示一种等效的方法。我使用了加等于，因为我们已经在这里计算了logits。这是现在的第二个分支。所以让我们。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_261.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_261.png)'
- en: look at logits and make sure that this is correct。 And we see that we have exactly
    the correct answer。
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 查看logits并确保这是正确的。我们看到我们得到了完全正确的答案。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_263.png)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_263.png)'
- en: Next up， we want to continue with logits here。 That is an outcome of a matrix
    multiplication and a。 bias offset in this linear layer。 So I've printed out the
    shapes of all these intermediate tensors。 We see that logits is of course 32 by
    27 as we've just seen。 Then the H here is 32 by 64。 So these。 are 64 dimensional
    hidden states。 And then this w matrix projects those 64 dimensional vectors into。
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们想要继续处理这里的logits。这是矩阵乘法和线性层中的偏置偏移的结果。因此，我打印出了所有这些中间张量的形状。我们看到logits当然是32乘27，正如我们刚才看到的。然后这里的H是32乘64。这是64维的隐藏状态。然后这个w矩阵将这64维向量投影到。
- en: 27 dimensions。 And then there's a 27 dimensional offset， which is a one dimensional
    vector。 Now we should note that this plus here actually broadcasts because H multiplied
    by w two will。 give us a 32 by 27。 And so then this plus B two is a 27 dimensional
    vector here。 Now in the rules of， broadcasting， what's going to happen with this
    bias vector is that this one dimensional vector of 27。
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 27维。然后有一个27维的偏移量，这是一个一维向量。现在我们应该注意，这里的加法实际上是广播，因为H乘以w2会给我们一个32乘27的结果。然后这个加B2在这里是一个27维的向量。现在在广播的规则中，这个偏置向量将会发生什么是，这个27维的一维向量。
- en: will get aligned with an padded dimension of one on the left。 And it will basically
    become a row。 vector。 And then it will get replicated vertically 32 times to make
    it 32 by 27。 And then there's an， element bias multiply。 Now， the question is
    how do we back propagate from logits to the hidden states。 the weight matrix w
    two and the bias B two。 And you might think that we need to go to some。
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 将与左侧填充维度为一对齐。基本上，它将变成一个行向量。然后它将垂直复制32次，形成32乘27的矩阵。接着，有一个元素偏置乘法。现在，问题是我们如何从logits反向传播到隐藏状态，即权重矩阵w2和偏置B2。你可能会认为我们需要去某个地方。
- en: matrix calculus。 And then we have to look up the derivative for a matrix multiplication。
    But。 actually you don't have to do any of that。 And you can go back to first principles
    and derive this。 yourself on a piece of paper。 And specifically what I like to
    do and I what I find works well for me。 is you find a specific small example that
    you then fully write out。
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵微积分。然后我们必须查找矩阵乘法的导数。但是，实际上你不需要做任何这些。你可以回到基本原理，在纸上自己推导出来。具体来说，我喜欢做的，发现对我有效的，是找一个具体的小例子，然后完全写出来。
- en: And then in process of analyzing， how that individual small example works。 you
    will understand a broader pattern。 And you'll be able。 to generalize and write
    out the full general formula for how these derivatives flow in an。 expression
    like this。 So let's try that out。 So part in the low budget production here， but。
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在分析的过程中，了解这个小例子是如何工作的。你将理解更广泛的模式。你将能够概括并写出这些导数在像这样的表达式中的完整一般公式。那么我们试试这个。所以在这里的低预算制作中，但。
- en: what I've done here is I'm writing it out on the piece of paper。 Really what
    we are interested in。 is we have a multiplied B plus C。 And that creates a D。
    And we have the derivative of the loss with。 respect to D。 And we'd like to know
    the derivative of the losses with respect to A， B and C。 Now these here are a
    little two dimensional examples of a matrix multiplication。 Two by two。
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里所做的就是在纸上写出来。实际上我们感兴趣的是，我们有 A 乘以 B 加上 C。这产生了 D。我们有损失相对于 D 的导数。我们想知道损失相对于
    A、B 和 C 的导数。这里是两个维度的矩阵乘法的示例，二维。
- en: times a two by two plus a two， a vector of just two elements， C one and C two
    gives me a two by two。 Now notice here that I have a bias vector here called C。
    And the bias vector is C one and C two。 But as I described over here， that bias
    vector will become a row vector in the broadcasting。 and will replicate vertically。
    So that's what's happening here as well。 C one C two is replicated。
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 乘以一个二维矩阵加上一个只有两个元素的向量 C one 和 C two，给我一个二维矩阵。现在注意到这里有一个叫 C 的偏置向量。偏置向量是 C one
    和 C two。但是正如我在这里描述的，那个偏置向量会在广播中变成行向量，并会在垂直方向上复制。所以这里也发生了这种情况。C one C two 被复制。
- en: vertically。 And we see how we have two rows of C one C two as a result。 So now
    when I say。 write it out， I just mean like this， basically break up this matrix
    multiplication into the actual。 thing that's going on under the hood。 So as a
    result of matrix multiplication and how it works。 D one one is the result of a
    dot product between the first row of A and the first column of B。
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 垂直地。我们看到结果是有两行 C one C two。那么现在当我说写出来时，我的意思就是像这样，基本上将这个矩阵乘法分解成实际在背后发生的事情。因此，矩阵乘法的结果以及它是如何工作的。D
    one one 是 A 的第一行与 B 的第一列之间的点积的结果。
- en: So a one one B one one plus a one two B two one plus C one。 And so on and so
    forth for all the。 other elements of D。 And once you actually write it out， it
    becomes obvious。 This is just a bunch。 of multiplies and ads。 And we know from
    micro grad how to differentiate multiplies and ads。
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 所以 a one one B one one 加上 a one two B two one 加上 C one，等等，其他所有 D 的元素依此类推。一旦你真的写出来，就会很明显。这只是一堆乘法和加法。我们知道从微分梯度如何区分乘法和加法。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_265.png)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_265.png)'
- en: And so this is not scary anymore。 It's not just a matrix multiplication。 It's
    just tedious。 unfortunately。 But this is completely tractable。 We have DL by D
    for all of these。 And we want。 DL by all these little other variables。 So how
    do we achieve that and how do we actually get the。 gradients？ Okay， so the low
    budget production continues here。 So let's for example derive the。
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这不再可怕了。这不仅仅是矩阵乘法。这只是繁琐，不幸的是。但这是完全可行的。我们有 DL 对所有这些的导数。我们想要 DL 对所有这些小变量。那么我们如何实现这一点，实际上如何获得梯度？好吧，低预算制作继续在这里。所以让我们举例推导。
- en: derivative of the loss with respect to a one one。 We see here that a one one
    occurs twice in our。 simple expression right here right here。 And influence is
    D one one and D one two。 So this is。 so what is DL by D a one one？ Well， it's
    DL by D one one times the local derivative of D one one。 which in this case is
    just B one one， because that's what's multiplying a one one here。 So。
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 关于损失相对于 a one one 的导数。我们在这里看到 a one one 在我们的简单表达式中出现了两次。影响是 D one one 和 D one
    two。那么，DL 对 D a one one 的导数是什么呢？它是 DL 对 a one one 的导数乘以 D one one 的局部导数，而在这种情况下就是
    B one one，因为这就是乘以 a one one 的内容。所以。
- en: and likewise here the local derivative of D one two with respect to a one one
    is just B one two。 And so B one two will in the chain rule therefore multiply
    DL by D one two。 And then because A one one， is used both to produce D one one
    and D one two。 we need to add up the contributions of both of， those sort of chains
    that are running in parallel。
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，D 一二 对于 A 一一 的局部导数就是 B 一二。因此，B 一二 在链式法则中将乘以 DL by D 一二。然后因为 A 一一 被用来生成 D
    一一 和 D 一二，我们需要将这两条平行链的贡献相加。
- en: And that's why we get a plus just adding up those， two， those two contributions。
    And that gives us DL by D a one one。 We can do the exact same analysis。 for the
    other one for all the other elements of A。 And when you simply write it out。 it's
    just super， simple taking ingredients on， you know， expressions like this。
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么我们通过将这两个贡献相加得到一个正值。这样我们就得到了对 A 的导数 DL by D a。一一。我们可以对 A 的所有其他元素进行完全相同的分析。当你简单地写出来时，这实际上是超级简单的，就像这样的表达式。
- en: you find that this matrix DL by D a， that we're after， right。 if we just arrange
    all of them in the same shape as A takes。 So A is just， two ratu matrix。 So DL
    by D a here will be also just the same shape tensor with the derivatives now。
    So DL by D a one one， etc。 And we see that actually we can express what we've
    written out here as a。
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现这个矩阵 DL by D a，正是我们想要的，如果我们将它们排列成与 A 相同的形状。因此 A 只是一个 2×2 矩阵。所以这里的 DL by
    D a 也将是一个形状相同的张量，现在包含导数。因此，DL by D a 一一，等等。我们实际上可以将这里写出的内容表示为一个。
- en: matrix multiply。 And so it just so happens that DL by that all of these formulas
    that we've derived。 here by taking gradients can actually be expressed as a matrix
    multiplication。 And in particular。 we see that it is the matrix multiplication
    of these two matrices。 So it is the DL by D and then。 matrix multiplying B but
    B transpose actually。 So you see that B two one and B one two have。
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法。因此，恰好是 DL by D 所有我们推导出的公式，通过取梯度实际上可以表示为矩阵乘法。特别地，我们看到这是这两个矩阵的矩阵乘法。所以这是 DL
    by D，然后矩阵乘以 B，但 B 实际上是转置的。因此，你会看到 B 二一 和 B 一二 有。
- en: changed place。 Whereas before we had of course B one one B one two B two one
    B two two。 So you see。 that this other matrix B is transposed。 And so basically
    what we have on story short just by。 doing very simple reasoning here by breaking
    up the expression in the case of a very simple example。 is that DL by D a is which
    is this is simply equal to DL by D D matrix multiplied with B transpose。
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 位置发生了变化。而之前我们当然有 B 一一、B 一二、B 二一、B 二二。所以你可以看到，这个其他矩阵 B 是转置的。因此，基本上，我们所拥有的故事很简单，通过在非常简单的例子中拆分表达式，得到
    DL by D a 其实是 DL by D D 矩阵与 B 转置的乘积。
- en: So that is what we have so far。 Now we also want the derivative with respect
    to B and C。
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们到目前为止所得到的。现在我们也想要关于 B 和 C 的导数。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_267.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_267.png)'
- en: Now for B I'm not actually doing the full derivation because honestly it's it's
    not deep it's just。 annoying it's exhausting。 You can actually do this analysis
    yourself。 You'll also find that if you。 take this these expressions and you differentiate
    with respect to B instead of A you will find that。 DL by D B is also a matrix
    multiplication。 In this case you have to take the matrix A and transpose it。
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 现在对于 B，我并没有进行完整的推导，因为老实说，这并不复杂，只是让人烦恼，令人疲惫。你可以自己做这个分析。你会发现，如果你取这些表达式并对 B 进行微分，而不是
    A，你会发现 DL by D B 也是一个矩阵乘法。在这种情况下，你需要对矩阵 A 进行转置。
- en: and matrix multiply that with DL by D D。 And that's what gives you a DL by D
    B。 And then here for the offsets C one and C two if you again just differentiate
    with respect to C one。 you will find an expression like this and C two an expression
    like this and basically you'll find。 that DL by D C is simply because they're
    just offsetting these expressions you just have to take。
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将其与 DL by D D 进行矩阵乘法。这就是给你 DL by D B 的原因。然后在偏移量 C 一和 C 二这里，如果你再次对 C 一 进行微分，你会发现像这样的表达式，C
    二则是像这样的表达式，基本上你会发现 DL by D C 是简单因为它们只是偏移这些表达式，你只需取。
- en: the DL by D D matrix of the derivatives of D and you just have to sum across
    the columns。 and that gives you the derivatives for C。 So long story short the
    backward pass of a matrix multiply。 is a matrix multiply and instead of just like
    we had D equals A times B plus C in a scalar case。 we sort of like arrive at something
    very very similar but now with a matrix multiplication instead。
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: D的导数与D的DL矩阵相乘，你只需对列求和。这会给你C的导数。简单来说，矩阵乘法的反向传播就是矩阵乘法，而不是像在标量情况下那样D等于A乘B加C。我们可以得到一个非常相似的结果，但现在是矩阵乘法。
- en: of a scalar multiplication。 So the derivative of D with respect to A is DL by
    D D matrix multiply B。 traspost and here it's A transpose multiply DL by D D。
    But in both cases matrix multiplication。 with the derivative and the other term
    in the multiplication。 And for C it is A sum。 Now I'll。
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 这是标量乘法的导数。因此，D对A的导数是DL与DD的矩阵乘B转置，这里是A转置乘以DL与DD。但在这两种情况下都是矩阵乘法，带有导数和乘法中的另一个项。对于C来说，是A的求和。现在我会。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_269.png)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_269.png)'
- en: tell you a secret I can never remember the formulas that we just arrived for
    back propagating。 from matrix multiplication and I can back propagate through
    these expressions just fine。 And the reason this works is because the dimensions
    have to work out。 So let me give you an example。 Say I want to create D H then
    what should D H be number one I have to know that the shape of D H。
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 我告诉你一个秘密，我永远记不住刚才得出的反向传播公式，尽管我可以很好地通过这些表达式进行反向传播。之所以能行是因为维度必须匹配。让我给你一个例子。假设我想创建D
    H，那么D H应该是什么，第一，我必须知道D H的形状。
- en: must be the same as the shape of H and the shape of H is 30 to by 64。 And then
    the other piece of。 information I know is that D H must be some kind of matrix
    multiplication of D logits with W2。 And D logits is 32 by 27 and W2 is 64 by 27。
    There is only a single way to make the shape or count。 in this case and it is
    indeed the correct result。 In particular here H needs to be 32 by 64。
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: H的形状必须与之相同，而H的形状是30到64。另一个信息是，D H必须是D logits与W2某种矩阵乘法的结果。D logits是32到27，W2是64到27。要使形状或计数匹配，这里只有一种方式，确实是正确的结果。特别是H需要是32到64。
- en: The only， way to achieve that is to take D logits and matrix multiply it with
    you see how I have to take W2。 but I have to transpose it to make the dimensions
    work out。 So W2 transpose。 And it's the only。 way to make these to make this multiply
    those two pieces to make the shapes work out。 And that。
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的方法是将D logits与W2进行矩阵乘法。你看，我必须将W2转置，以使维度匹配。所以是W2转置。这是唯一的方法来使这两个部分相乘以使形状匹配。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_271.png)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_271.png)'
- en: turns out to be the correct formula。 So if we come here we want D H which is
    D A and we see that D A。 is DL by DD matrix multiply B transpose。 So that's D
    logits multiply and B is W2。 So W2 transpose。 which is exactly what we have here。
    So there's no need to remember these formulas。 Similarly。
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明这是正确的公式。如果我们来到这里，我们想要D H，就是D A，我们看到D A是DL与DD的矩阵乘B转置。所以D logits乘以B，即W2的转置，这正是我们所拥有的。因此没有必要记住这些公式。同样。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_273.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_273.png)'
- en: now if I want D W2 well I know that it must be a matrix multiplication of D
    logits and H。
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 现在如果我想要D W2，我知道它必须是D logits与H的矩阵乘法。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_275.png)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_275.png)'
- en: and maybe there's a few transpose or there's one transpose in there as well。
    And I don't know。 which way it is so I have to come to W2 and I see that it's
    shaped it's 64 by 27。 and that has to come from some matrix multiplication of
    these two。 And so to get a 64 by 27 I need to take， H I need to transpose it and
    then I need to matrix multiply it。
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 可能有几个转置，或者其中也有一个转置。我不知道是什么方向，所以我必须查看W2，看到它的形状是64到27。这必须来自这两个矩阵的乘法。因此，要得到64到27，我需要取H并转置，然后进行矩阵乘法。
- en: So that will become 64 by 32 and then， I need to make your small by 32 by 27
    and that's going to give me a 64 by 27。 So I need to make， your small by this
    with D logits that shape just like that。 That's the only way to make the dimensions，
    work out and just use matrix multiplication。 And if we come here we see that that's
    exactly what's， here。 So A transpose A for us is H。
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这将变成64乘以32，然后，我需要将你的小调整为32乘以27，这将给我64乘以27。因此，我需要将你的小与这个D logits结合起来，形状正是这样。这是使维度匹配的唯一方法，使用矩阵乘法。如果我们来看这里，我们看到这正是这里的内容。因此A转置A对我们来说是H。
- en: multiply it with D logits。 So that's W2 and then D B2。
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 将其与D logits相乘。所以这是W2，然后是D B2。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_277.png)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_277.png)'
- en: is just the vertical sum and actually in the same way there's only one way to
    make the shapes。 work out。 I don't have to remember that it's a vertical sum along
    the zero of axis because that's。 the only way that this makes sense because B2
    shape is 27。 So in order to get a D logits here it's。 32 by 27。 So knowing that
    it's just some over D logits in some direction that direction must be。
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 只是垂直求和，实际上以同样的方式只有一种方法可以使形状匹配。我不必记得这是沿着零轴的垂直求和，因为这是。唯一合理的方法，因为B2的形状是27。因此为了得到这里的D
    logits，它是32乘以27。因此，知道这只是对D logits在某个方向上的求和，该方向必须是。
- en: zero because I need to eliminate this dimension。 So it's this。 So this is this
    kind of like the。 hacky way。 Let me copy paste and delete that and let me swing
    over here。 And this is our backward。
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 零，因为我需要消除这个维度。所以是这个。因此这有点像是。 应急方案。让我复制粘贴并删除这个，让我过来这里。这是我们的反向传播。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_279.png)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_279.png)'
- en: password the linear layer， hopefully。 So now let's uncomment these three and
    we're checking that we。
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 密码是线性层，希望如此。所以现在让我们取消注释这三个，我们来检查一下。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_281.png)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_281.png)'
- en: got all the three derivatives correct and run and we see that H， W2 and B2 are
    all exactly correct。
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 所有三个导数都正确并运行，我们看到H，W2和B2都是完全正确的。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_283.png)'
  id: totrans-359
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_283.png)'
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_284.png)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_284.png)'
- en: So we back propagate it through a linear layer。 Now next up we have derivative
    for the H already。
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们通过线性层进行反向传播。接下来我们已经有H的导数。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_286.png)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_286.png)'
- en: and we need to back propagate through 10H into H preact。 So we want to derive
    D H preact。 And here we have to back propagate through a 10H and we've already
    done this in micrograd。
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要通过10H反向传播到H preact。所以我们想要导出D H preact。在这里我们必须通过10H进行反向传播，我们在micrograd中已经做过这个。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_288.png)'
  id: totrans-364
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_288.png)'
- en: and we remember that 10H is a very simple backward formula。 Now unfortunately
    if I just put in D by。
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 我们记得10H是一个非常简单的反向公式。不幸的是，如果我只是把D放入。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_290.png)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_290.png)'
- en: dx of 10H of x into both from alpha it lets us down。 It tells us that it's a
    hyperbolic secant。
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: dx的10H进入α，这让我们感到困惑。它告诉我们这是一个双曲正割。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_292.png)'
  id: totrans-368
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_292.png)'
- en: function squared of x。 It's not exactly helpful but luckily Google image search
    does not let us down。
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: x的平方函数。它并不是特别有用，但幸运的是谷歌图片搜索没有让我们失望。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_294.png)'
  id: totrans-370
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_294.png)'
- en: and it gives us the simpler formula。 And in particular if you have that A is
    equal to 10H of z then D A。 by D z back propagating through 10H is just one minus
    A squared。 And take note that one minus A。 squared A here is the output of the
    10H not the input to the 10H z。 So the D A by D z is here。 formulated in terms
    of the output of that 10H。 And here also in Google image search we have the。
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 它给我们提供了更简单的公式。特别是如果你有A等于10H的z，那么D A。通过D z反向传播10H就是1减去A平方。请注意，1减去A。平方A在这里是10H的输出，而不是10H的输入z。因此D
    A通过D z在这里。以10H的输出为依据进行了公式化。在谷歌图片搜索中我们也有。
- en: full derivation if you want to actually take the actual definition of 10H and
    work through the math。 to figure out one minus 10H squared of z。 So one minus
    A squared is the local derivative。 In our case。
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的推导如果你想要实际使用10H的定义并进行数学计算。以找出1减去10H平方z。因此1减去A平方是局部导数。在我们的案例中。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_296.png)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_296.png)'
- en: that is one minus the output of 10H squared which here is H。 So it's H squared
    and that is the local， derivative and then times the chain rule D H。 So that is
    going to be our candidate implementation。
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是 1 减去 10H 的平方输出，这里是 H。所以它是 H 的平方，那就是局部导数，然后乘以链式法则 D H。所以这将是我们的候选实现。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_298.png)'
  id: totrans-375
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_298.png)'
- en: So if we come here and then uncomment this let's hope for the best and we have
    the right answer。
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果我们来到这里然后取消注释，让我们期待最好的结果，并且我们得到了正确的答案。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_300.png)'
  id: totrans-377
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_300.png)'
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_301.png)'
  id: totrans-378
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_301.png)'
- en: Okay next up we have D H P react and we want to back propagate into the gain
    the B and raw and the。 B and bias。 So here this is the best-term parameters B
    and gain and bias inside the。 best-term that take the B and raw that is exact
    unit Gaussian and they scale it and shift it。 And these are the parameters of
    the best-term。 Now here we have a multiplication but it's worth。
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，接下来我们有 D H P 激活，我们想要将其反向传播到 gain、B 和原始以及 B 和 bias。所以这里这是最佳项参数 B 和 gain 以及
    bias 在最佳项内，采用的是 B 和原始，这是精确的单位高斯，并对其进行缩放和偏移。这些是最佳项的参数。现在这里有一个乘法，但值得注意的是。
- en: noting that this multiply is very very different from this matrix multiply here。
    Matrix multiply。 our dot product between rows and columns of these matrices involved。
    This is an element。
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个相乘与这里的矩阵相乘是非常非常不同的。矩阵相乘。我们是这些矩阵的行与列之间的点积。这是一个元素。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_303.png)'
  id: totrans-381
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_303.png)'
- en: twice multiply so things are quite a bit simpler。 Now we do have to be careful
    with some of the。 broadcasting happening in this line of code though。 So you see
    how B and gain and B and bias are。
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 两次相乘，所以事情变得简单了很多。现在我们确实需要对这行代码中的一些广播操作保持谨慎。所以你可以看到 B 和 gain 以及 B 和 bias 是。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_305.png)'
  id: totrans-383
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_305.png)'
- en: one by 64 but H pre-act and B and raw are 32 by 64。 So we have to be careful
    with that and make。 sure that all the shapes work out fine and that the broadcasting
    is correctly back propagated。
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 一行 64，但 H 的预激活和 B 以及原始是 32 行 64。因此我们必须对此保持谨慎，并确保所有的形状都能正常工作，并且广播能正确回传。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_307.png)'
  id: totrans-385
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_307.png)'
- en: So in particular let's start with D B and gain。 So D B and gain should be and
    here this is again。 element twice multiply and whenever we have A times B equals
    C we saw that the local derivative。 here is just if this is A the local derivative
    is just the B the other。 So the local derivative is。 just B and raw and then times
    chain rule。 So D H pre-act。 So this is the candidate gradient。
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 所以特别地，让我们从 D B 和 gain 开始。因此 D B 和 gain 应该是，这里再次是元素两次相乘，无论何时我们有 A 乘以 B 等于 C，我们看到局部导数。这里如果这是
    A，局部导数就是 B 另一个。所以局部导数就是。只是 B 和原始，然后乘以链式法则。因此 D H 的预激活。这是候选梯度。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_309.png)'
  id: totrans-387
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_309.png)'
- en: Now again we have to be careful because B and gain is of size one by 64 but
    this here。 would be 32 by 64。 And so the correct thing to do in this case of course
    is that B and gain。 here is a rule vector of 64 numbers it gets replicated vertically
    in this operation。 And so。 therefore the correcting to do is to sum because it's
    being replicated and therefore all the。
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们再次需要小心，因为 B 和 gain 的大小是一行 64，但这里会是 32 行 64。因此，在这种情况下正确的做法当然是，B 和 gain 这里是一个包含
    64 个数字的规则向量，它在这个操作中被垂直复制。因此，应该做的更正是求和，因为它正在被复制，因此所有的。
- en: gradients in each of the rows that are now flowing backwards need to sum up
    to that same tensor B。 and gain。 So if to sum across all the zero all the examples
    basically which is the direction。 which this gets replicated。 And now we have
    to be also careful because we um being gain is of shape。 one by 64。 So in fact
    I need to keep them as true。 Otherwise I would just get 64。 Now I don't。
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 每一行的梯度现在反向流动需要加起来等于同样的张量 B 和 gain。因此如果要在所有零和所有示例中求和，基本上就是这个方向，这样它就被复制了。现在我们还需要小心，因为
    B 和 gain 的形状是一行 64。所以事实上，我需要将它们保持为真实值。否则我只会得到 64。现在我没有。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_311.png)'
  id: totrans-390
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_311.png)'
- en: actually really remember why the B and gain and the B and bias I made them be
    one by 64。
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，我真的记不清为什么我将 B 和 gain 以及 B 和 bias 设置为一行 64。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_313.png)'
  id: totrans-392
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_313.png)'
- en: But the biases B one and B two I just made them be one dimensional vectors they're
    not two dimensional。
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 但偏置 B one 和 B two 我只是让它们成为一维向量，而不是二维。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_315.png)'
  id: totrans-394
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_315.png)'
- en: tensors。 So I can't recall exactly why I left the gain and the bias as two dimensional
    but it。
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 张量。因此我不能确切回忆起为什么我把 gain 和 bias 设为二维，但它。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_317.png)'
  id: totrans-396
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_317.png)'
- en: doesn't really matter as long as you are consistent and you're keeping it the
    same。 So in this case。
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 只要你保持一致并保持相同，就没关系。因此在这种情况下。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_319.png)'
  id: totrans-398
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_319.png)'
- en: we're going to keep the dimension so that the tensor shapes work。 Next up we
    have B and raw。 So。
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将保持维度，以使张量形状工作。接下来我们有 B 和 raw。因此。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_321.png)'
  id: totrans-400
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_321.png)'
- en: D B and raw will be um B and gain multiplying D H preact。 That's our chain rule。
    Now what about。
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: D B 和 raw 将变为 um B 和 gain 乘以 D H preact。这就是我们的链式法则。现在怎么样呢？
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_323.png)'
  id: totrans-402
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_323.png)'
- en: the dimensions of this？ We have to be careful right so D H preact is 32 by 64。
    B and gain is。
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 这个维度是多少？我们必须小心，因此 D H preact 是 32 x 64。B 和 gain 是。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_325.png)'
  id: totrans-404
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_325.png)'
- en: one by 64。 So it will just get replicated and to create this multiplication
    which is the correct。
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 1 x 64。因此，它将被复制以创建这个乘法，这是正确的。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_327.png)'
  id: totrans-406
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_327.png)'
- en: thing because in a forward pass it also gets replicated in just the same way。
    So in fact we don't。 need the brackets here we're done and the shapes are already
    correct。 And finally for the bias。 very similar this bias here is very very similar
    to the bias we saw in the linear layer。 And we see。 that the gradients from H
    preact will simply flow into the biases and add up because these are just。
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 因为在前向传递中，它也以同样的方式被复制。因此实际上我们不需要在这里加括号，我们完成了，形状已经正确。最后对于偏置，这个偏置与我们在线性层看到的偏置非常相似。我们看到
    H preact 的梯度将简单地流入偏置并相加，因为这些只是。
- en: these are just offsets。 And so basically we want this to be D H preact but it
    needs to sum along。 the right dimension。 And in this case similar to the gain
    we need to sum across the zeroth dimension。 the examples because of the way that
    the bias gets replicated very quickly。 And we also want to。 have to keep them
    as true。 And so this will basically take this and sum it up and give us a one
    by 64。
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是偏移量。因此，我们基本上希望这成为 D H preact，但它需要沿着正确的维度求和。在这种情况下，类似于 gain，我们需要跨零维进行求和。这些示例是因为偏置复制得非常快。我们还希望保持它们的真实性。因此，这将基本上对其进行求和并给我们一个
    1 x 64 的结果。
- en: So this is the candidate implementation it makes all the shapes work。 Let me
    bring it up down here。
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是候选实现，它使所有形状都能工作。让我把它展示在这里。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_329.png)'
  id: totrans-410
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_329.png)'
- en: and then let me uncomment these three lines to check that we are getting the
    correct result for all the。
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 然后让我取消注释这三行，以检查我们是否得到了所有结果的正确结果。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_331.png)'
  id: totrans-412
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_331.png)'
- en: three tensors。 And indeed we see that all of that got back propagated correctly。
    So now we get to。
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 三个张量。事实上，我们看到所有这些都正确反向传播了。所以现在我们得到了。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_333.png)'
  id: totrans-414
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_333.png)'
- en: the batch norm layer。 We see how here B and gain and B and bias are the parameters
    so the back。 propagation ends。 But B and raw now is the output of the standardization。
    So here what I'm doing of。 course is I'm breaking up the batch norm into manageable
    pieces so we can back propagate through。 each line individually。 But basically
    what's happening is B and mean I is the sum。 So this is。
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 批量规范层。我们看到这里 B 和 gain 以及 B 和 bias 是参数，因此反向传播结束。但 B 和 raw 现在是标准化的输出。因此，这里我所做的当然是将批量规范分解为可管理的部分，以便我们可以逐行反向传播。但基本上发生的是
    B 和 mean I 是求和。因此，这就是。
- en: the B and mean I apologize for the variable naming。 B and diff is x minus mu。
    B and diff two is x minus， mu squared here inside the variance。 B and var is the
    variance。 So sigma square this is B and var。 And it's basically the sum of squares。
    So this is the x minus mu squared and then the sum。 Now you'll， notice one departure
    here。
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: B和mean，抱歉变量命名。B和diff是x减去mu。B和diff二是x减去mu的平方，这在方差内部。B和var是方差。因此sigma平方就是B和var。基本上是平方和。因此这是x减去mu的平方然后求和。现在你会注意到一个偏离。
- en: Here it is normalized as one over M which is the number of examples。 Here I
    am normalizing as one over N minus one instead of N。 And this is deliberate and
    I'll come。 back to that in a bit when we are at this line。 It is something called
    the Bessel's correction。 But this is how I want it in our case。 B and var in then
    becomes basically B and var plus epsilon。
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 这里将其标准化为一除以M，即样本数量。这里我标准化为一除以N减一，而不是N。这是有意为之，稍后我会再提到这一点。这被称为贝塞尔修正。但在我们的案例中我想这样。B和var变成基本的B和var加上epsilon。
- en: Epsilon is one negative five。 And then it's one over square root is the same
    as raising to the。 power of negative point five。 Because point five is square
    root and then negative makes it one over。 square root。 So B and var M is one over
    this denominator here。 And then we can see that B and。 var which is the x hat
    here is equal to the B and diff the numerator multiplied by the。
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: Epsilon是一个负五。然后一除以平方根与提升至负0.5次方是相同的。因为0.5是平方根，负数使其变为一除以平方根。因此B和var M是这里的分母一除以。而且我们可以看到，B和var，即这里的x
    hat等于B和diff分子乘以。
- en: B and var in。 And this line here that creates pre H pre act was the last piece
    we've already。 back propagated through it。 So now what we want to do is we are
    here and we have B and raw and we。 have to first back propagate into B and diff
    and B and var in。 So now we're here and we have D B。
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: B和var在。这一行创建pre H pre act是我们已经反向传播过的最后一块。因此现在我们要做的是，我们在这里，有B和raw，首先需要反向传播到B和diff和B和var在。所以现在我们在这里，有D
    B。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_335.png)'
  id: totrans-420
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_335.png)'
- en: and raw and we need to back propagate through this line。 Now I've written out
    the shapes here and。 indeed B and var in is a shape one by 64。 So there is a broadcasting
    happening here that we。 have to be careful with。 But it is just an element wise
    simple multiplication。 By now we should be。 pretty comfortable with that to get
    the B and diff。 We know that this is just B and var in。
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 和raw，我们需要通过这一行进行反向传播。现在我已经写出了这里的形状，实际上B和var在是一个1乘以64的形状。因此，这里发生了广播，我们必须小心。但是这只是逐元素的简单乘法。到现在我们应该对获得B和diff感到相当舒适。我们知道这只是B和var在。
- en: multiplied with D， B and raw。 And conversely to get D， B and var in we need
    to take B and diff。 and multiply that by D， B and raw。 So this is the candidate。
    But of course we need to make sure。
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 乘以D，B和raw。相反，要获得D，B和var在，我们需要取B和diff，并将其乘以D，B和raw。因此这是候选项。但当然，我们需要确保。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_337.png)'
  id: totrans-423
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_337.png)'
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_338.png)'
  id: totrans-424
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_338.png)'
- en: that broadcasting is obeyed。 So in particular B and var in multiplying with
    D， B and raw。 will be okay and give us 32 by 64 as we expect。 But D， B and var
    in would be taking a 32 by 64。 multiplying it by 32 by 64。 So this is a 32 by
    64。 But of course D， B， this B and var in is only。 one by 64。 So the second line
    here needs a sum across the examples。 And because there's this。
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 广播是遵循的。因此，特别是B和var在与D，B和raw相乘时，将是可以的，并给我们32乘以64的结果，正如我们预期的那样。但D，B和var在将是32乘以64，乘以32乘以64。因此，这是32乘以64。但是，当然，D，B，这个B和var在只是1乘以64。因此，这里第二行需要在样本间求和。而因为有这个。
- en: dimension here， we need to make sure that keep them history。 So this is the
    candidate。 Let's erase this and let's swing down here and implement it。 And let's
    comment out D， B and var。
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的维度，我们需要确保保持它们的历史。因此这是候选项。让我们擦掉这个，然后向下滑动并实现它。让我们注释掉D，B和var。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_340.png)'
  id: totrans-427
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_340.png)'
- en: in and D， B and diff。 Now， we'll actually notice that D。 B and diff by the way
    is going to be incorrect。
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 在和D，B和diff。现在，我们实际上会注意到D。B和diff，顺便说一下，将是错误的。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_342.png)'
  id: totrans-429
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_342.png)'
- en: So when I run this， B and var in this correct， B and diff is not correct。 And
    this is actually。
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 所以当我运行这个时，B和var在是正确的，B和diff是不正确的。这实际上是。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_344.png)'
  id: totrans-431
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_344.png)'
- en: expected because we're not done with B and diff。 So in particular， when we slide
    here， we see here。
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 预期之内，因为我们还没有完成B和diff。因此特别是，当我们滑动这里时，我们在这里看到。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_346.png)'
  id: totrans-433
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_346.png)'
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_347.png)'
  id: totrans-434
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_347.png)'
- en: that B and raw is a function of B and diff。 But actually B and var is a function
    of B and var。 which is a function of B and diff to， which is a function of B and
    diff。 So it comes here。 So B。 D and diff， these variable names are crazy。 I'm
    sorry。 It branches out into two branches。 and we've only done one branch of it。
    We have to continue our back propagation and eventually come。
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: B和raw是B和diff的一个函数。但实际上B和var是B和var的一个函数。这个又是B和diff的一个函数，因此又是B和diff的一个函数。它在这里出现。所以B、D和diff，这些变量名称真是奇怪。抱歉。它分支成两个分支，而我们只做了其中一个分支。我们必须继续我们的反向传播，最终回到这里。
- en: back to be in diff。 And then we'll be able to do a plus equals and get the actual
    current gradient。
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 回到差异中。然后我们就能做一个加等于，得到实际的当前梯度。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_349.png)'
  id: totrans-437
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_349.png)'
- en: For now， it is good to verify that CBMP also works。 It doesn't just lie to us
    and tell us that。 everything is always correct。 It can in fact detect when your
    gradient is not correct。 So that's good。
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，验证CBMP也能正常工作是好的。它不会只是对我们撒谎，告诉我们一切都是正确的。实际上，它可以检测到你的梯度不正确。因此这很好。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_351.png)'
  id: totrans-439
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_351.png)'
- en: to see as well。 Okay， so now we have the derivative here。 and we're trying to
    back propagate through， this line。 And because we're raising to a power of negative
    point five， I brought up the power rule。 and we see that basically we have that
    the B and var will now be we bring down the exponent， so。
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 也看看。好的，现在我们这里有导数。我们正在尝试通过这条线进行反向传播。因为我们提升到负0.5的幂，所以我提到了幂法则。我们看到基本上B和var现在会带下指数。
- en: negative point five times X， which is this。 And now raise to the power of negative
    point five minus。 one， which is a negative one point five。 Now， we would have
    to also apply a small chain。 rule here in our head， because we need to take further
    derivative of B and var with respect to。 this expression here inside the bracket。
    But because it's an element wise operation， and everything。
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 负0.5乘以X，就是这个。现在提升到负0.5减1的幂，也就是负1.5。现在，我们还需要在头脑中应用一个小的链式法则，因为我们需要进一步求B和var对这个括号内表达式的导数。但由于这是元素级操作，所有的。
- en: is fairly simple， that's just one。 And so there's nothing to do there。 So this
    is the local derivative， and then times the global derivative to create the chain
    rule。 This is just times the B and var。 So this is our candidate。 Let me bring
    this down and uncomment the check。
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 这相当简单，仅此而已。因此这里没有什么可做的。这是局部导数，然后乘以全局导数来创建链式法则。这只是乘以B和var。所以这是我们的候选者。让我把这个拉下来，并取消注释检查。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_353.png)'
  id: totrans-443
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_353.png)'
- en: And we see that we have the correct result。 Now， before we back propagate through
    the next line。
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到我们得到了正确的结果。现在，在我们通过下一条线进行反向传播之前。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_355.png)'
  id: totrans-445
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_355.png)'
- en: I want to briefly talk about the note here， where I'm using the bestness correction。
    dividing by n minus one， instead of dividing by n， when I normalize here， the
    sum of squares。
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 我想简要谈谈这里的说明，我在使用最佳性修正时，分母为(n-1)而不是n，当我在这里对平方和进行归一化时。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_357.png)'
  id: totrans-447
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_357.png)'
- en: Now， you'll notice that this is the departure from the paper， which uses one
    over n instead。 not one over n minus one。 There m is rn。 And so it turns out that
    there are two ways of estimating。 variance off an array。 One is the biased estimate，
    which is one over n。 And the other one is the。 unbiased estimate， which is one
    over n minus one。 Now， confusingly， in the paper， this is。
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你会注意到这是与论文的偏离，论文中使用的是1/n而不是1/(n-1)。这里的m是rn。因此，实际上有两种方法来估计数组的方差。一种是有偏估计，即1/n，另一种是无偏估计，即1/(n-1)。现在，令人困惑的是，在论文中，这是。
- en: not very clearly described， and also it's a detail that kind of matters， I think。
    They are using the biased version train time。 But later， when they are talking
    about the inference。 they are mentioning that when they do the inference， they
    are using the unbiased estimate。 which is the n minus one version in basically
    four inference， and to calibrate the running mean。
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 描述得不是很清楚，而且我认为这是一个重要的细节。他们在训练时使用了有偏版本。但后来，当他们谈论推理时，他们提到在推理时使用无偏估计，也就是基本上在推理中使用n减一版本，以校准运行均值。
- en: and running variance， basically。 And so they actually introduce a train test
    mismatch。 where in training， they use the biased version， and in the test time，
    they use the unbiased version。 I find this extremely confusing。 You can read more
    about the Bessel's correction， and why。 dividing by n minus one gives you a better
    estimate of the variance。
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 以及运行方差，基本上。因此，他们实际上引入了训练和测试的不匹配。在训练中，他们使用了有偏版本，而在测试时，他们使用无偏版本。我觉得这非常令人困惑。你可以阅读更多关于贝塞尔校正的信息，以及为什么将方差除以n减一能给出更好的估计。
- en: In a case where you have population， size it or samples for a population， there
    are very small。 And that is indeed the case for us， because we are dealing with
    many batches。 And these minimatches are a small sample of a larger， population，
    which is the entire training set。 And so it just turns out that if you just estimate，
    it using one over n。
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 在样本为一个种群的情况下，样本量非常小。而这确实是我们的情况，因为我们处理多个批次。这些小批次是更大种群的一个小样本，也就是整个训练集。因此，如果你仅仅使用1/n来估计，结果就会出现问题。
- en: that actually almost always underestimates the variance。 And it is a biased，
    estimator。 and it is advised to use the unbiased version and divide by n minus
    one。 And you can go。 through this article here that I liked that actually describes
    the full reasoning， and I'll。 link it in the video description。 Now， when you
    calculate the torso variance， you'll notice that。
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上几乎总是低估方差。而且这是一个有偏估计器。建议使用无偏版本并除以n减一。你可以参考我喜欢的这篇文章，它实际上描述了完整的推理，我会在视频描述中链接它。现在，当你计算整体方差时，你会注意到。
- en: they take the unbiased flag， whether or not you want to divide by n or n minus
    one。 Confusingly。 they do not mention what the default is for unbiased， but I
    believe unbiased by default， is true。 I'm not sure why the docs here don't cite
    that。 Now， in the batch norm， 1D， the documentation。 again is kind of wrong and
    confusing。 It says that the standard deviation is calculated via the。
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 他们会选择无偏标志，无论是否要除以n或n减一。令人困惑的是，他们没有提到无偏的默认值是什么，但我相信默认情况下无偏为真。我不确定为什么文档没有说明这一点。现在，在批归一化1D中，文档同样有点错误和混淆。它说标准差是通过。
- en: biased estimator。 But this is actually not exactly right。 And people have pointed
    out that it is not。
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 有偏估计器。但这实际上并不完全正确。人们已经指出这点。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_359.png)'
  id: totrans-455
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_359.png)'
- en: right in a number of issues system， because actually the rabbit hole is deeper，
    and they follow the。 paper exactly。 And they use the biased version for training。
    But when they're estimating the running， standard deviation， we are using the
    unbiased version。 So again， there's the train test mismatch。 So long story short。
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 在多个问题系统中，因为实际上兔子洞更深，他们确实严格遵循论文。而他们在训练时使用有偏版本。但在估计运行标准差时，我们使用的是无偏版本。因此，这又出现了训练和测试的不匹配。总之。
- en: I'm not a fan of train test discrepancies。 I basically kind of consider。
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 我不喜欢训练和测试之间的不一致。我基本上认为。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_361.png)'
  id: totrans-458
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_361.png)'
- en: the fact that we use the bias version， the training time， and the unbiased test
    time。 I。 basically consider this to be a bug。 And I don't think that there's a
    good reason for that。 It's not， really， they don't really go into the detail of
    the reasoning behind it in this paper。 So that's why， I basically prefer to use
    the bestness correction in my own work。 Unfortunately。
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在训练时使用有偏版本，而在测试时使用无偏版本的事实。我基本上认为这是一个bug。我认为没有合理的理由。实际上，他们没有深入探讨这篇论文中背后的推理。因此，这就是为什么我基本上在自己的工作中更倾向于使用贝塞尔校正。不幸的是。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_363.png)'
  id: totrans-460
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_363.png)'
- en: Bash norm does not take a keyword argument that tells you whether or not you
    want to use the。 unbiased version or the biased version in both training tests。
    And so therefore， anyone using。 batch normalization， basically in my view has
    a bit of a bug in the code。 And this turns out to be。 much less of a problem if
    your batch mini batch sizes are a bit larger。 But still。
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 批量规范不接受一个关键词参数，告诉你是否想在训练测试中使用无偏版本或有偏版本。因此，任何使用批量规范的人，基本上在我看来，代码中都有点错误。而这在你的批量小批量大小稍大的情况下，实际上是一个较小的问题。但仍然。
- en: I just might have， kind of a unpodable。 So maybe someone can explain why this
    is okay。 But for now。 I prefer to use the。
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 我可能有些不可理解。所以也许有人可以解释一下为什么这样可以。但现在。我更倾向于使用。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_365.png)'
  id: totrans-463
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_365.png)'
- en: unbiased version consistently both during training and at test time。 And that's
    why I'm using one。
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 无偏版本在训练和测试时始终一致。这就是我为什么使用一个。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_367.png)'
  id: totrans-465
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_367.png)'
- en: over N minus one here。 Okay， so let's now actually back propagate through this
    line。
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 在 N 减一上。好的，那么现在我们实际回传通过这行。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_369.png)'
  id: totrans-467
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_369.png)'
- en: So the first thing that I always like to do is I like to scrutinize the shapes
    first。 So in。
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我总是喜欢做的第一件事就是仔细审查形状。因此，在。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_371.png)'
  id: totrans-469
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_371.png)'
- en: particular here， looking at the shapes of what's involved， I see that B and
    var shape is one by 64。
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是在这里，查看所涉及的形状，我看到 B 和 var 的形状是 1x64。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_373.png)'
  id: totrans-471
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_373.png)'
- en: So it's a row vector and B and D if two dot shape is 32 by 64。 So clearly here。
    we're doing a sum over。
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是一个行向量，B 和 D 如果两个点的形状是 32x64。所以显然这里。我们在求和。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_375.png)'
  id: totrans-473
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_375.png)'
- en: the zero axis to squash the first dimension of of the shapes here using a sum。
    So that right away。
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 零轴以通过求和压缩这里的形状的第一维度。所以立刻。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_377.png)'
  id: totrans-475
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_377.png)'
- en: actually hints to me that there will be some kind of a replication or broadcasting
    in the backward pass。 And maybe you're noticing the pattern here， but basically
    anytime you have a sum in the forward pass。 that turns into a replication or broadcasting
    in the backward pass along the same dimension。 And conversely， when we have a
    replication or a broadcasting in the forward pass， that indicates a。
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上这暗示着在反向传播中会有某种复制或广播。也许你注意到了这里的模式，但基本上每当你在前向传播中有一个求和时，那会在同一维度的反向传播中变成复制或广播。反之，当我们在前向传播中有复制或广播时，这表明一个。
- en: variable reuse。 And so in the backward pass， that turns into a sum over the
    exact same dimension。 And so hopefully you're noticing that duality that those
    two are kind of like the opposite。
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 变量重用。因此在反向传播中，这变成了在完全相同的维度上的求和。希望你注意到这种对偶性，这两者有点像是相反的。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_379.png)'
  id: totrans-478
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_379.png)'
- en: of each other in the forward and backward pass。 Now， once we understand the
    shapes， the next thing。
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向和反向传播中相互作用。现在，一旦我们理解了形状，接下来要。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_381.png)'
  id: totrans-480
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_381.png)'
- en: I like to do always is I like to look at a two example in my head to sort of
    just like understand。 roughly how the variable dependencies go in the mathematical
    formula。 So here。 we have a two-dimensional。
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 我总是喜欢做的事是，我喜欢在脑海中查看一个例子，以便大致了解变量依赖关系在数学公式中的表现。所以这里。我们有一个二维。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_383.png)'
  id: totrans-482
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_383.png)'
- en: array at the end of two， which we are scaling by a constant。 And then we are
    summing vertically。
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后的数组中，有一个常数进行缩放。然后我们进行垂直求和。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_385.png)'
  id: totrans-484
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_385.png)'
- en: over the columns。 So if we have a two by two matrix A， and then we sum over
    the columns and scale。 we would get a row vector B1 B2。 And B1 depends on A in
    this way， where it's just some that are。 scaled of A and B2 in this way， where
    it's the second column， sum and scale。 And so looking at。
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 在列上。因此，如果我们有一个 2x2 矩阵 A，然后我们在列上求和并缩放。我们将得到一个行向量 B1 B2。B1 以这种方式依赖于 A，它是 A 的某些缩放和，而
    B2 以这种方式依赖于第二列，求和和缩放。所以查看。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_387.png)'
  id: totrans-486
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_387.png)'
- en: this basically， what we want to do now is we have the derivatives on B1 and
    B2， and we want to。 back propagate them into A's。 And so it's clear that just
    differentiating in your head， the local。 derivative here is 1 over n minus 1 times
    1 for each one of these A's。 And basically the derivative。 of B1 has to flow through
    the columns of A scaled by 1 over n minus 1。
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我们现在想做的是，我们有 B1 和 B2 的导数，我们想要将它们反向传播到 A。显然，只需在脑海中进行微分，这里的局部导数是 1 除以 n 减去
    1，针对每一个 A。基本上 B1 的导数必须通过 A 的列进行流动，缩放因子为 1 除以 n 减去 1。
- en: And that's roughly what's happening， here。 So intuitively， the derivative flow
    tells us that dBn。 dF2 will be the local derivative of this。
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 大致上就是这样。所以直观上，导数流告诉我们 dBn，dF2 将是这个的局部导数。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_389.png)'
  id: totrans-489
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_389.png)'
- en: operation。 And there are many ways to do this by the way， but I like to do something
    like this。 torched out one slide of Bn， dF2。 So I'll create a large array to the
    initial of ones。 And then I。 will scale it。 So 1。0 divided by n minus 1。 So this
    is a array of 1 over n minus 1。 And that's sort of， like the local derivative。
    And now for the chain rule。
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 操作。顺便说一句，这有很多方法来做到这一点，但我喜欢这样做。将 Bn，dF2 的一个切片导出。因此，我将创建一个初始为 1 的大数组。然后我会缩放它。所以
    1.0 除以 n 减去 1。这是一个 1 除以 n 减去 1 的数组。这就像是局部导数。现在用于链式法则。
- en: I will simply just multiply it by Bb and r。 And notice here what's going to
    happen。 This is 32 by 64。 And this is just 1 by 64。 So I'm letting。 the broadcasting
    do the replication because internally in PyTorch， basically dBn， which is 1 by
    64。 row vector， well， in this multiplication， get copied vertically until the
    two are of the same。
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 我会简单地乘以 Bb 和 r。注意这里会发生什么。这是 32 乘以 64。这只是 1 乘以 64。所以我让广播进行复制，因为在 PyTorch 内部，基本上
    dBn 是 1 乘以 64 的行向量，在这个乘法中，会被垂直复制，直到两者形状相同。
- en: shape。 And then there will be an element to us multiply。 And so that the broadcasting
    is basically。 doing the replication。 And I will end up with the derivatives of
    dBn， dF2 here。 So this is the。
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 形状。然后将有一个元素供我们乘以。因此，广播基本上是进行复制。最终我将得到 dBn，dF2 的导数。所以这是这个。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_391.png)'
  id: totrans-493
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_391.png)'
- en: candidate solution。 Let's bring it down here。 Let's uncomment this line where
    we check it。 And let's。
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 候选解决方案。让我们把它放在这里。让我们取消注释这一行，检查一下。然后让我们。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_393.png)'
  id: totrans-495
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_393.png)'
- en: hope for the best。 And indeed we see that this is the correct formula。 Next
    up， let's differentiate。
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 希望一切顺利。确实我们看到这是正确的公式。接下来，让我们进行微分。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_395.png)'
  id: totrans-497
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_395.png)'
- en: here into Bn， dF。 So here we have that Bn， dF is element y squared to create
    Bn， dF2。 So this is a。 relatively simple derivative because it's a simple element
    wise operation。 So it's kind of like the。 scalar case。 And we have that dBn， dF
    should be， if this is x squared， then derivative of it is 2x。 right？ So it's simply
    2 times Bn， dF that's the local derivative。 And then times chain rule。
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Bn，dF 中。所以这里我们有 Bn，dF 是元素 y 的平方，以创建 Bn，dF2。所以这是一个相对简单的导数，因为这是一个简单的逐元素操作。所以这有点像标量情况。如果这是
    x 的平方，那么它的导数是 2x，对吧？所以它就是 2 乘以 Bn，dF，这是局部导数。然后乘以链式法则。
- en: And the shape of these is the same， they are of the same shape。 So times this。
    So that's the backward。
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 这些的形状是相同的，它们的形状相同。所以乘以这个。这就是反向传播。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_397.png)'
  id: totrans-500
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_397.png)'
- en: pass for this variable。 Let me bring it down here。 And now we have to be careful
    because we already。
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 把它带到这里。现在我们必须小心，因为我们已经。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_399.png)'
  id: totrans-502
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_399.png)'
- en: calculated the Bn， dF， right？ So this is just the end of the other， you know。
    other branch coming back， to Bn， dF。 Because Bn， dF will already back propagate
    it to way over here from Bn。 raw。 So we now completed。
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 计算了 Bn，dF，对吧？所以这只是另一个分支回到 Bn，dF。因为 Bn，dF 会已经从 Bn 的原始值反向传播到这里。所以我们现在完成了。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_401.png)'
  id: totrans-504
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_401.png)'
- en: the second branch。 And so that's why I have to do plus equals。 And if you recall。
    we had an incorrect。
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个分支。所以这就是我为什么要加等于。如果你记得，我们之前有一个不正确的。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_403.png)'
  id: totrans-506
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_403.png)'
- en: derivative for Bn， dF4。 And I'm hoping that once we append this last missing
    piece。 we have the exact。
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: Bn的导数，dF4。我希望一旦我们附加这最后缺失的部分。我们得到了准确的。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_405.png)'
  id: totrans-508
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_405.png)'
- en: correctness。 So let's run。 And Bn， dF2， Bn， dF now actually shows the exact
    correct derivative。
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 正确性。因此，让我们运行。现在，Bn，dF2，Bn，dF实际上显示了准确的导数。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_407.png)'
  id: totrans-510
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_407.png)'
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_408.png)'
  id: totrans-511
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_408.png)'
- en: So that's comforting。 Okay， so let's now back propagate through this line here。
    The first thing we do， of course， is we check the shapes。 And I wrote them out
    here。 And basically。 the shape of this is 32 by 64。 HP Bn is the same shape。 But
    Bn， mean i is a row vector， 1 by 64。 So this minus here will actually do broadcasting。
    And so we have to be careful with that。 And as。
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这让人感到安心。好的，现在让我们通过这一行进行反向传播。我们首先要做的，当然是检查形状。我在这里写下来了。基本上。这个的形状是32乘以64。HP Bn也是相同的形状。但是Bn，mean
    i是一个行向量，1乘以64。所以这里的减法实际上会进行广播。因此我们必须小心。还有。
- en: a hint to us， again， because of the duality， a broadcasting in a forward pass
    means a variable。 reuse。 And therefore there will be a sum in the backward pass。
    So let's write out the backward pass。 here now。 Back propagate into the HP Bn。
    Because these are the same shape。 then the local derivative， for each one of the
    elements here is just one for the corresponding element in here。
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 这给了我们一个提示，再一次，由于对偶性，前向传播中的广播意味着变量的重用。因此，在反向传播中将会有一个求和。所以让我们现在写出反向传播。反向传播到HP
    Bn。因为这些形状相同。每个元素的局部导数只是对应元素的1。
- en: So basically， what this means is that the gradient just simply copies。 It's
    just a variable assignment， it's， quality。 So I'm just going to clone this tensor
    just for safety to create an exact copy of。 dBn。 And then here to back propagate
    into this one， what I'm inclined to do here is， dBn。 mean i will basically be
    what is the local derivative？ Well， it's negative torch dot 1。
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，这意味着梯度只是简单地复制。这只是一个变量赋值，它是，质量。因此，我将克隆这个张量以安全起见，创建一个精确的。dBn。然后在这里进行反向传播，我倾向于做的是，dBn。意思是我基本上会得到什么是局部导数？嗯，它是负的torch
    dot 1。
- en: like of the shape of b and f。 And then times the derivative here dBn。 And this
    here is the back propagation for the replicated b and mean i。 So I still have
    to。 back propagate through the replication in the broadcasting。 And I do that
    by doing a sum。 So I'm。 going to take this whole thing and I'm going to do a sum
    over the zero dimension， which was the。
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 形状与b和f类似。然后乘以这里的导数dBn。这是对复制的b和mean i的反向传播。因此，我仍然需要。通过复制和广播进行反向传播。我通过进行求和来做到这一点。所以我。将整个内容进行求和，我将在零维上进行求和。
- en: replication。 So if you scrutinize this by the way。 you'll notice that this is
    the same shape as that。 And so what I'm doing here doesn't actually make that
    much sense because it's just a。 array of ones multiplying dBn。 So in fact， I can
    just do this。 And there's equivalent。
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 复制。所以如果你仔细审视一下。你会注意到这个和那个是相同形状。因此，我在这里所做的实际上没有太多意义，因为这只是一个全是1的数组在乘以dBn。因此，实际上，我可以这样做。这样是等效的。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_410.png)'
  id: totrans-517
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_410.png)'
- en: So this is the candidate backward pass。 Let me copy it here。 And then let me
    comment out this one。
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是候选的反向传播。让我在这里复制它。然后让我注释掉这个。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_412.png)'
  id: totrans-519
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_412.png)'
- en: and this one。 Enter。 And it's wrong。 Damn。 Actually， sorry， this is supposed
    to be wrong。
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 还有这个。输入。结果是错误的。该死。实际上，抱歉，这本该是错误的。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_414.png)'
  id: totrans-521
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_414.png)'
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_415.png)'
  id: totrans-522
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_415.png)'
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_416.png)'
  id: totrans-523
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_416.png)'
- en: And it's supposed to be wrong because we are back propagating from a b and f
    into h preb and。 and but we're not done because b and mean i depends on h preb
    and there will be a second portion of。 that derivative coming from this second
    branch。 So we're not done yet and we expect it to be。 incorrect。 So there you
    go。 So let's not back propagate from b and mean i into h preb and。
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 而且应该是错误的，因为我们正在从b和f反向传播到h preb和。但是我们还没有完成，因为b和mean i依赖于h preb，因此会有来自这个第二分支的第二部分导数。所以我们还没有完成，我们预期它是。错误的。好了，所以让我们不要从b和mean
    i反向传播到h preb。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_418.png)'
  id: totrans-525
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_418.png)'
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_419.png)'
  id: totrans-526
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_419.png)'
- en: And so here again， we have to be careful because there's a broadcasting along
    or there's a sum。 along the zero dimension。 So this will turn into broadcasting
    in the backward pass now。 And I'm。 going to go a little bit faster on this line
    because it is very similar to the line that we had before。 and multiple as in
    the past。 In fact， so d h preb and will be the gradient will be scaled by one
    over n。
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在这里，我们必须小心，因为沿着零维度有广播或求和。因此这将转变为反向传递中的广播。我要在这一行上加快速度，因为这与我们之前的行非常相似。并且像以前一样进行乘法。实际上，d
    h preb将会被梯度缩放为1/n。
- en: And then basically this gradient here， the b and mean i is going to be scaled
    by one over n。 And then， it's going to flow across all the columns and deposit
    itself into d h preb and。 So what we want， is this thing scaled by one over n。
    We'll put the constant up front here。 So scale down the gradient， and now we need
    to replicate it across all the across all the rows here。
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 然后基本上这个梯度这里，b和mean i将会被缩放为1/n。然后，它会流遍所有的列并沉积到d h preb中。所以我们想要的，就是这个东西被缩放为1/n。我们会把常数放在前面。因此缩小梯度，现在我们需要在这里的所有行中复制它。
- en: So we I like to do that， by torch dot one slide of basically h preb and。 And
    I will let broadcasting do the work of replication。
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢通过torch dot一个基本上是h preb的滑动来做这件事。我将让广播来完成复制的工作。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_421.png)'
  id: totrans-530
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_421.png)'
- en: So this is the h preb and hopefully we can plus equals that。 So this here is
    broadcasting and then this is the scaling。 So this should be correct。
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是h preb，希望我们可以使用加法赋值。这是广播，然后这是缩放。所以这应该是正确的。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_423.png)'
  id: totrans-532
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_423.png)'
- en: Okay。 So that completes the back propagation of the bathroom layer and we are
    now here。
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。这完成了浴室层的反向传播，我们现在在这里。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_425.png)'
  id: totrans-534
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_425.png)'
- en: Let's back propagate through the linear layer one here。 Now because everything
    is getting a little。
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们反向传播到线性层一。在这里，因为一切都变得有点复杂。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_427.png)'
  id: totrans-536
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_427.png)'
- en: vertically crazy， I copy pasted the line here and let's just back propagate
    through this one line。
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 这条线看起来很疯狂，我在这里复制粘贴了一行，让我们反向传播通过这一行。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_429.png)'
  id: totrans-538
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_429.png)'
- en: So first of course we inspect the shapes and we see that this is 32 by 64。 M
    cat is 32 by 30。 W one is 30 30 by 64 and B one is just 64。 So as I mentioned
    back propagating through linear。
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，当然我们检查形状，我们看到这是32乘64。M cat是32乘30。W one是30乘30乘64，而B one则是64。所以如我所提到的，反向传播通过线性层。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_431.png)'
  id: totrans-540
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_431.png)'
- en: layers is fairly easy just by matching the shapes。 So let's do that。 We have
    that d M cat。 Should be some interest multiplication of d h preb and with w one
    and one transpose thrown in there。 So to make a MCAT be 32 by 30， I need to take
    d h preb and 32 by 64 and multiply it by w one dot transpose。 To get d w one，
    I need to end up with 30 by 64。 So to get that， I need to take。
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 层之间的匹配非常简单。所以我们来做这个。我们有d M cat。应该是d h preb与w one的某种有趣的乘法，再加上一个转置。因此为了使MCAT变为32乘30，我需要将d
    h preb（32乘64）与w one的转置相乘。为了得到d w one，我需要最终得到30乘64。所以为了达到这个，我需要。
- en: em cat transpose and multiply that by d h preb and finally to get d B one。 This
    is a addition and we saw that basically I need to just sum the elements in d h
    preb。 and along some dimension and to make the dimensions work out。 I need to
    sum along the zero access here， to eliminate this dimension and we do not keep
    them so that we want to just get a single one。
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: em cat转置并将其与d h preb相乘，最后得到d B one。这是一个加法，我们发现基本上我只需要在d h preb中对元素求和。并沿某个维度进行求和，为了使维度正确，我需要沿着零轴求和，以消除这个维度，我们不保留它，这样我们只想得到一个单一的结果。
- en: dimensional vector of 64。 So these are the claimed derivatives。 Let me put that
    here and let me。
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 维度向量为64。这些是声称的导数。让我把它放在这里。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_433.png)'
  id: totrans-544
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_433.png)'
- en: uncomment three lines and cross our fingers。 Everything is great。 Okay， so we
    now continue。
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 取消注释三行代码，然后祈祷一切顺利。一切都很好。好的，我们现在继续。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_435.png)'
  id: totrans-546
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_435.png)'
- en: almost there。 We have the derivative of em cat and we want to derivative， we
    want to back propagate。
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 快到了。我们有`em cat`的导数，我们想要导数，我们想要反向传播。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_437.png)'
  id: totrans-548
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_437.png)'
- en: into em。 So I again copied this line over here。 So this is the forward pass
    and then this is the。
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 进入`em`。所以我又把这一行复制到了这里。这是前向传播，然后这是。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_439.png)'
  id: totrans-550
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_439.png)'
- en: shapes。 So remember that the shape here was 32 by 30 and the original shape
    of em was 32 by 3 by 10。 So this layer in the forward pass as you recall that
    the concatenation of these three 10 dimensional。 character vectors。 And so now
    we just want to undo that。 So this is actually relatively。
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 因此请记住，这里的形状是32乘以30，而`em`的原始形状是32乘以3乘以10。所以在前向传播中的这一层，正如你回忆的那样，是这三个10维字符向量的连接。因此我们现在只想撤销那个。因此这实际上相对简单。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_441.png)'
  id: totrans-552
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_441.png)'
- en: straightforward operation because the backward pass of the， what is the view？
    View is just a。 re-representation of the array。 It's just a logical form of how
    you interpret the array。 So let's just， reinterpret it to be what it was before。
    So in other words， the em is not 32 by 30。 It is basically， the em cat。 But if
    you view it as the original shape， so just em dot shape。
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单的操作，因为反向传播的视图是什么？视图只是数组的重新表示。这只是你如何解释数组的逻辑形式。所以让我们重新解释一下，回到之前的状态。换句话说，`em`不是32乘以30。它基本上是`em
    cat`。但是如果你把它视为原始形状，那么就是`em dot shape`。
- en: you can pass in， tuples into view。 And so this should just be okay。 We just
    rerepresent that view and then we。
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将元组传入视图。因此，这应该没问题。我们只是重新表示那个视图，然后我们。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_443.png)'
  id: totrans-555
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_443.png)'
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_444.png)'
  id: totrans-556
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_444.png)'
- en: uncomment this line here and hopefully， yeah， so the derivative of em is correct。
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里取消注释这一行，希望，好的，`em`的导数是正确的。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_446.png)'
  id: totrans-558
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_446.png)'
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_447.png)'
  id: totrans-559
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_447.png)'
- en: So in this case， we just have to re-represent the shape of those derivatives
    into the original view。 So now we are at the final line。 And the only thing that's
    left to back propagate through。 is this indexing operation here， M is C at XB。
    So as I did before， I copy pasted this line here。
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 因此在这种情况下，我们只需将这些导数的形状重新表示为原始视图。所以现在我们到了最后一行。唯一剩下的就是通过反向传播的这个索引操作，`M`是`C`在`XB`处。因此，正如我之前所做的，我在这里复制粘贴了这一行。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_449.png)'
  id: totrans-561
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_449.png)'
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_450.png)'
  id: totrans-562
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_450.png)'
- en: And let's look at the shapes of everything that's involved and remind ourselves
    how this worked。 So em dot shape was 32 by 3 by 10。 So it's 32 examples。 And then
    we have three characters。 Each。 one of them has a 10 dimensional embedding。 And
    this was achieved by taking the lookup table C。 which have 27 possible characters，
    each of them 10 dimensional。 And we looked up at the rows。
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看所有相关内容的形状，并提醒自己这是如何工作的。所以`em dot shape`是32乘以3乘以10。这是32个示例。然后我们有三个字符。每个字符都有一个10维的嵌入。这是通过使用查找表C来实现的，C有27个可能的字符，每个字符是10维的。我们查看了这些行。
- en: that were specified inside this tensor XB。 So XB is 32 by 3。 And it's basically
    giving us for each。 example， the identity or the index of which character is part
    of that example。 And so here。 I'm showing the first five rows of three of this
    tensor XB。 And so we can see that， for example。 here it was the first example
    in this batch is that the first character in the first character。
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个张量`XB`中指定的内容。因此，`XB`是32乘以3。它基本上为每个示例提供了该示例中哪个字符的身份或索引。因此在这里，我展示了这个张量`XB`的前五行中的三个。所以我们可以看到，例如，这个批次中的第一个示例是第一个字符中的第一个字符。
- en: and the fourth character comes into the neural net。 And then we want to predict
    the next character。 in a sequence after the character is 114。 So basically what's
    happening here is there are。 integers inside XB。 And each one of these integers
    is specifying which row of C we want to。 pluck out， right？ And then we arrange
    those rows that we've plucked out into three， two by three by。
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 第四个字符进入神经网络。然后我们想要预测序列中在字符114之后的下一个字符。因此，基本上发生的事情是`XB`中有整数。这些整数分别指定我们要提取的`C`的哪一行，对吗？然后我们将提取的行整理成三个、两个乘以三的形状。
- en: 10 tensor， and we just package them in， we just package them into this tensor。
    And now what's。 happening is that we have D amp。 So for every one of these basically
    plucked out rows。 we have their， gradients now， but they're arranged inside this
    32 by three by 10 tensor。 So all we have to do now， is we just need to route this
    gradient backwards through this assignment。
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 10 维张量，我们只需将它们打包，我们只需将它们打包到这个张量中。现在发生的事情是我们有 D amp。所以对于每一个基本上挑出来的行，我们现在有它们的梯度，但它们排列在这个
    32 乘 3 乘 10 的张量中。所以我们现在所要做的就是将这个梯度向后路由通过这个赋值。
- en: So we need to find which row， of C did every one of these 10 dimensional embeddings
    come from。 And then we need to deposit them into， DC。 So we just need to undo
    the indexing。 And of course。 if any of these rows of C was used， multiple times，
    which almost certainly is the case。 like the row one and one was used multiple
    times， then we have to remember that the gradients that arrive there have to add。
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们需要找出这10维嵌入的每一个是来自 C 的哪一行。然后我们需要将它们存入 DC。所以我们只需要撤销索引。当然，如果 C 的任何行被多次使用，这几乎是肯定的。比如第一行被多次使用，那么我们必须记住到达那里梯度需要相加。
- en: So for each occurrence， we have to have an addition。 So let's now write this
    out。 And I don't actually know of like a， much better way to do this than a for
    loop unfortunately in Python。 So maybe someone can come up with a。
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 所以对于每个出现，我们必须进行相加。那么现在我们来写出来。我实际上不知道在 Python 中有没有比 for 循环更好的方法。所以也许有人可以想出一个。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_452.png)'
  id: totrans-569
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_452.png)'
- en: vectorized efficient operation， but for now let's just use for loops。 So let
    me create a torch dot。 zeros like C to initialize just 27 by 10 tensor of all
    zeros。 And then honestly， 4k in range。
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 向量化的高效操作，但现在我们就用 for 循环。让我创建一个 torch 的 zeros，像 C 一样初始化一个全零的27乘10的张量。然后老实说，4k
    在范围内。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_454.png)'
  id: totrans-571
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_454.png)'
- en: xb dot shape at zero。 Maybe someone has a better way to do this， but for J in
    range。 xb dot shape at one。 This is going to iterate over all the all the elements
    of xb。 all these integers。
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: xb 的形状在零处。也许有人有更好的方法来做到这一点，但对于 J 在范围内。xb 的形状在一处。这将迭代 xb 的所有元素，所有这些整数。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_456.png)'
  id: totrans-573
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_456.png)'
- en: And then let's get the index at this position。 So the index is basically xb
    at kj。 So that an example of that like 11 or 14 and so on。 And now in the forward
    pass， we took。 we basically took the row of C at index， and we deposited it into
    m at k aj。 That's what happened。 That's where they are packaged。 So now we need
    to go backwards and we just need to route。
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们获取这个位置的索引。所以索引基本上是 xb 在 kj 的位置。这就像 11 或 14 等的例子。现在在前向传播中，我们取了 C 的一行在索引，并把它存入
    m 在 k aj。这就是发生的事情。这就是它们打包的位置。所以现在我们需要向后操作，我们只需要路由。
- en: dm at the position kj。 We now have these derivatives for each position and it's
    10， dimensional。 And it just needs to go into the correct row of C。 So dC rather
    at ix is this。 but plus equals because there could be multiple occurrences， like
    the same row could have been。 used many， many times。 And so all of those derivatives
    will just go backwards through the indexing and。
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 在 kj 的位置 dm。我们现在为每个位置都有这些导数，并且是10维的。它只需要进入 C 的正确行。因此 dC 实际上在 ix 是这个。但是加等于，因为可能会有多次出现，比如同一行可能被使用过很多次。所以所有这些导数都将通过索引向后传递。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_458.png)'
  id: totrans-576
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_458.png)'
- en: they will add。 So this is my candidate solution。 Let's copy it here。 Let's uncomment
    this and。
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 它们将相加。这是我的候选解决方案。我们把它复制到这里。让我们取消注释这个。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_460.png)'
  id: totrans-578
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_460.png)'
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_461.png)'
  id: totrans-579
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_461.png)'
- en: cross our fingers。 Hey， so that's it。 We've back propagated through this entire
    beast。
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉我们的手指。嘿，所以就这样。我们已经通过整个庞然大物进行了反向传播。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_463.png)'
  id: totrans-581
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_463.png)'
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_464.png)'
  id: totrans-582
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_464.png)'
- en: So there we go。 Totally made sense。 So now we come to exercise two。 It basically
    turns out that in。
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们开始吧。这完全说得通。那么现在我们进入第二个练习。结果基本上是。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_466.png)'
  id: totrans-584
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_466.png)'
- en: this first exercise， we were doing way too much work。 We were back propagating
    way too much。 And it。 was all good practice and so on， but it's not what you would
    do in practice。 And the reason for that。
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个第一次练习中，我们做了太多的工作。我们反向传播得太多了。这都是好的练习等等，但这不是你在实际中会做的事情。原因是。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_468.png)'
  id: totrans-586
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_468.png)'
- en: is for example， here I separated out this loss calculation over multiple lines
    and I broke it up。 all all too like its smallest atomic pieces and we back propagated
    through all of those individually。
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这里我将损失计算分开成多行，并将其分解为最小的原子部分，我们分别对所有这些进行了反向传播。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_470.png)'
  id: totrans-588
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_470.png)'
- en: But it turns out that if you just look at the mathematical expression for the
    loss。 then actually you can do the differentiation on pen and paper and a lot
    of terms cancel and。 simplify。 And the mathematical expression you end up with
    can be significantly shorter and easier to。 implement than back propagating through
    all the little pieces of everything you've done。 So before。
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 但事实证明，如果你只看损失的数学表达式，实际上你可以用纸和笔进行微分，很多项会相互抵消并简化。而你最终得到的数学表达式可以显著更短，并且比通过你所做的所有小部分反向传播要容易实现得多。所以在此之前。
- en: we had this complicated forward pass going from logits to the loss， but in PyTorch，
    everything can。 just be glued together into a single call at that cross entropy。
    You just pass in logits and the。 labels and you get the exact same loss as I verify
    here。 So our previous loss and the fast loss coming， from the chunk of operations
    as a single mathematical expression is the same。
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有这个复杂的前向传递，从 logits 到损失，但在 PyTorch 中，一切都可以简单地拼接成一个单一的交叉熵调用。你只需传入 logits 和标签，你就得到了与我在这里验证的完全相同的损失。因此，我们之前的损失和来自一系列操作的快速损失作为一个单一的数学表达式是相同的。
- en: but it's much much faster。
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 但这要快得多。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_472.png)'
  id: totrans-592
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_472.png)'
- en: and forward pass。 It's also much much faster and backward pass。 And the reason
    for that is if you。
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 向前传递。反向传递也要快得多。原因是如果你。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_474.png)'
  id: totrans-594
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_474.png)'
- en: just look at the mathematical form of this and differentiate again， you will
    end up with a very。 small and short expression。 So that's what we want to do here。
    We want to in a single operation or in a， single go or like very quickly go directly
    into D logits and we need to implement D logits as a。 function of logits and YBs。
    But it will be significantly shorter than whatever we did here。
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 只需查看这个数学形式并再次微分，你将最终得到一个非常小且简短的表达式。这就是我们想要在这里做的。我们希望在单个操作中，或一次性快速直接进入 D logits，并且我们需要将
    D logits 实现为 logits 和 YBs 的函数。但这将比我们在这里所做的任何事情显著更短。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_476.png)'
  id: totrans-596
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_476.png)'
- en: where to get to D logits we have to go all the way here。 So all of this work
    can be skipped in a。
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得到 D logits，我们必须走到这里。因此所有这些工作都可以在一个。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_478.png)'
  id: totrans-598
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_478.png)'
- en: much much simpler mathematical expression that you can implement here。 So you
    can give it a shot。 yourself， basically look at what exactly is the mathematical
    expression of loss and differentiate。 with respect to the logits。 So let me show
    you a hint。 You can of course try it fully yourself。
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 更简单的数学表达式，你可以在这里实现。所以你可以尝试一下，基本上看一下损失的数学表达式是什么，并对 logits 进行微分。所以让我给你一个提示。你当然可以完全自己尝试。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_480.png)'
  id: totrans-600
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_480.png)'
- en: but if not， I can give you some hint of how to get started mathematically。 So
    basically what's happening here is we have logits， then there's the softmax that
    takes the。 logits and gives you probabilities， then we are using the identity
    of the correct next。 character to pluck out a row of probabilities， take the negative
    log of it to get our negative。
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果不是，我可以给你一些数学上的提示，帮助你开始。所以基本上这里发生的事情是我们有 logits，然后是 softmax，它接受 logits 并给你概率，然后我们使用正确下一个字符的身份来提取出一行概率，取它的负对数以得到我们的负值。
- en: log probability。 And then we average up all the log probabilities or negative
    log probabilities to。 get our loss。 So basically what we have is for a single
    individual example， rather， we have that。 loss is equal to negative log probability，
    where p here is kind of like thought of as a vector of。 all the probabilities。
    So at the y position， where y is the label。 And we have that p here。
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 对数概率。然后我们将所有的对数概率或负对数概率平均起来，以获得我们的损失。基本上，对于单个示例，我们得到的损失等于负对数概率，这里的p可以看作是所有概率的向量。在y位置，y是标签。我们得到了这里的p。
- en: of course， is the softmax。 So the i component of p of this probability vector
    is just the softmax function。 So raising all the logits basically to the power
    of e and normalizing， so everything， comes to one。 Now if you write out p of y
    here， you can just write out the softmax。 and then basically what we're interested
    in is we're interested in the derivative of the loss。
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这是softmax。所以这个概率向量的p的i分量就是softmax函数。将所有的logits基本上都提高到e的幂并进行归一化，所有结果加起来等于一。现在如果你在这里写出p
    of y，你可以直接写出softmax。我们基本上感兴趣的是损失的导数。
- en: with respect to the ith logit。 And so basically it's a d by dli of this expression
    here， where we。 have l indexed with the specific label y。 And on the bottom we
    have a sum over j of e to the lj。 and the negative log of all that。 So potentially
    give it a shot pen and paper and see if you can。 actually derive the expression
    for the loss by dli， and then we're going to implement it here。
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上这是这个表达式的d by dli，其中我们有以特定标签y为索引的l。在底部，我们有一个j的和e的lj，以及所有这些的负对数。所以可能试试用笔和纸，看你是否能推导出损失对dli的表达式，然后我们将在这里实现它。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_482.png)'
  id: totrans-605
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_482.png)'
- en: Okay， so I am going to give away the result here。 So this is some of the math
    I did to derive the。
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我将在这里透露结果。这是我推导出来的一些数学内容。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_484.png)'
  id: totrans-607
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_484.png)'
- en: gradients analytically。 And so we see here that I'm just applying the rules
    of calculus from your。 first or second year of bachelor's degree if you took it。
    And we see that the expression is。 actually simplified quite a bit。 You have to
    separate out the analysis in the case where。 the ith index that you're interested
    in inside logits is either equal to the label， or it's not。
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 从分析上来看梯度。因此我们在这里看到，我只是应用了你们本科第一年或第二年的微积分规则，如果你上过这门课。我们看到表达式实际上简化了很多。你需要在分析中分开考虑，当你感兴趣的logits的第i个索引要么等于标签，要么不等。
- en: equal to the label。 And then the expression is simplify and cancel in a slightly
    different way。 And what we end up with is something very， very simple。 We either
    end up with basically p at i。 where p is again this vector of probabilities after
    a softmax， or p at i minus one， where we just。 simply subtract to one。 But in
    any case， we just need to calculate the softmax p and then in the。
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 等于标签。然后表达式以稍微不同的方式简化和抵消。我们最终得到的东西非常简单。我们要么最终得到基本的p at i，这里的p再次是经过softmax处理的概率向量，要么是p
    at i减去一。无论如何，我们只需要计算softmax p，然后在。
- en: correct dimension， we need to subtract to one。 And that's the gradient， the
    form that it takes。 analytically。 So let's implement this basically。 And we have
    to keep in mind that this is only done。
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 正确的维度，我们需要减去一。那就是梯度的形式。从分析上来看。因此，让我们基本上实现这一点。我们必须记住，这仅仅是针对第i个logit完成的。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_486.png)'
  id: totrans-611
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_486.png)'
- en: for a single example。 But here we are working with batches of examples。 So we
    have to be careful。
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单个示例。但这里我们处理的是示例的批次。因此我们必须小心。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_488.png)'
  id: totrans-613
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_488.png)'
- en: of that。 And then the loss for a batch is the average loss over all the examples。
    So in other。 words， it's the example for all the individual examples is the loss
    for each individual example。 summed up and then divided by n。 And we have to back
    propagate through that as well and be careful。
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 的结果中。然后一个批次的损失是所有示例的平均损失。换句话说，对于所有单个示例的损失，总和后再除以n。我们还需要通过这一过程进行反向传播，并且要小心。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_490.png)'
  id: totrans-615
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_490.png)'
- en: with it。 So d logits is going to be f dot softmax。 PyTorch has a softmax function
    that you can call。 And we want to apply the softmax on the logits。 And we want
    to go in the dimension that is one。 So basically we want to do the softmax along
    the rows of these logits。 Then at the correct。 positions， we need to subtract
    a one。 So d logits at iterating over all the rows and indexing into。
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 因此d logits将是f dot softmax。PyTorch有一个你可以调用的softmax函数。我们希望在logits上应用softmax，并且希望在维度1上进行。所以基本上我们希望在这些logits的行上执行softmax。然后在正确的位置，我们需要减去1。因此d
    logits在遍历所有行并索引时。
- en: the columns provided by the correct labels inside yb。 We need to subtract one。
    And then finally。 it's the average loss that is the loss。 And in the average，
    there's a one over n of all the losses。 added up。 And so we need to also back
    propagate through that division。 So the gradient has to be。 scaled down by n as
    well， because of the mean。 But this otherwise should be the result。 So now。
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 由正确标签提供的列在yb内。我们需要减去1。最后，这是平均损失，它是损失。在平均值中，所有损失加起来有一个1/n。因此我们还需要通过这个除法进行反向传播。因此，梯度也必须缩放为n，因为均值。但除此之外，这应该是结果。所以现在。
- en: if we verify this， we see that we don't get an exact match。 But at the same
    time， the maximum。 difference from logits from PyTorch and our d logits here is
    on the order of 5e negative 9。 So it's a tiny， tiny number。 So because of floating
    point of wantiness。 we don't get the exact bitwise， result。 But we basically get
    the correct answer。 Approximately。 Now。
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们验证这一点，我们会发现没有得到完全匹配。但同时，从PyTorch的logits和我们这里的d logits之间的最大差异约为5e负9。所以这是一个微小的数字。因此，由于浮点的不精确性，我们没有得到确切的位级结果。但基本上，我们得到了正确的答案。大约。现在。
- en: I'd like to pause here briefly。
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 我想在这里稍作停顿。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_492.png)'
  id: totrans-620
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_492.png)'
- en: before we move on to the next exercise， because I'd like us to get an intuitive
    sense of what。 the logits is。 Because it has a beautiful and very simple explanation，
    honestly。 So here。 I'm taking the logits and I'm visualizing it。 And we can see
    that we have a batch of 32 examples。 of 27 characters。 And what is the logits
    intuitively， right？
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进行下一个练习之前，因为我想让我们对logits有一个直观的理解。因为它有一个美丽而非常简单的解释，老实说。因此在这里。我正在获取logits并对其进行可视化。我们可以看到，我们有32个例子的批次，包含27个字符。那么直观上logits是什么，对吧？
- en: The logits is the probabilities that the， probabilities matrix in a forward
    pass。 But then here。 these black squares are the positions of， the correct indices
    where we subtracted a 1。 And so what is this doing， right？ These are the， derivatives
    on the logits。 And so let's look at just the first row here。 So that's what I'm
    doing。
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: Logits是前向传播中概率矩阵的概率。但是这里，这些黑色方块是我们减去1的正确索引的位置。那么这在做什么，对吧？这些是logits上的导数。因此让我们看看这里的第一行。这就是我正在做的事情。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_494.png)'
  id: totrans-623
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_494.png)'
- en: here。 I'm collecting the probabilities of these logits and that I'm taking just
    the first row。 And this is the probability row。 And then the logits of the first
    row and multiplying by n just。 for us so that we don't have the scaling by n in
    here and everything is more interpretable。 We see。
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里。我正在收集这些logits的概率，并且我只取第一行。这是概率行。然后第一行的logits乘以n，仅仅是为了不在这里缩放n，所有内容都更易于解释。我们看到了。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_496.png)'
  id: totrans-625
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_496.png)'
- en: that it's exactly equal to the probability， of course， but then the position
    of the correct index。 has a minus equals 1。 So minus 1 on that position。 And so
    notice that if you take the logits at 0。
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 当然它正好等于概率，但正确索引的位置减去1。所以在那个位置减去1。所以注意，如果你在0处获取logits。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_498.png)'
  id: totrans-627
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_498.png)'
- en: and you sum it， it actually sums to 0。 And so you should think of these gradients
    here at each cell。 as like a force。 We are going to be basically pulling down
    on the probabilities of the incorrect。 characters。 And we're going to be pulling
    up on the probability at the correct index。 And that's。 what's basically happening
    in each row。 And the amount of push and pull is exactly equalized。
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你求和，它实际上会和为0。因此你应该把这些梯度视为每个单元的一个力。我们基本上会拉低不正确字符的概率。并且我们将拉高正确索引的概率。这就是每行基本上发生的事情。推动和拉动的量是完全平衡的。
- en: because the sum is zero。 So the amount to which we pulled down on the probabilities
    and the。 demand that we push up on the probability of the correct character is
    equal。 So it's sort of the。 repulsion and the attraction are equal。 And think
    of the neural map now as a like a massive。 pulley system or something like that，
    we're up here on top of the logits and we're pulling up。
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 因为总和为零。因此，我们对概率的下拉和对正确字符概率的上推是相等的。所以这有点像排斥和吸引是平衡的。现在把神经网络视为一个巨大的滑轮系统，我们在logits的顶部拉升。
- en: we're pulling down the probabilities of incorrect and pulling up the probability
    of the correct。 And in this complicated pulley system， because everything is mathematically
    just determined。 just think of it as sort of like this tension translating to
    this complicating pulley mechanism。 And then eventually we get a tug on the weights
    and the biases。 And basically in each update。
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在下拉错误的概率，同时提升正确的概率。在这个复杂的滑轮系统中，因为一切都是数学上确定的。可以把它视作这种张力转化为复杂的滑轮机制。最终我们在权重和偏置上得到了拉动。在每次更新中，基本上都是这样。
- en: we just kind of like tug in the direction that we like for each of these elements。
    And the parameters， are slowly given in to the tug。 And that's what training and
    neural net kind of like looks like， on a high level。 And so I think the forces
    of push and pull in these gradients are actually， very intuitive here。
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 我们就是在这些元素的方向上轻轻拉动。参数也会慢慢适应这种拉动。这就是高层次上训练和神经网络的样子。我认为这些梯度中的推拉力量实际上是非常直观的。
- en: We're pushing and pulling on the correct answer and the incorrect answers。 And
    the amount of force that we're applying is actually proportional to the probabilities
    that。 came out in the forward pass。 And so for example， if our probabilities came
    out exactly correct。 so they would have had zero everywhere except for one at
    the correct position。
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在正确答案和错误答案之间推拉。我们施加的力量实际上与前向传播中产生的概率成正比。例如，如果我们的概率完全正确，那么它们会在正确位置上为1，其余位置为0。
- en: then the the logits would be all row of zeros for that example。 There would
    be no push and pull。 So the amount to which your prediction is incorrect is exactly
    the amount by which you're going to。 get a pull or a push in that dimension。 So
    if you have for example a very confidently。 mispredicted element here， then what's
    going to happen is that element is going to be pulled down。
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: 那么对于这个例子，logits将会是全零的行。没有推拉的效果。因此，预测的不准确程度正好等于你在那个维度上将要获得的推或拉的量。如果你在这里有一个非常自信的错误预测，那么这个元素将会被拉下去。
- en: very heavily。 And the correct answer is going to be pulled up to the same amount。
    And the other。 characters are not going to be influenced too much。 So the amount
    to which you mispredict is then。 proportional to the strength of the pull。 And
    that's happening independently in all the dimensions of。
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 将会被强烈拉动。正确答案将被提升到同样的程度，而其他字符不会受到太大影响。因此，你的错误预测程度与拉动的强度成正比。这是在所有维度中独立发生的。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_500.png)'
  id: totrans-635
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_500.png)'
- en: this of this tensor。 And it's sort of very intuitive and very used to think
    through。 And that's basically， the magic of the cross entropy loss and what is
    doing dynamically in the backward pass of the neural。
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: 这是这个张量。想象一下，这种思考方式非常直观且常用。这基本上就是交叉熵损失的魔力，以及它在神经网络反向传播中的动态作用。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_502.png)'
  id: totrans-637
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_502.png)'
- en: mat。 So now we get to exercise number three， which is a very fun exercise。 depending
    on your definition， of fun。 And we are going to do for batch normalization exactly
    what we did for cross entropy loss in。 exercise number two。 That is we are going
    to consider it as a glued single mathematical expression。 and back propagate through
    it in a very efficient manner， because we are going to derive a much。
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来进行第三个练习，这是一个非常有趣的练习，具体取决于你对“有趣”的定义。我们将对批量归一化做的事情与第二个练习中的交叉熵损失相同。也就是说，我们将其视为一个粘合的单一数学表达式，并以非常高效的方式进行反向传播，因为我们将推导出一个更高效的结果。
- en: simpler formula for the backward pass of batch normalization。 And we're going
    to do that using。 pen and paper。 So previously we've broken up batch normalization
    into all of the little intermediate。 pieces and all the atomic operations inside
    it。 And then we back propagate it through it one by one。 Now we just have a single
    sort of forward pass of a batch room。 And it's all glued together。
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 更简单的批量归一化反向传播公式。我们将使用纸和笔来完成。因此之前我们将批量归一化拆分成所有小的中间部分和所有的原子操作。然后我们一个个地进行反向传播。现在我们只需进行一次批量处理的前向传播。一切都粘合在一起。
- en: And we see that we get these as same result as before。 Now for the batch backward
    pass。 we'd like to， also implement a single formula basically for back propagating
    through this entire operation。 That is the batch normalization。 So in the forward
    pass previously， we took H pre-bn。 the hidden states of the pre-bacterialization
    and created H pre-act。
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到得到的结果与之前相同。现在对于批量反向传播。我们希望基本上实现一个单一的公式，通过整个操作进行反向传播。也就是批量归一化。因此在之前的前向传播中，我们取
    H pre-bn。预归一化的隐藏状态并创建 H pre-act。
- en: which is the hidden states just， before the activation。 In the batch normalization
    paper。 H pre-bn is X and H pre-act is Y。 So in the backward pass。 what we'd like
    to do now is we have D H pre-act and we'd like to produce， D H pre-bn。 And we'd
    like to do that in a very efficient manner。 So that's the name of the game。
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏状态就是，在激活之前。在批量归一化的论文中。H pre-bn 是 X，H pre-act 是 Y。所以在反向传播中。我们现在想做的是，我们有 D H
    pre-act，想要生成 D H pre-bn。我们希望以一种非常高效的方式做到这一点。这就是目标。
- en: Calculate D H pre-bn given D H pre-act。 And for the purposes of this exercise。
    we're going to ignore， gamma and beta and their derivatives because they take
    on a very simple form in a very similar way。 to what we did up above。 So let's
    calculate this given that right here。 So to help you。
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 给定 D H pre-act 计算 D H pre-bn。为了这个练习，我们将忽略 gamma 和 beta 及其导数，因为它们的形式非常简单，与我们之前的计算方式非常相似。那么让我们在这里计算这个。为了帮助您。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_504.png)'
  id: totrans-643
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_504.png)'
- en: a little bit like I did before， I started off the implementation here on pen
    and paper。 And I took two sheets of paper to derive the mathematical formulas
    for the backward pass。 And basically to set up the problem， just write out the
    mu sigma square variance。 X i hat and Y i exactly as in the paper except for the
    Bessel correction。
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 有点像我之前做的，我开始在纸上进行实现。我用两张纸推导反向传播的数学公式。基本上要设置问题，只需写出mu sigma平方方差。X i hat 和 Y i
    完全按照论文的方式，除了贝塞尔修正。
- en: And then in the backward pass， we have the derivative of the loss with respect
    to all the。 elements of Y。 And remember that Y is a vector。 There's multiple numbers
    here。 So we have all the derivatives with respect to all the Ys。 And then there's
    a gamma and a beta。 And this is kind of like the compute graph。 The gamma and
    the beta， there's the X hat。 And then。
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在反向传播中，我们有损失对所有元素 Y 的导数。请记住，Y 是一个向量。这里有多个数字。因此，我们有关于所有 Y 的导数。然后有一个 gamma 和
    beta。这有点像计算图。gamma 和 beta，有 X hat。然后是。
- en: the mu and the sigma square and the X。 So we have D L by D Y i and we won't
    D L by D X i for all the。 I's in these vectors。 So this is the compute graph and
    you have to be careful because。 I'm trying to note here that these are vectors。
    There's many nodes here inside X， X hat and Y。 But mu and sigma， sorry， sigma
    square are just individual scalars， single numbers。
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: mu 和 sigma 平方以及 X。因此我们有 D L 对 D Y i 的导数，我们不会 D L 对 D X i 的导数，针对这些向量中的所有 I。这是计算图，您必须小心，因为。我在这里注意到这些是向量。这里面有许多节点在
    X，X hat 和 Y 中。但 mu 和 sigma，抱歉，sigma 平方只是单独的标量，单个数字。
- en: So you have to be careful with that。 You have to imagine there's multiple nodes
    here or you're。 going to get your math wrong。 So as an example， I would suggest
    that you go in the following order。 one， two， three， four in terms of the back
    propagation。 So back propagate into X hat。 then to sigma square， then into mu
    and then into X。 Just like an anthropological sort in。
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您必须对此小心。您必须想象这里有多个节点，否则您会搞错数学。例如，我建议您按照以下顺序进行。一，二，三，四，关于反向传播。因此反向传播到 X hat，然后到
    sigma 平方，再到 mu，然后到 X。就像人类学排序一样。
- en: micro grad， we would go from right to left。 You're doing the exact same thing
    except you're doing。 it with symbols and on a piece of paper。 So for number one，
    I'm not giving away too much。 If you。 want dl of the X i hat， then we just take
    dl by dy and multiply by gamma because of this expression。 here where any individual
    Y i is just gamma times X i hat plus beta。
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: micro grad，我们将从右到左进行。你所做的完全相同，只是用符号在纸上进行。所以对于第一项，我不会透露太多。如果你想要X i hat的dl，那么我们只需对y取dl并乘以gamma，因为这里的这个表达式，其中任何个体Y
    i只是gamma乘以X i hat加上beta。
- en: So it doesn't help you too much， there。 But this gives you basically the derivatives
    for all the X hats。 And so now try to go through， this computational graph and
    derive what is dl by d sigma square。 And then what is dl by d mu， and then what
    is dl by dx eventually。 So give it a go and I'm going to be revealing the answer
    one， piece at a time。 Okay。
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这对你帮助不大。但这基本上给你所有X hats的导数。因此，现在尝试通过这个计算图推导出dl对sigma square的导数是什么。然后dl对mu的导数是什么，最后dl对x的导数又是什么。试试看，我会逐步揭示答案。好的。
- en: so to get dl by d sigma square， we have to remember again， like I mentioned，
    that there are many Xs。 X hats here。 And remember that sigma square is just a
    single individual number， here。 So when we look at the expression for dl by d
    sigma square， we have that we have to actually。 consider all the possible paths
    that we basically have that there's many X hats and they all feed off。
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，要获得dl对sigma square的导数，我们必须再次记住，正如我提到的，这里有许多Xs。X hats。并记住sigma square只是这里的一个单一数值。因此，当我们查看dl对sigma
    square的表达式时，我们必须考虑所有可能的路径，因为基本上有许多X hats，它们都与之相关。
- en: from the all depend on sigma square。 So sigma square has a large fan out。 There's
    lots of arrows。 coming out from sigma square into all the X hats。 And then there's
    a back propagating signal from。 each X hat into sigma square。 And that's why we
    actually need to sum over all those eyes from i。 equal to one to m of the dl by
    d Xi hat， which is the global gradient times the Xi hat by d sigma。
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都依赖于sigma square。因此，sigma square的影响范围很大。有许多箭头从sigma square指向所有X hats。然后每个X
    hat都有一个向sigma square反向传播的信号。这就是为什么我们实际上需要对i从1到m的dl对X i hat进行求和，这就是全局梯度乘以X i hat对sigma的导数。
- en: square， which is the local gradient of this operation here。 And then mathematically。
    I'm just working， it out here and I'm simplifying and you get a certain expression
    for dl by d sigma square。 We're going to be using this expression when we back
    propagate into mu and then eventually into， X。 So now let's continue our back
    propagation into mu。 So what is dl by d mu？ Now again， be careful。
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: square，这是此操作的局部梯度。然后在数学上。我在这里进行计算并简化，你将获得dl对sigma square的某种表达式。我们将在反向传播到mu然后最终到X时使用这个表达式。那么现在让我们继续对mu的反向传播。那么dl对mu的导数是什么？现在再次小心。
- en: that mu influences X hat and X hat is actually lots of values。 So for example，
    if our mini batch。 size is 32， as it is in our example that we were working on，
    then this is 32 numbers and 32。 arrows going back to mu。 And then mu going to
    sigma square is just a single arrow because sigma。 square is scalar。 So in total，
    there are 33 arrows emanating from mu。 And then all of them have。
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: mu影响X hat，实际上X hat是许多值。因此，例如，如果我们的迷你批次大小是32，就像我们正在处理的例子一样，那么这就是32个数字和32个指向mu的箭头。然后，mu指向sigma
    square仅仅是一个箭头，因为sigma square是标量。因此，总共有33个箭头从mu发出。然后它们都有。
- en: gradients coming into mu and they all need to be summed up。 And so that's why
    when we look at the。 expression for dl by d mu， I am summing up over all the gradients
    of dl by d Xi hat times d Xi hat by。 d mu。 So that's this arrow and the 32 arrows
    here。 And then plus the one arrow from here。 which is dl， by d sigma square times
    d sigma square by d mu。
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度进入mu，它们都需要被相加。因此，这就是当我们查看dl对mu的表达式时，我在所有dl对X i hat的梯度和dX i hat对dmu的梯度之间进行求和的原因。所以这是这个箭头和这里的32个箭头。然后加上来自这里的一个箭头，即dl对sigma
    square的梯度乘以d sigma square对d mu的导数。
- en: So now we have to work out that expression。 And let me just reveal the rest
    of it。 Simplifying here is not complicated， the first term。 And you， just get
    an expression here。 For the second term though， there's something really interesting
    that， happens。 When we look at d sigma square by d mu and we simplify， at one
    point， if we assume that in。
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们必须计算那个表达式。让我揭示剩下的部分。在这里简化并不复杂，第一个项。你只是得到一个表达式。对于第二个项，发生了一些非常有趣的事情。当我们查看d
    sigma square对d mu并简化时，在某一点，如果我们假设在。
- en: a special case where mu is actually the average of Xi's， as it is in this case。
    then if we plug that， in， then actually the gradient vanishes and becomes exactly
    zero。 And that makes the entire second term， cancel。 And so these。 if you just
    have a mathematical expression like this， and you look at d sigma， square by d
    mu。
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 是一个特殊情况，其中`mu`实际上是`Xi`的平均值，就像在这个案例中那样。如果我们把它代入，那么梯度实际上会消失并变为零。这使得整个第二项抵消。因此，如果你只得到这样的数学表达式，并查看`d
    sigma`平方对`d mu`的。
- en: you would get some mathematical formula for how mu impacts sigma square。 But
    if。 it is the special case that mu is actually equal to the average， as it is
    in the case of。 rationalization， that gradient will actually vanish and become
    zero。 So the whole term cancels。 and we just get a fairly straightforward expression
    here for dl by d mu。 Okay。
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 你将得到一个关于`mu`如何影响`sigma square`的数学公式。但如果这是一个特殊情况，即`mu`实际上等于平均值，就像在合理化的情况下那样，那么梯度将实际上消失并变为零。因此整个项抵消，我们在这里得到一个相对简单的表达式，即`dl`对`d
    mu`的导数。好的。
- en: and now we get to the， craziest part， which is deriving dl by d Xi。 which is
    ultimately what we're after。 Now let's count。 First of all。 how many numbers are
    there inside X？ As I mentioned， there are 32 numbers。 There。 are 32 little Xi's。
    And let's count the number of arrows emanating from each Xi。 There's an arrow。
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们进入最疯狂的部分，就是推导`dl`对`d Xi`的导数，这最终是我们要追求的。现在我们来数一数。首先，`X`里面有多少个数字？正如我提到的，有32个数字。这里有32个小`Xi`。现在让我们数一数从每个`Xi`发出的箭头数量。这里有一个箭头。
- en: going to mu， an arrow going to sigma square。 And then there's an arrow going
    to X hat。 But this。 arrow here， let's scrutinize that a little bit。 Each Xi hat
    is just a function of Xi and all the。 other scalars。 So Xi hat only depends on
    Xi and none of the other X's。 And so therefore。 there are actually in this single
    arrow， there are 32 arrows。 But those 32 arrows are going。
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: 指向`mu`的箭头，指向`sigma square`的箭头。然后还有指向`X hat`的箭头。但这个箭头，让我们仔细审视一下。每个`Xi hat`只是`Xi`和其他标量的一个函数。因此，`Xi
    hat`只依赖于`Xi`，而不依赖于其他的`X`。因此，在这个单一的箭头中，实际上有32个箭头。但这32个箭头正。
- en: exactly parallel。 They don't interfere。 They're just going parallel between
    X and X hat。 You can。 look at it that way。 And so how many arrows are emanating
    from each Xi？ There are three arrows， mu。 sigma， square， and the associated X
    hat。 And so in back propagation， we now need to apply the chain。 rule。 And we
    need to add up those three contributions。
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: 完全平行。它们没有干扰。它们只是沿着`X`和`X hat`平行。你可以这样看。那么，从每个`Xi`发出的箭头有多少？有三个箭头，`mu`、`sigma`、平方，以及相关的`X
    hat`。所以在反向传播中，我们现在需要应用链式法则。我们需要将这三项贡献相加。
- en: So here's what that looks like if I just write， that out。 We have， we're going
    through。 we're changing through mu， sigma square and through X hat。 And， those
    three terms are just here。 Now we already have three of these。 We have dl by dx
    i hat。 We have， dl by d mu。 which we derived here。 And we have dl by d sigma square，
    which we derived here。 But we。
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我把它写出来，就像这样。这就是我们的过程。我们正在通过`mu`、`sigma square`和`X hat`进行变换。这三项就在这里。现在我们已经有了三个这样的项。我们有`dl`对`dx
    i hat`的导数。我们有`dl`对`d mu`的导数，这是我们在这里推导的。还有`dl`对`d sigma square`的导数，这是我们在这里推导的。但我们。
- en: need three other terms here。 The this one， this one， and this one。 So I invite
    you to try to derive。 them。 It's not that complicated。 You're just looking at
    these expressions here and differentiating。 with respect to Xi。 So give it a shot。
    But here's the result。 Or at least what I got。 Yeah。 I'm just， I'm just differentiating
    with respect to Xi for all of these expressions。 And， honestly。
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要三个其他术语。这个，这个，还有这个。所以我邀请你尝试推导它们。并不复杂。你只需查看这些表达式，并对`Xi`进行求导。试试看。不过这是结果。或者至少是我得到的。是的。我只是在对所有这些表达式进行`Xi`的求导。老实说。
- en: I don't think there's anything too tricky here。 It's basic calculus。 Now it
    gets a little。 bit more tricky is we are now going to plug everything together。
    So all of these terms。 multiply it with all of these terms and add it up according
    to this formula。 And that gets a。
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这里没有什么太棘手的。这是基本的微积分。现在变得稍微复杂一点，因为我们现在要把所有内容整合在一起。所以所有这些项与所有这些项相乘，并根据这个公式相加。这将得到一个。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_506.png)'
  id: totrans-664
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_506.png)'
- en: little bit hairy。 So what ends up happening is you get a large expression。 And
    the thing to be。 very careful with here， of course， is we are working with a dl
    by dx i for specific i here。 But when we are plugging in some of these terms，
    like say， this term here， dl by d sigma squared。 you see how dl by d sigma squared，
    I end up with an expression。 And I'm iterating over little i's。
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: 有点复杂。所以最终发生的事情是你得到一个大的表达式。当然，在这里需要非常小心的是，我们正在处理特定的`dl by dx i`。但是当我们插入一些这些项，比如说这个项，`dl
    by d sigma squared`。你会看到`dl by d sigma squared`，我最终得到了一个表达式。我在对小的i进行迭代。
- en: here。 But I can't use i as the variable when I plug in here， because this is
    a different i from。 this i。 This i here is just a place or a local variable for
    a for loop in here。 So here when I。 plug that in， you notice that I rename the
    i to a j。 Because I need to make sure that this j is not。 that this j is not this
    i。 This j is like a little local iterator over 32 terms。 And so you have to。
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里。但是我不能用i作为变量来插入，因为这个i与这个i不同。这里的i只是这里循环中的一个局部变量。所以在这里，当我插入时，你会注意到我将i重命名为j。因为我需要确保这个j不是这个i。这个j就像一个局部迭代器，遍历32个项。所以你必须。
- en: be careful with that。 When you're plugging in the expressions from here to here，
    you may have to。 rename i's into j's。 You have to be very careful what is actually
    an i with respect to dl by d xi。 So some of these are j's。 Some of these are i's。
    And then we simplify this expression。 And I guess like the big thing to notice
    here is a bunch of terms just kind of come out to the。
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: 对此小心。当你将这里的表达式插入到这里时，你可能需要将i重命名为j。你必须非常小心，什么实际上是与`dl by d xi`相关的i。所以这些中有一些是j，有一些是i。然后我们简化这个表达式。我想这里需要注意的一个重要点是，一堆项会自然地出来。
- en: front and you can refactor them。 There's a sigma squared plus epsilon raised
    to the power of negative。 three over two。 This sigma squared plus epsilon can
    be actually separated out into three terms。 Each of them are sigma squared plus
    epsilon to the negative one over two。 So the three of them。 multiplied is equal
    to this。 And then those three terms can go different places because of the。
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的，你可以重构它们。这里有一个`sigma squared`加上`epsilon`，并且这个值的指数为负的三分之二。这个`sigma squared`加上`epsilon`实际上可以分解成三个项。每一个都是`sigma
    squared`加`epsilon`的负一分之二。所以这三者相乘等于这个。然后这三个项可以因为。
- en: multiplication。 So one of them actually comes out to the front and will end
    up here outside。 One of。 them joins up with this term and one of them joins up
    with this other term。 And then when you。 simplify the expression， you'll notice
    that some of these terms that are coming out are just the。 xi hats。 So you can
    simplify just by rewriting that。 And what we end up with at the end is a fairly。
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: 乘法。所以其中一个实际上会到达前面，并最终出现在这里。一个与这个项结合，另一个与这个其他项结合。当你简化表达式时，你会注意到一些出来的项只是`xi hats`。所以你可以通过重新写来简化。最后我们得到的是一个相当。
- en: simple mathematical expression over here that I cannot simplify further。 But
    basically you'll。 notice that it only uses the stuff we have and it derives the
    thing we need。 So we have dl by dy。 for all the i's。 And those are used plenty
    of times here。 And also in the additional what we're。 using is these xi hats and
    xj hats。 And they just come from the forward pass。 And otherwise this is。
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个简单的数学表达式，我无法进一步简化。但是基本上你会注意到，它只使用了我们拥有的东西，并推导出我们需要的东西。所以我们有`dl by dy`，对于所有的i。这些在这里使用了很多次。而且在附加的部分，我们使用的是这些`xi
    hats`和`xj hats`。它们来自于前向传播。除此之外，这就是。
- en: a simple expression and it gives us dl by d xi for all the i's。 And that's ultimately
    what we're。 interested in。 So that's the end of a batch norm backward pass analytically。
    Let's now implement。 this final result。 Okay， so I implemented the expression
    into a single line of code here。 And。 you can see that the max diff is tiny。 So
    this is the correct implementation of this formula。 Now。
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的表达式，它给我们`dl by d xi`对于所有的i。这就是我们最终感兴趣的。所以这是批量归一化反向传播的解析结束。现在让我们实现这个最终结果。好吧，我将表达式实现成了一行代码。你可以看到最大差异很小。所以这是这个公式的正确实现。现在。
- en: I'll just basically tell you that getting this formula here from this mathematical
    expression。 was not trivial。 And there's a lot going on packed into this one formula。
    And this is all。 exercised by itself。 Because you have to consider the fact that
    this formula here is just for a。 single neuron and a batch of 32 examples。 But
    what I'm doing here is I'm actually， we actually。
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
- en: have 64 neurons。 And so this expression has to in parallel evaluate the batch
    norm backward pass。 for all those 64 neurons in parallel independently。 So this
    has to happen basically in every single。 column of the inputs here。 And in addition
    to that， you see how there are a bunch of sums here。 and we need to make sure
    that when I do those sums that they broadcast correctly onto everything。
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
- en: else that's here。 And so getting this expression is just like highly non trivial。
    And I invite you。 to basically look through it and step through it。 And it's a
    whole exercise to make sure that this。 checks out。 But once all the shapes agree
    and once you convince yourself that it's correct。 you can also verify that PyTorch
    gets the exact same answer as well。 And so that gives you a lot。
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
- en: of peace of mind that this mathematical formula is correctly implemented here
    and broadcast it。 correctly and replicated in parallel for all of the 64 neurons
    inside this batch term layer。
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_508.png)'
  id: totrans-676
  prefs: []
  type: TYPE_IMG
- en: Okay， and finally exercise number four asks you to put it all together。 And
    here we have a。 redefinition of the entire problem。 So you see that we reinstallize
    the neural net from scratch and。
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_510.png)'
  id: totrans-678
  prefs: []
  type: TYPE_IMG
- en: everything。 And then here， instead of calling the loss that backward， we want
    to have the manual。
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_512.png)'
  id: totrans-680
  prefs: []
  type: TYPE_IMG
- en: back propagation here as we derived it up above。 So go up copy paste all the
    chunks of code that。 we've already derived， put them here and drive your own gradients，
    and then optimize this neural。
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_514.png)'
  id: totrans-682
  prefs: []
  type: TYPE_IMG
- en: net， basically using your own gradients all the way to the calibration of the
    batch norm and the。 evaluation of the loss。 And I was able to achieve quite a
    good loss， basically the same loss you。 would achieve before。 And that shouldn't
    be surprising because all we've done is we've。
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_516.png)'
  id: totrans-684
  prefs: []
  type: TYPE_IMG
- en: really gotten into loss and backward and we've pulled out all the code and inserted
    it here。 But those gradients are identical and everything is identical and the
    results are identical。
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_518.png)'
  id: totrans-686
  prefs: []
  type: TYPE_IMG
- en: It's just that we have full visibility on exactly what goes on under the hood
    of。
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_520.png)'
  id: totrans-688
  prefs: []
  type: TYPE_IMG
- en: lot that backward in this specific case。 Okay， and this is all of our code。
    This is the full。 backward pass using basically the simplified backward pass for
    the cross entropy loss and the。 batch normalization。 So back propagating through
    cross entropy， the second layer， the 10 h null。 linearity， the batch normalization
    through the first layer and through the embedding。 And so you。
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种特定情况下很多是反向传播。好的，这就是我们的所有代码。这是完整的。反向传播，基本上是交叉熵损失和批量归一化的简化反向传播。因此，反向传播通过交叉熵，第二层，10
    h null。线性，批量归一化通过第一层和嵌入。因此你。
- en: see that this is only maybe what is this 20 lines of code or something like
    that。 And that's what。 gives us gradients。 And now we can potentially erase loss
    and backward。 So the way I have the code。
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: 看到这大概只有 20 行代码之类的。这就是。给我们梯度的东西。现在我们可以潜在地删除损失和反向。因此我有代码的方式。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_522.png)'
  id: totrans-691
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_522.png)'
- en: set up is you should be able to run this entire cell once you fill this in。
    And this will run for。
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: 设置是你应该能够在填入这些内容后运行整个单元。这将运行。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_524.png)'
  id: totrans-693
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_524.png)'
- en: only 100 iterations and then break。 And it breaks because it gives you an opportunity
    to check your。
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: 只需 100 次迭代，然后中断。它中断是因为它给你一个机会来检查你的。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_526.png)'
  id: totrans-695
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_526.png)'
- en: gradients against pytorch。 So here are gradients we see are not exactly equal。
    They are approximately， equal。 And the differences are tiny， one in negative nine
    or so。 And I don't exactly know where they're， coming from， to be honest。 So once
    we have some confidence that the gradients are basically correct。
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: 对比 pytorch 的梯度。我们看到的梯度并不完全相等。它们大约是相等的。差异很小，大约在负九的数量级。我老实说不太清楚这些差异来自哪里。因此，一旦我们对梯度的基本正确性感到有信心。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_528.png)'
  id: totrans-697
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_528.png)'
- en: we can take out the gradient checking。 We can disable this breaking statement。
    And then we can。 basically disable loss of backward。 We don't need it anywhere。
    It feels amazing to say that。 And then here， when we are doing the update， we're
    not going to use p。grad。 This is the old way。 of pytorch。 We don't have that anymore
    because we're not doing it backward。 We are going to use。
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以去掉梯度检查。我们可以禁用这个中断语句。然后我们可以。基本上禁用反向的损失。我们不需要它。这真是太好了。然后在这里，当我们进行更新时，我们不会使用
    p.grad。这是 pytorch 的旧方式。我们不再有这个，因为我们不再进行反向传播。我们将使用。
- en: this update where we you see that I'm iterating over， I've arranged the grads
    to be in the same。 order as the parameters， and I'm zipping them up the gradients
    and the parameters into p and grad。 And then here， I'm going to step with just
    a grad that we derived manually。 So the last piece。 is that none of this now requires
    gradients from pytorch。 And so one thing you can do here。
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: 这个更新，你看到我在迭代，我将梯度安排为与参数在同一顺序，并且我将梯度和参数压缩成 p 和 grad。然后在这里，我将用我们手动推导的梯度进行一步。因此最后一部分。是现在这些都不需要来自
    pytorch 的梯度。所以在这里你可以做的一件事。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_530.png)'
  id: totrans-700
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_530.png)'
- en: is you can do with torch。no。grad and offset this whole code block。 And really
    what you're saying。
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以用 torch.no.grad 来处理整个代码块。实际上你是在说。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_532.png)'
  id: totrans-702
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_532.png)'
- en: is you're telling pytorch that， hey， I'm not going to call backward on any of
    this。 And this last。
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: 你是在告诉 pytorch，嘿，我不会对任何这个调用 backward。而这个最后。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_534.png)'
  id: totrans-704
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_534.png)'
- en: pytorch to be a bit more efficient with all of it。 And then we should be able
    to just run this。 And it's running。 And you see that loss of the backward is commented
    out and we're optimizing。
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: 让 pytorch 更加高效。然后我们应该能够直接运行这个。它正在运行。你会看到反向的损失已被注释掉，我们正在优化。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_536.png)'
  id: totrans-706
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_536.png)'
- en: So we're going to leave this run and hopefully we get a good result。 Okay， so
    I allowed the neural。
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们将结束这个运行，希望能得到一个好的结果。好的，我允许神经网络。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_538.png)'
  id: totrans-708
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_538.png)'
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_539.png)'
  id: totrans-709
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_539.png)'
- en: out to finish optimization。 Then here， I calibrated the batch on parameters
    because I did not keep。
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: 最后完成优化。因此在这里，我根据参数校准了批处理，因为我没有保留。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_541.png)'
  id: totrans-711
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_541.png)'
- en: track of the running mean and very variance in their training loop。 Then here，
    I ran the loss。
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的训练循环中跟踪运行均值和方差。然后在这里，我运行了损失。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_543.png)'
  id: totrans-713
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_543.png)'
- en: and you see that we actually obtained a pretty good loss very similar to what
    we've achieved。 before。 And then here， I'm sampling from the model， and we see
    some of the name like gibberish。 that we're sort of used to。 So basically， the
    model worked and samples， pretty decent results。 compared to what we're used to。
    So everything is the same。 But of course， the big deal is that we。
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到我们实际上获得了一个相当好的损失，和之前的结果非常相似。然后在这里，我正在从模型中进行采样，我们看到一些像胡言乱语的名字。这是我们有点习惯的。因此，基本上模型有效，并且样本结果相当不错，和我们习惯的相比。所以一切都是一样的。但当然，关键是我们。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_545.png)'
  id: totrans-715
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_545.png)'
- en: did not use lots of backward。 We did not use pytorch autograd。 And we estimated
    our gradients。 ourselves by hand。 And so hopefully you're looking at this， the
    backward pass of this neural net。 and you're thinking to yourself， actually， that's
    not too complicated。 Each one of these layers is。 like three lines of code or
    something like that。 And most of it is fairly straightforward。
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有使用很多的反向传播。我们没有使用PyTorch的自动求导。我们手动估计了梯度。因此，希望你在看这个神经网络的反向传递时，会觉得其实并不复杂。每一层大约只有三行代码。而且大部分内容都是相当直接的。
- en: potentially with the notable exception of the batch normalization backward pass。
    Otherwise。
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: 可能除了批量归一化的反向传递之外，其他都是一样的。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_547.png)'
  id: totrans-718
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_547.png)'
- en: it's pretty good。 Okay， and that's everything I wanted to cover for this lecture。
    So hopefully。 you found this interesting。 And what I liked about it， honestly，
    is that it gave us a very nice。 diversity of layers to back propagate through。
    And I think it gives a pretty nice and comprehensive。 sense of how these backward
    passes are implemented and how they work。 And you'd be able to derive。
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: 这相当不错。好的，这就是我想在本讲中涵盖的所有内容。希望你觉得这很有趣。老实说，我喜欢的是，它给了我们非常好的多样性层以进行反向传播。我认为它提供了一个相当不错且全面的理解，关于这些反向传递是如何实现的以及它们是如何工作的。你也能够推导出。
- en: them yourself。 But of course， in practice， you probably don't want to， and you
    want to use the。 pytorch autograd。 But hopefully， you have some intuition about
    how gradients flow backwards through。 the neural net， starting at the loss， and
    how they flow through all the variables and all the。 intermediate results。 And
    if you understood a good chunk of it， and if you have a sense of that。
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以自己来做。但当然，在实践中，你可能不想这样，而是希望使用PyTorch的自动求导。但希望你对梯度是如何在神经网络中反向流动的有一些直觉，从损失开始，如何流经所有变量和中间结果。如果你理解了大部分内容，并且对这些有一些感觉。
- en: then you can count yourself as one of these buff dogees on the left， instead
    of the dogees on the。
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你可以把自己算作左边这些强壮的狗狗，而不是右边的狗狗。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_549.png)'
  id: totrans-722
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_549.png)'
- en: right here。 Now， in the next lecture， we're actually going to go to recurrent
    neural nets， LSTMs。 and all the other variants of Arnis。 And we're going to start
    to complexify the。 architecture and start to achieve better log likelihoods。 And
    so I'm really looking forward to that。 And。
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: 就在这里。现在，在下一讲中，我们将实际上讨论递归神经网络、LSTM以及所有其他变体。我们将开始复杂化架构并实现更好的对数似然。因此，我真的很期待这个。
- en: '![](img/86f8499f4a173e8882bce59ebef07fb4_551.png)'
  id: totrans-724
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86f8499f4a173e8882bce59ebef07fb4_551.png)'
- en: I'll see you then。
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: 那我们下次见。
