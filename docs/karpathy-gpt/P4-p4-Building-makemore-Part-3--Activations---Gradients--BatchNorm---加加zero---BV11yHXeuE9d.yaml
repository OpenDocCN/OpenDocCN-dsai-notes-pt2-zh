- en: P4：p4 Building makemore Part 3： Activations & Gradients, BatchNorm - 加加zero
    - BV11yHXeuE9d
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P4：p4 Building makemore Part 3：激活值与梯度，BatchNorm - 加加zero - BV11yHXeuE9d
- en: Hi everyone。 Today we are continuing our implementation of Makemore。 Now in
    the last lecture we implemented the multi-layer perceptron along the lines of。
    Benjio et al 2003 for character level language modeling。 So we followed this paper，
    took in a。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 大家好。今天我们继续实施Makemore。在上一次讲座中，我们实现了基于Benjio等人2003年提出的字符级语言建模的多层感知器。所以我们遵循了这篇论文，取了一些。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_1.png)'
- en: few characters in the past and used an MLP to predict the next character in
    a sequence。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 过去的一些字符，并使用多层感知器预测序列中的下一个字符。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_3.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_3.png)'
- en: So what we'd like to do now is we'd like to move on to more complex and larger
    neural networks。 like recurrent neural networks and their variations like the
    grew LSTM and so on。 Now before we do that though we have to stick around the
    level of multi-layer perceptron for a bit。 longer。 And I'd like to do this because
    I would like us to have a very good intuitive understanding。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们现在想要做的是，继续向更复杂和更大的神经网络发展，比如递归神经网络及其变体，如GRU和LSTM等。不过，在此之前，我们必须再停留在多层感知器的层面上稍微长一些。我想这样做，因为我希望我们能有一个非常好的直观理解。
- en: of the activations in the neural net during training and especially the gradients
    that are。 flowing backwards and how they behave and what they look like。 And this
    is going to be very。 important to understand the history of the development of
    these architectures because we'll see that。 recurrent neural networks while they
    are very expressive in that they are a universal approximator。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练期间神经网络中激活值的表现，尤其是向后传播的梯度，以及它们的表现和外观。这对于理解这些架构的发展历史非常重要，因为我们会看到，递归神经网络尽管在表达能力上很强大，但仍是一个通用逼近器。
- en: and can in principle implement all the algorithms。 We'll see that they are not
    very easily optimizable。 with the first order gradient based techniques that we
    have available to us and that we use all the。 time。 And the key to understanding
    why they are not optimizable easily is to understand the。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 从原则上讲，它们可以实现所有算法。我们将看到它们不是很容易优化的，使用的第一阶基于梯度的技术也不是很有效，且我们一直在使用这些技术。理解它们为什么不易优化的关键在于理解。
- en: the activations and the gradients and how they behave during training。 And we'll
    see that a lot。 of the variance since recurrent neural networks have tried to
    improve that situation。 And so。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 激活值和梯度，以及它们在训练中的表现。我们会看到，由于递归神经网络尝试改善这种情况，因此有很多的方差。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_5.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_5.png)'
- en: that's the path that we have to take and let's go start it。 So the starting
    code for this lecture is， largely the code from before but I've cleaned it up
    a little bit。 So you'll see that we are importing， all the torch and map plotlet
    utilities。 We're reading into words just like before。 These are。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们必须走的路径，让我们开始吧。这次讲座的起始代码主要是之前的代码，但我稍微清理了一下。所以你会看到我们导入了所有的Torch和Matplotlib工具。我们像之前一样读取单词。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_7.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_7.png)'
- en: eight example words。 There's a total of 32，000 of them。 Here's a vocabulary
    of all the lowercase。 letters and the special dot token。 Here we are reading the
    dataset and processing it and。 creating three splits， the train， dev and the test
    split。 Now in MLP， this is the identical。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 八个示例单词。总共有32,000个。这里是所有小写字母和特殊点标记的词汇表。我们正在读取数据集并处理它，创建三个分割：训练集、开发集和测试集。在多层感知器中，这是完全相同的。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_9.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_9.png)'
- en: same MLP except you see that I removed a bunch of magic numbers that we had
    here。 And instead we。 have the dimensionality of the embedding space of the characters
    and the number of hidden units。 in the hidden layer。 And so I've pulled them outside
    here so that we don't have to go and change all。 these magic numbers all the time。
    With the same neural net with 11，000 parameters that we optimize。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的多层感知器，不过我移除了这里的一堆魔法数字。取而代之的是，我们有字符嵌入空间的维度和隐藏层中隐藏单元的数量。所以我把它们提取出来，这样我们就不需要每次都去更改这些魔法数字。使用相同的神经网络，我们优化了11,000个参数。
- en: now over 200，000 steps with batch size of 32。 And you'll see that I refactored
    the code here a little。 bit but there are no functional changes。 I just created
    a few extra variables， a few more comments。 and I removed all the magic numbers
    and otherwise is the exact same thing。 Then when we optimized。 we saw that our
    loss looked something like this。 We saw that the train and val loss were about
    2。16。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在经过超过 200,000 步，批次大小为 32。你会看到我在这里稍微重构了一下代码，但没有功能上的变化。我只是创建了一些额外的变量，增加了一些注释，并去掉了所有魔法数字，其余部分完全一样。然后当我们进行优化时，我们看到我们的损失看起来大致是这样的。我们看到训练和验证损失大约为
    2.16。
- en: and so on。 Here I refactored the code a little bit for the evaluation of arbitrary
    splits。 So you pass in a string of which split you'd like to evaluate。 And then
    here， depending on train。 val or test， I index in and I get the correct split。
    And then this is the forward pass of the。 network and evaluation of the loss and
    printing it。 So just making it nicer。 One thing that you'll。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 诸如此类。在这里，我对任意分割的评估稍作了重构。所以你传入一个字符串，指定想要评估的分割。然后根据训练、验证或测试，我进行索引，得到正确的分割。接着这是网络的前向传播和损失评估及打印。这样做让它更简洁。你会注意到的一件事是。
- en: notice here is I'm using a decorator torch。no grad。 which you can also look
    up and read documentation， of。 Basically what this decorator does on top of a
    function is that whatever happens in this function。 is seen by torch to never
    require any gradients。 So it will not do any of the bookkeeping that it。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这里我使用了装饰器 `torch.no_grad`，你也可以查阅文档。基本上，这个装饰器在函数上会使得该函数中的所有操作都被 `torch` 视为不需要计算梯度。因此，它不会进行任何账务处理。
- en: does to keep track of all the gradients in anticipation of an eventual backward
    pass。 It's almost as if all the tensors that get created here have a requires
    grad of false。 And so it just makes everything much more efficient because you're
    telling torch that I will not call。 dot backward on any of this computation and
    you don't need to maintain the graph under the hood。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做可以跟踪所有梯度，以便将来进行反向传播。就好像这里创建的所有张量都有 `requires_grad` 设置为 `false`。这使得一切都更高效，因为你告诉
    `torch` 我不会在这些计算上调用 `dot.backward`，因此你不需要在后台维护图。
- en: So that's what this does。 And you can also use a context manager with torch
    dot no grad and you。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是它的作用。你也可以使用上下文管理器与 `torch.no_grad` 结合使用。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_11.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_11.png)'
- en: can let those up。 Then here we have the sample from a model just as before。
    Just a poor pass。 of a neural net getting the distribution sampling from it adjusting
    the context window and repeating。 until we get the special and token。 And we see
    that we are starting to get much nicer looking words。 simple from the model。 It's
    still not amazing and they're still not fully named like but it's much。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 可以让它们消失。然后在这里，我们有一个模型的样本，就像之前一样。只是神经网络的一个简单前向传播，从中获取分布进行采样，调整上下文窗口并重复，直到得到特殊和标记。我们看到开始出现更好看的词。模型生成的结果仍然不算惊艳，且仍然没有完全命名，但已好很多。
- en: better than when we had it with the bagram model。 So that's our starting point。
    Now the first thing。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 比我们使用 Bagram 模型时要好。这就是我们的起点。现在第一件事。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_13.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_13.png)'
- en: I would like to scrutinize is the initialization。 I can tell that our network
    is very improperly。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我想要仔细审查的是初始化。我可以看出我们的网络初始化得非常不当。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_15.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_15.png)'
- en: configured at initialization。 And there's multiple things wrong with it but
    let's just start with。 the first one。 Look here on the zero federation the very
    first iteration。 We are recording a loss。 of 27 and this rapidly comes down to
    roughly one or two or so。 So I can tell that the initialization， is all messed
    up because this is way too high。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始化时配置。有很多问题，但我们先从第一个开始。看看零联邦的第一次迭代。我们记录到的损失是27，而这很快降到大约一或二。所以我可以告诉你，初始化是完全混乱的，因为这个值太高了。
- en: In training of neural nets it is almost always， the case that you will have
    a rough idea for what loss to expect at initialization。 And that， just depends
    on the loss function and the problem set up。 In this case I do not expect 27。
    I expect， a much lower number and we can calculate it together。 Basically at initialization
    what we'd like is that。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络的训练中，几乎总是能对初始化时的损失有一个大致的预期。而这完全取决于损失函数和问题的设置。在这种情况下，我不期望得到27。我期望一个更低的数字，我们可以一起计算。基本上，在初始化时我们希望是这样的。
- en: there's 27 characters that could come next for any one training example。 At
    initialization we have。 no reason to believe any characters to be much more likely
    than others。 And so we'd expect that。 the probability distribution that comes
    out initially is a uniform distribution assigning about equal。 probability to
    all the 27 characters。 So basically what we like is the probability for any character。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何一个训练示例，可能会出现27个字符。在初始化时，我们没有理由相信任何字符比其他字符更可能。因此，我们预期最初输出的概率分布是均匀分布，给所有27个字符分配大致相等的概率。所以基本上，我们喜欢的是任何字符的概率。
- en: would be roughly one over 27。 That is the probability we should record and then
    the loss is the。 negative log probability。 So let's wrap this in a tensor and
    then then we can take the log of it。 and then the negative log probability is
    the loss we would expect which is 3。29 much much lower than 27。 And so what's
    happening right now is that at initialization the neural net is creating probability。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 概率大约是1/27。这是我们应该记录的概率，然后损失是负对数概率。所以我们把它包装在一个张量中，然后可以对其取对数，负对数概率就是我们期望的损失，即3.29，远低于27。因此，现在发生的事情是，在初始化时，神经网络正在生成概率。
- en: distributions that are all messed up。 Some characters are very confident and
    some characters are very。 not confident。 And then basically what's happening is
    that the network is very confidently wrong。 and that makes that's what makes it
    record very high loss。 So here's a smaller four-dimensional。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 有些字符非常自信，而有些字符则非常不自信。基本上，发生的事情是网络非常自信地出错，这就是造成记录非常高损失的原因。因此，这里有一个较小的四维示例。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_17.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_17.png)'
- en: example of the issue。 Let's say we only have four characters and then we have
    logits that come out。 of the neural net and they are very， very close to zero。
    Then when we take the softmax of all zeros。 we get probabilities that are a diffuse
    distribution。 So sums to one and is exactly uniform。 And then in this case if
    the label is say two， it doesn't actually matter if the label is two or。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 问题的例子。假设我们只有四个字符，神经网络输出的logits非常接近零。当我们对所有零进行softmax时，我们得到的是一个分散的概率分布。总和为1，完全均匀。在这种情况下，如果标签是2，其实并不重要标签是2还是。
- en: three or one or zero because it's a uniform distribution we're recording the
    exact same loss。 in this case 1。38。 So this is the loss we would expect for a
    four-dimensional example。 And I can see of course that as we start to manipulate
    these logits we're going to be changing the loss。 here。 So it could be that we
    lock out and by chance this could be a very high number like you know。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 三、一个或零，因为它是均匀分布，所以我们记录的损失是完全相同的，在这种情况下是1.38。因此，这是我们对于四维示例预期的损失。我当然可以看到，随着我们开始操纵这些logits，损失会发生变化。所以有可能我们偶然间锁定，这可能是一个很高的数字，比如你知道的。
- en: five or something like that。 Then in that case we'll record a very low loss
    because we're signing。 the correct probability at initialization by chance to
    the correct label。 Much more likely it is that， some other dimension will have
    a high logit and then what will happen is we start to record。 much higher loss。
    And what can come what can happen is basically the logits come out like。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 5或者类似的情况。那么在这种情况下，我们会记录非常低的损失，因为我们在初始化时偶然将正确的概率分配给了正确的标签。更可能的是，某个其他维度会有一个高logit，然后发生的事情是我们开始记录更高的损失。而且发生的事情基本上是logits输出的分布都是混乱的。
- en: something like this you know and they take on extreme values and we record really
    high loss。 For example if we have torched out a random of four so these are uniform。
    so these are normally distributed numbers for them。 And here we can also print
    the logits。 probabilities that come out of it and loss。 And so because these logits
    are near zero for the most。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 像这样，你知道它们会取极端值，我们记录的损失非常高。例如，如果我们随机选择四个字符，这些字符是均匀分布的。它们通常是正态分布的数字。在这里，我们也可以打印出logits、概率以及损失。因此，因为这些logits大部分接近零。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_19.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_19.png)'
- en: part the loss that comes out is is okay。 But suppose this is like times 10 now。
    You see how because these are more extreme values it's very unlikely that you're
    going to be guessing。 the correct bucket and then you're confidently wrong and
    recording very high loss。 If your。 logits are coming up even more extreme you
    might get extremely you know same losses like infinity。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 损失的一部分是可以的。但是假设这像乘以10一样。你可以看到，由于这些是更极端的值，很不可能猜测正确的桶，然后你自信地错误并记录非常高的损失。如果你的logits更加极端，你可能会得到极端的损失，比如无穷大。
- en: even at initialization。 So basically this is not good and we want the logits
    to be roughly zero。 when the network is initialized。 In fact the logits can don't
    have to be just zero they just。 have to be equal。 So for example if all the logits
    are one then because of the normalization inside。 the softmax this will actually
    come out okay。 But by symmetry we don't want it to be any arbitrary。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在初始化时也是如此。所以基本上这不好，我们希望网络初始化时logits大致为零。实际上，logits不一定是零，只需要相等。例如，如果所有logits都是1，那么由于softmax内部的归一化，这实际上是可以的。但从对称性来看，我们不希望它是任意的。
- en: positive or negative number we just want it to be all zeros and record the loss
    that we expect at。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 正数或负数我们希望它全部为零，并记录我们预期的损失。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_21.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_21.png)'
- en: initialization。 So let's now concretely see where things go wrong in our example。
    Here we have the。 initialization let me reinitialize the neural net and here let
    me break after the very first iteration。 so we only see the initial loss which
    is 27。 So that's way too high and intuitively now we can。 expect the variables
    involved and we see that the logits here if we just print some of these。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化。所以现在让我们具体看看在我们的例子中哪里出错。这里是初始化，让我重新初始化神经网络，并在第一次迭代后中断。这样我们只看到初始损失为27。这太高了，直观上我们可以预期涉及的变量，并且我们看到这里的logits如果我们打印一些。
- en: If we just print the first row we see that the logits take on quite extreme
    values。 and that's what's creating the fake confidence in incorrect answers and
    makes the loss。 get very very high。 So these logits should be much much closer
    to zero。 So now let's think through。 how we can achieve logits coming out of this
    neural net to be more closer to zero。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们打印第一行，我们看到logits具有相当极端的值，这正是造成错误答案中虚假自信的原因，并使损失非常高。因此，这些logits应该更接近零。那么现在让我们思考一下，如何才能让这个神经网络输出的logits更接近零。
- en: You see here that， logits are calculated as the hidden states multiplied by
    w2 plus b2。 So first of all currently we're， initializing b2 as random values
    of the right size but because we want roughly zero we don't。 actually want to
    be adding a bias of random numbers so in fact I'm going to add a times zero here。
    to make sure that b2 is just basically zero at initialization and second this
    is h multiplied by。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，这里logits是通过隐藏状态乘以w2加上b2计算得出的。所以首先，我们将b2初始化为合适大小的随机值，但因为我们想要大致为零，我们实际上不希望添加随机数的偏差，所以我将在这里添加一个乘以零的项，以确保b2在初始化时基本为零，第二个是h乘以。
- en: w2。 So if we want logits to be very very small then we would be multiplying
    w2 and making that smaller。 So for example if we scale down w2 by 0。1 all the
    elements then if I do again just a very。 first iteration you see that we are getting
    much closer to what we expect。 So roughly what we want。 is about 3。29 this is
    4。2。 I can make this maybe even smaller 3。32 okay so we're getting closer and。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: w2。所以如果我们希望logits非常非常小，那么我们就会乘以w2并使其更小。例如，如果我们将w2缩小到0.1的所有元素，然后如果我再进行第一次迭代，你会看到我们离预期的更近。所以我们大致想要的是3.29，这是4.2。我可以把这个值甚至更小变成3.32，好的，所以我们越来越接近。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_23.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_23.png)'
- en: closer。 Now you're probably wondering can we just set this to zero then we get
    of course exactly what。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 更接近。现在你可能想知道我们是否可以将其设置为零，那么我们当然会得到正是我们所期望的。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_25.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_25.png)'
- en: we're looking for at initialization and the reason I don't usually do this is
    because I'm very nervous。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在初始化时所寻找的，我通常不这样做的原因是因为我非常紧张。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_27.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_27.png)'
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_28.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_28.png)'
- en: and I'll show you in a second why you don't want to be setting w's or weights
    of a neural net exactly。 to zero。 You usually want it to be small numbers instead
    of exactly zero。 For this output layer in。 this specific case I think it would
    be fine but I'll show you in a second where things go wrong。 very quickly if you
    do that。 So let's just go with 0。01。 In that case our loss is close enough。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我会在一秒钟后向你展示为什么不想将神经网络的权重设置为零。你通常希望它是小数字，而不是完全为零。在这个特定情况下的输出层，我认为这样做是可以的，但我会在一秒钟后展示，如果这样做，事情会很快出错。所以我们就设定为0.01。在这种情况下，我们的损失足够接近。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_30.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_30.png)'
- en: but has some entropy it's not exactly zero it's got some little entropy and
    that's used for。 symmetry breaking as we'll see in a second。 Logits are now coming
    out much closer to zero and everything。 is well and good。 So if I just erase these
    and I now take away the break statement。 we can run the optimization with this
    new initialization and let's just see what losses we， record。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 但是有一些熵，它并不完全为零，存在一些小的熵，这用于对称破缺，正如我们稍后会看到的那样。Logits现在接近零，一切看起来很好。因此，如果我删除这些并去掉中断语句，我们可以使用这个新的初始化进行优化，让我们看看我们记录的损失。
- en: Okay so I'll let it run and you see that we started off good and then we came
    down a bit。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我让它运行，你会看到我们一开始很好，然后稍微下降了一些。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_32.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_32.png)'
- en: The plot of the loss now doesn't have this hockey shape appearance because basically
    what's。 happening in the hockey stick the very first few iterations of the loss
    what's happening during the。 optimization is the optimization is just squashing
    down the logits and then it's rearranging the。 logits。 So basically we took away
    this easy part of the loss function where just the the weights were。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在损失的图形没有这种曲棍球形状的出现，因为基本上在曲棍球棒的最初几次迭代中，损失的发生是优化仅仅在压缩logits，然后重新排列logits。因此，我们基本上去掉了损失函数的这个简单部分，那里的权重只是。
- en: just being shrunk down and so therefore we don't we don't get these easy gains
    in the beginning。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 只是被压缩了，因此我们一开始并没有获得这些轻松的收益。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_34.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_34.png)'
- en: and we're just getting some of the hard gains of training the actual neural
    net and so there's no。 hockey stick appearance。 So good things are happening in
    that both number one loss at initialization is。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在获得训练实际神经网络的一些艰难收益，因此没有曲棍球棒的出现。所以好事情发生了，初始化时的损失是。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_36.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_36.png)'
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_37.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_37.png)'
- en: what we expect and the loss doesn't look like a hockey stick and this is true
    for any neural。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所期望的，损失看起来不像一个曲棍球棒，这对任何神经网络来说都是如此。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_39.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_39.png)'
- en: likelihood train and something to look at for。 And second the loss that came
    out is actually quite。 a bit improved。 Unfortunately I erased what we had here
    before。 I believe this was 2。12 and this。 was 2。16 so we get a slightly improved
    result and the reason for that is because we're spending more。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 可能性训练和一些需要关注的内容。其次，输出的损失实际上是相当的。稍微有一些改善。不幸的是，我之前的内容被我删除了。我相信这是2.12，而这个是2.16，因此我们得到稍微改善的结果，原因在于我们花费了更多的时间。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_41.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_41.png)'
- en: cycles more time optimizing the neural net actually instead of just spending
    the first。 several thousand iterations probably just squashing down the weights
    because they are so。 way too high in the beginning of the initialization。 So something
    to look out for and that's number one。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在优化神经网络时花费更多的时间，而不仅仅是在前几千次迭代中，可能只是压缩权重，因为它们在初始化的开始阶段过高。因此，需要关注的事项就是这一点。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_43.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_43.png)'
- en: Now let's look at the second problem。 Let me reinitialize our neural net and
    let me reintroduce。 the break statement。 So we have a reasonable initial loss。
    So even though everything is looking good on， the level of the loss and we get
    something that we expect there's still a deeper problem working。 inside this neural
    net and its initialization。 So the logits are now okay。 The problem now is with。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看第二个问题。让我重新初始化我们的神经网络，并让我重新引入断点语句。因此我们有一个合理的初始损失。尽管在损失的层面上看起来一切都很好，并且我们得到了一些预期的结果，但这个神经网络及其初始化内部仍然存在更深层的问题。因此，logits
    现在没问题。现在的问题是。
- en: the values of H the activations of the hidden states。 Now if we just visualize
    this vector。 sorry this tensor H it's kind of hard to see but the problem here
    roughly speaking is you see how many。 of the elements are one or negative one。
    Now recall that torch dot 10 H the 10 H function is a squashing。 function。 It
    takes arbitrary numbers and it squashes them into a range of negative one and
    one and it。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: H 的值是隐藏状态的激活值。现在如果我们只是可视化这个向量，抱歉这个张量 H，有点难以看清，但问题大致上是你会看到多少元素是 1 或负 1。现在回想一下
    torch dot 10 H，10 H 函数是一个压缩函数。它接受任意数字并将其压缩到负一到一的范围内，并且。
- en: does so smoothly。 So let's look at the histogram of H to get a better idea of
    the distribution of。 the values inside this tensor。 We can do this first。 Well
    we can see that H is 32 examples and 200。 activations in each example。 We can
    view it as negative one to stretch it out into one large。 vector and we can then
    call two list to convert this into one large Python list of floats。 And then。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 一切都是如此平滑。因此，让我们查看 H 的直方图，以更好地了解这个张量内部值的分布。我们可以首先做到这一点。好吧，我们可以看到 H 有 32 个示例，每个示例有
    200 个激活。我们可以将其视为负一，扩展成一个大的向量，然后调用 to list 将其转换为一个大型的 Python 浮点数列表。然后。
- en: we can pass this into PLT dot hissed for histogram and we say we want 50 bins
    and a semicolon to suppress。 a bunch of output we don't want。 So we see this histogram
    and we see that most of the values by far。 take on value of negative one and one。
    So this 10 H is very very active and we can also look at。 basically why that is
    we can look at the preactivations that feed into the 10 H。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个传递到 PLT dot hissed 来生成直方图，并且我们说我们想要 50 个箱子，并用分号来抑制我们不想要的输出。因此我们看到这个直方图，并且可以看到大多数值明显是负一和一。因此，这个
    10 H 是非常活跃的，我们还可以看看，基本上这是什么原因，我们可以查看输入到 10 H 的预激活值。
- en: And we can see that the distribution of the preactivations are is very very
    broad。 These take。 numbers between negative 15 and 15 and that's why in a torch
    dot 10 H everything is being squashed。 and capped to be in the range of negative
    one and one and lots of numbers here take on very。 extreme values。 Now if you
    are new to neural networks you might not actually see this as an issue。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到预激活值的分布非常广泛。这些值在负 15 到 15 之间，这就是为什么在 torch dot 10 H 中所有值都被压缩，并限制在负一到一的范围内，许多值在这里取非常极端的值。现在如果你对神经网络不太了解，可能不会将其视为问题。
- en: but if you're well versed in the dark arts of back propagation and then have
    an intuitive。 sense of how these gradients flow through a neural net you are looking
    at your distribution of 10 H。 activations here and you are sweating。 So let me
    show you why。 We have to keep in mind that during。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果你对反向传播的黑暗艺术非常熟悉，并且对这些梯度如何在神经网络中流动有直观的理解，你会看到你在这里的 10 H 激活的分布，而感到紧张。那么让我给你展示一下原因。我们必须记住，在此期间。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_45.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_45.png)'
- en: back propagation just like we saw in micro grad we are doing backward pass starting
    at the loss。 and flowing through the network backwards。 In particular we're going
    to back propagate through。 this torch dot 10 H and this layer here is made up
    of 200 neurons for each one of these examples。 and it implements an element twice
    10 H。 So let's look at what happens in 10 H in the backward pass。
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播就像我们在微梯度中看到的一样，我们从损失开始进行反向传递，并向后流经网络。特别是我们将通过这个 torch dot 10 H 进行反向传播，这一层由每个示例的
    200 个神经元组成，并实现了一个元素两次的 10 H。因此让我们看看在反向传递中 10 H 发生了什么。
- en: We can actually go back to our previous micro grad code in the very first lecture
    and see how we。 implemented 10 H。 We saw that the input here was X and then we
    calculate T which is the 10 H of X。 So that's T and T is between negative one
    and one it's the output of the 10 H and then in the backward。 pass how do we back
    propagate through a 10 H。 We take out that grad and then we multiply it this。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上可以回到第一次讲座的微梯度代码，看看我们是如何实现10 H的。我们看到这里的输入是X，然后我们计算T，它是X的10 H。所以T在负一和一之间，是10
    H的输出，然后在反向传播中，我们如何通过10 H反向传播？我们取出那个梯度，然后乘以它。
- en: is the chain rule with the local gradient which took the form of one minus T
    squared。 So what。 happens if the outputs of your 10 H are very close to negative
    one or one。 If you plug in T。 equals one here you're going to get zero multiplying
    out that grad。 No matter what out that grad is。 we are killing the gradient and
    we're stopping effectively the backward propagation through this。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 是链式法则与局部梯度的形式，表现为1减去T的平方。那么，如果你的10 H输出非常接近负一或一，会发生什么呢？如果这里代入T等于一，你会得到零，乘以那个梯度，无论那个梯度是什么，我们都在消除梯度，有效地阻止了向后传播。
- en: 10 H unit。 Similarly when T is negative one this will again become zero and
    out that grad just stops。 and intuitively this makes sense because this is a 10
    H neuron and what's happening is if its。 output is very close to one then we are
    in the tail of this 10 H and so changing basically the input。 is not going to
    impact the output of the 10 H too much because it's in the flat region of the
    10 H。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 10 H单元。同样，当T为负一时，这将再次变为零，那个梯度就停止了，从直观上看，这是有意义的，因为这是一个10 H神经元，如果它的输出非常接近一，那么我们就在这个10
    H的尾部，所以改变输入不会对10 H的输出产生太大影响，因为它位于10 H的平坦区域。
- en: and so therefore there's no impact on the loss and so indeed the weights and
    the biases along。 with this 10 H neuron do not impact the loss because the output
    of this 10 H unit is in the flat。 region of the 10 H and there's no influence
    we can we can be changing them whatever we want。 however we want and the loss
    is not impacted that's so that's another way to justify that。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 因此对损失没有影响，确实这个10 H神经元的权重和偏置对损失没有影响，因为这个10 H单元的输出位于10 H的平坦区域，改变它们不会影响损失，这也是另一种证明方法。
- en: indeed the gradient would be basically zero it vanishes。 Indeed when T equals
    zero we get。 one times out that grad so when the 10 H takes on exactly value of
    zero then out that grad is just。 passed through so basically what this is doing
    right is if T is equal to zero then this the 10 H unit is。 sort of inactive and
    gradient just passes through but the more you are in the flat tails the more。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，梯度基本上会变为零，消失。确实当T等于零时，我们得到一乘以那个梯度，因此当10 H恰好取零值时，那个梯度就被传递过来。所以这基本上意味着，如果T等于零，那么这个10
    H单元是处于不活跃状态，梯度就会传递，但你越处于平坦尾部。
- en: the gradient is squashed so in fact you'll see that the gradient flowing through
    10 H can only ever。 decrease in the amount that it decreases is proportional through
    a square here depending on how far you。 are in the flat tails of this 10 H and
    so that's kind of what's happening here and through this。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度被压缩，实际上你会看到通过10 H的梯度只能减少，其减少的量与这个10 H的平坦尾部的位置成平方比例，这就是这里发生的事情。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_47.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_47.png)'
- en: the concern here is that if all of these outputs H are in the flat regions of
    negative one and one。 then the gradients that are flowing through the network
    will just get destroyed at this layer。 Now there is some redeeming quality here
    and that we can actually get a sense of the problem。 here as follows。 I wrote
    some code here and basically what we want to do here is we want to take a look。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的担忧是，如果所有这些输出H都在负一和一的平坦区域，那么通过网络流动的梯度将在这一层被摧毁。现在这里有一些挽救的特质，我们可以实际感知到问题，我写了一些代码，我们想要查看的正是这些。
- en: at H take the absolute value and see how often it is in the in the flat region
    so say greater than 0。99。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在H取绝对值并查看它在平坦区域中出现的频率，比如大于0.99。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_49.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_49.png)'
- en: and what you get is the following and this is a Boolean tensor so in the Boolean
    tensor you get a。 white if this is true and a black if this is false and so basically
    what we have here is the 32。 examples and a 200 hidden neurons and we see that
    a lot of this is white and what that's telling us。 is that all these 10 H neurons
    were very very active and they're in a flat tail and so in all these。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你得到的是以下内容，这是一个布尔张量。在布尔张量中，如果这个为真则为白色，如果为假则为黑色。基本上，我们这里有32个示例和200个隐藏神经元，我们看到很多是白色的，这告诉我们所有这些10
    H神经元都非常活跃，并且它们处于一个平坦的尾部，因此在所有这些。
- en: cases the backward gradient would get destroyed。 Now we would be in a lot of
    trouble if for any one。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，反向梯度将被破坏。如果对于任何一个。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_51.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_51.png)'
- en: of these 200 neurons if it was the case that the entire column is white because
    in that case we。 have what's called the dead neuron and this could be a 10 H neuron
    where the initialization of the。 weights and the biases could be such that no
    single example ever activates this 10 H in the。 sort of active part of the 10
    H。 If all the examples land in the tail then this neuron will never learn。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这200个神经元中，如果整个列是白色，那么我们就有了所谓的死神经元。这可能是一个10 H神经元，其中权重和偏置的初始化可能使得没有单个示例能激活这个10
    H在其活跃部分。如果所有示例都落在尾部，那么这个神经元将永远不会学习。
- en: it is a dead neuron and so just scrutinizing this and looking for columns of
    completely white we see。 that this is not the case so I don't see a single neuron
    that is all of you know white and so therefore。 it is the case that for every
    one of these 10 H neurons we do have some examples that activate them。 in the
    active part of the 10 H and so some gradients will flow through and this neuron
    will learn。
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个死神经元，因此仔细检查这个并寻找完全白色的列，我们看到并非如此。因此，我没有看到任何单个神经元是全白的，因此对于每一个这些10 H神经元，我们确实有一些示例可以激活它们。在10
    H的活跃部分，因此一些梯度会流动，这个神经元将会学习。
- en: and neuron will change and it will move and it will do something but you can
    sometimes get it。 yourself in cases where you have dead neurons and the way this
    manifests is that for 10 H neurons。 this would be when no matter what inputs you
    plug in from your data set this 10 H neuron always fires。 completely one or completely
    negative one and then it will just not learn because all the gradients。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元会改变，它会移动并执行某些操作，但在某些情况下，你可能会发现自己处于死神经元的情境中。对于10 H神经元来说，这意味着无论你从数据集中插入什么输入，这个10
    H神经元总是完全地激活或完全负激活，然后它将不会学习，因为所有的梯度。
- en: will be just zero that this is true not just for 10 H but for a lot of other
    non-linearities。
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这不仅对10 H是正确的，对许多其他非线性函数也是如此。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_53.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_53.png)'
- en: that people use in neural networks so we certainly use 10 H a lot but sigmoid
    will have the exact same。 issue because it is a squashing neuron and so the same
    will be true for sigmoid but but you know。 basically the same will actually apply
    to sigmoid the same will also apply to a relu so relu has。 a completely flat region
    here below zero so if you have a relu neuron then it is a pass through。
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 人们在神经网络中使用的函数，所以我们确实在使用10 H，但sigmoid会有完全相同的问题，因为它是一个压缩神经元，因此sigmoid也将面临同样的情况。基本上，这同样适用于sigmoid，同样也适用于relu，因此relu在零以下有一个完全平坦的区域。如果你有一个relu神经元，那么它就是一个通道。
- en: if it is positive and if it's if the pre-activation is negative it will just
    shut it off。 since the region here is completely flat then during back propagation
    this would be exactly。
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果它是正的，而预激活是负的，它将被关闭。由于这里的区域完全平坦，因此在反向传播时这将是完全的。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_55.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_55.png)'
- en: zeroing out the gradient like all of the gradient would be set exactly to zero
    instead of just like。 a very very small number depending on how positive or negative
    t is and so you can get for example a。
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 将梯度置零，所有梯度将被设定为零，而不仅仅是一个非常非常小的数字，这取决于t的正负，因此你可以得到例如一个。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_57.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_57.png)'
- en: dead relu neuron and a dead relu neuron would basically look like basically
    what it is is if a。 neuron with a relu nonlinearity never activates so for any
    examples that you plug in in the data set。 it never turns on it's always in this
    flat region then this relu neuron is a dead neuron its weights。 and bias will
    never learn they will never get a gradient because the neuron never activated。
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 死的 ReLU 神经元基本上看起来就是，如果一个带有 ReLU 非线性的神经元从不激活，那么对于数据集中你插入的任何例子，它永远不会打开，总是在这个平坦的区域，那么这个
    ReLU 神经元就是一个死神经元，它的权重和偏差将永远不会学习，它们永远得不到梯度，因为这个神经元从未激活。
- en: and this can sometimes happen at initialization because the weights in the biases
    just make it so。 that by chance some neurons are just forever dead but it can
    also happen during optimization。 if you have like a too high learning rate for
    example sometimes you have these neurons that。 get too much of a gradient and
    they get knocked out of the data manifold and what happens is that。
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况有时会在初始化时发生，因为权重和偏差使得一些神经元不幸地永远处于死亡状态，但也可能在优化过程中发生。如果你有一个过高的学习率，举个例子，有时这些神经元会得到过多的梯度，从而被击出数据流形，结果是。
- en: from then on no example ever activates its neuron so this neuron remains dead
    forever so it's kind。 of like a permanent brain damage in a in a mind of a network
    and so sometimes what can happen is。 if your learning rate is very high for example
    and you have a neural net with a relu neurons。 you train the neural net and you
    get some last loss but then actually what you do is you go through。
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 从那时起，没有任何例子能激活它的神经元，所以这个神经元将永远处于死状态，这有点像网络中一种永久性的大脑损伤。有时，如果你的学习率非常高，举个例子，你有一个带有
    ReLU 神经元的神经网络，你训练这个神经网络，得到了一些损失，但实际上你做的是通过某些部分进行训练。
- en: the entire training set and you forward your examples and you can find neurons
    that never activate。 they are dead neurons in your network and so those neurons
    will will never turn on and usually what。 happens is that during training these
    relu neurons are changing moving etc and then because of a。 high gradient somewhere
    by chance they get knocked off and then nothing ever activates them and from。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 整个训练集你前馈你的例子，可以发现从未激活的神经元。它们是你网络中的死神经元，所以这些神经元将永远不会激活，通常在训练过程中，这些 ReLU 神经元会改变、移动等，然后因为某处的高梯度，它们被随机击退，然后没有任何东西激活它们。
- en: then on they are just dead so that's kind of like a permanent brain damage that
    can happen to。 some of these neurons these other nonlinearities like leaky relu
    will not suffer from this issue。 as much because you can see that it doesn't have
    flat tails you'll almost always get gradients。 and elu is also fairly frequently
    used it also might suffer from this issue because it has flat。
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 从那时起，它们就死了，这就像是一些神经元可能发生的永久性大脑损伤。这些其他非线性函数如 Leaky ReLU 不会那么严重地遭受这个问题，因为你可以看到它没有平坦的尾部，几乎总是会得到梯度。而
    ELU 也相对常用，它也可能遭受这个问题，因为它有平坦。
- en: parts so that's just something to be aware of and something to be concerned
    about and in this case。
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这只是需要注意的事情，也是需要担忧的事情，在这种情况下。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_59.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_59.png)'
- en: we have way too many activations h that take on extreme values and because there's
    no column of。
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有太多激活 h 取极端值，而且因为没有列。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_61.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_61.png)'
- en: white i think we will be okay and indeed the network optimizes and gives us
    a pretty decent。
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 白色，我认为我们会没事，实际上网络优化得很好，给了我们一个相当不错的结果。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_63.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_63.png)'
- en: loss but it's just not optimal and this is not something you want especially
    during initialization。
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有损失，但这并不是最优的，这不是你想要的，特别是在初始化期间。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_65.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_65.png)'
- en: and so basically what's happening is that this h pre-activation that's flowing
    to 10h。
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 所以基本上发生的事情是，这个流向 10h 的 h 预激活。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_67.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_67.png)'
- en: it's it's too extreme it's too large it's creating very it's creating a distribution
    that is too。 saturated in both sides of the 10h and it's not something you want
    because it means that there's。 less training for these neurons because they update
    less frequently so how do we fix this well h。
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这太极端了，太大了，导致分布在两侧过于饱和，这不是你想要的，因为这意味着这些神经元的训练减少，因为它们更新不那么频繁。那么我们该如何解决这个问题呢？
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_69.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_69.png)'
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_70.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_70.png)'
- en: pre-activation is mcat which comes from c so these are uniform Gaussian but
    then it's multiplied by。 w1 plus b1 and h pre-act is too far off from zero and
    that's causing the issue so we want this。 pre-activation to be closer to zero
    very similar to what we have with logis so here we want actually。 something very
    very similar now it's okay to set the biases to very small number we can either。
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 前激活是来自c的mcat，这些是均匀的高斯分布，但然后它乘以w1加b1，h前激活离零太远，这造成了问题，所以我们希望这个前激活更接近于零，类似于我们在logis中所拥有的。所以这里我们实际上希望得到非常相似的东西，现在把偏置设置为非常小的数字是可以的，我们可以选择。
- en: multiply by 001 to get like a little bit of entropy I sometimes like to do that
    just so that。 there's like a little bit of variation in diversity in the original
    initialization of these 10h。 neurons and I find in practice that that can help
    optimization a little bit and then the weights we。
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 乘以0.01以获取一点熵，我有时喜欢这样做，这样在这10个神经元的初始状态中会有一点变化和多样性，我发现实际上这有助于优化，接下来是权重。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_72.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_72.png)'
- en: can also just like squash so let's multiply everything by 0。1 let's rerun the
    first batch。
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以像这样压缩，所以我们乘以0.1，重新运行第一批。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_74.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_74.png)'
- en: and now let's look at this and well first let's look at here you see now because
    we multiply。
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看这一点，首先让我们看看这里，你现在看到，因为我们乘以。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_76.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_76.png)'
- en: double it by 0。1 we have a much better histogram and that's because the pre-activations
    are now。
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 乘以0.1后，我们有了更好的直方图，这因为前激活现在。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_78.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_78.png)'
- en: between negative 1。5 and 1。5 and this we expect much much less white okay there's
    no white so。
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在负1.5和1.5之间，我们希望有更少的白色，好的，这里没有白色。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_80.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_80.png)'
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_81.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_81.png)'
- en: basically that's because there are no neurons that saturated above 0。99 in either
    direction so。
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上是因为没有神经元在任何方向上饱和超过0.99，所以。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_83.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_83.png)'
- en: this is actually a pretty decent place to be maybe we can go up a little bit。
    sorry am I changing w1 here so maybe we can go to 0。2 okay so maybe something
    like this is a nice。
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上是一个相当不错的状态，也许我们可以稍微增加一点。抱歉，我这里在改变w1，也许我们可以调整到0.2，好的，也许像这样是不错的选择。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_85.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_85.png)'
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_86.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_86.png)'
- en: distribution so maybe this is what our initialization should be so let me now
    erase these and let me。
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 分布，所以也许这就是我们初始化的方式，现在让我擦掉这些，让我。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_88.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_88.png)'
- en: starting with initialization let me run the full optimization without the break
    and let's see what。
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 从初始化开始，让我进行完整的优化而不打断，看看结果是什么。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_90.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_90.png)'
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_91.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_91.png)'
- en: we get okay so the optimization finished and I rerun the loss and this is the
    result that we get。
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，优化完成，我重新运行损失，这就是我们得到的结果。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_93.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_93.png)'
- en: and then just as a reminder I put down all the losses that we saw previously
    in this lecture。 so we see that we actually do get an improvement here and just
    as a reminder we started off with a。 validation loss of 2。17 when we started by
    fixing the softmax being confidently wrong we came down to。 2。13 and by fixing
    the 10H layer being way too saturated we came down to 2。
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，我把我们在这次讲座中看到的所有损失都列了下来。所以我们看到这里确实有改进，提醒一下，我们开始时的验证损失是2.17，当我们通过修正softmax使其自信地错误后，下降到了2.13，通过修正10H层过饱和，降到了2。
- en: 10 and the reason this is， happening of course is because our initialization
    is better and so we're spending more time being。 productive training instead of
    not very productive training because our gradients are set to zero。
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 10，这当然是因为我们的初始化更好，因此我们花更多的时间进行有效训练，而不是效率不高的训练，因为我们的梯度设置为零。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_95.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_95.png)'
- en: and we have to learn very simple things like the overconfidence of the softmax
    in the beginning。
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须学习一些非常简单的东西，比如最开始的softmax的过度自信。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_97.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_97.png)'
- en: and we're spending cycles just like squashing down the weight matrix so this
    is illustrating。 basically initialization and its impact on performance just by
    being aware of the internals of these。 neural nets and their activations and their
    gradients now we're working with a very small。 network this is just one layer
    multi layer perception so because the network is so shallow。
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们花费周期去压缩权重矩阵，这基本上是在说明初始化及其对性能的影响，通过了解这些神经网络及其激活和梯度的内部情况。现在我们处理的是一个非常小的网络，这只是一个多层感知器的单层，因此由于网络非常浅。
- en: the optimization problem is actually quite easy and very forgiving so even though
    our。 initialization was terrible the network still learned eventually it just
    got a bit worse result。 this is not the case in general though once we actually
    start working with much deeper networks。 that have say 50 layers things can get
    much more complicated and these problems stack up。
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 优化问题实际上相当简单，并且非常宽容，因此即使我们的初始化非常糟糕，网络仍然最终学习到了，只是结果稍微差一些。然而，一旦我们开始处理更深的网络，比如说有50层的网络，情况就会变得复杂，这些问题会层层叠加。
- en: and so you can actually get into a place where the network is basically not
    training at all。 if your initialization is bad enough and the deeper your network
    is and the more complex it is the less。 forgiving it is to some of these errors
    and so something to be definitely be aware of。 and something to scrutinize something
    to plot and something to be careful with and。
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你实际上可能会遇到网络基本上完全不训练的情况。如果你的初始化足够糟糕，并且你的网络越深、越复杂，对这些错误的宽容度就越低，因此这点一定要引起注意，需要仔细检查、绘图，并要小心处理。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_99.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_99.png)'
- en: yeah okay so that's great that that worked for us but what we have here now
    is all these。
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，好的，这对我们有效，但现在我们有的全是这些。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_101.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_101.png)'
- en: magic numbers like point two like where do I come up with this and how am I
    supposed to set these。 if I have a large neural net with lots and lots of layers
    and so obviously no one does this by hand。 there's actually some relatively principled
    ways of setting these scales that I would like to introduce。
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 像点二这样的魔法数字，我从哪里想到这些，应该如何设置这些。如果我有一个层数很多的大型神经网络，显然没有人会手动进行这种设置。实际上，有一些相对有原则的方法来设置这些尺度，我想在这里介绍一下。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_103.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_103.png)'
- en: to you now so let me paste some code here that I prepared just to motivate the
    discussion of this。
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来贴一些我准备的代码，以激励对这个话题的讨论。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_105.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_105.png)'
- en: so what I'm doing here is we have some random input here x that is drawn from
    a gosh in。 and there's 1000 examples that are 10 dimensional and then we have
    a weight in layer here that is。 also initialized using gosh in just like we did
    here and we these neurons in the head and layer。 look at 10 inputs and there are
    200 neurons in this hidden layer and then we have here just like here。
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里做的事情是，我们有一些随机输入x，这些输入是从高斯分布中抽取的。这里有1000个样本，都是10维的，然后我们在这一层有一个权重。这个权重的初始化也是使用高斯分布，就像我们在这里所做的一样。这些神经元在隐藏层中查看10个输入，这里有200个神经元，然后我们这里就像之前那样。
- en: in this case the multiplication x multiplied by w to get the pre-activations
    of these neurons。 and basically the analysis here looks at okay suppose these
    are uniform gosh in and these weights。
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，乘法x乘以w以获得这些神经元的预激活值。基本上，这里的分析是考虑假设这些是均匀的高斯分布以及这些权重。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_107.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_107.png)'
- en: are uniform gosh in if I do x times w and we forget for now the bias and the
    nonlinearity。 then what is the mean and the standard deviation of these goshs
    so in the beginning here the input。 is just a normal gosh in distribution mean
    zero and the standard deviation is one and a。 standard deviation again is just
    the measure of a spread of the gosh in but then once we multiply。
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我做x乘以w，并且现在暂时忽略偏置和非线性，那么这些gosh的均值和标准偏差是什么？所以在这里一开始，输入仅仅是一个均值为0，标准偏差为1的正态gosh
    in分布。标准偏差再次只是gosh in扩展的度量，但一旦我们乘以。
- en: here and we look at the histogram of y we see that the mean of course stays
    the same it's about zero。 because this is a symmetric operation but we see here
    that the standard deviation has expanded to。 three so the input standard deviation
    was one but now it's grown to three and so what you're seeing。 in the histogram
    is that this gosh in is expanding and so we're expanding this gosh in from the
    input。
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们查看y的直方图，看到均值当然保持不变，大约是零。因为这是一个对称操作，但我们看到标准偏差扩展到了三。因此，输入的标准偏差是1，但现在增长到了3，所以你在直方图中看到的是这个gosh
    in正在扩展，我们正在从输入中扩展这个gosh in。
- en: and we don't want that we want most of the neural nets to have relatively similar
    activations。 so unit gosh in roughly throughout the neural net as the question
    is how do we scale these w's。 to preserve the to preserve this distribution to
    remain a gosh in and so intuitively if I multiply。 here these elements of w by
    a larger number like say by five then this gosh in grows and grows in。
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们并不想这样，我们希望大多数神经网络具有相对相似的激活。因此，整个神经网络中的单位gosh in大致相同，问题是我们如何缩放这些w，以保持这种分布仍然是gosh
    in。因此，直观上，如果我将这些w的元素乘以一个更大的数字，比如说乘以5，那么这个gosh in就会不断增长。
- en: standard deviation so now we're at 15 so basically these numbers here in the
    output y take on more。 and more extreme values but if we scale it down like say
    point two then conversely this gosh in is。 getting smaller and smaller and it's
    shrinking and you can see that the standard deviation is 0。6。 and so the question
    is what do I multiply by here to exactly preserve the standard deviation to be
    one。
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 标准偏差现在是15，因此基本上这些输出y中的数字变得越来越极端。但如果我们将其缩小，比如说0.2，那么相反，这个gosh in正在变得越来越小，正在缩小，你可以看到标准偏差是0.6。那么问题是，我在这里乘以什么才能准确保持标准偏差为1。
- en: and it turns out that the correct answer mathematically when you work out through
    the variance。 of this multiplication here is that you are supposed to divide by
    the square root of the fan in the。 fan in is the basically the number of input
    elements here 10 so we are supposed to divide by 10 square。 root and this is one
    way to do the square root you raise it to a power of 0。5 that's the same as。
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，当你通过方差计算这个乘法时，正确的数学答案是你应该除以fan in的平方根。fan in基本上是输入元素的数量，这里是10，所以我们应该除以10的平方根。这是一种计算平方根的方法，即将其提高到0.5的幂，这是相同的。
- en: doing a square root so when you divide by the square root of 10 then we see
    that the output。 gosh in it has exactly standard deviation of y now unsurprisingly
    a number of papers have looked into。
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 进行平方根运算时，当你除以10的平方根时，我们看到输出。真是的，它的标准偏差正好是y的标准偏差，现在毫无疑问，有很多论文对此进行了研究。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_109.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_109.png)'
- en: how but to best initialize neural networks and in the case of multilay perceptions
    we can have。 fairly deep networks that have these nonlinearities in between and
    we want to make sure that the。 activations are well behaved and they don't expand
    to infinity or shrink all the way to 0。 and the question is how do we initialize
    the weights so that these activations take on reasonable。
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如何最佳初始化神经网络？在多层感知器的情况下，我们可以有相当深的网络，里面有这些非线性，我们希望确保激活表现良好，并且它们不会扩展到无穷大或缩小到0。问题是我们如何初始化权重，以便这些激活在整个网络中采取合理的。
- en: values throughout the network now one paper that has studied this in quite a
    bit detail that is。
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 值。现在，有一篇论文对此进行了相当详细的研究。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_111.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_111.png)'
- en: often referenced is this paper by Kaiming Hetal called Delving Deep Interactifiers
    now in this。
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 经常被提到的是Kaiming Hetal的论文，标题是《深入研究互动激活器》。在这篇论文中。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_113.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_113.png)'
- en: case they actually study convolutional neural networks and they study especially
    the relu nonlinearity。 and the p relu nonlinearity instead of a 10H nonlinearity
    but the analysis is very similar and。
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 他们实际上研究卷积神经网络，特别是relu非线性激活函数，以及p relu非线性激活函数，而不是10H非线性，但分析是非常相似的。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_115.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_115.png)'
- en: basically what happens here is for them the the relu nonlinearity that they
    care about quite a bit。
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，他们关心的relu非线性激活函数就是在这里。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_117.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_117.png)'
- en: here is a squashing function where all the negative numbers are simply clamped
    to 0。 so the positive numbers are passed through but everything negative is just
    set to 0。 and because you are basically throwing away half of the distribution
    they find in their analysis of。
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个压缩函数，所有负数都简单地限制为0，因此正数被通过，但所有负数都被设置为0。因为你基本上丢弃了一半的分布，他们在分析中发现。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_119.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_119.png)'
- en: the forward activations in the neural net that you have to compensate for that
    with a gain。 and so here they find that basically when they initialize their weights
    they have to do it with。 a zero-mingation whose standard deviation is square root
    of 2 over the fanon what we have here is we。 are initializing the Gaussian with
    the square root of fanon this NL here is the fanon so what we have。
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中，前向激活你必须用增益进行补偿。因此，在这里他们发现，基本上当他们初始化权重时，必须用标准差为fanon的平方根除以2来进行零均值初始化。我们在这里初始化的是具有fanon平方根的高斯分布，这里的NL就是fanon，所以我们有。
- en: is square root of 1 over the fanon because we have the division here now they
    have to add this factor。
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 是fanon的平方根，因为我们这里有除法，现在他们必须添加这个因子。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_121.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_121.png)'
- en: of 2 because of the relu which basically discards half of the distribution and
    clamps at a 0 and so。 that's where you get an initial factor now in addition to
    that this paper also studies not just the。 sort of behavior of the activations
    in the forward pass of the neural net but it also studies the。 back propagation
    and we have to make sure that the gradients also are well-behaved and so。
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 因为relu基本上丢弃了分布的一半，并将其限制在0，所以你会得到一个初始因子。此外，本文还研究了神经网络前向传播中激活的行为，以及反向传播，我们必须确保梯度也表现良好。
- en: because ultimately they end up updating our parameters and what they find here
    through a lot of the。
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 因为最终它们会更新我们的参数，而他们在这里找到的东西经过很多分析后。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_123.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_123.png)'
- en: analysis that i am watching to read through but it's not exactly approachable
    what they find is。 basically if you properly initialize the forward pass the backward
    pass is also。 approximately initialized up to a constant factor that has to do
    with the size of the number of。 hidden neurons in an early and late layer but
    basically they find empirically that this is not。
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我正在观看的分析有点难以接近，他们发现基本上如果你正确初始化前向传播，反向传播也会近似初始化，直到与隐藏神经元数量的大小有关的一个常数因子，但他们经验性地发现这不是。
- en: a choice that matters too much now this timing initialization is also implemented
    in PyTorch。
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这个选择非常重要，现在这个时间初始化也在PyTorch中实现。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_125.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_125.png)'
- en: so if you go to torch。nn。init documentation you'll find kind of normal and in
    my opinion this is。 probably the most common way of initializing neural networks
    now and it takes a few keyword。 arguments here so number one it wants to know
    the mode would you like to normalize the activations。 or would you like to normalize
    the gradients to to be always gosh in with zero mean and unit or。
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果你去torch.nn.init文档，你会发现类似于正态分布，在我看来，这可能是初始化神经网络的最常见方式，现在它接受几个关键字参数，所以第一，它想知道你想要归一化激活还是想要归一化梯度，使其始终以零均值和单位方差。
- en: one standard deviation and because they find the paper that this doesn't matter
    too much most of the。 people just leave it as the default which is pen it and
    then second pass in the nonlinearity that you。 are using because depending on
    the nonlinearity we need to calculate a slightly different gain。 and so if your
    nonlinearity is just linear so there's no nonlinearity then the gain here will
    be。
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 一个标准差，因为他们发现这篇论文说这并不太重要，大多数人只是把它保留为默认值，也就是pen，然后第二次传递非线性，这取决于你使用的非线性，因为我们需要计算略微不同的增益。因此，如果你的非线性只是线性，那么这里的增益将是。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_127.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_127.png)'
- en: one and we have the exact same kind of formula that we've got up here but if
    the nonlinearity is。
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 一样的公式在这里，但如果非线性是。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_129.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_129.png)'
- en: something else we're going to get a slightly different gain and so if we come
    up here to the top we。 see that for example in the case of relu this gain is a
    square root of two and the reason it's a。 square root because in this paper you
    see how the two is inside of the square root so the gain。 is a square root of
    two in a case of linear or identity we just get a gain of one in a case of。
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他一些地方我们会得到略微不同的增益，所以如果我们到这里的顶部，我们会看到，例如在relu的情况下，这个增益是根号二，原因是这个。在这篇论文中你可以看到二在根号内部，所以在这个情况下增益是根号二，而在线性或恒等的情况下，我们只是得到增益为一。
- en: tenh which is what we're using here the advised gain is a five over three and
    intuitively why do we。 need a gain on top of the initialization is because tenh
    just like relu is a contractive。
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: tenh是我们在这里使用的，建议的增益是五分之三，直观上我们为什么需要在初始化的基础上增加增益，因为tenh就像relu一样是收缩的。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_131.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_131.png)'
- en: transformation so what that means is you're taking the output distribution from
    this matrix。
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 转换，这意味着你正在从这个矩阵中提取输出分布。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_133.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_133.png)'
- en: multiplication and then you are squashing it in some way now relu squashes it
    by taking everything。 below zero and clamping it to zero tenh also squashes it
    because it's a contract to operation it will。 take the tails and it will squeeze
    them in and so in order to fight the squeezing in we need to boost。 the weights
    a little bit so that we renormalize everything back to standard unit standard
    deviation。
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 乘法，然后你以某种方式压缩它，现在relu通过将所有低于零的值夹紧到零来压缩它，而tenh也压缩它，因为它是一个收缩操作，它会把尾部挤压进来，因此为了抵消这种压缩，我们需要稍微提升权重，以便将所有内容重新归一化回标准单位标准差。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_135.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_135.png)'
- en: so that's why there's a little bit of a gain that comes out now i'm skipping
    through this。
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是为什么现在会有一点增益，我在跳过这个部分。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_137.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_137.png)'
- en: section a little bit quickly and i'm doing that actually intentionally and the
    reason for that is。 because about seven years ago when this paper was written
    you have to actually be extremely careful。 with the activations and ingredients
    and their ranges and their histograms and you have to be。 very careful with the
    precise setting of gains and the scrutinizing of the null linearity is used and。
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 快速进行这个部分，我实际上是故意这样做的，原因是大约七年前，当这篇论文写成时，你必须对激活函数、成分及其范围和直方图非常小心，你必须非常注意增益的精确设置，以及对使用的非线性的仔细审查。
- en: so on and everything was very finicky and very fragile and very properly arranged
    for the neural。 left to train especially if your neural net was very deep but
    there are a number of modern。 innovations that have made everything significantly
    more stable and more well behaved and it's become。 less important to initialize
    these networks exactly right and some of those modern innovations for。
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 所以一切都非常麻烦、脆弱，并且对神经网络的训练非常精细安排，特别是如果你的神经网络非常深，但现代有许多创新使得一切显著更加稳定，更加规范，并且初始化这些网络变得不那么重要，其中一些现代创新是。
- en: example are residual connections which we will cover in the future the use of
    a number of normalization。 layers like for example batch normalization layer normalization
    group normalization we're going to。 go into a lot of these as well and number
    three much better optimizers not just a cast ingredient。 scent the simple optimizer
    we're basically using here but a slightly more complex optimizers like。
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 示例是残差连接，我们将在未来讨论使用多种归一化层，比如批量归一化、层归一化、组归一化，我们将深入探讨这些内容，以及更好的优化器，不仅仅是这里使用的简单优化器，而是稍微复杂一些的优化器。
- en: rms prop and especially adam and so all of these modern innovations make it
    less important for you。 to precisely calibrate the initialization of the neural
    net all that being said in practice what。 should we do in practice when I initialize
    these neural nets I basically just normalize my weights。 by the square root of
    the fan in so basically roughly what we did here is what I do now if we。
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: rms prop和特别是adam等所有这些现代创新使得你不必精确校准神经网络的初始化，所有这些都说完后，实际上我们应该怎么做呢？在初始化这些神经网络时，我基本上只是通过fan
    in的平方根来归一化我的权重。所以，基本上我们在这里做的与我现在做的差不多。
- en: want to be exactly accurate here we and go back in it of kind of normal this
    is how good implemented。 we want to set the standard deviation to be gain over
    the square root of fan in right。
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准确无误，我们回到这种正常的实现方式。我们希望将标准差设置为增益除以fan in的平方根。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_139.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_139.png)'
- en: so to set the standard deviation of our weights we will proceed as follows basically
    when we have。 a torch dot random and let's say I just create a thousand numbers
    we can look at the standard。 deviation of this and of course that's one that's
    the amount of spread let's make this a bit bigger。 so it's closer to one so that's
    the spread of the Gaussian of zero mean and unit standard deviation。
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 为了设置我们权重的标准差，我们将按照以下步骤进行，基本上，当我们有一个torch的随机数，并假设我创建一千个数字，我们可以查看这些数字的标准差，当然，这是一个分布的量，让我们把这个数字稍微放大一点，以便更接近1，所以这是均值为0和单位标准差的高斯分布的扩展。
- en: now basically when you take these and you multiply by say point two that basically
    scales down the。 Gaussian and that makes its standard deviation point two so basically
    the number that you multiply。 by here ends up being the standard deviation of
    this Gaussian so here this is a standard deviation。 point two Gaussian here when
    we sample our w one but we want to set the standard deviation to gain。
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，基本上当你将这些数值乘以比如说0.2时，实际上就是缩小了高斯分布，使其标准差为0.2。因此，你在这里乘以的数字最终成为这个高斯分布的标准差。这里的标准差是0.2的高斯分布，当我们对w1进行采样时，但我们希望将标准差设置为增益。
- en: over square root of fan mode which is fan in so in other words we want to multiply
    by gain。
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在fan in的平方根上乘以增益，换句话说，我们想要乘以增益。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_141.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_141.png)'
- en: which for 10 h is five over three five over three is the gain and then times。
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 对于10 h，增益是5/3，然后乘以。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_143.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_143.png)'
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_144.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_144.png)'
- en: I guess our divide square root of the fan in and in this example here the fan
    in most 10。
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我想我们要除以fan in的平方根，在这个例子中，fan in最多是10。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_146.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_146.png)'
- en: and I just noticed actually here the fan in for w one is actually an embed times
    block size。 which as you all recall is actually 30 and that's because each character
    is 10 dimensional but then。 we have three of them and we concatenate them so actually
    the fan in here was 30 and I should。 have used 30 here probably but basically
    we want 30 square root so this is the number this is what。
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我刚注意到，这里w1的fan in实际上是嵌入乘以块大小。大家还记得，这实际上是30，因为每个字符是10维的，但我们有三个字符并且进行连接，所以这里的fan
    in实际上是30，我应该在这里使用30，但基本上我们需要30的平方根，所以这是这个数字，这就是我们要的。
- en: our standard deviation we want to be and this number turns out to be point three
    whereas here。 just by fiddling with it and looking at the distribution and making
    sure it looks okay。 we came up with point two and so instead what we want to do
    here is we want to make the standard。 deviation be five over three which is our
    gain divide this amount times point two square root。
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望标准差为，而这个数值恰好是0.3，而在这里。通过调整和查看分布，确保它看起来不错，我们得出了0.2，因此我们在这里想做的是使标准差为5/3，也就是我们的增益，再将这个数值乘以0.2的平方根。
- en: and these brackets here are not that necessary but I'll just put them here for
    clarity。 this is basically what we want this is the timing in it in our case for
    a 10H nonlinearity and this。 is how we would initialize the neural mat and so
    we're multiplying by point three instead of。
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这些括号在这里并不是很必要，但我放在这里以便于理解。这基本上就是我们想要的，这在我们的10H非线性中是时间的表现，这就是我们如何初始化神经矩阵，因此我们乘以0.3而不是。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_148.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_148.png)'
- en: multiplying by point two and so we can we can initialize this way and then we
    can train the。
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 乘以0.2，因此我们可以这样初始化，然后我们可以进行训练。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_150.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_150.png)'
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_151.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_151.png)'
- en: neural mat and see what we got okay so I trained the neural mat and we end up
    in roughly the same spot。 so looking at the value she lost we now get 2。10 and
    previously we also had 2。10。 there's a little bit of a difference but that's just
    randomness the process I suspect。
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 神经矩阵，看看我们得到了什么，好的，我训练了神经矩阵，最终我们大致处于同一个位置，所以查看损失值，我们现在得到了2.10，之前我们也得到了2.10。有一点差异，但我怀疑那只是过程中的随机性。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_153.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_153.png)'
- en: but the big deal of course is we get to the same spot but we did not have to
    introduce any。
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，最重要的是我们达到了同一个位置，但我们并不需要引入任何。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_155.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_155.png)'
- en: magic numbers that we got from just looking at histograms and guess and checking
    we have。 something that is semi-principled and will scale us to much bigger networks
    and something that we。 can sort of use as a guide so I mentioned that the precise
    setting of these initializations。
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从观察直方图和试验中得出的魔法数字，是一种半原则性的方法，能够扩展到更大的网络，并且可以作为我们的指导，因此我提到这些初始化的精确设置。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_157.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_157.png)'
- en: is not as important today due to some modern innovations and I think now is
    a pretty good time。
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 由于一些现代创新，今天这些初始化的精确设置并不是那么重要，我认为现在是个不错的时机。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_159.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_159.png)'
- en: to introduce one of those modern innovations and that is batch normalization
    so batch normalization。 came out in 2015 from a team at Google and it was an extremely
    impactful paper because it made it。 possible to train very deep neural nets quite
    reliably and it basically just worked so here's。
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 介绍这些现代创新之一，即批量归一化，批量归一化在2015年由谷歌的一个团队提出，这是一篇极具影响力的论文，因为它使得训练非常深的神经网络变得相当可靠，基本上就是有效的，所以这里是。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_161.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_161.png)'
- en: what batch normalization does and what's implemented basically we have these
    hidden states H pre-act。 right and we were talking about how we don't want these
    these pre-activation states to be way too。 small because then the 10H is not doing
    anything but we don't want them to be too large because。 then the 10H is saturated
    in fact we want them to be roughly roughly gosh so zero mean and a unit。
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化的作用及其基本实现，我们有这些隐藏状态H预激活。对，我们在谈论如何不希望这些预激活状态太小，因为那样10H就没有任何作用，但我们也不希望它们太大，因为那样10H就饱和了，实际上我们希望它们大致为高斯，即零均值和单位。
- en: or one standard deviation at least at initialization so the insight from the
    batch normalization paper。 is okay you have these hidden states and you'd like
    them to be roughly gosh in then why not take。 the hidden states and just normalize
    them to be gosh in and it sounds kind of crazy but you can。 just do that because
    standardizing hidden states so that their unit gosh in is a perfectly。
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 或者至少在初始化时一个标准差，所以来自批量归一化论文的洞察是，好的，你有这些隐藏状态，你希望它们大致为高斯，那么为什么不直接将隐藏状态归一化为高斯呢？这听起来有点疯狂，但你可以这样做，因为将隐藏状态标准化，使其单位为高斯，是完全可以的。
- en: differentiable operation as we'll see and so that was kind of like the big insight
    in this paper。 and when I first read it my mind was blown because you can just
    normalize these hidden states and if。 you'd like unit gosh in states in your network
    at least initialization you can just normalize them。 to be unit gosh in so let's
    see how that works so we're going to scroll to our pre-activations here。
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 可微分操作，正如我们将看到的，这就是这篇论文中的重大见解。当我第一次读到它时，我的脑海被震撼了，因为你可以规范化这些隐藏状态。如果你希望网络中的状态是单位高斯状态，至少在初始化时，你可以将它们标准化为单位高斯状态。现在，让我们看看这如何运作，我们将滚动到我们的预激活部分。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_163.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_163.png)'
- en: just before they enter into the 10H now the idea again is remember we're trying
    to make these。 roughly gosh in and that's because if these are way too small numbers
    then the 10H here is kind of。 inactive but if these are very large numbers then
    the 10H is way to saturate it and graze it in the。 flow so we'd like this to be
    roughly gosh in so the insight in batch normalization again is that。
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在它们进入10H之前，想法再次是要记住我们试图让这些数值大致符合高斯分布。这是因为如果这些数值太小，那么这里的10H会变得有些不活跃；但如果这些数值非常大，那么10H就会饱和并影响流动，因此我们希望这些数值大致符合高斯分布。所以批量归一化的关键在于。
- en: we can just standardize these activations so they are exactly gosh in so here
    H pre-act。 has a shape of 32 by 200 32 examples by 200 neurons in the end layer
    so basically what we can do is we。 can take H pre-act and we can just calculate
    the mean and the mean we want to calculate across the。 zero dimension and we want
    to also keep them as true so that we can easily broadcast this so the。
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接标准化这些激活，使它们完全符合高斯分布。因此H预激活的形状是32乘以200，32个样本，200个神经元在最后一层。基本上，我们可以拿H预激活来计算均值，而均值的计算是沿着零维度进行的，并且我们还希望将它们保留为真实值，以便我们可以轻松地进行广播。
- en: shape of this is one by 200 in other words we are doing the mean over all the
    elements in the batch。 and similarly we can calculate the standard deviation of
    these activations。 and that will also be one by 200 now in this paper they have
    the。
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这个形状是1乘以200，换句话说，我们是在对批次中所有元素进行均值计算。同样，我们也可以计算这些激活的标准差，这样的结果也是1乘以200。现在在这篇论文中，他们有。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_165.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_165.png)'
- en: sort of prescription here and see here we are calculating the mean which is
    just taking the。 average value of any neurons activation and then the standard
    deviation is basically kind of like。 the measure of the spread that we've been
    using which is the distance of every one of these values。 away from the mean and
    that squared and averaged that's the variance and then if you want to take。
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的公式可以看到，我们正在计算均值，即取任意神经元激活的平均值，而标准差基本上是我们使用的扩散度量，表示每个值与均值的距离的平方平均，这就是方差。如果你想进行。
- en: the standard deviation you will square root the variance to get the standard
    deviation so these。 are the two that we're calculating and now we're going to
    normalize or standardize these x's by。 subtracting the mean and dividing by the
    standard deviation so basically we're taking H pre-act and。
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 标准差是通过对方差开方来得到的，因此我们计算的就是这两个值。现在我们要通过减去均值并除以标准差来对这些x进行归一化或标准化，基本上我们是在处理H预激活。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_167.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_167.png)'
- en: we subtract the mean and then we divide by the standard deviation this is exactly
    what these two。
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们减去均值，然后除以标准差，这正是这两者所做的。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_169.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_169.png)'
- en: STD and mean are calculating oops sorry this is the mean and this is the variance
    you see how the。 sigma is the standard deviation usually so this is sigma square
    which is the variance is the square。 of the standard deviation so this is how
    you standardize these values and what this will do is that every。
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 标准差和均值的计算，抱歉，这里是均值，这里是方差。你会看到σ是标准差，通常情况下，这就是σ的平方，即方差，是标准差的平方。这就是我们如何标准化这些值的方式，这样做的结果是每个。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_171.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_171.png)'
- en: single neuron now and its firing rate will be exactly unit Gaussian on these
    32 examples at least。 of this batch that's why it's called batch normalization
    we are normalizing these batches。 and then we could in principle train this notice
    that calculating the mean and。 their standard deviation these are just mathematical
    formulas they're perfectly differentiable all this。
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 单个神经元的激活率将在这32个示例中恰好是单位高斯分布，因此称其为批量归一化，我们正在对这些批次进行归一化，然后我们原则上可以训练这个。注意计算均值和标准差，这些只是数学公式，它们是完全可微分的，所有这些。
- en: is perfectly differentiable and we can just train this the problem is you actually
    won't achieve。 a very good result with this and the reason for that is we want
    these to be roughly Gaussian but only。 at initialization but we don't want these
    to be forced to be Gaussian always we would like to。 allow the neural net to move
    this around to potentially make it more diffuse to make it more sharp to。
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 这是完全可微分的，我们可以直接训练这个，问题在于你实际上不会获得非常好的结果，原因是我们希望这些在初始化时大致呈高斯分布，但我们不希望这些始终被强制为高斯分布，我们希望允许神经网络调整这些分布，使其更加分散或更加尖锐。
- en: make some 10 h neurons maybe more trigger more trigger happy or less trigger
    happy so we'd like。 this distribution to move around and we'd like the back propagation
    to tell us how the distribution。 should move around and so in addition to this
    idea of standardizing the activations at any point。
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 使一些10个h神经元，可能更多，触发更频繁或不那么频繁，因此我们希望这个分布能够移动，我们希望反向传播告诉我们这个分布应该如何移动。因此，除了在任何时刻标准化激活的想法之外。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_173.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_173.png)'
- en: in the network we have to also introduce this additional component in the paper
    here describe。
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络中，我们还必须引入这部分附加组件，文中对此进行了描述。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_175.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_175.png)'
- en: the scale and shift and so basically what we're doing is we're taking these
    normalized inputs and。 we are additionally scaling them by some gain and offsetting
    them by some bias to get our final output。
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 进行缩放和平移，因此基本上我们所做的是将这些归一化的输入进行缩放，并通过某个增益进行附加缩放，并通过某个偏差进行偏移，以获得最终输出。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_177.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_177.png)'
- en: from this layer and so what that amounts to is the following we are going to
    allow a batch。 normalization gain to be initialized at just once and the once
    will be in the shape of one by n hidden。
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 从这一层开始，所以这意味着我们将允许批量归一化增益仅初始化一次，这一次将采用一行n个隐藏单元的形状。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_179.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_179.png)'
- en: and then we also will have a bn bias which will be charged at zeros and it will
    also be of the shape。 n by one by n hidden and then here the bn gain will multiply
    this and the bn bias will offset it here。
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们还将有一个bn偏差，它将初始化为零，形状为n乘以一乘以n个隐藏单元，然后这里的bn增益将乘以它，而bn偏差将对此进行偏移。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_181.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_181.png)'
- en: so because this is initialized to one and this to zero at initialization each
    neurons firing values。 in this batch will be exactly unit Gaussian and will have
    nice numbers no matter what the。 distribution of the HP Act is coming in coming
    out it will be unit Gaussian for each neuron and that's。 roughly what we want
    at least at initialization and then during optimization we'll be able to back。
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在初始化时将其初始化为一，将其初始化为零，因此这一批次中每个神经元的激活值将恰好是单位高斯分布，无论HP激活的分布如何，输出都将是每个神经元的单位高斯分布。这大致是我们在初始化时想要的，然后在优化过程中，我们将能够进行反向传播。
- en: propagate to bn gain and bn bias and change them so the network is given the
    full ability。 to do with this whatever it wants internally here we just have to
    make sure that we。 include these in the parameters of the neural mat because they
    will be trained with back propagation。 so let's initialize this and then we should
    be able to train。
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 将传播到bn增益和bn偏差，并进行更改，使网络获得充分的能力。内部处理时，网络可以随意使用这些，我们只需确保将这些包含在神经网络的参数中，因为它们将通过反向传播进行训练。因此，让我们初始化这些，然后我们应该能够进行训练。
- en: and then we're going to also copy this line which is the best normalization
    layer。
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们还将复制这一行，即最佳归一化层。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_183.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_183.png)'
- en: here on the single line of code and we're going to swing down here and we're
    also going to。 do the exact same thing at test time here， so similar to train
    time we're going to normalize and then scale and that's going to give us our。
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一行代码中，我们将下滑，并且在测试时也会做完全相同的事情，因此在训练时，我们将进行归一化，然后缩放，这将给我们带来我们的结果。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_185.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_185.png)'
- en: train and validation loss and we'll see in a second that we're actually going
    to change this。 a little bit but for now I'm going to keep it this way so I'm
    just going to wait for this to。
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和验证损失，我们稍后会看到我们实际上会对其进行一些更改，但现在我会保持这个状态，所以我会等着这个。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_187.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_187.png)'
- en: converge okay so I allowed the neural nets to converge here and when we scroll
    down we see that。 our validation loss here is 2。10 roughly which I wrote down
    here and we see that this is actually。 kind of comparable to some of the results
    that we've achieved previously now I'm not actually。 expecting an improvement
    in this case and that's because we are dealing with a very simple neural。
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 允许神经网络在这里收敛，当我们向下滚动时，我们看到我们的验证损失大约是2.10，我在这里写下了这一点，并且我们看到这实际上与我们之前取得的一些结果相当。现在我并不期待在这种情况下有改善，因为我们处理的是一个非常简单的神经网络。
- en: nut that has just a single hidden layer so in fact in this very simple case
    of just one hidden layer。
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 只有一个隐藏层的神经网络，事实上在这个非常简单的情况下只有一个隐藏层。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_189.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_189.png)'
- en: we were able to actually calculate what the scale of W should be to make these
    pre-activations。
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能够实际计算出W的规模应该是什么，以使这些预激活。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_191.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_191.png)'
- en: already have a roughly Gaussian shape so the rationalization is not doing much
    here but you might。 imagine that once you have a much deeper neural nut that has
    lots of different types of operations。 and there's also for example residual connections
    which we'll cover and so on it will become basically。
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这些激活已经大致呈高斯分布，因此合理化在这里没有发挥太多作用，但你可以想象，一旦你有一个更深的神经网络，包含许多不同类型的操作，以及我们将讨论的残差连接等，这将变得相对复杂。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_193.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_193.png)'
- en: very very difficult to tune the scales of your weight matrices such that all
    the activations。 throughout the neural net are roughly Gaussian and so that's
    going to become very quickly intractable。
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 调整权重矩阵的规模非常困难，以使得整个神经网络的激活大致呈高斯分布，这将很快变得难以处理。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_195.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_195.png)'
- en: but compared to that it's going to be much much easier to sprinkle rationalization
    layers throughout。
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 但与此相比，在整个神经网络中散布合理化层将容易得多。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_197.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_197.png)'
- en: the neural net so in particular it's common to look at every single linear layer
    like this one this。 is a linear layer multiplying by weight matrix and adding
    bias or for example convolutions which we'll。 cover later and also perform basically
    a multiplication with weight matrix but in a more spatially structured。 format
    it's customary to take these linear layer or convolutional layer and append a。
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中，通常会查看每个线性层，比如这个。这是一个通过权重矩阵相乘并加上偏置的线性层，或者例如卷积层，我们将在后面讨论，它也基本上以更具空间结构的格式进行权重矩阵的乘法。
- en: rationalization layer right after it to control the scale of these activations
    at every point in the。 neural nut so we'd be adding these bathroom layers throughout
    the neural nut and then this controls。
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在它之后添加一个**合理化层**，以控制神经网络中每个点的激活规模。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_199.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_199.png)'
- en: the scale of these activations throughout the neural nut it doesn't require
    us to do perfect。
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个神经网络中，这些激活的规模不需要我们做到完美。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_201.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_201.png)'
- en: mathematics and care about the activation distributions for all these different
    types of neural。
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 数学上关注这些不同类型神经网络的激活分布。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_203.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_203.png)'
- en: nut or lego building blocks that you might want to introduce into your neural
    nut and it significantly。
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 网络或你可能想引入到神经网络中的乐高积木，并且它显著地。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_205.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_205.png)'
- en: stabilizes the training and that's why these layers are quite popular now the
    stability offered by。 rationalization actually comes at a terrible cost and that
    cost is that if you think about what's。 happening here something something terribly
    strange and unnatural is happening it used to be that。 we have a single example
    feeding into a neural nut and then we calculate this activations and its。
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 这稳定了训练，因此这些层现在相当流行，而归一化提供的稳定性实际上是以巨大的代价为代价的，如果你考虑这里发生的事情，某些事情非常奇怪且不自然，以前我们有一个单一的样本输入到神经网络中，然后计算这个激活及其。
- en: logits and this is a deterministic sort of process so you arrive at some logits
    for this example。 and then because of efficiency of training we suddenly started
    to use batches of examples but。 those batches of examples were processed independently
    and it was just an efficiency thing but now suddenly。 in batch normalization because
    of the normalization through the batch we are coupling these examples。
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: logits 这是一个确定性的过程，因此你会得到这个样本的一些 logits，然后由于训练的效率，我们突然开始使用样本批次，但这些样本批次是独立处理的，这只是一种效率提升，但现在，批量归一化由于批次的归一化使得这些样本相互耦合。
- en: mathematically and in the forward pass and backward pass of the neural nut so
    now the hidden state。 activations HP Act and your logits for any one input example
    are not just a function of that。 example and its input but they're also a function
    of all the other examples that happen to come for。 a ride in that batch and these
    examples are sampled randomly and so what's happening is for example。
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学上，神经网络的前向传播和反向传播中的隐藏状态、激活值 HP Act 和任何一个输入样本的 logits 不仅仅是该样本及其输入的函数，还同时是该批次中其他所有样本的函数，这些样本是随机抽取的，因此发生的情况是，例如。
- en: when you look at HP Act that's going to feed into H the hidden state activations
    for for example for。 any one of these input examples is going to actually change
    slightly depending on what other examples。 there are in the batch and depending
    on what other examples happen to come for a ride H is going to。 change suddenly
    and it's going to like jitter if you imagine sampling different examples because。
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 当你查看 HP Act 时，它将用于 H 隐藏状态激活，对于这些输入样本中的任何一个，其实际变化会根据批次中的其他样本而略有不同，因此，H 将会突然变化，如果你想象不同样本的抽取，它会像抖动一样。
- en: the statistics of the mean and standard deviation are going to be impacted and
    so you'll get a jitter。 for H and you'll get a jitter for logits and you think
    that this would be a bug or something undesirable。 but in a very strange way this
    actually turns out to be good in neural network training。 and as a side effect
    and the reason for that is that you can think of this as kind of like a。
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 均值和标准差的统计量将受到影响，因此你会得到 H 的抖动和 logits 的抖动，你会认为这会是一个错误或不希望的事情，但在某种非常奇怪的方式中，这实际上在神经网络训练中是有益的，副作用是你可以将其视为一种。
- en: regularizer because what's happening is you have your input and you get your
    H and then depending on。 the other examples this is generating a bit and so what
    that does is that it's effectively padding。 out any one of these input examples
    and it's introducing a little bit of entropy and because。 of the padding out it's
    actually kind of like a form of a data augmentation which we'll cover in。
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化器，因为发生的情况是你有你的输入并得到你的 H，然后根据其他样本，这会生成一些噪声，因此这实际上是在有效地填充这些输入样本中的任何一个，并引入一点熵，因为。
- en: the future and it's kind of like augmenting the input a little bit and jittering
    it and that makes。 it harder for the neural nets to overfit these concrete specific
    examples so by introducing all。 this noise it actually like pats out the examples
    and it regularizes the neural net and that's one of。 the reasons why the seemingly
    as a second order effect this is actually a regularizer and that。
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 填充实际上有点像一种数据增强形式，我们将在未来讨论，这有点像对输入进行小幅增强并抖动，这使得神经网络更难以过拟合这些具体的样本，因此通过引入所有这些噪声，它实际上像是填充这些样本并且它正则化了神经网络，这就是为什么看似作为二阶效应的原因实际上这是一个正则化器。
- en: has made it harder for us to remove the use of batch normalization because basically
    no one。 likes this property that the examples in the batch are coupled mathematically
    and in the forward pass。 and at least all kinds of like strange results we'll
    go into some of that in a second as well。 and it leads to a lot of bugs and and
    so on and so no one likes this property and so people have tried。
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得我们更难去除批量归一化的使用，因为基本上没有人喜欢这种特性，即批量中的示例在数学上是耦合的，在前向传递中也是如此。至少各种奇怪的结果我们稍后会详细讨论，这导致了很多错误，所以没有人喜欢这种特性，因此人们尝试过。
- en: to deprecate the use of batch normalization and move to other normalization
    techniques that do。 not couple the examples of batch examples are linear normalization，
    instance normalization。 group normalization and so on and we'll come we'll come
    and sound these later。 but basically long story short batch normalization was
    the first kind of normalization later to be。
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望弃用批量归一化，转向不耦合示例的其他归一化技术，如线性归一化、实例归一化、组归一化等，我们稍后会详细讨论。但总之，批量归一化是第一种归一化技术。
- en: introduced it worked extremely well it happens to have this regularizing effect
    it stabilized。 training and people have been trying to remove it and move to some
    of the other normalization。 techniques but it's been hard because it just works
    quite well and some of the reason that it works。 quite well is again because of
    this regularizing effect and because of the because it is quite。
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 引入后效果极佳，正好具有这种正则化效果，稳定了训练。人们一直在尝试去除它，转向一些其他归一化技术，但这很困难，因为它的效果相当好。而它能如此有效的原因之一正是因为这种正则化效果。
- en: effective at controlling the activations and their distributions so that's kind
    of like the brief。 story of batch normalization and i'd like to show you one of
    the other weird sort of outcomes of。
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 有效地控制激活值及其分布，这就是批量归一化的简要历史。我想给你展示一些其他奇怪的结果。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_207.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_207.png)'
- en: this coupling so here's one of the strange outcomes that i only lost over previously。
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我之前提到的一个奇怪结果。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_209.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_209.png)'
- en: when i was valuing the loss on the validation set basically once we've trained
    a neural nut。
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 当我在验证集上评估损失时，基本上一旦我们训练了神经网络。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_211.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_211.png)'
- en: we'd like to deploy it in some kind of a setting and we'd like to be able to
    feed in a single。 individual example and get a prediction out from our neural
    nut but how do we do that when our。
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望在某种环境中部署它，并且希望能够输入一个单独的示例并从我们的神经网络中得到预测。但当我们的示例之间存在耦合时，我们该如何做到这一点呢？
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_213.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_213.png)'
- en: neural nut now in the forward pass estimates the statistics of the mean understanding
    deviation。 of a batch the neural nut expects batches as an input now so how do
    we feed in a single example。 and get sensible results out and so the proposal
    in the batch normalization paper is the following。
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络现在在前向传递中估计一个批次的均值和标准差的统计量，神经网络现在期望批次作为输入。那么我们该如何输入一个单独的示例并获得合理的输出结果呢？因此，批量归一化论文中的提议如下。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_215.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_215.png)'
- en: what we would like to do here is we would like to basically have a step after
    training that。 calculates and sets the batch room mean and standard deviation
    a single time over the training set。 and so i wrote this code here in interest
    of time and we're going to call what's called calibrate。 the batch room statistics
    and basically what we do is torch nut no grad telling by torch that。
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要做的基本上是在训练后进行一步，计算并设置整个训练集上的批量均值和标准差，因此我写了这段代码以节省时间，我们将称之为校准批量统计。基本上我们做的是使用`torch.no_grad()`告诉`torch`。
- en: none of this we will call the doc backward on and it's going to be a bit more
    efficient。 we're going to take the training set get the pre-activations for every
    single training example。 and then one single time estimate the mean and standard
    deviation or the entire training set。 and then we're going to get B and mean and
    B and standard deviation and now these are fixed。
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会称这个文档为反向文档，它将变得更加高效。我们将获取训练集，为每个训练示例获取预激活值。然后我们将一次性估算整个训练集的均值和标准差。接着我们将获得
    B 和均值，以及 B 和标准差，这些都是固定的。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_217.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_217.png)'
- en: numbers estimating of the entire training set and here instead of estimating
    it dynamically。 we are going to instead here use B and mean and here we're just
    going to use B and standard deviation。
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 估算整个训练集的数字，在这里我们不再动态估算，而是使用 B 和均值，并且我们将仅使用 B 和标准差。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_219.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_219.png)'
- en: and so at test time we are going to fix these clamp them and use them during
    inference and now。
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 因此在测试时，我们将固定这些值，限制它们，并在推理期间使用它们，现在。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_221.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_221.png)'
- en: you see that we get basically identical result but the benefit that we've gained
    is that we can。
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现我们基本上得到了相同的结果，但我们获得的好处是我们可以。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_223.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_223.png)'
- en: now also forward a single example because the mean and standard deviation are
    now fixed。
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 现在也只需前向传播一个示例，因为均值和标准差现在是固定的。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_225.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_225.png)'
- en: uh sort of tensors that said nobody actually wants to estimate this mean and
    standard deviation。 as a second stage after neural network training because everyone
    is lazy and so this batch。 normalization paper actually introduced one more idea
    which is that we can we can estimate。 the mean and standard deviation in a running
    manner running manner during training of the neural。
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，有一种张量的说法是没有人真正想在神经网络训练后的第二阶段估算这个均值和标准差，因为每个人都懒惰，因此这篇批量归一化的论文实际上引入了一个新想法，即我们可以在神经网络训练期间以运行的方式估算均值和标准差。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_227.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_227.png)'
- en: network and then we can simply just have a single stage of training and on the
    side of that training。
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 网络，然后我们可以简单地进行一次训练阶段，并在该训练的旁边进行。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_229.png)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_229.png)'
- en: we are estimating the running mean and standard deviation so let's see what
    that would look like。
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在估算运行中的均值和标准差，所以让我们看看这会是什么样子。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_231.png)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_231.png)'
- en: let me basically take the mean here that we are estimating on the batch and
    let me call this B and。 mean on the i-th iteration and then here this is B and
    STD and i and the mean comes here and the。 STD comes here so so far I've done
    nothing I've just moved around and I created these extra。 variables for the mean
    and standard deviation and I put them here so so far nothing has changed。
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 让我基本上获取我们在批量上估算的均值，并将其称为 B 和第 i 次迭代的均值，然后这里是 B 和标准差 i，均值在这里，标准差在这里，所以到目前为止我什么都没做，只是移动了一下，创建了这些额外的均值和标准差变量，并把它们放在这里，所以到目前为止没有任何改变。
- en: but what we're good to do now is we're going to keep a running mean of both
    of these values。
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们现在要做的就是保持这两个值的运行均值。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_233.png)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_233.png)'
- en: during training so let me swing up here and let me create a bn mean underscore
    running。 and i'm going to initialize it at zeros and then bn STD running which
    i'll initialize at once。
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练期间，让我在这里稍微调整一下，创建一个 bn mean underscore running。我将其初始化为零，然后 bn STD running
    将初始化为一。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_235.png)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_235.png)'
- en: because in the beginning because of the way we initialized w1 and b1 h-criact
    will be roughly。 unit caution so the mean will be roughly zero and the standard
    deviation roughly one so i'm going to。 initialize these that way but then here
    i'm going to update these and in PyTorch these mean and。 standard deviation that
    are running they're not actually part of the gradient based optimization。
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 因为在开始时，由于我们初始化 w1 和 b1 的方式，h-criact 大致会是单位矩阵，因此均值大致为零，标准差大致为一，所以我将以这种方式初始化这些，但随后我会更新这些，在
    PyTorch 中，这些运行中的均值和标准差实际上并不是梯度优化的一部分。
- en: we're never going to derive gradients with respect to them they're they're updated
    on the side of。 training and so what we're going to do here is we're going to
    say with torch。nograd telling PyTorch that。
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会对它们推导梯度，它们是在训练的旁边更新的。因此，我们要做的是告诉PyTorch使用torch.no_grad。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_237.png)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_237.png)'
- en: the update here is not supposed to be building out a graph because there will
    be no dot backward。 but this running mean is basically going to be 0。999 times
    the current value， plus 0。001 times the this value this new mean and in the same
    way bn STD running will be mostly。 what it used to be but it will receive a small
    update in the direction of what the current。
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 此处的更新不应构建出一个图，因为没有反向传播。但这个运行均值基本上将是0.999乘以当前值，加上0.001乘以这个新均值。同样，bn STD运行将大致保持原样，但会在当前值的方向上收到小更新。
- en: standard deviation is and as you're seeing here this update is outside and on
    the side of the。 gradient based optimization and it's simply being updated not
    using gradient descent it's just being。 updated using a janky like smooth sort
    of running mean manner and so while the network is training。 and these pre-activations
    are sort of changing and shifting around during back propagation。
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 标准差是，如你所见，这次更新在梯度优化的外部和旁边。它不是使用梯度下降进行更新，而是以一种类似平滑的运行均值方式被更新。因此，在网络训练期间，这些预激活值在反向传播中会发生变化和调整。
- en: we are keeping track of the typical mean and standard deviation and estimating
    them once and。
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在跟踪典型的均值和标准差，并进行一次估计。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_239.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_239.png)'
- en: when i run this now i'm keeping track of this in a running manner and what we're
    hoping for of。
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 当我现在运行这个时，我正在以一种运行的方式跟踪这个，而我们希望的是。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_241.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_241.png)'
- en: course is that the mean mean underscore running and bn mean underscore STD are
    going to be very。 similar to the ones that we calculated here before and that
    way we don't need a second stage because。
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，均值均值下划线运行和bn均值下划线STD将与我们之前计算的非常相似，因此我们不需要第二个阶段。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_243.png)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_243.png)'
- en: we've sort of combined the two stages and we've put them on the side of each
    other if you want to。 look at it that way and this is how this is also implemented
    in the bastard normalization layer。
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这两个阶段结合在一起，放在彼此旁边，你可以这样看待它。这也是在“混蛋归一化”层中实现的方式。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_245.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_245.png)'
- en: in pytorch so during training the exact same thing will happen and then later
    when you're using。
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中，所以在训练期间，将发生完全相同的事情，稍后当你使用。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_247.png)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_247.png)'
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_248.png)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_248.png)'
- en: inference it will use the estimated running mean of both the mean as to deviation
    of those hidden。
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 推理时，将使用估计的运行均值，以及那些隐藏层的均值和标准差。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_250.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_250.png)'
- en: states so let's wait for the optimization converge and hopefully the running
    mean as。 derivation are roughly equal to these two and then we can simply use
    it here and we don't need this。
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 状态，所以我们等待优化收敛，希望运行均值和导数大致等于这两个，然后我们可以简单地在这里使用它，我们不需要这个。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_252.png)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_252.png)'
- en: stage of explicit calibration at the end okay so the optimization finished i'll
    rerun the explicit。 estimation and then the bn mean from the explicit estimation
    is here and bn mean from the running。
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 明确校准的阶段结束，好的，优化完成后，我将重新运行明确的估计，然后来自明确估计的bn均值在这里，来自运行的bn均值。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_254.png)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_254.png)'
- en: estimation during the during the optimization you can see it's very very similar
    it's not identical。 but it's pretty close and in the same way bn STD is this and
    bn STD running is this as you can see。 that once again they are fairly similar
    values not identical but pretty close and so then here。 instead of bn mean we
    can use the bn mean running instead of bn STD we can use bn STD running。
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 在优化过程中进行的估计，你可以看到它非常相似，并不是完全相同，但相当接近，以同样的方式，bn STD 是这个，bn STD 运行是这个，正如你所看到的，再次它们的值相当相似，不完全相同，但相当接近，因此在这里，我们可以用
    bn mean 运行代替 bn mean，用 bn STD 运行代替 bn STD。
- en: and hopefully the validation loss will not be impacted too much okay so it's
    basically identical。
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 希望验证损失不会受到太大的影响，所以基本上是相同的。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_256.png)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_256.png)'
- en: and this way we've eliminated the need for this explicit stage of calibration
    because we are doing。
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们消除了显式校准阶段的需要，因为我们正在进行中。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_258.png)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_258.png)'
- en: it in line over here okay so we're almost done with batch normalization there
    are only two more。 notes that i'd like to make number one i've skipped a discussion
    over what is this plus epsilon doing。 here this epsilon is usually like some small
    fixed number for example one in negative five by default。 and what it's doing
    is that it's basically preventing a division by zero in the case that the variance。
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里的行上，所以我们快完成批量归一化了，还有两个笔记我想提一下，第一，我跳过了关于这个正数 epsilon 的讨论，这个 epsilon 通常是某个小的固定数值，例如负五的默认值，而它的作用基本上是防止在方差为零的情况下出现除以零的情况。
- en: over your batch is exactly zero in that case here we normally have a division
    by zero but because。 of the plus epsilon this is going to become a small number
    in the denominator instead and things。 will be more well behaved so feel free
    to also add a plus epsilon here of a very small number。 it doesn't actually substantially
    change the result i'm going to skip it in our case just because。
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你的批次的结果恰好为零，这里我们通常会遇到除以零的情况，但由于有加上了 epsilon，这将使得分母变成一个很小的数，从而使情况变得更加稳定，因此可以在这里添加一个很小的正数
    epsilon，实际上这并不会实质性地改变结果，我在我们的案例中会跳过它。
- en: this is unlikely to happen in our very simple example here and the second thing
    i want you to。
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况在我们非常简单的例子中不太可能发生，我希望你能注意到。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_260.png)'
  id: totrans-359
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_260.png)'
- en: notice is that we're being wasteful here and it's very subtle but right here
    where we are adding the。 bias into each preact these biases now are actually useless
    because we're adding them to the each。 preact but then we are calculating the
    mean for every one of these neurons and subtracting it so。 whatever bias you add
    here is going to get subtracted right here and so these biases are not doing anything。
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到我们在这里有些浪费，这很微妙，但在这里我们将偏差添加到每个预激活中，这些偏差实际上是无用的，因为我们将它们添加到每个预激活中，但然后我们为这些神经元计算均值并进行减法，因此你在这里添加的任何偏差都会在这里被减去，所以这些偏差没有任何作用。
- en: in fact they're being subtracted out and they don't impact the rest of the calculation
    so if you look。 at b1。grad it's actually going to be zero because it's being subtracted
    out and doesn't actually have。
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上它们被减去了，并且对其余计算没有影响，因此如果你查看 b1.grad，它实际上将为零，因为它被减去了，并且实际上没有。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_262.png)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_262.png)'
- en: any effect and so whenever you're using batch normalization layers then if you
    have any weight。 layers before like a linear or a conv or something like that
    you're better off coming here and just。 like not using bias so you don't want
    to use bias and then here you don't want to add it because。
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，每当你使用批量归一化层时，如果在前面有任何权重层，比如线性层或卷积层，最好来这里，直接不使用偏差，所以你不想使用偏差，在这里你也不想添加它，因为。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_264.png)'
  id: totrans-364
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_264.png)'
- en: it's that spurious instead we have this batch normalization bias here and that
    batch normalization bias is。 now in charge of the biasing of this distribution
    instead of this b1 that we had here originally and。
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的伪影变成了这个批量归一化偏差，现在这个批量归一化偏差负责这个分布的偏移，而不是我们最初的 b1。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_266.png)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_266.png)'
- en: so basically the batch normalization layer has its own bias and there's no need
    to have a bias in。 the layer before it because that bias is going to be subtracted
    out anyway so that's the other small。 detail to be careful with sometimes it's
    not going to do anything catastrophic this b1 will just be。 useless it will never
    get any gradient it will not learn it will stay constant and it's just wasteful。
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 所以基本上批量归一化层有自己的偏置，而前一层就不需要再有偏置，因为那个偏置无论如何都会被减去。这是需要小心的另一个小细节，有时候它不会造成灾难性的后果，这个b1将会变得无用，它不会获得任何梯度，不会学习，会保持不变，这只是浪费。
- en: but it doesn't actually really impact anything otherwise okay so I rearranged
    the code a little。
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 但它并不会对其他任何事情产生实际影响。好的，所以我稍微调整了一下代码。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_268.png)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_268.png)'
- en: bit with comments and I just wanted to give a very quick summary of the batch
    normalization layer。
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 我想对批量归一化层做一个非常简短的总结。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_270.png)'
  id: totrans-371
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_270.png)'
- en: we are using batch normalization to control the statistics of activations in
    the neural net it is。 common to sprinkle batch normalization layer across the
    neural net and usually we will place it after。 layers that have multiplications
    like for example a linear layer or a convolutional layer which we。
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用批量归一化来控制神经网络中激活值的统计特性。通常会在神经网络中各处添加批量归一化层，通常会将其放在像线性层或卷积层等乘法层之后。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_272.png)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_272.png)'
- en: may cover in the future now the batch normalization internally has parameters
    for the gain and the bias。 and these are trained using back propagation it also
    has two buffers the buffers are the mean。 and the standard deviation the running
    mean and the running mean of the standard deviation and these。
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化内部有增益和偏置的参数，这些参数通过反向传播进行训练。此外，它还有两个缓冲区，这些缓冲区分别是均值和标准差、运行均值和标准差。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_274.png)'
  id: totrans-375
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_274.png)'
- en: are not trained using back propagation these are trained using this janky update
    of kind of like。
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数不是通过反向传播训练的，而是通过一种稍微不太正常的更新方式来训练的。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_276.png)'
  id: totrans-377
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_276.png)'
- en: a running mean update so these are sort of the parameters and the buffers of
    batch room layer。 and then really what it's doing is it's calculating the mean
    and standard deviation of the activations。 that are feeding into the batch room
    layer over that batch then it's centering that batch to be unit。 gosh in and then
    it's offsetting and scaling it by the learned bias and gain and then on top of
    that。
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 运行均值更新，这些是批量归一化层的参数和缓冲区。它的实际操作是计算输入批次的激活值的均值和标准差，然后将该批次中心化为单位均值，再通过学习到的偏置和增益进行偏移和缩放。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_278.png)'
  id: totrans-379
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_278.png)'
- en: it's keeping track of the mean standard deviation of the inputs and it's maintaining
    this running。 mean standard deviation and this will later be used at inference
    so that we don't have to。 re-estimate the mean standard deviation all the time
    and in addition that allows us to basically。 forward individual examples at test
    time so that's the batch normalization layer it's a fairly complicated。
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 它跟踪输入的均值和标准差，并维护这个运行均值和标准差，这将在推理时使用，以免我们需要不断重新估计均值和标准差。此外，这还使我们在测试时能够基本上处理单个示例。这就是批量归一化层，它相当复杂。
- en: layer but this is what it's doing internally now i wanted to show you a little
    bit of a real example。
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 这一层的内部运作是这样的，现在我想给你展示一个真实的例子。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_280.png)'
  id: totrans-382
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_280.png)'
- en: so you can search reznet which is a residual neural network and these are contacts
    of neural。 arcs used for image classification and of course we haven't come in
    reznets in detail so i'm not。 going to explain all the pieces of it but for now
    just note that the image feeds into a reznet on。 the top here and there's many
    many layers with repeating structure all the way to predictions。
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以搜索一下ResNet，它是一种残差神经网络，这些是用于图像分类的神经网络架构。当然，我们还没有详细讨论ResNet，所以我不会解释所有的部分，但现在请注意，图像在上方输入到ResNet，并且有许多重复结构的层，一直到最终的预测。
- en: of what's inside that image this repeating structure is made up of these blocks
    and these blocks are。 just sequentially stacked up in this deep neural network
    now the code for this the block basically。 that's used and repeated sequentially
    in series is called this bottleneck block bottleneck block。 and there's a lot
    here this is all pytorch and of course we haven't covered all of it but i want。
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 在那个图像内部的结构是由这些块组成的，这些块在这个深度神经网络中是顺序堆叠的。这个块的代码基本上是用作并在序列中重复的，称为瓶颈块。瓶颈块有很多，这里都是
    pytorch，当然我们还没有覆盖全部，但我想。
- en: to point out some small pieces of it here in the init is where we initialize
    the neural net so this。
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我想指出一些小的部分，初始化是在这里，我们初始化神经网络。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_282.png)'
  id: totrans-386
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_282.png)'
- en: code of block here is basically the kind of stuff we're doing here we're initializing
    all the layers。 and in the forward we are specifying how the neural net acts once
    you actually have the input。
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的代码块基本上是我们正在做的事情，我们初始化所有的层。在前向传播中，我们指定神经网络在实际接收到输入后如何工作。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_284.png)'
  id: totrans-388
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_284.png)'
- en: so this code here is along the lines of what we're doing here and now these
    blocks are replicated。 and stacked up serially and that's what a residual network
    would be and so notice what's happening here。 come one these are convolutional
    layers and these convolutional layers basically they're the same。 thing as a linear
    layer except convolutional layers don't apply convolutional layers are used。
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这里的代码与我们正在做的事情是一致的，现在这些块被复制并串联堆叠，这就是残差网络的样子，因此请注意这里发生的事情。这些是卷积层，这些卷积层基本上与线性层是相同的，只是卷积层不应用卷积层。
- en: for images and so they have spatial structure and basically this linear multiplication
    and bias。 offset are done on patches instead of a map instead of the full input
    so because these images have。 structure spatial structure convolution is just
    basically do wx plus b but they do it on overlapping。 patches of the input but
    otherwise it's wx plus b then we have the normal layer which by default。
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像，它们具有空间结构，基本上这个线性乘法和偏置。偏移是在图像块上进行的，而不是整个输入图，因此由于这些图像具有。结构，空间结构，卷积基本上就是做
    wx 加 b，但它们是在输入的重叠块上进行的，但其他情况下就是 wx 加 b。然后我们有默认的归一化层。
- en: here is initialized to be a batch norm in 2d so two dimensional batch normalization
    layer and then。 we have a nonlinearity like relu so instead of here they use relu
    we are using 10h in this case。 but both both are just nonlinearities and you can
    just use them relatively interchangeably。 for very deep networks relu typically
    empirically work a bit better so see the motif that's being。
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 这里被初始化为二维批归一化，所以是二维批归一化层。然后我们有一个非线性，例如 relu，因此在这里他们使用 relu，而在这种情况下我们使用的是 10h。但两者都是非线性，你可以相对可互换地使用它们。对于非常深的网络，relu
    通常在经验上表现得更好，所以看看这个模式。
- en: repeated here we have convolution batch normalization rather convolution batch
    normalization。 etc and then here this is residual connection that we haven't covered
    yet but basically that's the。 exact same pattern we have here we have a weight
    layer like a convolution or like a linear layer。 batch normalization and then
    10h which is nonlinearity but basically a weight layer。
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 这里重复的部分是卷积批归一化，确切地说是卷积批归一化，等等。然后这里是我们还没有覆盖的残差连接，但基本上这就是我们这里的确切模式，我们有一个权重层，如卷积或线性层，批归一化，然后是10h，非线性，但基本上是一个权重层。
- en: a normalization layer and nonlinearity and that's the motif that you would be
    stacking up when you。 create these deep neural networks exactly as it's done here
    and one more thing i'd like you to notice。
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 一个归一化层和非线性，这正是你在创建这些深度神经网络时会堆叠的元素，正如这里所做的那样，还有一件事我想请你注意。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_286.png)'
  id: totrans-394
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_286.png)'
- en: is that here when they are initializing the conv layers like conv one by one
    the depth for that is。 right here and so it's initializing an nn。conf2d which
    is a convolutional layer in pytorch and there's。 much of keyword arguments here
    that i am not going to explain yet but you see how there's bias equals。 false
    the bias equals false is exactly for the same reason as bias is not used in our
    case you see。
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 这里在初始化 conv 层（如 conv 一对一）时，深度正是这里，因此它正在初始化一个 nn。conf2d，这是pytorch中的一个卷积层，还有许多关键字参数我还不打算解释，但你可以看到有
    bias 等于 false，bias 等于 false 的原因和在我们案例中不使用偏差的原因完全相同。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_288.png)'
  id: totrans-396
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_288.png)'
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_289.png)'
  id: totrans-397
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_289.png)'
- en: how i erase the use of bias and the use of bias is spurious because after this
    weight layer there's。 a batch normalization and the batch normalization subtracts
    that bias and that has its own bias。 so there's no need to introduce these spurious
    parameters it wouldn't hurt performance it's just。 useless and so because they
    have this motif of conv master and relu they don't need a bias here。
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 我是如何消除偏差的使用，偏差的使用是虚假的，因为在这个权重层之后有一个批归一化，而批归一化减去那个偏差，并且它有自己的偏差。因此没有必要引入这些虚假的参数，这对性能没有影响，只是无用的，因此因为他们有这个
    conv master 和 relu 的主题，他们不需要在这里添加偏差。
- en: because there's a bias inside here so by the way this example here is very easy
    to find just do。 resnet pytorch and uh it's this example here so this is kind
    of like the stock implementation of a。 residual neural network in pytorch and
    you can find that here but of course i haven't covered many。 of these parts yet
    and i would also like to briefly descend into the definitions of these。
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这里有一个偏差，所以这个例子很容易找到，只需搜索。 resnet pytorch，嗯，就是这个例子，所以这有点像pytorch中残差神经网络的基本实现，你可以在这里找到，但当然我还没有覆盖很多这些部分，我也想简要深入这些定义。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_291.png)'
  id: totrans-400
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_291.png)'
- en: pytorch layers and the parameters that they take now instead of a convolutional
    layer we're going to。 look at a linear layer because that's the one that we're
    using here this is a linear layer and i haven't。
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: pytorch层及其参数，现在我们要看的是线性层，因为这就是我们在这里使用的。这是一个线性层，我还没有。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_293.png)'
  id: totrans-402
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_293.png)'
- en: covered covered convolutions yet but as i mentioned convolutions are basically
    linear layers except。 on patches so a linear layer performs a wx plus b except
    here they're calling the w a transpose。 so it's called case wx plus b very much
    like we did here to initialize this layer you need to know。
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 还没有覆盖卷积，但正如我提到的，卷积基本上是线性层，只是在小块上进行处理，因此线性层执行的是 wx 加 b，而在这里他们称 w 为转置，因此称为 wx
    加 b，和我们在这里初始化这个层的方法非常相似，你需要知道。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_295.png)'
  id: totrans-404
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_295.png)'
- en: the fan in the fan out and that's so that they can initialize this w this is
    the fan in and the fan。
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: fan in 和 fan out，这样他们就可以初始化这个 w，这是 fan in 和 fan out。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_297.png)'
  id: totrans-406
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_297.png)'
- en: out so they know how how big the weight matrix should be you need to also pass
    in whether you。 whether or not you want a bias and if you set it to false then
    no bias will be inside this layer。 and you may want to do that exactly like in
    our case if your layer is followed by a normalization。 layer such as batch norm
    so this allows you to basically disable bias in terms of the initialization。
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 他们知道权重矩阵应该有多大，你还需要传入是否希望有偏差，如果你将其设置为 false，那么这个层内部将没有偏差。你可能想这样做，就像在我们的案例中，如果你的层后面有一个归一化层，比如批归一化，这样可以基本上在初始化时禁用偏差。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_299.png)'
  id: totrans-408
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_299.png)'
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_300.png)'
  id: totrans-409
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_300.png)'
- en: if we swing down here this is reporting the variables used inside this linear
    layer and our linear layer。
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们向下看，这是报告在这个线性层中使用的变量，我们的线性层。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_302.png)'
  id: totrans-411
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_302.png)'
- en: here has two parameters the weight and the bias in the same way they have a
    weight and a bias。
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两个参数：权重和偏差，正如它们有一个权重和一个偏差一样。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_304.png)'
  id: totrans-413
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_304.png)'
- en: and they're talking about how they initialize it by default so by default pytorch
    will initialize。 your weights by taking the fan in and then doing one over fan
    in square root and then instead of a。 normal distribution they are using a uniform
    distribution so it's very much the same thing but。 they are using a one instead
    of five over three so there's no gain being calculated here the gain。
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 他们讨论如何默认初始化，所以默认情况下pytorch将通过获取输入的数量来初始化你的权重，然后做一个输入数量的平方根的倒数，而不是使用正常分布，他们使用均匀分布，所以非常相似，但。它们使用的是1而不是5/3，因此没有计算增益。
- en: is just one but otherwise is exactly one over the square root of fan in exactly
    as we have here。 so one over the square root of k is the is the scale of the weights
    but when they are drawing the。
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个，但在其他方面与这里的完全相同，即为输入的平方根的倒数。所以，输入的平方根的倒数是权重的尺度，但当它们被绘制时。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_306.png)'
  id: totrans-416
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_306.png)'
- en: numbers they're not using a Gaussian by default they're using a uniform distribution
    by default。
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 它们默认不使用高斯分布，而是使用均匀分布。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_308.png)'
  id: totrans-418
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_308.png)'
- en: and so they draw uniformly from negative square root of k to square root of
    k but it's the exact。
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 所以它们在负平方根k到平方根k之间均匀绘制，但这是完全准确的。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_310.png)'
  id: totrans-420
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_310.png)'
- en: same thing and the same motivation from for with respect to what we've seen
    in this lecture and。
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的事情，以及关于我们在这节课上看到的内容的相同动机。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_312.png)'
  id: totrans-422
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_312.png)'
- en: the reason they're doing this is if you have a roughly Gaussian input this will
    ensure that out of this。 layer you will have a roughly Gaussian output and you
    basically achieve that by scaling the weights by。
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的原因是，如果你有一个大致高斯输入，这将确保你从这个层输出一个大致高斯输出，基本上通过权重的缩放来实现。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_314.png)'
  id: totrans-424
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_314.png)'
- en: one over the square root of fan in so that's what this is doing and then the
    second thing is the。
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 输入的平方根的倒数，所以这就是它所做的，第二件事是。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_316.png)'
  id: totrans-426
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_316.png)'
- en: baster normalization layer so let's look at what that looks like in pytorch
    so here we have a one。
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化层，所以让我们看看在pytorch中它的样子，这里我们有一个。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_318.png)'
  id: totrans-428
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_318.png)'
- en: dimensional baster normalization layer exactly as we are using here and there
    are a number of keyword。 arguments going into it as well so we need to know the
    number of features for us that is 200。 and that is needed so that we can initialize
    these parameters here the gain the bias and the buffers。 for the running mean
    and standard deviation then they need to know the value of epsilon here。
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用的是维度归一化层，并且有多个关键字参数输入，因此我们需要知道特征的数量，对我们来说是200。这是必要的，以便初始化这些参数，包括增益、偏差和缓冲区，用于运行均值和标准差，然后它们需要知道这里的epsilon值。
- en: and by default this is one negative five you don't typically change this too
    much。 then they need to know the momentum and the momentum here as they explain
    is basically used。
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，这个值是1负5，你通常不会太改动这个。然后它们需要知道动量，动量在这里被解释为基本使用。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_320.png)'
  id: totrans-431
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_320.png)'
- en: for these running mean and running standard deviation so by default the momentum
    here is point one。 the momentum we are using here in this example is 0。001 and
    basically you may want to change this。
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些运行均值和运行标准差，默认动量是0.1。我们在这个例子中使用的动量是0.001，基本上你可能想要改变这个值。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_322.png)'
  id: totrans-433
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_322.png)'
- en: sometimes and roughly speaking if you have a very large batch size then typically
    what you'll see is。 that when you estimate the mean and standard deviation for
    every single batch size if it's large。 enough you're going to get roughly the
    same result and so therefore you can use slightly higher。 momentum like point
    one but for a batch size as small as 32 the mean and standard deviation here。
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，粗略地说，如果你有一个非常大的批次大小，通常你会看到的是，当你估计每个批次大小的均值和标准差时，如果它足够大，你将得到大致相同的结果，因此你可以使用稍高的动量，如0.1，但对于小至32的批次大小，这里的均值和标准差。
- en: might take on slightly different numbers because there's only 32 examples we
    are using to estimate。 the mean and standard deviation so the value is changing
    around a lot and if your momentum is point。 one that that might not be good enough
    for this value to settle and converge to the actual mean。 and standard deviation
    over the entire training set and so basically if your batch size is very small。
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会采用略有不同的数值，因为我们使用的只有32个示例来估计均值和标准差，所以这个值变化很大，如果你的动量是0.1，那可能不足以让这个值收敛到整个训练集的实际均值和标准差，因此如果你的批量大小非常小。
- en: momentum of point one is potentially dangerous and it might make it so that
    the running mean。 and standard deviation is thrashing too much during training
    and it's not actually converging properly。 affine equals true determines whether
    this batch normalization layer has these learnable affine。
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 0.1的动量是潜在危险的，可能导致运行均值和标准差在训练期间波动过大，而没有正确收敛。affine为真决定这个批量归一化层是否具有可学习的仿射变换。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_324.png)'
  id: totrans-437
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_324.png)'
- en: parameters the gain and the bias and this is almost always kept to true i'm
    not actually sure why you。
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 参数增益和偏差几乎总是保持为真，我其实不太确定为什么你。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_326.png)'
  id: totrans-439
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_326.png)'
- en: would want to change this to false then track running stats is determining whether
    or not。
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想将其更改为假，那么跟踪运行统计是决定是否。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_328.png)'
  id: totrans-441
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_328.png)'
- en: batch normalization layer of PyTorch will be doing this and one reason you may
    you may want to skip。
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch的批量归一化层会做这个，可能你会想跳过的一个原因。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_330.png)'
  id: totrans-443
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_330.png)'
- en: the running stats is because you may want to for example estimate them at the
    end as a stage two。
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 运行统计是因为你可能希望在结束时估计它们作为第二阶段。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_332.png)'
  id: totrans-445
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_332.png)'
- en: at the like this and in that case you don't want the batch normalization layer
    to be doing all this。 extra compute that you're not going to use and finally we
    need to know which device we're going。 to run this batch normalization on a CPU
    or a GPU and what the data type should be half precision。 single precision double
    precision and so on so that's the batch normalization layer otherwise。
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你不希望批量归一化层进行所有这些额外的计算，因为你不会使用，最后我们需要知道我们要在哪个设备上运行这个批量归一化，是在CPU上还是GPU上，以及数据类型应该是半精度、单精度、双精度等等，所以这就是批量归一化层的要求。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_334.png)'
  id: totrans-447
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_334.png)'
- en: they link to the paper it's the same formula we've implemented and everything
    is the same exactly as。
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 它们链接到论文，公式是我们实现的相同的一切也完全一样。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_336.png)'
  id: totrans-449
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_336.png)'
- en: we've done here okay so that's everything that i wanted to cover for this lecture
    really what i。
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里做了，这就是我想在这次讲座中覆盖的所有内容。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_338.png)'
  id: totrans-451
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_338.png)'
- en: wanted to talk about is the importance of understanding the activations and
    the gradients。 and their statistics in neural works and this becomes increasingly
    important especially as you。 make your neural works bigger larger and deeper we
    looked at the distributions basically at the。 output layer and we saw that if
    you have two confident mispredictions because the activations are too。
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 我想谈的是理解激活值和梯度及其统计数据的重要性。在神经网络中，这变得越来越重要，尤其是当你让你的神经网络变得更大、更深时。我们基本上看了输出层的分布，我们看到如果你有两个置信度较高的错误预测，是因为激活值太。
- en: messed up at the last layer you can end up with these hockey stick losses and
    if you fix this you。 get a better loss at the end of training because your training
    is not doing wasteful work then we。 also saw that we need to control the activations
    we don't want them to you know squash to zero。 or explode to infinity and because
    that you can run into a lot of trouble with all of these。
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一层搞砸了，你可能会得到这些曲棍球棒损失，如果你修复了这个问题，你会在训练结束时获得更好的损失，因为你的训练没有做无用的工作，我们还看到我们需要控制激活值，我们不希望它们压缩为零或爆炸到无穷大，因为这样你会遇到很多麻烦。
- en: nonlinearities in these neural nets and basically you want everything to be
    fairly homogeneous throughout。 the neural net you want roughly gosh in activations
    throughout the neural net let me talk about okay。
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 这些神经网络中的非线性，基本上你希望整个网络保持相对均匀。你希望在整个神经网络中大致均匀地激活，让我谈谈，好吧。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_340.png)'
  id: totrans-455
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_340.png)'
- en: if we want roughly gosh in activations how do we scale these weight matrices
    and biases during。 initialization of the neural net so that we don't get you know
    so everything is as control as possible。
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望激活大致均匀，我们如何在神经网络初始化期间缩放这些权重矩阵和偏置，以便我们不会。让一切尽可能可控。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_342.png)'
  id: totrans-457
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_342.png)'
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_343.png)'
  id: totrans-458
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_343.png)'
- en: so that gave us a large boost in improvement and then I talked about how that
    strategy is not。 actually possible for much much deeper neural nets because when
    you have much deeper neural。 nets with lots of different types of layers it becomes
    really really hard to precisely set the。 weights and the biases in such a way
    that the activations are roughly uniform throughout the。
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们带来了巨大的改进，然后我谈到了这个策略对于更深的神经网络来说并不。实际上是可能的，因为当你有更深的神经网络和许多不同类型的层时，准确设置。权重和偏置变得非常困难，以至于激活在整个网络中大致均匀。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_345.png)'
  id: totrans-460
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_345.png)'
- en: neural net so then I introduced the notion of the normalization layer now there
    are many normalization。 layers that people use in practice bachelor normalization
    layer normalization this is normalization。 group normalization we haven't covered
    most of them but I've introduced the first one and also。
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络，所以我引入了归一化层的概念，现在有许多归一化。层在实践中被人们使用，学士归一化层，层归一化，这就是归一化。组归一化，我们还没有涵盖大多数，但我引入了第一个，还有。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_347.png)'
  id: totrans-462
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_347.png)'
- en: the one that I believe came out first and that's called bachelor normalization
    and we saw how。 bachelor normalization works this is a layer that you can sprinkle
    throughout your deep neural net。 and the basic idea is if you want roughly gosh
    in activations well then take your activations and。 take the mean and standard
    deviation and center your data and you can do that because the centering。
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 我相信首先出现的是称为学士归一化的层，我们看到了。学士归一化是如何工作的，这是一个可以在深度神经网络中散布的层。基本想法是，如果你希望激活大致均匀，那么就取你的激活值并。取均值和标准差，并对数据进行中心化，你可以这样做，因为中心化。
- en: operation is differentiable but and on top of that we actually had to add a
    lot of bells and whistles。 and that gave you a sense of the complexities of the
    bachelor normalization layer because now。 we're centering the data that's great
    but suddenly we need the gain and the bias and now those are。 trainable and then
    because we are coupling all the training examples now suddenly the question is。
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 操作是可微的，但除此之外，我们实际上还需要添加许多额外的功能。这让你感受到学士归一化层的复杂性，因为现在。我们正在对数据进行中心化，这很好，但突然间我们需要增益和偏置，而这些都是。可训练的，然后由于我们耦合了所有的训练示例，现在突然间问题是。
- en: how do you do the inference or to do to do the inference we need to now estimate
    these mean and。
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何进行推理，或者说我们现在需要估计这些均值和。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_349.png)'
  id: totrans-466
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_349.png)'
- en: standard deviation once or the entire training set and then use those at inference
    but then no one。
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 标准差一次或整个训练集，然后在推理时使用这些，但没有人。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_351.png)'
  id: totrans-468
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_351.png)'
- en: likes to do stage two so instead we fold everything into the bachelor normalization
    layer during training。 and try to estimate these in the running manner so that
    everything is a bit simpler and that gives。 us the bachelor normalization layer
    and as i mentioned no one likes this layer it causes a huge。 amount of bugs and
    intuitively it's because it is coupling examples in the forward pass of the neural。
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 喜欢进行第二阶段，因此我们在训练期间将所有内容折叠到学士归一化层中。并试图以运行的方式估计这些，以便一切变得更简单，这给了。我们学士归一化层，正如我提到的，没有人喜欢这个层，它会导致大量的。错误，直观上是因为它在神经网络的前向传递中耦合示例。
- en: net and i've shocked myself in the foot with this layer over and over again
    in my life and i don't。 want you to suffer the same so basically try to avoid
    it as much as possible some of the other。 alternatives to these layers are for
    example group normalization or layer normalization and。 those have become more
    common in more recent deep learning but we haven't covered those yet。
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 网络，我在生活中多次用这个层给自己带来了麻烦，我不希望你们遭受同样的痛苦，所以基本上尽量避免它。对于这些层的其他一些替代方案，例如群归一化或层归一化，最近在深度学习中变得更加常见，但我们还没有介绍这些。
- en: but definitely bachelor normalization was very influential at the time when
    it came out in roughly。 2015 because it was kind of the first time that you could
    train reliably much deeper neural nets。 and fundamentally the reason for that
    is because this layer was very effective at controlling the。 statistics of the
    activations in the neural net so that's the story so far and that's all i wanted。
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 但绝对来说，批量归一化在大约2015年推出时非常有影响力，因为那是你可以可靠地训练更深神经网络的第一次。根本原因是这个层在控制神经网络中的激活统计方面非常有效，所以到目前为止就是这个故事，这就是我想说的全部。
- en: to cover and in the future lecture so hopefully we can start going into recurrent
    neural nets and。 recurrent neural nets as we'll see are just very very deep networks
    because you you unroll the loop。 and when you actually optimize these neural nets
    and that's where a lot of this analysis around。 the activation statistics and
    all these normalization layers will become very very important for good。
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 要覆盖的内容，以及未来的讲座，希望我们可以开始深入探讨递归神经网络，递归神经网络就像我们看到的，是非常非常深的网络，因为你解开了循环。当你真正优化这些神经网络时，这些激活统计和所有这些归一化层的分析就变得非常重要了。
- en: performance so we'll see that next time bye okay so i lied i would like us to
    do one more summary。
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 性能，我们下次会看到这一点，拜拜，好吧，我说谎了，我想我们再做一个总结。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_353.png)'
  id: totrans-474
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_353.png)'
- en: here as a bonus and i think it's useful as to have one more summary of everything
    i've presented in。 this lecture but also i would like us to start by tortuifying
    our code a little bit so it looks much。 more like what you would encounter in
    pytorch so you'll see that i will structure our code into。 these modules like
    a linear module and a batch room module and i'm putting the code inside these。
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 这是作为一个额外的内容，我认为有一个额外的总结对我在这次讲座中展示的所有内容是有用的，但我还希望我们先将代码进行一些调整，使其更像在PyTorch中会遇到的样子。你会看到，我会将我们的代码结构分成这些模块，比如线性模块和批量归一化模块，我将代码放在这些模块内。
- en: modules so that we can construct neural networks very much like we would construct
    them in pytorch。 and i will go through this in detail so we'll create our neural
    net then we will do the optimization。
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 模块，以便我们可以构建神经网络，方式非常像我们在PyTorch中构建它们的方式。我会详细讲解这一点，因此我们将创建我们的神经网络，然后进行优化。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_355.png)'
  id: totrans-477
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_355.png)'
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_356.png)'
  id: totrans-478
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_356.png)'
- en: loop as we did before and then the one more thing that i want to do here is
    i want to look at the。
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 像之前那样进行循环，然后我在这里想做的另一件事是看一下。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_358.png)'
  id: totrans-480
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_358.png)'
- en: activation statistics both in the forward pass and in the backward pass and
    then here we have the。 evaluation and sampling just like before so let me rewind
    all the way up here and go a little bit。
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 激活统计在前向传播和反向传播中都有，然后在这里我们有评估和采样，和之前一样，所以让我回到这里，再讲一点。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_360.png)'
  id: totrans-482
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_360.png)'
- en: slower so here i am creating a linear layer you'll notice that tort。nn has lots
    of different types。 of layers and one of those layers is the linear layer tort。nn
    takes a number of input features。 output features whether or not we should have
    bias and then the device that we want to place this。 layer on and the data type
    so i will omit these two but otherwise we have the exact same thing。
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 慢一点，所以我在这里创建一个线性层，你会注意到`torch.nn`有很多不同类型的层，其中之一就是线性层。`torch.nn`接受一些输入特征、输出特征、是否需要偏置、我们希望将此层放置的设备以及数据类型，因此我将省略这两个参数，但其余的我们有完全相同的东西。
- en: we have the fan in which is the number of inputs fan out the number of outputs
    and whether or not。 we want to use a bias and internally inside this layer there's
    a weight and a bias if you'd like it。 it is typical to initialize the weight using
    say random numbers drawn from gosh in and then here's。 the coming initialization
    that we discussed already in this lecture and that's a good default and also。
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有输入的fan in，输出的fan out，以及我们是否想使用偏置。在这个层内部，有一个权重和一个偏置，如果你需要。通常使用从高斯分布中抽取的随机数来初始化权重，接下来就是我们在本讲中讨论的初始化，这是一种好的默认值。
- en: the default that i believe pytorch uses and by default the bias is usually initialized
    to zeros。 now when you call this module this will basically calculate w times
    x plus b if you have nb。 and then when you also call that parameters on this module
    it will return the tensors。 that are the parameters of this layer now next we
    have the batch normalization layer so。
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 我相信PyTorch使用的默认值，默认情况下偏置通常初始化为零。现在，当你调用这个模块时，它基本上会计算w乘以x加上b，如果你有nb。当你也调用这个模块上的参数时，它将返回该层的张量参数。接下来，我们有批量归一化层。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_362.png)'
  id: totrans-486
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_362.png)'
- en: i've written that here and this is very similar to pytorch and then dot batch
    normal 1d layer as shown。
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经在这里写下来了，这与PyTorch的dot batch norm 1d层非常相似。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_364.png)'
  id: totrans-488
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_364.png)'
- en: here so i'm kind of taking these three parameters here the dimensionality the
    epsilon that we'll use。 in the division and the momentum that we will use in keeping
    track of these running stats the running。 mean and the running variance um now
    pytorch actually takes quite a few more things but i'm assuming。 some of their
    settings so for us a find will be true that means that we will be using a gamma。
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我在获取这三个参数：维度、我们将在除法中使用的epsilon，以及我们将在跟踪这些运行统计时使用的动量，运行均值和运行方差。实际上，PyTorch会处理更多内容，但我假设一些他们的设置，因此我们认为会是正确的，这意味着我们将使用gamma。
- en: beta after the normalization the track running stats will be true so we will
    be keeping track of。 the running mean and the running variance in the in the past
    room our device by default is the cpu。 and the data type by default is float float
    32 so those are the defaults otherwise um we are。 taking all the same parameters
    in this bathroom layer so first i'm just saving them now here's。
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 在归一化之后，跑步统计数据的beta将会是真实的，所以我们会进行跟踪。运行均值和运行方差在过去的房间中，我们的设备默认是CPU，而数据类型默认是float32，所以这些是默认值。否则，我们在这个批处理层中保持所有相同的参数，所以我现在只是在保存它们。
- en: something new there's a dot training which by default is true and pytorch and
    in modules also。 have this attribute that training and that's because many modules
    and batch norm is included in that。 have a different behavior whether you are
    training your or what and whether you are running it in。 an evaluation mode and
    calculating your evaluation laws or using it for inference on some test examples。
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些新的东西，比如默认为true的dot训练，PyTorch和模块也有这个属性，那就是训练。这是因为许多模块，包括批量归一化，具有不同的行为，具体取决于你是在训练还是在评估模式下运行，并计算评估损失，或者在一些测试示例上进行推断。
- en: and batch norm is an example of this because when we are training we are going
    to be using the。 mean and the variance estimated from the current batch but during
    inference we are using the running。 mean and running variance and so also if we
    are training we are updating mean and variance but if。 we are testing then these
    are not being updated they're kept fixed and so this flag is necessary。
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化就是这个例子的一个，因为当我们在训练时，我们将使用当前批次估计的均值和方差，但在推断时，我们使用运行均值和运行方差，因此，如果我们在训练，我们在更新均值和方差，但如果我们在测试，则这些不会被更新，而是保持不变，因此这个标志是必要的。
- en: and by default true just like in pytorch now the parameters of batch norm 1d
    are the gamma and the。 beta here and then the running mean and running variance
    are called buffers in pytorch nomenclature。
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下为true，就像在PyTorch中一样，Batch Norm 1D的参数是gamma和beta，然后运行均值和运行方差在PyTorch的命名法中称为缓冲区。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_366.png)'
  id: totrans-494
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_366.png)'
- en: and these buffers are trained using exponential moving average here explicitly
    and they are not。 part of the back propagation is the cast gradient descent so
    they are not sort of like parameters of。 this layer and that's why when we have
    a parameters here we only return gamma and beta we do not return。 the mean and
    the variance this is trained sort of like internally here every forward pass using。
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 这些缓冲区使用指数移动平均明确训练，它们不是反向传播的一部分，不是梯度下降的参数。因此，当我们有参数时，我们只返回gamma和beta，而不返回均值和方差。这些在每次前向传递时都是内部训练的。
- en: exponential moving average so that's the initialization now in a forward pass
    if we are training then we。 use the mean and the variance estimated by the batch
    or you pull up the paper here we calculate the。 mean and the variance now up above
    i was estimating the standard deviation and keeping track of the。 standard deviation
    here in the running standard deviation instead of running variance but let's。
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这是初始化，现在在前向传递中，如果我们正在训练，那么我们使用由批次估计的均值和方差，或者查看论文，这里我们计算均值和方差。上面我估计了标准差并跟踪了运行标准差，而不是运行方差，但让我们。
- en: follow the paper exactly here they calculate the variance which is the standard
    deviation squared。 and that's what's kept track of in the running variance instead
    of a running standard deviation。 but those two would be very very similar i believe
    if we are not training then we use running mean。 and variance we normalize and
    then here i am calculating the output of this layer and i'm also。
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 完全按照论文的要求，这里计算方差，即标准差的平方。这是运行方差中跟踪的内容，而不是运行标准差。但我相信，如果我们不训练，这两者将非常相似。我们使用运行均值和方差进行归一化，然后在这里计算该层的输出。
- en: assigning it to an attribute called dot out now dot out is something that i'm
    using in our modules。 here this is not what you would find in PyTorch we are slightly
    deviating from it i'm creating a。 dot out because i would like to very easily
    maintain all those variables so that we can create。
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 将其分配给名为dot out的属性，现在dot out是我在模块中使用的。这不是您在PyTorch中会发现的，我们稍微偏离了它。我创建了一个dot out，因为我希望非常方便地维护所有这些变量，以便我们可以创建。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_368.png)'
  id: totrans-499
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_368.png)'
- en: statistics of them and plot them but PyTorch and modules will not have a dot
    out attribute。
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 统计它们并绘制图形，但PyTorch和模块不会有dot out属性。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_370.png)'
  id: totrans-501
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_370.png)'
- en: and finally here we are updating the buffers using again as i mentioned exponential
    moving average。 provided given the provided momentum and importantly you'll notice
    that i'm using the。 Torstop no-grat context manager and i'm doing this because
    if we don't use this then PyTorch will。 start building out an entire computational
    graph out of these tensors because it is expecting that。
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们再次使用指数移动平均更新缓冲区。如我所提到的，提供了动量，并且重要的是，您会注意到我使用了Torstop无梯度上下文管理器。我这样做是因为如果不使用它，PyTorch将开始根据这些张量构建整个计算图，因为它期望这样。
- en: we will eventually call that backward but we are never going to be calling that
    backward on anything。 that includes running mean and running variance so that's
    why we need to use this context manager。 so that we are not sort of maintaining
    them using all this additional memory so this will make it。 more efficient and
    it's just telling PyTorch that while we know backward we just have a bunch of
    tensors。
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 最终我们将调用反向传播，但我们永远不会在任何包含运行均值和运行方差的内容上调用它。因此，我们需要使用这个上下文管理器，以免使用额外的内存来维护它们，这样会更高效，并且它只是在告诉PyTorch，在我们知道反向传播时，我们只有一堆张量。
- en: we want to update them that's it and then we return okay now scrolling down
    we have the 10H layer。
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要更新它们，仅此而已，然后我们返回。现在向下滚动，我们有10H层。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_372.png)'
  id: totrans-505
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_372.png)'
- en: this is very very similar to Torstop 10H and it doesn't do too much it just
    calculates 10H as you。 might expect so that's Torstop 10H and there's no parameters
    in this layer but because these are。 layers it now becomes very easy to sort of
    like stack them up into basically just a list。 and we can do all the initializations
    that we're used to so we have the initial sort of embedding。
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 这与Torstop 10H非常相似，它的功能并不复杂，只是计算10H，正如你所期望的那样，所以这就是Torstop 10H，并且这一层没有参数，但由于这些是层，现在将它们堆叠起来变得很简单，基本上就是一个列表，我们可以进行所有我们习惯的初始化，所以我们有初步的嵌入。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_374.png)'
  id: totrans-507
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_374.png)'
- en: matrix we have our layers and we can call them sequentially and then again with
    Torstop no-grat。
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵，我们有我们的层，我们可以顺序调用它们，然后再次使用Torstop无梯度。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_376.png)'
  id: totrans-509
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_376.png)'
- en: there's some initializations here so we want to make the output softmax a bit
    less confident。 like we saw and in addition to that because we are using a six
    layer multi layer perception here。 so you see how I'm stacking linear 10H linear
    10H etc I'm going to be using the game here and I'm。 going to play with this in
    a second so you'll see how when we change this what happens to this。
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些初始化，所以我们希望让输出softmax稍微不那么自信。正如我们所看到的，此外，因为我们使用的是六层的多层感知器，所以你会看到我如何堆叠线性10H、线性10H等，我将在这里使用游戏，并且我将在一秒钟内进行调整，所以你会看到当我们改变这个时，它会发生什么。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_378.png)'
  id: totrans-511
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_378.png)'
- en: statistics finally the primers are basically the embedding matrix and all the
    primers in all the。 layers and notice here I'm using a double list comprehension
    if you want to call it that but。 for every layer in layers and for every parameter
    in each of those layers we are just stacking up。 all those piece all those parameters
    now in total we have 46 000 parameters and I'm telling。
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 统计信息，最后，基元基本上是嵌入矩阵，以及所有层中的所有基元，请注意，我在使用双重列表推导，如果你愿意这样称呼的话，但对于每一层中的每一个参数，我们只是在堆叠所有这些片段，所有这些参数，现在总共有46000个参数，我在告诉。
- en: patters that all of them require gradient then here we have everything here
    we are actually。 mostly used to we are sampling batch we are doing forward pass
    the forward pass now is just a linear。 application of all the layers in order
    followed by the cross entropy and then in the backward pass。 you'll notice that
    for every single layer I now iterate over all the outputs and I'm telling patters。
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些模式都需要梯度，这里我们有一切，实际上我们大多数是习惯于抽样批次，我们在进行前向传播，前向传播现在只是所有层的线性应用，随后是交叉熵，然后在反向传播中，你会注意到对于每一层，我现在迭代所有输出，并告诉模式。
- en: to retain the gradient of them and then here we are already used to all the
    all the gradients。 sent to none do the backward to fill in the gradients do an
    update using stochastic gradient sent。
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 保留它们的梯度，这里我们已经习惯了所有梯度，发送为无，进行反向传播以填充梯度，使用随机梯度进行更新。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_380.png)'
  id: totrans-515
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_380.png)'
- en: and then track some statistics and then I am going to break after a single iteration
    now here in this。
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 然后跟踪一些统计信息，之后我将在单次迭代后中断，现在在这里。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_382.png)'
  id: totrans-517
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_382.png)'
- en: cell in this diagram I'm visualizing the histograms the histograms of the forward
    pass activations。 and I'm specifically doing it at the 10 h layers so iterating
    over all the layers except for the very。 last one which is basically just the
    softmax layer if it is a 10 h layer and I'm using a 10 h。 layer just because they
    have a finite output negative one to one and so it's very easy to visualize here。
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图表中的单元格中，我正在可视化前向传播激活的直方图，我特别在10h层中进行，所以迭代所有层，除了最后一层，基本上就是softmax层，如果这是一个10h层，我使用10h层只是因为它们有有限的输出，从负一到一，所以在这里可视化非常简单。
- en: so you see negative one to one it's a finite range and it is to work with I
    take the out tensor from that。 layer into t and then I'm calculating the mean
    the standard deviation and the percent saturation。 of t and the way I define the
    percent saturation is that t dot absolute value is greater than 0。97。 so that
    means we are here at the tails of the 10 h and remember that when we are in the
    tails of。
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你看到从负一到一，这是一个有限范围，且可以进行操作。我将从那个层输出的张量取出，赋值给t，然后我计算t的均值、标准差和百分比饱和度。我定义百分比饱和度的方式是t的绝对值大于0.97。所以这意味着我们在10H的尾部，记住当我们在尾部时。
- en: the 10 h that will actually stop gradients so we don't want this to be too high
    now here I'm calling。 torch dot histogram and then I'm plotting this histogram
    so basically what this is doing is that。
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 10H实际上会停止梯度，因此我们不希望这个值过高。现在在这里我调用torch.dot.histogram，然后我绘制这个直方图，因此基本上这正在做的事情是。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_384.png)'
  id: totrans-521
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_384.png)'
- en: every different type of layer and they all have a different color we are looking
    at how many。 values in these testers take on any of the values below on this axis
    here so the first layer is。 fairly saturated here at 20% so you can see that it's
    got tails here but then everything sort of。 stabilizes and if we had more layers
    here it would actually just stabilize at around the standard。
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 每种不同类型的层都有不同的颜色，我们正在查看这些测试者中有多少值取这个轴上任何值，因此第一层在20%处相当饱和，所以你可以看到它有尾部，但之后一切都会稳定，如果我们这里有更多层，它实际上会在标准附近稳定。
- en: deviation of about 0。65 and the saturation would be roughly 5% and the reason
    that this stabilizes and。 gives us a nice distribution here is because gain is
    set to 5 over 3 now here this gain you see that。
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差约为0.65，饱和度大约为5%，而这能稳定并给我们带来良好分布的原因是增益设置为5/3，现在在这里你看到这个增益。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_386.png)'
  id: totrans-524
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_386.png)'
- en: by default we initialize with one over square root of fan in but then here during
    initialization I。 come in and I iterate over all the layers and if it's a linear
    layer I boost that by the gain。 Now we saw that one so basically if we just do
    not use a gain then what happens if I redraw this。
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，我们用1除以fan in的平方根进行初始化，但在这里初始化时，我会遍历所有层，如果是线性层，我就会通过增益进行提升。现在我们看到1，所以基本上如果我们不使用增益，那么重新绘制这个会发生什么。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_388.png)'
  id: totrans-526
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_388.png)'
- en: you will see that the standard deviation is shrinking and the saturation is
    coming to zero。 and basically what's happening is the first layer is you know
    pretty decent but then further layers。 are just kind of like shrinking down to
    zero and it's happening slowly but it's shrinking to zero。 and the reason for
    that is when you just have a sandwich of linear layers alone then a then。
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到标准差在缩小，饱和度接近于零。基本上，第一层还算不错，但进一步的层就像是缩小到零，并且这一过程缓慢发生，但它正在缩小到零。原因是当你仅仅有一层线性层的“夹心”时，那么就会。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_390.png)'
  id: totrans-528
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_390.png)'
- en: initializing our weights in this manner we saw previously would have conserved
    the standard deviation。 of one but because we have this interspersed 10H layers
    in there these 10 linear layers are。 squashing functions and so they take your
    distribution and they slightly squash it and so。 some gain is necessary to keep
    expanding it to fight the squashing so it just turns out that 5 over 3。
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式初始化我们的权重，之前我们看到过，这会保持标准差为1，但因为我们有这个交错的10H层，这些10个线性层是压缩函数，因此它们会稍微压缩你的分布，所以需要一些增益来持续扩展，以对抗压缩，因此结果就是5/3。
- en: is a good value so if we have something too small like one we saw that things
    will come towards zero。 but if it's something too high let's do two then here
    we see that um。
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个好的值，所以如果我们有一个太小的值，比如1，我们看到事物会向零靠近。但如果是一个太高的值，比如2，那么我们看到。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_392.png)'
  id: totrans-531
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_392.png)'
- en: well let me do something a bit more extreme because so it's a bit more visible
    let's try three。
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，让我做一些更极端的事情，因为这样会更明显，让我们试试三。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_394.png)'
  id: totrans-533
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_394.png)'
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_395.png)'
  id: totrans-534
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_395.png)'
- en: okay so we see here that the saturation is going to be way too large okay so
    three would create。
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，所以我们看到这里的饱和度会过大，好的，所以三会产生。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_397.png)'
  id: totrans-536
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_397.png)'
- en: way too saturated activations so 5 over 3 is a good setting for a sandwich of
    linear layers。
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 过于饱和的激活，所以5/3是线性层组合的良好设置。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_399.png)'
  id: totrans-538
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_399.png)'
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_400.png)'
  id: totrans-539
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_400.png)'
- en: with 10H activations and it roughly stabilizes the standard deviation at a reasonable
    point。
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 使用10H激活，标准差在合理范围内大致稳定。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_402.png)'
  id: totrans-541
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_402.png)'
- en: now honestly i have no idea where 5 over 3 came from in pytorch when we were
    looking at the coming。
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 说实话，我不知道在PyTorch中5/3是从哪里来的，当时我们在查看这些内容。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_404.png)'
  id: totrans-543
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_404.png)'
- en: initialization i see empirically that it stabilizes this sandwich of linear
    and 10H and that the。 saturation is in a good range but i didn't actually know
    if this came out of some math formula i tried。 searching briefly for where this
    comes from but i wasn't able to find anything but certainly we。
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 从初始化来看，我经验上发现它稳定了这个线性和10H的组合，并且饱和度处于良好范围内，但我实际上并不知道这是否来自某个数学公式。我曾尝试简单搜索一下这个来源，但没有找到任何东西，但可以肯定的是我们。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_406.png)'
  id: totrans-545
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_406.png)'
- en: see that empirically these are very nice ranges our saturation is roughly 5%
    which is a pretty good。 number and this is a good setting of the gain in this
    context similarly we can do the exact same。 thing with the gradients so here is
    a very same loop if it's a 10H but instead of taking the layer。 that out i'm taking
    the grad and then i'm also showing the mean and aesthetic deviation and i'm。
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 实证数据显示这些都是非常好的范围，我们的饱和度大约是5%，这是个不错的数字，在这种情况下这是增益的良好设置。同样，我们可以用梯度做完全相同的事情，这里是一个非常相同的循环，如果是10H，但我不再取出层，而是取出梯度，然后展示均值和标准偏差，我。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_408.png)'
  id: totrans-547
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_408.png)'
- en: plotting the histogram of these values and so you'll see that the gradient distribution
    is fairly。 reasonable and in particular what we're looking for is that all the
    different layers in this。 sandwich has roughly the same gradient things are not
    shrinking or exploding so we can for example。
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制这些值的直方图，你会看到梯度分布相当合理，特别是我们希望的是，这个组合中的不同层的梯度大致相同，既没有缩小也没有爆炸，因此我们可以。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_410.png)'
  id: totrans-549
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_410.png)'
- en: come here and we can take a look at what happens if this gain was way too small
    so this was 0。5。 then you see the first of all the activations are shrinking to
    zero but also the gradients are。 doing something weird the gradients started out
    here and then now they're like expanding out。 and similarly if we for example
    have a too high of a gain so like three then we see that also the。
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 来这里，我们可以看看如果这个增益过小会发生什么，比如设置为0.5。这时你会看到所有激活都在缩小到零，而梯度也出现了奇怪的现象，梯度从这里开始，而现在它们像是扩展出去。同样，如果我们有一个过高的增益，比如三，那么我们也会看到。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_412.png)'
  id: totrans-551
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_412.png)'
- en: gradients have there's some asymmetry going on where as you go into deeper and
    deeper layers。
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度存在一些不对称现象，随着层数加深而加剧。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_414.png)'
  id: totrans-553
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_414.png)'
- en: the activations are also changing and so that's not what we want and in this
    case we saw that。 without the use of batch term as we are going through right
    now we have to very carefully set。
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 激活也在变化，所以这不是我们想要的。在这种情况下，我们看到，如果不使用批量归一化，如今我们必须非常小心地设置。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_416.png)'
  id: totrans-555
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_416.png)'
- en: those gains to get nice activations in both the forward pass and the backward
    pass。
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 这些增益在前向传递和反向传递中都能产生良好的激活。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_418.png)'
  id: totrans-557
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_418.png)'
- en: now before we move on to pass normalization i would also like to take a look
    at what happens。
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续进行归一化之前，我还想看看会发生什么。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_420.png)'
  id: totrans-559
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_420.png)'
- en: when we have no 10H units here so erasing all the 10H nonlinearities but keeping
    the gain at 5/3。 we now have just a giant linear sandwich so let's see what happens
    to the activations。 as we saw before the correct gain here is one that is the
    standard deviation preserving gain， so 1。667 is too high and so what's going to
    happen now is the following。
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们这里没有10H单元时，去掉所有10H的非线性，但保持增益为5/3。我们现在只剩下一个巨大的线性夹心层，所以让我们看看激活发生了什么。正如我们之前看到的，这里的正确增益是保持标准差的增益，因此1.667太高了，因此接下来将发生以下情况。
- en: oh i have to change this to be linear so we are because there's no more 10H
    players。 and let me change this to linear as well so what we're seeing is the
    activations started out on the blue。 and have by layer four become very diffuse
    so what's happening to the activations is this。
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 哦，我得把这个改成线性，因为没有更多的10H单元了。让我也把这个改成线性，所以我们看到的激活一开始是蓝色的。到第四层时变得非常分散，所以激活发生了什么呢？
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_422.png)'
  id: totrans-562
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_422.png)'
- en: and with the gradients on the top layer the activation the gradient statistics
    are the purple。 and then they diminish as you go down deeper in the layers and
    so basically have an asymmetry。 like in the neural net and you might imagine that
    if you have very deep neural networks say like 50。 layers or something like that
    this just this is not a good place to be so that's why before。
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 在最上层的梯度激活中，梯度统计是紫色的。然后随着层数的加深，它们逐渐减小，因此基本上在神经网络中存在不对称性。你可以想象，如果你有非常深的神经网络，比如50层，或者类似的层数，这显然不是一个好的状态，所以在这之前。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_424.png)'
  id: totrans-564
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_424.png)'
- en: batch normalization this was incredibly tricky to to set in particular if this
    is too large of。
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化之前，这种设置非常棘手，尤其是当这个值过大时。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_426.png)'
  id: totrans-566
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_426.png)'
- en: a gain this happens and if it's too little to gain then this happens also the
    opposite of that。
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 增益再次发生，如果增益过小，那么也会发生相反的情况。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_428.png)'
  id: totrans-568
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_428.png)'
- en: basically happens here we have a um shrinking and a diffusion depending on which
    direction we look。
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上在这里发生了，我们有一个缩小和扩散，具体取决于我们观察的方向。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_430.png)'
  id: totrans-570
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_430.png)'
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_431.png)'
  id: totrans-571
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_431.png)'
- en: at it from and so certainly this is not what you want and in this case the correct
    setting of the。
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 从中可以看出，这显然不是你想要的，在这种情况下，正确的设置是。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_433.png)'
  id: totrans-573
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_433.png)'
- en: gain is exactly one just like we're doing at initialization and then we see
    that the statistics。
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 增益正好是1，就像我们在初始化时做的那样，然后我们看到统计数据。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_435.png)'
  id: totrans-575
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_435.png)'
- en: for the forward and the backward pass are well behaved and so the reason i want
    to show you this is。
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 正向和反向传播表现良好，所以我想向你展示这个原因是。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_437.png)'
  id: totrans-577
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_437.png)'
- en: but basically like getting neuralness to train before these normalization layers
    and before the。
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 但基本上，就像让神经网络在这些归一化层之前进行训练一样。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_439.png)'
  id: totrans-579
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_439.png)'
- en: use of advanced optimizers like atom which we still have to cover and residual
    connections and so on。 training neural lines basically look like this it's like
    a total balancing act you have to make。 sure that everything is precisely orchestrated
    and you have to care about the activations and。 ingredients and their statistics
    and then maybe you can train something but it was basically。
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 使用像Adam这样的高级优化器，我们仍需涵盖的残差连接等。训练神经网络基本上看起来就是这样，这就像一场完全的平衡行动，你必须确保一切都精确协调，你必须关注激活和其统计数据，然后也许你能训练出一些东西，但基本上是这样的。
- en: impossible to train very deep networks and this is fundamentally the reason
    for that。 it you'd have to be very very careful with your initialization um the
    other point here is。 you might be asking yourself by the way i'm not sure if i
    covered this why do we need these 10H。 layers at all why do we include them and
    then have to worry about the game and the reason for that。
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 训练非常深的网络几乎是不可能的，这基本上就是原因所在。你必须非常非常小心地进行初始化，嗯，另一点是。你可能会问，顺便说一下，我不确定我是否讲过，为什么我们需要这些10H层呢？我们为什么要包括它们，然后还要担心增益？原因就在于此。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_441.png)'
  id: totrans-582
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_441.png)'
- en: of course is that if you just have a stack of linear layers then certainly we're
    getting very easily。
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果你只是有一堆线性层，那么我们肯定会非常容易。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_443.png)'
  id: totrans-584
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_443.png)'
- en: nice activations and so on but this is just a massive linear sandwich and it
    turns out that it。
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 好的激活等等，但这只是一个庞大的线性三明治，事实证明。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_445.png)'
  id: totrans-586
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_445.png)'
- en: collapses to a single linear layer in terms of its representation power so if
    you were to plot。 the output as a function of the input you're just getting a
    linear function no matter how many linear。 layers you stack up you still just
    end up with a linear transformation all the wx plus b's just。 collapse into a
    large wx plus b with slightly different w's as likely different b but interestingly。
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 在表示能力方面，模型最终简化为一个单一的线性层，因此如果你绘制输入的函数输出，无论你堆叠多少线性层，得到的结果都是一个线性函数。所有的 wx 加上 b
    最终都合并为一个大的 wx 加上 b，只是 w 和 b 可能有所不同，但有趣的是。
- en: even though the forward pass collapses to just a linear layer because of back
    propagation and。 the dynamics of the backward pass the optimization is really
    is not identical you actually end up with。 all kinds of interesting dynamics in
    the backward pass because of the the way the chain rule is。 calculating it and
    so optimizing a linear layer by itself and optimizing a sandwich of 10 linear。
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传播简化为仅一个线性层，但由于反向传播和反向传播的动态，优化过程实际上并不完全相同。你实际上会在反向传播中遇到各种有趣的动态，这是因为链式法则的计算方式。因此，优化单个线性层和优化十个线性层的“三明治”是不同的。
- en: layers in both cases those are just a linear transformation in the forward pass
    but the training。 dynamics would be different and there's entire papers that analyze
    in fact like infinitely layered。 linear layers and so on and so there's a lot
    of things too that you can play with there but。 basically the 10-ish linearities
    allow us to turn this sandwich from just a linear。
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，前向传播都是线性变换，但训练动态是不同的，实际上有整篇论文分析像无限层线性层等内容，这里有很多可以尝试的事情，但基本上这十个线性层允许我们将这个三明治从仅仅是线性变换。
- en: function into a neural network that can in principle approximate any arbitrary
    function。 okay so now i've reset the code to use the linear 10-ish sandwich like
    before and i reset everything。
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 函数可以嵌入到神经网络中，原则上可以近似任何任意函数。好的，现在我将代码重置为使用大约 10 个线性层的“三明治”结构，并重置了所有内容。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_447.png)'
  id: totrans-591
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_447.png)'
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_448.png)'
  id: totrans-592
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_448.png)'
- en: so the gains five over three we can run a single step of optimization and we
    can look at the。
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 所以增益为五除以三，我们可以运行一次优化步骤，并观察。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_450.png)'
  id: totrans-594
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_450.png)'
- en: activation statistics of the forward pass and the backward pass but i've added
    one more plot here。
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 前向传播和反向传播的激活统计，但我在这里增加了一个图。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_452.png)'
  id: totrans-596
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_452.png)'
- en: that i think is really important to look at when you're training your neural
    nets and to consider。 and ultimately what we're doing is we're updating the parameters
    of the neural net so we care about。 the parameters and their values and their
    gradients so here what i'm doing is i'm actually iterating。 over all the parameters
    available and then i'm only um restricting it to the two-dimensional。
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这是在训练神经网络时需要关注的重要因素，最终我们所做的是更新神经网络的参数，因此我们关心的是参数及其值和梯度。因此，我实际上是在遍历所有可用的参数，然后我只将其限制在二维。
- en: parameters which are basically the weights of these linear layers and i'm skipping
    the biases。 and i'm skipping the um gammas and the betas in the bathroom just
    for simplicity but you can also。 take a look at those as well but what's happening
    with the weights is um instructive by itself。 so here we have all the different
    weights their shapes uh so this is the embedding layer the first。
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数基本上是这些线性层的权重，我跳过了偏置项，也跳过了简单起见的伽玛和贝塔，但你也可以查看它们。权重的变化本身就很有启发性。所以这里我们有所有不同权重的形状，这里是嵌入层的第一个。
- en: linear layer all the way to the very last linear layer and then we have the
    mean the standard deviation。 of all these parameters the histogram and you can
    see that actually doesn't look that amazing。 so there's some trouble and paradise
    even though these gradients look okay there's something weird。
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 线性层一直到最后一层，然后我们有所有这些参数的均值和标准差的直方图，你可以看到这实际上看起来并不那么出色。因此，即使这些梯度看起来还可以，依然有些奇怪。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_454.png)'
  id: totrans-600
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_454.png)'
- en: going on here i'll get to that in a second and the last thing here is the gradient
    to data ratio。 so sometimes i like to visualize this as well because what this
    gives you a sense of is what is the。 scale of the gradient compared to the scale
    of the actual values and this is important because。 we're going to end up taking
    a step update um that is the learning rate times the gradient onto the。
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生了什么，我稍后会解释，最后一点是数据与梯度的比率。因此，有时我也喜欢将其可视化，因为这可以让你感知梯度的规模与实际值的规模相比如何，这很重要，因为我们将进行一次步长更新，即学习率乘以梯度。
- en: data and so the gradient has too large of magnitude if the numbers in there
    are too large compared to。 the numbers in data then you'd be in trouble but in
    this case the gradient to data is our。 low numbers so the values inside grad are
    one thousand times smaller than the values inside。 data in these weights most
    of them now notably that is not true about the last layer and so the。
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 数据，因此梯度的大小过大，如果里面的数字与数据相比太大，那么你会陷入困境，但在这种情况下，数据的梯度是较低的数字，所以在这些权重中，grad的值比data中的值小一千倍，尤其是在最后一层。
- en: last layer actually here the output layer is a bit of a troublemaker in the
    way that this is。 currently arranged because you can see that the um last layer
    here in pink takes on values that。 are much larger than some of the values inside
    inside the neural nut so the standard deviations。 are roughly one in negative
    three throughout except for the last but last layer which actually。
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一层实际上是有点麻烦，因为目前的安排使得这一层（粉色）取值比神经元内部的一些值大得多。所以标准差大致在负三附近，除了最后一层。
- en: has roughly one in negative two standard deviation of gradients and so the gradients
    on the last。 layer are currently about 100 times greater sorry 10 times greater
    than all the other weights inside。 the neural nut and so that's problematic because
    in the simplest stochastic gradient in this sense。 setup you would be training
    this last layer about 10 times faster than you would be training the。
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度的标准差大约在负二附近，所以最后一层的梯度目前大约是其他神经元权重的100倍，抱歉，是10倍，这很成问题，因为在最简单的随机梯度下降设置中，你将训练最后一层的速度比训练其他层快约10倍。
- en: other layers at initialization now this actually like kind of fixes itself a
    little bit if you train。
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 其他层在初始化时，现在实际上如果你进行训练，这会有点自我修复。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_456.png)'
  id: totrans-606
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_456.png)'
- en: for a bit longer so for example if i greater than one thousand only then do
    a break let me。
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 稍微长一点，例如，如果我大于1000，那么就中断，让我。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_458.png)'
  id: totrans-608
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_458.png)'
- en: reinitialize and then let me do it one thousand steps and after one thousand
    steps we can look at the。
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 重新初始化，然后让我进行1000步，经过1000步后我们可以查看。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_460.png)'
  id: totrans-610
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_460.png)'
- en: forward pass okay so you see how the neurons are a bit are saturating a bit
    and we can also look。
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 前向传播还可以，所以你可以看到神经元有一点饱和，我们也可以观察一下。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_462.png)'
  id: totrans-612
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_462.png)'
- en: at the backward pass but otherwise they look good they're about equal and there's
    no shrinking to。
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播时，除此之外它们看起来良好，基本相等，没有收缩。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_464.png)'
  id: totrans-614
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_464.png)'
- en: zero or exploding to infinities and you can see that here in the weights things
    are also stabilizing。
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 除零或爆炸到无穷大，你可以在权重中看到这里的东西也在稳定。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_466.png)'
  id: totrans-616
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_466.png)'
- en: a little bit so the tails of the last pink layer are actually coming coming
    in during the optimization。 but certainly this is like a little bit troubling
    especially if you are using a very simple update。
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_468.png)'
  id: totrans-618
  prefs: []
  type: TYPE_IMG
- en: rule like stochastic gradient descent instead of a modern optimizer like atom
    now i'd like to show。
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_470.png)'
  id: totrans-620
  prefs: []
  type: TYPE_IMG
- en: you one more plot that i usually look at when i train neural networks and basically
    the gradient。 to data ratio is not actually that informative because what matters
    at the end is not the gradient。 to data ratio but the update to the data ratio
    because that is the amount by which we will actually。
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_472.png)'
  id: totrans-622
  prefs: []
  type: TYPE_IMG
- en: change the data in these tensors so coming up here what i'd like to do is i'd
    like to introduce。 a new update to data ratio it's going to be less than we're
    going to build it out every single。 iteration and here i'd like to keep track
    of basically the ratio every single iteration。 so without any ingredients i'm
    comparing the update which is learning rate times the。
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
- en: times the gradient that is the update that we're going to apply to every parameter。
    socio-material world of parameters and then i'm taking the basically standard
    deviation of the。 update we're going to apply and divided by the actual content
    the data of that parameter and its。 standard deviation so this is the ratio of
    basically how great are the updates to the values in these。
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
- en: tensors then we're going to take a log of it and actually i'd like to take a
    log 10 just so it's a。 nice service realization so we're going to be basically
    looking at the exponents of the。 of this division here and then that item to pop
    out the float and we're going to be keeping track。 of this for all the parameters
    and adding it to this UD tensor so now let me re-inertilize in。
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_474.png)'
  id: totrans-626
  prefs: []
  type: TYPE_IMG
- en: one thousand iterations we can look at the activations the gradients and the
    parameter gradients as we。
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_476.png)'
  id: totrans-628
  prefs: []
  type: TYPE_IMG
- en: did before but now i have one more plot here to introduce now what's happening
    here is where every。
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_478.png)'
  id: totrans-630
  prefs: []
  type: TYPE_IMG
- en: interval will be parameters and i'm constraining it again like i did here to
    just two weights so。
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_480.png)'
  id: totrans-632
  prefs: []
  type: TYPE_IMG
- en: the number of dimensions in these sensors is two and then i'm basically plotting
    all of these。 update ratios over time so when i plot this i plot those ratios
    and you can see that they。
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_482.png)'
  id: totrans-634
  prefs: []
  type: TYPE_IMG
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_483.png)'
  id: totrans-635
  prefs: []
  type: TYPE_IMG
- en: evolve over time during initialization that they account certain values and
    then these updates。 are like start stabilizing usually during training then the
    other thing that i'm plotting here is i'm。 plotting here like an approximate value
    that is a rough guide for what it roughly should be and。 it should be like roughly
    one in negative three and so that means that basically there's some values。
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始化期间随时间演变，它们会计算某些值，然后这些更新。通常在训练期间开始稳定，另外我在这里绘制的是一个近似值，作为粗略的指导，它应该大约是负三左右，这意味着基本上有一些值。
- en: in this tensor and they take on certain values and the updates to them at every
    single iteration。 are no more than roughly one thousand of the actual like magnitude
    in those tensors if this。 was much larger like for example if this was um if the
    log of this was like saying negative one。 this is actually updating those values
    quite a lot they're undergoing a lot of change but the。
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个张量中，它们具有某些值，并且在每一次迭代中的更新不超过那些张量实际大小的千分之一。如果这个值大得多，比如说，如果这个对数是负一，那么它实际上在更新这些值相当多，它们经历了很大的变化，但。
- en: reason that the final rate the final layer here is an outlier is because this
    layer was artificially。 shrunk down to keep the softmax in come unconfident so
    here you see how we multiply the weight by point。
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一层是异常值的原因是因为这一层被人为缩小，以保持softmax的输出不自信，所以这里你会看到我们将权重乘以点。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_485.png)'
  id: totrans-639
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_485.png)'
- en: one in the initialization to make the last layer prediction less confident that
    made that artificially。
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始化时将最后一层的预测变得不那么自信，这是人为造成的。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_487.png)'
  id: totrans-641
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_487.png)'
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_488.png)'
  id: totrans-642
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_488.png)'
- en: made the values inside that tensor way too low and that's why we're getting
    temporarily a very high。 ratio but you see that that stabilizes over time once
    that weight starts to learn starts to learn。 but basically i like to look at the
    evolution of this update ratio for all my parameters usually。 and i like to make
    sure that it's not too much above one negative three roughly so around negative
    three。
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 使得那个张量内部的值过低，这就是为什么我们暂时得到一个非常高的比率，但你会看到，随着时间的推移它会稳定下来，一旦权重开始学习。基本上，我喜欢观察这个更新比率的演变，通常针对我的所有参数。我喜欢确保它不超过负三。
- en: on this log plot if it's below negative three usually that means that the parameters
    are not。
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个对数图上，如果低于负三，通常意味着参数没有。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_490.png)'
  id: totrans-645
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_490.png)'
- en: training fast enough so if our learning rate was very low let's do that experiment
    let's initialize。 and then let's actually do a learning rate of say one in negative
    three here so 0。001 if you're。 learning is way too low this plot will typically
    reveal it so you see how all of these updates are。 way too small so the size of
    the update is basically 10，000 times in magnitude to the size of the。
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 训练速度太快，如果我们的学习率非常低，那就进行这个实验，初始化一下。然后我们实际设置一个学习率，比如这里的负三，即0.001。如果你的学习太低，这个图通常会显示出来，你会看到所有的更新都太小了，所以更新的大小基本上是10,000倍于。
- en: numbers in that tensor in the first place so this is a symptom of training way
    too slow。 so this is another way to sometimes set the learning rate and to get
    a sense of what that。 learning rate should be and ultimately this is something
    that you would keep track of。
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: 在那个张量中的数字首先是一个训练速度过慢的症状。所以这也是有时设置学习率的一种方式，以了解学习率应该是什么，最终这是你需要跟踪的内容。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_492.png)'
  id: totrans-648
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_492.png)'
- en: if anything the learning rate here is a little bit on the higher side because
    you see that。 we're above the black line of negative three we're somewhere around
    negative 2。5 it's like okay。
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: 如果说这里的学习率有点高，那是因为你可以看到我们在负三的黑线之上，差不多在负2.5左右，感觉还可以。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_494.png)'
  id: totrans-650
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_494.png)'
- en: and but everything is like somewhat stabilizing and so this looks like a pretty
    decent setting of。 of learning rates and so on but this is something to look at
    and when things are。 miscalibrated you will see very quickly so for example everything
    looks pretty well behaved right。
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: 一切看起来都有点稳定，这看起来是一个相当不错的学习率设置，但这是需要关注的，当事情误校准时，你会很快看到，比如说，一切看起来都相当正常，对吧。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_496.png)'
  id: totrans-652
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_496.png)'
- en: but just as a comparison when things are not properly calibrated what does that
    look like。 let me come up here and let's say that for example what do we do let's
    say that we forgot to apply。 this fan in normalization so the weights inside the
    linear layers are just sampled from a gosh in。 in all those stages what happens
    to our how do we notice that something's off well the activation。
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: 但作为比较，当事情没有正确校准时，情况是什么样的呢？让我上来，比如说我们做了什么，假设我们忘记应用这个风扇归一化，所以线性层内部的权重只是从高斯分布中采样。在所有这些阶段，我们如何注意到有什么问题呢？激活。
- en: plot will tell you whoa your neurons are way too saturated the gradients are
    going to be all messed。 up the histogram for these weights are going to be all
    messed up as well and there's a lot of。 asymmetry and then if we look here I suspect
    it's all going to be also pretty messed up so you see。 there's a lot of discrepancy
    in how fast these layers are learning and some of them are learning。
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 图表会告诉你，哇，你的神经元过于饱和，梯度会完全混乱，这些权重的直方图也会搞砸，存在很多不对称。如果我们在这里看，我怀疑这也会相当混乱，所以你会看到，这些层学习的速度有很大的差异，有些正在学习。
- en: way too fast so negative 1 negative 1。5 those are very large numbers in terms
    of this ratio again。 you should be somewhere on negative 3 and not much more about
    that so this is how。 miscalibrations of your neural nets are going to manifest
    and these kinds of plots here are a good。
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 太快了，所以负1和负1.5，这些在这个比例上是非常大的数字。你应该在负3左右，而不是更多，这就是神经网络误校准的表现，这些图表是一个很好的指示。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_498.png)'
  id: totrans-656
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_498.png)'
- en: way of sort of bringing those miscalibrations sort of to your attention and
    so you can address them。
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 一种引起你注意并解决那些误校准的方法。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_500.png)'
  id: totrans-658
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_500.png)'
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_501.png)'
  id: totrans-659
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_501.png)'
- en: okay so far we've seen that when we have this linear 10-H sandwich we can actually
    precisely。 calibrate the gains and make the activations the gradients and the
    parameters and the updates。 all look pretty decent but it definitely feels a little
    bit like balancing of a pencil on your。 finger and that's because this gain has
    to be very precisely calibrated so now let's introduce。
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到，当我们有这个线性10-H夹心时，我们实际上可以精确校准增益，使得激活、梯度、参数和更新看起来相当不错，但确实感觉有点像在你的手指上平衡铅笔，因为这个增益必须非常精确地校准，所以现在让我们引入。
- en: Bachelorette's Bachelorette's into the mix and let's let's see how that tops
    fix the problem。 so here I'm going to take the Bachelorette 1D class and I'm going
    to start placing it inside。 and as I mentioned before the standard typical place
    you would place it is between the linear。 layer so right after it but before the
    nonlinearity but people have definitely played with that and。
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: 将单身女郎的元素加入其中，让我们看看如何修复这个问题。所以在这里，我将把单身女郎1D类放进去。正如我之前提到的，典型的放置位置是在线性层之间，所以在其之后但在非线性之前，但人们确实尝试过其他方式。
- en: in fact you can get very similar results even if you place it after the nonlinearity。
    and the other thing that I wanted to mention is it's totally fine to also place
    it at the end。 after the last linear layer and before the loss function so this
    is potentially fine as well。 and in this case this would be output would be vocab
    size。
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，即使你在非线性之后放置它，你也可以得到非常相似的结果。我想提到的另一件事是，将其放在最后一个线性层之后和损失函数之前也是完全可以的，所以在这种情况下，这样的输出将是词汇大小。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_503.png)'
  id: totrans-663
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_503.png)'
- en: now because the last layer is a best-room we would not be changing the weight
    to make the。 softmax less confident we'd be changing the gamma because gamma remember
    in the。 best-room is the variable that multiplicative way interacts with the output
    of that normalization。 so we can initialize this sandwich now we can train and
    we can see that the activations are。
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_505.png)'
  id: totrans-665
  prefs: []
  type: TYPE_IMG
- en: going to of course look very good and they are going to necessarily look at
    because now before。 every single 10H layer there is a normalization in the bachelor
    so this is unsurprisingly all。 looks pretty good it's going to be standard deviation
    of roughly 0。65 2% and roughly equal。 standard deviation throughout the entire
    layers so everything looks very homogeneous。
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
- en: the gradients look good the weights look good and their distributions and then
    the updates。
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_507.png)'
  id: totrans-668
  prefs: []
  type: TYPE_IMG
- en: also look pretty reasonable we're going above negative three a little bit but
    not by too much。 so all the parameters are training and roughly the same rate
    here。
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_509.png)'
  id: totrans-670
  prefs: []
  type: TYPE_IMG
- en: but now what we've gained is we are going to be slightly less。 brittle with
    respect to the gain of these so for example I can make the gain be say 0。2 here。
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_511.png)'
  id: totrans-672
  prefs: []
  type: TYPE_IMG
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_512.png)'
  id: totrans-673
  prefs: []
  type: TYPE_IMG
- en: which is much slower than when we have with the 10H but as we'll see the activations
    will。
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_514.png)'
  id: totrans-675
  prefs: []
  type: TYPE_IMG
- en: actually be exactly unaffected and that's because of again this explicit normalization。
    the gradients are going to look okay the weight gradients are going to look okay。
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_516.png)'
  id: totrans-677
  prefs: []
  type: TYPE_IMG
- en: but actually the updates will change and so even though the forward and backward
    pass to a very。
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_518.png)'
  id: totrans-679
  prefs: []
  type: TYPE_IMG
- en: large extent look okay because of the backward pass of the batch norm and how
    the scale of the。 incoming activations interacts in the batch norm and it's a
    backward pass this is actually changing。 the um the scale of the updates on these
    parameters so the gradients of these weights are affected。
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_520.png)'
  id: totrans-681
  prefs: []
  type: TYPE_IMG
- en: so we still don't get it completely free pass to pass in arbitrary weights here
    but everything else。
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_522.png)'
  id: totrans-683
  prefs: []
  type: TYPE_IMG
- en: is significantly more robust in terms of the forward backward and the weight
    gradients。 it's just that you may have to retune your learning rate if you are
    changing sufficiently。 the the scale of the activations that are coming into the
    batch norms so here for example this um。 we changed the gains of these linear
    layers to be greater and we're seeing that the updates are。
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
- en: coming out lower as a result and then finally we can also if we are using batch
    norms we don't。
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_524.png)'
  id: totrans-686
  prefs: []
  type: TYPE_IMG
- en: actually need to necessarily let me reset this to one so there's no gain we
    don't necessarily even。 have to um normalize back then in sometimes so if i take
    out the fan in so these are just now。
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，我们不需要一定将其重置为1，因此没有增益，我们甚至不一定需要。 有时不进行归一化，所以如果我去掉输入，那么这些现在只是。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_526.png)'
  id: totrans-688
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_526.png)'
- en: a random gosh in we'll see that because of batch norm this will actually be
    relatively well behaved。 so this is a look of course in the forward pass look
    good the gradients look good。 the backward the weight updates look okay a little
    bit of fat tails in some of the layers。
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: 一个随机的输入将显示出由于批归一化，这实际上会相对表现良好。因此，前向传播的表现不错，梯度看起来也很好。反向传播的权重更新也不错，但某些层中有一些极端值。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_528.png)'
  id: totrans-690
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_528.png)'
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_529.png)'
  id: totrans-691
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_529.png)'
- en: and this looks okay as well but as you as you can see we're significantly below
    negative three so。 we'd have to bump up the learning rate of this batch norm so
    that we are training more properly。 and in particular looking at this roughly
    looks like we have to 10x the learning rate to get to about。
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来也还可以，但正如你所见，我们明显低于负三。因此，我们需要提高这个批归一化的学习率，以便更好地训练。特别是看来我们需要将学习率提高到约10倍。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_531.png)'
  id: totrans-693
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_531.png)'
- en: one in negative three so we've come here and we would change this to be update
    of 1。0 and if。
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: 在负三，所以我们来这里，我们将其更改为更新1.0。如果。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_533.png)'
  id: totrans-695
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_533.png)'
- en: you and i re-emitialize then we'll see that everything still of course looks
    good and now。
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: 你和我重新初始化，那么我们会看到一切仍然看起来不错，现在。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_535.png)'
  id: totrans-697
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_535.png)'
- en: we are roughly here and we expect this to be an okay training run so long story
    short we are。
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: 我们大致在这里，我们期待这将是一次不错的训练运行。长话短说，我们是。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_537.png)'
  id: totrans-699
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_537.png)'
- en: significantly more robust to the gain of these linear layers whether or not
    we have to apply the。
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: 对这些线性层的增益显著更具鲁棒性，无论我们是否需要应用。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_539.png)'
  id: totrans-701
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_539.png)'
- en: fan in and then we can change the gain but we actually do have to worry a little
    bit about the。
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: 输入和增益可以更改，但我们确实需要稍微担心一下。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_541.png)'
  id: totrans-703
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_541.png)'
- en: update scales and making sure that the learning rate is properly calibrated
    here but the activations。 of the forward backward pass and the updates are all
    are looking significantly more well behaved。
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: 更新缩放，并确保学习率在这里得到适当校准，但前向和反向传播的激活以及更新都表现得明显更好。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_543.png)'
  id: totrans-705
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_543.png)'
- en: except for the global scale that is potentially being adjusted here okay so
    now let me summarize。 there are three things i was hoping to achieve with this
    section number one i wanted to introduce。 you to bachelor normalization which
    is one of the first modern innovations that we're looking into。
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: 除了可能在这里被调整的全局规模，好吧，现在让我总结一下。我希望通过这一部分实现三件事，第一，我想介绍。你了解学士归一化，这是我们正在研究的首批现代创新之一。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_545.png)'
  id: totrans-707
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_545.png)'
- en: that helped stabilize very deep neural networks and their training and i hope
    you understand how。 the bachelorization works and how it would be used in your
    own network number two i was hoping to。
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: 这有助于稳定非常深的神经网络及其训练，我希望你能理解。学士化是如何工作的，以及它如何在你的网络中使用。我原本希望能。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_547.png)'
  id: totrans-709
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_547.png)'
- en: pytorchify some of our code and wrap it up into these modules so like linear
    bachelor monday 10h etc these。
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: 将我们的部分代码转换为PyTorch，并将其包装到这些模块中，比如线性层、学士、星期一、10h等。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_549.png)'
  id: totrans-711
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_549.png)'
- en: are layers or modules and they can be stacked up into neural nets like lego
    building blocks。
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: 这些层或模块可以像乐高积木一样堆叠成神经网络。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_551.png)'
  id: totrans-713
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_551.png)'
- en: and these layers actually exist in pytorch and if you import torch and then
    then you can actually。
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 这些层实际上存在于PyTorch中，如果你导入torch，那么你可以实际地。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_553.png)'
  id: totrans-715
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_553.png)'
- en: the way i've constructed it you can simply just use pytorch by prepending an
    endot to all these。 different layers and actually everything will just work because
    the api that i have developed here。 is identical to the api that pytorch uses
    and the implementation also is basically as far as i'm aware。
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_555.png)'
  id: totrans-717
  prefs: []
  type: TYPE_IMG
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_556.png)'
  id: totrans-718
  prefs: []
  type: TYPE_IMG
- en: identical to the one in pytorch and number three i try to introduce you to the
    diagnostic tools that。
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_558.png)'
  id: totrans-720
  prefs: []
  type: TYPE_IMG
- en: you would use to understand whether your neural network is in a good state dynamically
    so we are。
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_560.png)'
  id: totrans-722
  prefs: []
  type: TYPE_IMG
- en: looking at the statistics and histograms and activation of the forward pass
    activation activations。
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_562.png)'
  id: totrans-724
  prefs: []
  type: TYPE_IMG
- en: the backward pass gradients and then also we're looking at the weights that
    are going to be updated。 as part of stochosigarity in ascent and we're looking
    at their means standard deviations and。
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_564.png)'
  id: totrans-726
  prefs: []
  type: TYPE_IMG
- en: also the ratio of gradients to data or even better the updates to data and we
    saw that typically we。
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_566.png)'
  id: totrans-728
  prefs: []
  type: TYPE_IMG
- en: don't actually look at it as a single snapshot frozen in time at some particular
    iteration。 typically people look at this as a over time just like i've done here
    and they look at these。 update to data ratios and they make sure everything looks
    okay and in particular i said that um。 one in negative three or basically negative
    three on the lock scale is a good uh rough。
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
- en: heuristic for what you want this ratio to be and if it's way too high then probably
    the learning。 rate or the updates are equal to too big and if it's way too small
    that the learning rate is。 probably too small so that's just some of the things
    that you may want to play with when you。
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_568.png)'
  id: totrans-731
  prefs: []
  type: TYPE_IMG
- en: try to get your neural network to work very well now there's a number of things
    i did not try to。
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_570.png)'
  id: totrans-733
  prefs: []
  type: TYPE_IMG
- en: achieve i did not try to beat our previous performance as an example by introducing
    the。 bathroom layer actually i did try and i found that i used the learning rate
    finding mechanism。 that i've described before i tried to train the bathroom layer
    a bathroom neural net and i actually。
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_572.png)'
  id: totrans-735
  prefs: []
  type: TYPE_IMG
- en: ended up with results that are very very similar to what we've obtained before
    and that's because。
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_574.png)'
  id: totrans-737
  prefs: []
  type: TYPE_IMG
- en: our performance now is not bottlenecked by the optimization which is what bash
    norm is helping with。 the performance at the stage is bottlenecked by what i suspect
    is the context length of our。 context so currently we are taking three characters
    to predict the fourth one and i think we need to go。 beyond that and we need to
    look at more powerful architectures like recurrent neural networks and。
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
- en: transformers in order to further push um the lock bar abilities that we're achieving
    on this data set。
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器进行全面解释，以进一步推动我们在这个数据集上所取得的能力。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_576.png)'
  id: totrans-740
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_576.png)'
- en: and i also did not try to have a full explanation of all of these activations
    the gradients and the。
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: 而且我也没有尝试对所有这些激活、梯度和。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_578.png)'
  id: totrans-742
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_578.png)'
- en: backward pass and the statistics of all these gradients and so you may have
    found some of the。
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播和所有这些梯度的统计，因此你可能发现了一些。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_580.png)'
  id: totrans-744
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_580.png)'
- en: parts here unintuitive and maybe you're slightly confused about okay if i change
    the gain here。
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的某些部分不太直观，可能你会稍微困惑，好吧，如果我在这里改变增益。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_582.png)'
  id: totrans-746
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_582.png)'
- en: how come that we need a different learning rate and i didn't go into the full
    detail because you'd。
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们需要不同的学习率，我没有详细讨论，因为你可能。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_584.png)'
  id: totrans-748
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_584.png)'
- en: have to actually look at the backward pass of all these different layers and
    get an intuitive。 understanding of how all that works and i did not go into that
    in this lecture the purpose really。
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须实际查看所有这些不同层的反向传播，并直观理解这一切是如何运作的，而我在这次讲座中并没有深入探讨这一点，目的真的。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_586.png)'
  id: totrans-750
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_586.png)'
- en: was just to introduce you to the diagnostic tools and what they look like but
    there's still a lot of。 work remaining on the intuitive level to understand the
    initialization the backward pass and how all。 that interacts but you shouldn't
    feel too bad because honestly we are getting to the cutting edge of。
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: 只是为了向你介绍诊断工具及其外观，但在直观层面上，理解初始化、反向传播及其相互作用仍有很多工作要做，但你不应该感到太糟糕，因为老实说，我们正处于前沿领域。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_588.png)'
  id: totrans-752
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_588.png)'
- en: where the field is we certainly haven't i would say solved initialization and
    we haven't solved。 back propagation and these are still very much an active area
    of research people are still trying。 to figure out where's the best way to initialize
    these networks what is the best update rule to use。 and so on so none of this
    is really solved and we don't really have all the answers to all the。
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个领域，我们可以肯定地说，初始化问题尚未解决，反向传播也没有解决，这些仍然是非常活跃的研究领域，人们仍在努力弄清楚初始化这些网络的最佳方法，使用什么是最佳更新规则等等，所以这些问题并没有真正解决，我们并没有对所有。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_590.png)'
  id: totrans-754
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_590.png)'
- en: to you know all these cases but at least you know we're making progress and
    at least we have some。
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
  zh: 情况有确切的答案，但至少你知道我们在取得进展，至少我们有一些。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_592.png)'
  id: totrans-756
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_592.png)'
- en: tools to tell us whether or not things are on the right track for now so i think
    we've made。
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: 工具可以告诉我们目前事情是否在正确的轨道上，所以我认为我们取得了一些进展。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_594.png)'
  id: totrans-758
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_594.png)'
- en: positive progress in this lecture and i hope you enjoyed that and i will see
    you next time。
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: 在这次讲座中取得了积极的进展，我希望你喜欢这次讲座，下次再见。
- en: '![](img/7e929a646a3b3c13dd787fc42b498be7_596.png)'
  id: totrans-760
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e929a646a3b3c13dd787fc42b498be7_596.png)'
