- en: P1：p1 The spelled-out intro to neural networks and backpropagation： bu - 加加zero
    - BV11yHXeuE9d
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P1：p1 神经网络和反向传播的详细介绍：bu - 加加zero - BV11yHXeuE9d
- en: Hello， my name is Andre and I've been training deep neural networks for a bit
    more than a decade and in this lecture。 I'd like to show you what neural network
    training looks like under the hood。 So in particular。 we are going to start with
    a blank jubyter notebook and by the end of this lecture。 we will define and train
    in your own that。 You know we get to see everything that goes on under the hood
    and exactly。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 嗨，我的名字是安德烈，我已经训练深度神经网络超过十年，在这次讲座中，我想向你展示神经网络训练的内部机制。因此，特别地，我们将从一个空白的Jupyter笔记本开始，到这次讲座结束时，我们将定义并训练你自己的网络。你会看到在这个过程中发生的所有事情。
- en: sort of how that works and intuitive level。 Now specifically what I would like
    to do is I would like to take you through。 building of micrograd。 Now micrograd
    is this library that I released on github about two years ago。 but at the time
    I only uploaded this source code and you'd have to go in by yourself and really。
    figure out how it works。 So in this lecture I will take you through it step by
    step and kind of comment on all the pieces of it。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 关于它的工作原理和直观层面。现在我特别想带你通过构建micrograd。micrograd是我大约两年前在GitHub上发布的这个库，但当时我只上传了源代码，你需要自己去弄清楚它是如何工作的。因此，在这次讲座中，我会一步一步带你了解它，并对它的所有部分进行评论。
- en: So what is micrograd and why is it interesting？ Okay， Micrograd is basically
    an autograd engine。 Autograd is short for automatic gradient and really what it
    does is it implements， back propagation。 Now back propagation is this algorithm
    that allows you to efficiently evaluate the gradient of。 some kind of a loss function
    with respect to the weights of a neural network and what that allows us to do
    then is we can。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 那么什么是micrograd，为什么它有趣？好吧，micrograd基本上是一个自动梯度引擎。自动梯度是automatic gradient的缩写，实际上它实现了反向传播。反向传播是一种算法，允许你有效地评估某种损失函数相对于神经网络权重的梯度，这使我们能够进行更进一步的操作。
- en: editively tune the weights of that neural network to minimize the loss function
    and therefore improve the accuracy of the network。 So back propagation would be
    at the mathematical core of any modern deep neural network library like say PyTorch
    or Jax。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 微调神经网络的权重以最小化损失函数，从而提高网络的准确性。因此，反向传播将是任何现代深度神经网络库的数学核心，例如PyTorch或Jax。
- en: So the functionality of micrograd is I think best illustrated by an example。
    So if we just scroll down here， you'll see that micrograd basically allows you
    to build out mathematical expressions and。 here what we are doing is we have an
    expression that we're building out where you have two inputs A and B and。 you'll
    see that A and B are negative four and two but we are wrapping those。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: micrograd的功能我认为最好通过一个例子来说明。因此，如果我们向下滚动，你会看到micrograd基本上允许你构建数学表达式。在这里，我们有一个正在构建的表达式，其中有两个输入A和B，你会看到A和B分别是负四和二，但我们将这些值封装到micrograd的一部分中。
- en: values into this value object that we are going to build out as part of micrograd。
    So this value object will wrap the numbers themselves and then we are going to
    build out a mathematical expression here where A and B are。 transformed into C，
    D and eventually E， F and G and I'm showing some of the function some of the functionality
    of micrograd and the operations that it supports。 So you can add two value objects。
    You can multiply them。 You can raise them to a constant power。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个数学表达式，在这个表达式中，A和B被转换为C、D，并最终变为E、F和G，我在展示micrograd的一些功能和它所支持的操作。所以你可以将两个值对象相加，可以将它们相乘，可以将它们提升到常数的幂。
- en: You can offset by one， negate， squash at zero， square， divide by constant， divide
    by it， etc。 And so we're building out an expression graph with these two inputs
    A and B and we're creating an output value of G and。 micrograd will in the background
    build out this entire mathematical expression。 So it will for example know that
    C is also a value。 C was a result of an addition operation and。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过偏移、取反、在零点压缩、平方、常数相除等方式进行操作。因此，我们正在用这两个输入A和B构建一个表达式图，并创建一个输出值G，micrograd将在后台构建整个数学表达式。因此，它会知道C也是一个值，C是一个加法操作的结果。
- en: the child nodes of C are A and B because the and all maintain pointers to A
    and B value objects。 So we'll basically know exactly how all of this is laid out
    and。 then not only can we do what we call the forward pass where we actually look
    at the value of G。 Of course， that's pretty straightforward。 We will access that
    using the dot data attribute and so the output of the forward pass the value of
    G is。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: C 的子节点是 A 和 B，因为它们都保持指向 A 和 B 值对象的指针。因此，我们将基本上确切知道这一切是如何布局的。然后，我们不仅可以进行所谓的前向传递，实际上查看
    G 的值。当然，这非常简单。我们将使用 dot data 属性访问它，因此前向传递的输出 G 的值是。
- en: 0。7 it turns out but the big deal is that we can also take this G value object
    and we can call dot backward and。 this will basically， initialize back propagation
    at the node G。 And what back propagation is going to do is it's going to start
    at G and it's going to go backwards through that expression graph。 and it's going
    to recursively apply the chain rule from calculus and。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是 0.7，但重要的是我们还可以调用这个 G 值对象的 dot backward，这基本上会在节点 G 处初始化反向传播。反向传播将从 G 开始，向后遍历那个表达式图，并递归应用微积分中的链式法则。
- en: what that allows us to do then is we're going to evaluate basically the derivative
    of G with respect to all the internal。 nodes like E， D and C but also with respect
    to the inputs A and B and。 then we can actually query this derivative of G with
    respect to A for example， that's A dot grad。 In this case， it happens to be one
    thirty eight and the derivative of G with respect to B which also happens to be
    here。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们能够基本上评估 G 相对于所有内部节点（如 E、D 和 C）以及输入 A 和 B 的导数。然后，我们实际上可以查询 G 关于 A 的导数，例如
    A dot grad。在这种情况下，它恰好是 一百三十八，而 G 关于 B 的导数也恰好在这里。
- en: 645 and this derivative we'll see soon is very important information because
    it's telling us how A and B are affecting。 B through this mathematical expression。
    So in particular A that grad is one thirty eight。 So if we slightly， nudge A and
    make it slightly larger。 one thirty eight is telling us that G will grow and the
    slope of that growth is going to be one thirty eight and。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 六百四十五及其导数，我们很快会看到，这非常重要，因为它告诉我们 A 和 B 如何通过这个数学表达式影响 B。特别地，A 的梯度是一百三十八。因此，如果我们稍微调整
    A 使其稍微增大，一百三十八告诉我们 G 将增长，而这个增长的斜率将是 一百三十八。
- en: the slope of growth of B is going to be six hundred forty five。 So that's going
    to tell us about how G will respond if A and B get tweaked a tiny amount in a
    positive direction。 Okay， Now you might be confused about what this expression
    is that we built out here and this expression by the way is completely meaningless。
    I just made it up。 I'm just flexing about the kinds of operations that are supported
    by micro grad。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: B 的增长斜率将是六百四十五。这将告诉我们，如果 A 和 B 轻微向正方向调整，G 将如何响应。好吧，现在你可能对我们这里构建的这个表达式感到困惑，顺便说一下，这个表达式是完全没有意义的。我只是随便编的。我只是展示微分梯度所支持的操作类型。
- en: What we actually really care about are neural networks。 But it turns out that
    neural networks are just mathematical expressions just like this one。 but actually
    slightly bit less crazy even。 Neural networks are just a mathematical expression。
    They take the input data as an input and they take the weights of a neural。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们真正关心的是神经网络。但事实证明，神经网络就像这个数学表达式一样。实际上，神经网络稍微不那么疯狂。神经网络仅仅是一个数学表达式。它们将输入数据作为输入，并获取神经网络的权重。
- en: network as an input and some mathematical expression and the output are your
    predictions of your neural net or the loss function。 We'll see this in a bit。
    But basically neural networks just happen to be a certain class of mathematical
    expressions。 But back propagation is actually significantly more general。 It doesn't
    actually care about neural networks at all。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 网络作为输入，以及一些数学表达式，输出则是你的神经网络预测值或损失函数。我们稍后会看到这一点。基本上，神经网络只是一类特定的数学表达式。但反向传播实际上要普遍得多，它根本不关心神经网络。
- en: It only tells about arbitrary mathematical expressions。 And then we happen to
    use that machinery for training of neural networks。 Now one more note I would
    like to make at this stage is that as you see here micro grad is a scalar valued
    autograd engine。 So it's working on the you know level of individual scalars like
    negative four and two。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 它仅仅讲述任意的数学表达式。然后我们恰好使用这一机制来训练神经网络。现在我想在这个阶段再提一点，正如你在这里看到的，micrograd是一个标量值的自动求导引擎。所以它是在单个标量的层面上工作的，比如负四和二。
- en: And we're taking neural nets and we're breaking them down all the way to these
    atoms of individual scalars and all the little pluses and times。 And it's just
    excessive。 And so obviously you would never be doing any of this in production。
    It's real just put down for pedagogical reasons because it allows us to not have
    to deal with these。 dimensional tensors that you would use in modern deep neural
    network library。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将神经网络分解到这些单个标量的原子和所有的小加法和乘法。其实这太过繁琐了。因此，显然你在生产环境中绝不会这样做。这纯粹是出于教学目的，因为它让我们不必处理这些现代深度神经网络库中会用到的多维张量。
- en: So this is really done so that you understand and refactor out back propagation
    and chain rule and understanding of your training。 And then if you actually want
    to train bigger networks， you have to be using these tensors。 But none of the
    math changes。 This is done purely for efficiency。 We are basically taking scale
    value all the scale values。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做是为了让你理解并重构反向传播和链式法则以及对训练的理解。如果你确实想训练更大的网络，你必须使用这些张量。但数学没有任何变化。这完全是出于效率。我们基本上将所有标量值打包成张量，这些张量就是这些标量的数组。
- en: We're packaging them up into tensors which are just arrays of these scalars。
    And then because we have these large arrays， we're making operations on those
    large arrays that allows us to take advantage of the parallelism in a computer。
    And all those operations can be done in parallel and then the whole thing runs
    faster。 But really none of the math changes and that done purely for efficiency。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然后因为我们有了这些大数组，我们对这些大数组进行操作，使我们能够利用计算机的并行性。所有这些操作都可以并行进行，然后整体运行得更快。但实际上，数学没有任何变化，这纯粹是为了效率。
- en: So I don't think that it's pedagogically useful to be dealing with tensors from
    scratch。 And I think and that's why I fundamentally wrote micrograd because you
    can understand how things work at the fundamental level。 And then you can speed
    up later。 Okay， so here's the fun part。 My claim is that micrograd is what you
    need to train neural networks and everything else is just efficiency。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我认为从零开始处理张量在教学上并没有什么用。这就是我根本上编写micrograd的原因，因为你可以在基础层面理解事物是如何运作的。然后你可以再加速。好了，接下来是有趣的部分。我的主张是，micrograd就是你训练神经网络所需的，其他的只是效率。
- en: So you'd think that micrograd would be a very complex piece of code。 And that
    turns out to not be the case。 So if we just go to micrograd and you will see that
    there's only two files here in micrograd。 This is the actual engine。 It doesn't
    know anything about neural nets。 And this is the entire neural nets library on
    top of micrograd。 So engine and and and dot pi。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你会认为micrograd会是一段非常复杂的代码，但事实并非如此。所以如果我们去看看micrograd，你会发现这里只有两个文件。这是实际的引擎。它对神经网络一无所知。这是整个基于micrograd的神经网络库。所以引擎和和和.dot
    pi。
- en: So the actual back propagation autograd engine that gives you the power of neural
    networks is literally。 a hundred lines of code of like very simple Python。 Which
    we'll understand by the end of this lecture。 And then and then and then that pie。
    This neural network library built on top of the autograd engine is like a joke。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 所以实际的反向传播自动求导引擎，赋予你神经网络的能力，实际上就是一百行简单的Python代码。我们将在本讲座结束时理解这一点。然后，那个派。这种建立在自动求导引擎之上的神经网络库简直就像一个笑话。
- en: It's like we have to define what is a neuron and then we have to define what
    is a layer of neurons。 and then we define what is a multilateral perceptron which
    is just a sequence of layers of neurons。 And so it's just a total joke。 So basically
    there's a lot of power that comes from only a hundred and fifty lines of code。
    And that's only need to understand to understand neural network training and everything
    else is just efficiency。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们必须定义什么是神经元，然后定义什么是神经元层。接着我们定义什么是多层感知器，它仅仅是神经元层的一个序列。所以这简直是个笑话。基本上，仅仅一百五十行代码就蕴含了巨大的能力。而这只是理解神经网络训练所需的，其他的都是效率问题。
- en: And of course there's a lot two efficiency but fundamentally that's all that's
    happening。 Okay so now let's dive right in and implement micro grad step by step。
    The first thing I'd like to do is I'd like to make sure that you have a very good
    understanding。 intuitively of what a derivative is and exactly what information
    it gives you。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，效率上还有很多内容，但从根本上来说，这就是所有发生的事情。好的，那么现在让我们一步一步深入实施微观梯度。首先，我想确保你对导数有非常好的直观理解，以及它究竟给你提供了什么信息。
- en: So let's start with some basic imports that I copy based in every Jupyter Notebook
    always。 And let's define the function scalar value function f of x as follows。
    So I just make this up randomly。 I just want to scale a value function that takes
    a single scalar x and returns a single scalar。 Y。 And we can call this function
    of course so we can pass in say 3。0 and get 20 back。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们从一些基本的导入开始，我在每个Jupyter Notebook中都会复制这些。让我们定义标量值函数f(x)如下。我随便编造的，只想要一个接受单个标量x并返回单个标量y的标值函数。当然我们可以调用这个函数，比如传入3.0，得到20的返回值。
- en: Now we can also plot this function to get a sense of its shape。 You can tell
    from the mathematical expression that this is probably a parabola。 It's a quadratic。
    And so if we just create a set of， scalar values that we can feed in using for
    example a range from negative five to five and steps up。 point two five。 So x
    is just from negative five to five not including five in steps up point two five。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以绘制这个函数，以便了解它的形状。你可以从数学表达式中看出，这很可能是一个抛物线。它是一个二次函数。因此，如果我们创建一组标量值，比如使用从负五到五的范围，步长为点二五。那么x的范围就是从负五到五，但不包括五，步长为点二五。
- en: And we can actually call this function on this not by array as well。 So we get
    a set of y's if we call f on x's。 And these y's are basically also applying function
    on every one of these elements independently。 And we can plot this using matplotlib。
    So if you'll see that plot x is in y's and we get a nice parabola。 So previously
    here we fed in 3。0 somewhere here and we received 20 back which is here the y-coordinate。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上也可以在这个非数组上调用这个函数。所以如果我们在x上调用f，就会得到一组y。这些y基本上也是对每一个元素独立应用函数。我们可以使用matplotlib绘制这个图。如果你看到那个图，x在y中，我们得到一个漂亮的抛物线。因此之前我们在这里输入3.0，得到了20的返回值，这里的y坐标就是。
- en: So now I'd like to think through what is the derivative of this function at
    any single input point x。 Right so what is the derivative at different points
    x of this function。 Now if you remember back to your calculus class you've probably
    derived derivatives。 So we take this mathematical expression 3x square minus 4x
    plus five and you would write out on a。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我想思考一下这个函数在任意输入点x的导数是什么。对吧，那么这个函数在不同点x的导数是什么？如果你还记得你的微积分课，你可能已经推导过导数。所以我们拿这个数学表达式3x²
    - 4x + 5，你会在一张纸上写出来。
- en: piece of paper and you would apply the product rule and all the other rules
    and derive the mathematical。 expression of the great derivative of the original
    function。 And then you could plug in different。 taxes and see what the derivative
    is。 We're not going to actually do that because no one in neural。 networks actually
    writes out the expression for neural net。 It would be a massive expression。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 你会应用乘积法则和其他所有法则，推导出原始函数的伟大导数的数学表达式。然后你可以代入不同的值，看看导数是什么。我们并不打算真正这样做，因为在神经网络中没有人会写出神经网络的表达式。那会是一个庞大的表达式。
- en: It would be you know thousands since thousands of terms no one actually derives
    the derivative of。 course。 And so we're not going to take this kind of symbolic
    approach。 Instead what I'd like to do is I'd like to look at the definition of
    derivative and just make sure。 that we really understand what derivative is measuring
    what is telling you about the function。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道成千上万的项，实际上没有人推导导数。因此，我们不会采取这种符号方法。相反，我想做的是看看导数的定义，确保我们真正理解导数在测量什么，它告诉你关于这个函数的什么。
- en: And so if we just look at derivative。 We see that。 okay so this is not a very
    good definition of derivative。 This is a definition。 of what it means to be differentiable。
    But if you remember from your calculus it is the limit。 as h goes to zero of f
    of x plus h minus f of x over h。 So basically what it's saying is if you。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们只看导数，我们会发现，这不是一个很好的导数定义。这是一个关于可微性的定义。但是如果你还记得微积分，它是h趋近于零时f的x加h减去f的x除以h的极限。所以基本上它在说的是如果你。
- en: slightly bump up， you're at some point x that you're interested in or a and
    if you slightly bump up。 you know you slightly increase it by small number h。
    How does the function respond with what。 sensitivity does it respond？ Where does
    the slope at that point？ Does the function go up or does it。 go down and by how
    much？ And that's the slope of that function， the slope of that response at that。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 轻微增加，你在某个你感兴趣的点x或a，如果你稍微增加一点，你知道你稍微增加了一个小数h。这个函数如何响应？它的敏感度是什么？在那个点的斜率在哪里？这个函数是上升还是下降？下降多少？这就是这个函数的斜率，响应的斜率在那一点。
- en: point。 And so we can basically evaluate the derivative here numerically by taking
    a very small h。 Of， course the definition would ask us to take h to zero。 We're
    just going to pick a very small h。 0。001。 And let's say we're interested in 0。3。0。
    So we can look at f of x of course as 20。 And now f of x plus h。 So if we slightly
    nudge x in a positive direction， how is the function。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 点。因此，我们基本上可以通过取一个非常小的h来数值评估导数。定义当然要求我们将h取到零。我们只是选择一个非常小的h，0.001。假设我们对0.3感兴趣。所以我们可以把f的x看作是20。现在f的x加h。如果我们稍微将x向正方向推动，函数会怎样。
- en: going to respond？ And just looking at this， do you expect f of x plus h to be
    slightly greater than 20？
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 它会如何响应？仅仅看看这个，你期望f的x加上h稍微大于20吗？
- en: Or do you expect to be slightly lower than 20？ And since this 3 is here and
    this is 20。 if we slightly go positively， the function will respond positively。
    So you'd expect this to be。 slightly greater than 20。 And by how much is telling
    you the strength of that slope， right？
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 或者你期望略低于20？既然这里有3，而这里是20，如果我们稍微正向移动，函数会正向响应。因此你会期望这个值稍微大于20。而这个值告诉你斜率的强度，对吧？
- en: The size of the slope。 So f of x plus h to the power of f of x， this is how
    much the。 function responded in the positive direction。 And we have to normalize
    by the run。 So we have。 the rise over run to get the slope。 So this of course
    is just a numerical approximation of the。 slope because we have to make age very，
    very small to converge to the exact amount。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 斜率的大小。所以f的x加h与f的x之差，这是函数在正方向响应的程度。我们必须通过运行来归一化。所以我们有rise over run来得到斜率。因此，这当然只是斜率的数值近似，因为我们必须让h非常非常小，以便收敛到确切的值。
- en: Now if I'm doing， too many zeros， at some point， I'm gonna get an incorrect
    answer because we're using floating。 point arithmetic and the representations
    of all these numbers in computer memory is finite。 And at some point we get into
    trouble。 So we can converge towards the right answer with this approach。 But basically
    at 3， the slope is 14。 And you can see that by taking 3x squared minus 4x plus
    5。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在如果我在做太多零，某个时候，我会得到一个不正确的答案，因为我们使用的是浮点算术，而计算机内存中所有这些数字的表示是有限的。在某个时候，我们会遇到麻烦。因此，我们可以通过这种方法收敛到正确答案。但基本上，在3时，斜率是14。你可以通过计算3x平方减去4x加5来看到这一点。
- en: and differentiating it in our head。 So 3x squared would be 6x minus 4。 And then
    we plug in x equals 3。 So that's 18 minus 4 is 14。 So this squared。 So that's
    at 3。 Now how about the slope at say negative 3？ Would you expect？ What would
    you expect for the slope？
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 并在脑海中进行微分。因此3x平方的导数是6x减去4。然后我们代入x等于3。所以是18减去4等于14。所以这是在3时的情况。那么在负3时的斜率如何？你会期待什么？
- en: Now telling the exact value is really hard， but what is the sign of that slope？
    So at negative 3。 if we slightly go in the positive direction at x， the function
    would actually go down。 And so that tells you that the slope would be negative。
    So we'll， get a slight number below 20。 And so if we take the slope， we expect
    something negative， negative 22。 Okay。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在确切的值很难确定，但那个斜率的符号是什么呢？所以在负3时，如果我们稍微在正方向上移动x，函数实际上会下降。这告诉你斜率是负的。所以我们将得到一个略低于20的数字。因此如果我们取斜率，我们期望得到负值，负22。好的。
- en: And at some point here， of course， the slope would be 0。 Now for this specific
    function。 I looked it up previously and it's at point 2 over 3。 So at roughly
    2 over 3， that's somewhere here。 this derivative would be 0。 So basically at that
    precise point， yeah， at that precise point。 if we nudge in a positive direction，
    the function doesn't respond。 This stays the same almost。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在某个时刻，当然，斜率会是0。现在对于这个特定的函数，我之前查过，它在2/3点。所以大约在2/3，那里这个导数会是0。因此，基本上在那个精确点，是的，在那个精确点，如果我们朝正方向轻推，函数不会响应。这几乎保持不变。
- en: And so that's why the slope is 0。 Okay。 Now let's look at a bit more， complex
    case。 So we're going to start， you know， complexifying a bit。 So now we have a
    function。 here with output variable D that is a function of three scalar inputs，
    A， B and C。 So A。 B and C are， some specific values， three inputs into our expression
    graph and a single output D。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是斜率为0的原因。好的。现在让我们看看一个更复杂的案例。所以我们将开始，你知道，稍微复杂化一下。所以现在我们有一个函数。这里输出变量D是三个标量输入A、B和C的函数。因此A、B和C是一些特定值，三输入到我们的表达式图中，以及单一输出D。
- en: And so， if we just print D， we get four。 And now what I have to do is I'd like
    to again look at the。 derivatives of D with respect to A， B and C。 And think through
    again， just the intuition of。 what this derivative is telling us。 So in order
    to evaluate this derivative， we're going to get a。 bit hacky here。 We're going
    to again have a very small value of H。 And then we're going to fix。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，如果我们打印D，我们得到四。现在我必须做的是，我想再次查看D对A、B和C的导数。并再次思考这个导数告诉我们的直觉。因此，为了评估这个导数，我们将有点小技巧。我们将再次有一个非常小的H值。然后我们将固定我们感兴趣的输入一个微小的量。这是通过H来归一化以获得斜率。
- en: the inputs at some values that we're interested in。 So these are the， this is
    the point A， B， C。 at which we're going to be evaluating the derivative of D with
    respect to all A。 B and C at that point。 So there are the inputs。 And now we have
    D one is that expression。 And then we're going to， for， example， look at the derivative
    of D with respect to A。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这些是，点A、B、C。在这些点上，我们将评估D对所有A、B和C的导数。因此，输入就是这些。现在D1是那个表达式。然后我们将，举个例子，查看D对A的导数。
- en: So we'll take A and we'll bump it by H。 And， then we'll get D two to be the
    exact same function。 And now we're going to print， you know， F1 D1 is D1 D2 is
    D2 and print slope。 So the derivative or slope here will be， of course， D2， minus
    D1 divided H。 So D2 minus D1 is how much the function increased when we bumped
    the specific。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们将取A，并将其增加H。然后，我们将使D二与之完全相同。现在我们要打印，你知道，F1 D1是D1，D2是D2并打印斜率。所以这里的导数或斜率当然是D2减去D1再除以H。D2减去D1就是当我们增加特定值时，函数增加了多少。
- en: input that we're interested in by a tiny amount。 And this is the normalized
    by H to get the slope。 So， yeah。 So this， so I just from this， we're going to
    print D1， which we know is four。 Now D2。 will be bumped A will be bumped by H。
    So let's just think through a little bit what D2 will be。 printed out here。 In
    particular， D1 will be four will D2 be a number slightly greater than four or。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，是的。所以我从中打印D1，我们知道是四。现在D2将是A增加H。让我们稍微想一下D2在这里将打印出什么。特别是D1将是四，D2是一个略大于四的数字，还是。
- en: slightly lower than four。 And it's going to tell us the sign of the derivative。
    So。 we're bumping A by H。 B is minus three， CS10。 So you can just intuitively
    think through this。 derivative and what it's doing。 A will be slightly more positive
    and but B is a negative number。 So if A is slightly more positive， because B is
    negative three， we're actually going to be adding。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 略低于四。这将告诉我们导数的符号。所以，我们将A增加H。B是负三，CS10。所以你可以直观地思考一下这个导数及其作用。A会稍微更正，但B是一个负数。因此，如果A稍微更正，由于B是负三，我们实际上会在某些值上增加输入，这些值是我们感兴趣的。
- en: less to D。 So you'd actually expect that the value of the function will go down。
    So let's just see this。 Yeah。 And so we went from four to 3。9996。 And that tells
    you that the slope will be negative。 And then will be a negative number。 because
    we went down。 And then the exact number of slope will be。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对D减少。因此你会期望函数的值会下降。让我们看看这个。是的。所以我们从4变成了3.9996。这告诉你斜率将是负的。然后将是一个负数，因为我们下降了。然后斜率的确切数值将是。
- en: the exact number of slope is negative three。 And you can also convince yourself
    that negative。 three is the right answer mathematically and analytically， because
    if you have A times B plus C。 and you are， you know， you have calculus， then differentiating
    A times B plus C with respect to A。 gives you just B。 And indeed， the value of
    B is negative three， which is the derivative that we。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 斜率的确切数值是负三。你也可以通过数学和分析来确认负三是正确的答案，因为如果你有A乘以B加上C。你知道，你有微积分，然后对A的A乘以B加C求导，仅仅得到B。确实，B的值是负三，这是我们得到的导数。
- en: have。 So you can tell that that's correct。 So now if we do this with B， so if
    we bump B by a little。 bit in a positive direction， we'd get different slopes。
    So what is the influence of B on the output D？ So if we bump B by tiny amount
    in a positive direction。 then because A is positive， we'll be adding more to D。
    Right。 So， and now what is the。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 有。所以你可以判断这是正确的。那么现在如果我们对B这样做，所以如果我们稍微向正方向增加B，我们会得到不同的斜率。那么B对输出D的影响是什么？所以如果我们在正方向上稍微增加B。那么因为A是正的，我们将会向D添加更多，对吧。所以，现在。
- en: what is the sensitivity？ What is， the slope of that addition？
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 敏感性是什么？那个加法的斜率是什么？
- en: And it might not surprise you that this should be two。 And why is， it two？ Because
    D of D by DB。 the fractional respect to B would be would give us A， and the value
    of A is， two。 So that's also working well。 And then if C gets bumped a tiny amount
    in H by H， then of course。 H times B is unaffected。 And now C becomes slightly
    bit higher。 What does that do to the function？
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能并不让你感到惊讶，这应该是二。为什么是二？因为D关于B的导数，关于B的分数会给我们A，而A的值是二。所以这也是正常工作的。如果C在H上稍微增加了一点，那么当然。H乘以B不受影响。现在C稍微高了一点。这对函数有什么影响？
- en: It makes， it slightly bit higher， because we're simply adding C。 And it makes
    it slightly bit higher by the exact， same amount that we added to C。 And so that
    tells you that the slope is one。 That will be the。 the rate at which D will increase
    as we scale C。 Okay， so we now have some intuitive sense of。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得它稍微高了一点，因为我们只是简单地添加了C。并且它使得它稍微高了与我们添加到C的量完全相同。因此，这告诉你斜率是1。这将是D随着C的缩放而增加的速率。好的，所以我们现在对这个有一些直观的理解。
- en: what this derivative is telling you about the function。 And we'd like to move
    to neural networks。 Now， as I mentioned， neural networks will be pretty massive
    expressions， mathematical， expressions。 So we need some data structures that maintain
    these expressions。 And that's。 what we're going to start to build out now。 So
    we're going to build out this value object that I showed。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这个导数告诉你关于这个函数的什么。我们想转向神经网络。正如我提到的，神经网络将是相当庞大的表达式，数学表达式。因此我们需要一些数据结构来维护这些表达式。这就是我们现在要开始构建的内容。所以我们要构建我展示的这个值对象。
- en: you in the read me page of micro grad。 So let me copy paste a skeleton of the
    first very simple。 value object。 So class value takes a single scalar value that
    it wraps and keeps track of。 And。 that's it。 So we can， for example， do value
    of 2。0， and then we can get， we can look at its content。 And Python will internally
    use the wrapper function to return this string。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 你在micro grad的自述页面上。所以让我复制粘贴第一个非常简单的值对象的框架。所以类值接受一个单一的标量值，它包装并跟踪它。就这样。所以我们可以，例如，做值为2.0，然后我们可以查看它的内容。Python将在内部使用包装函数返回这个字符串。
- en: So this is a value object with data equals two that we're creating here。 Now
    we'd like to do is like， we'd like to be able to have not just like two values。
    but we'd like to do a blocky， right？ We'd， like to add them。 So currently。 you
    get an error because Python doesn't know how to add two value， objects。 So we
    have to tell it。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个数据等于2的值对象，我们在这里创建。现在我们想要做的是，不仅仅有两个值，而是想做一个块状的，对吧？我们想把它们加起来。因此，目前你会得到一个错误，因为Python不知道如何添加两个值对象。所以我们必须告诉它。
- en: So here's addition。 So you have to basically use these special double。 underscore
    methods in Python to define these operators for these objects。 So if we call the。
    if we use this plus operator， Python will internally call a dot add of B。 That's
    what will happen internally。 And so B will be the other。 And self will be a。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这里是加法。你基本上必须在Python中使用这些特殊的双下划线方法为这些对象定义这些运算符。所以如果我们调用，如果我们使用这个加法运算符，Python将在内部调用B的dot
    add。这就是内部会发生的事情。因此，B将是另一个，而self将是一个。
- en: And so we see that what we're going to return is a new， value object。 And it's
    just going to be wrapping the plus of their data。 But remember now， because。 data
    is the actual like numbered Python number。 So this operator here is just the typical
    floating。 point plus addition now， it's not an addition of value objects。 And
    we'll return a new value。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到我们要返回的是一个新的值对象。它将只是包装它们数据的加法。但是请记住，现在，因为数据实际上是像数字的Python数字。因此，这里的运算符只是典型的浮点加法，现在不是值对象的加法。我们将返回一个新的值。
- en: So now a plus B should work。 And it should print value off negative one， because
    that's two plus。 minus three。 There we go。 Okay， let's now implement multiply
    just so we can recreate this expression。 here。 So multiply， I think it won't surprise
    you， will be fairly similar。 So instead of at。 we're going to be using mall。 And
    then here， of course， we want to do tons。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在a加B应该能工作。它应该打印出负一的值，因为这是二加负三。好了，现在让我们实现乘法，以便重现这个表达式。所以乘法，我想你不会惊讶，它将是相当相似的。因此，我们将使用mall代替at。在这里，当然我们想做很多。
- en: And so now we can create， a C value object， which will be 10。0。 And now we should
    be able to do a times B。 Well， let's just do a， times B first。 That's value of
    negative six now。 And by the way， I skipped over this a little bit。 suppose that
    I didn't have the wrapper function here， then it's just that you'll get some kind
    of。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以创建一个C值对象，它将是10.0。现在我们应该能够执行a乘以B。好吧，我们先来做a乘以B。这现在的值是负六。顺便提一下，我略过了这一点。假设我这里没有包装函数，那么你只会得到某种结果。
- en: an ugly expression。 So what rep is doing is it's providing us a way to print
    out like a nicer。 looking expression in Python。 So we don't just have something
    cryptic， we actually are， you know。 it's value of negative six。 So this gives
    us a times。 And then this we should now be able to。 add C to it， because we've
    defined and told the Python how to do mall and add。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一个丑陋的表达式。所以rep的作用是提供一种在Python中打印出更美观表达式的方式。这样我们就不仅仅是得到一些神秘的东西，而是得到了负六的值。所以这给了我们一个乘法。然后我们现在应该能够将C添加到其中，因为我们已经定义并告诉Python如何执行mall和add。
- en: And so this will call， this will basically be equivalent to a dot mall of B。
    And then this new value object will be dot， add of C。 And so let's see if that
    work。 Yep。 So that worked。 Well， that gave us four， which is， what we expect from
    before。 And I believe we can just call them manually as well。 There we go。 So，
    Yeah。 Okay。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这将调用，这基本上相当于B的dot mall。然后这个新值对象将是dot add C。让我们看看是否有效。是的。这有效。好吧，那给了我们四，这是我们之前预期的结果。我相信我们也可以手动调用它们。好了。
- en: So now what we are missing is the connected tissue of this expression。 As I
    mentioned。 we want to keep these expression graphs。 So we need to know and keep
    pointers about what values。 produce what other values。 So here， for example， we
    are going to introduce a new variable， which。 will call children。 And by default，
    it will be an empty tuple。 And then we're actually going to。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们缺少的是这个表达式的连接部分。正如我提到的，我们希望保留这些表达式图。所以我们需要了解并保持指向哪些值产生其他值的指针。例如，在这里，我们将引入一个新变量，称为children。默认情况下，它将是一个空元组。然后我们实际上将会。
- en: keep a slightly different variable in the class， which will call underscore
    private， which will be。 the set of children。 This is how I done， I did it in the
    original micro grad looking at my code here。 I can't remember exactly the reason
    I believe it was efficiency。 But this underscore children。 will be a tuple for
    convenience。 But then when we actually maintain it in the class， it will be。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在类中保持一个略微不同的变量，称为_私有，它将是children的集合。这是我在原始微型梯度中做到的，查看我的代码，我不记得确切的原因，我相信这是出于效率。但是这个_
    children将是一个元组以方便起见。但当我们在类中实际维护它时，它将是。
- en: just this set。 I believe for efficiency。 So now when we are creating a value
    like this with a。 constructor， children will be empty and private will be the
    empty set。 But when we are creating a。 value through addition or multiplication，
    we're going to feed in the children of this value。 which in this case is self
    another。 So those are the children here。 So now we can do d dot prep。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这个集合。我相信这是为了效率。所以现在当我们用构造函数创建一个值时，children将是空的，private将是空集合。但是当我们通过加法或乘法创建一个值时，我们将传入这个值的children。在这种情况下是self另一个。所以这些就是这里的children。现在我们可以做`d
    dot prep`。
- en: And we'll see that the children of the， we now know， are this a value。 of negative
    six and value of 10。 And this， of course， is the value resulting from a times
    b。 and the C value， which is 10。 Now the last piece of information we don't know，
    so we know that the。 children of every single value， but we don't know what operation
    created this value。 So we need one。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会看到，children现在知道的是这个值，**负六**和**十**。当然，这是由`a times b`得到的值，C值是10。现在我们不知道的最后一块信息是，我们知道每个值的children，但不知道是什么操作创建了这个值。所以我们需要一个。
- en: more element here， let's call it underscore pop。 And by default， this is the
    hookedy set for leaves。 And then we'll just maintain it here。 And now the operation
    will be just a simple string。 And in the case of addition， it's plus in the case
    of multiplication is times。 So now we not just have d dot prep， we also have a
    d dot op。 And we know that D was produced by an。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有更多的元素，我们称之为**下划线弹出**。默认情况下，这是叶子的钩子集合。然后我们将把它保持在这里。现在的操作将是一个简单的字符串。在加法的情况下，是加，在乘法的情况下，是乘。所以现在我们不仅有`d
    dot prep`，还有`d dot op`。我们知道D是由一个产生的。
- en: addition of those two values。 And so now we have the full mathematical expression。
    And we're building， out this data structure。 And we know exactly how each value
    came to be。 by word expression and from， what other values。 Now。 because these
    expressions are about to get quite a bit larger， we'd like a way。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个值的相加。所以现在我们有了完整的数学表达式。我们正在构建这个数据结构。我们确切知道每个值是如何通过字面表达式以及从其他值而来的。现在，由于这些表达式即将变得相当复杂，我们希望找到一种方式。
- en: to nicely visualize these expressions that we're building out。 So for that，
    I'm going to copy。 paste a bunch of slightly scary code that's going to visualize
    this these expression graphs for us。 So here's the code and I'll explain that
    in a bit。 But first， let me just show you what this。 code does。 Basically， what
    it does is it creates a new function draw dot that we can call on some。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 好好地可视化我们正在构建的这些表达式。为此，我将复制粘贴一些稍微吓人的代码，用于可视化这些表达式图。这里是代码，我稍后会解释。但是首先，让我给你展示一下这段代码的作用。基本上，它创建了一个新的函数`draw
    dot`，我们可以在某个根节点上调用它。
- en: root node。 And then it's going to visualize it。 So if we call draw dot on D，
    which is this final。 value here， that is eight times B plus C。 It creates something
    like this。 So this is D。 And you see that， this is a times B， creating an attribute
    value plus C gives us the output node D。 So that's draw， out of D。 And I'm not
    going to go through this in complete detail。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 然后它将进行可视化。所以如果我们在D上调用`draw dot`，这是这个最终值，**八乘B加C**。它会生成类似这样的东西。所以这是D。你看到的是，这个是`a
    times B`，生成一个属性值，加上C给我们输出节点D。所以这就是D的绘制。我不会详细讲解这一部分。
- en: You can take a look at graph， this and it's API。 A graph is is a open source
    graph visualization software。 And what we're doing， here is we're building out
    this graph in the graph is API。 And you can basically see that trace， is this
    helper function that enumerates all the nodes and edges in the graph。 So that
    just builds， a set of all the nodes and edges。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看看图表，以及它的API。图是一种开源图形可视化软件。我们在这里做的是在图的API中构建这个图。你基本上可以看到，跟踪是这个帮助函数，它枚举图中的所有节点和边。所以这只构建了一个包含所有节点和边的集合。
- en: And then we iterate for all the nodes and we create special。 node objects for
    them in using dot node。 And then we also create edges using dot dot edge。 And
    the only thing that's like slightly tricky here is you'll notice that I basically
    add these。 fake nodes， which are these operation nodes。 So for example， this node
    here is just like a plus。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们遍历所有节点，并使用`dot node`为它们创建特殊的节点对象。然后我们还使用`dot dot edge`创建边。唯一稍微复杂的地方是，你会注意到我基本上添加了这些假节点，这些是操作节点。例如，这个节点就像一个加号。
- en: node。 And I create these special op nodes here。 And I connect them accordingly。
    So these nodes。 of course， are not actual nodes in the original graph。 They're
    not actually a value object。 The。 only value objects here are the things in squares。
    Those are actual value objects or representations， thereof。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 节点。我在这里创建这些特殊的操作节点，并相应地连接它们。因此，这些节点当然不是原始图中的实际节点。它们实际上不是值对象。这里唯一的值对象是方框中的内容。那些才是真正的值对象或其表示。
- en: And these op nodes are just created in this draw dot routine so that it looks
    nice。 Let's also add labels to these graphs just so we know what variables are
    where。 So let's create。 a special underscore label。 Or let's just do label equals
    empty by default and save it in each node。 And then here we're going to do label
    is a label is the label is C。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这些操作节点只是为了让它看起来美观而在这个绘图例程中创建的。我们还要给这些图添加标签，以便知道变量的位置。因此，让我们创建一个特殊的下划线标签。或者我们默认让标签为空，并将其保存在每个节点中。在这里，我们将标签设置为C。
- en: And then let's create a special equals a times B。 And the label will be， it's
    kind of not。 And he will be a plus C and D dot label will be， okay。 So nothing
    really changes。 I just added this new function， new variable。 And then here， when
    we are printing this。 I'm going to print the label here。 So this will be a percent
    S bar。 And this will be end up label。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然后让我们创建一个特殊的等于A乘以B。标签将是，它有点不。然后它将是A加C，而D的标签将是，好吧。因此，没有什么真正改变。我只是添加了这个新函数、新变量。然后在打印时，我将在这里打印标签。所以这将是一个百分比S条。这将最终成为标签。
- en: And so now we have the label on the left here。 So it says， A be creating me。
    And then E plus C creates D。 Just like we have it here。 And finally。 let's make
    this expression just one layer deeper。 So D will not be the final output node。
    Instead。 after D， we are going to create a new value object called F。 We're going
    to start。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们在左侧有了标签。因此它显示，A创造了我。然后E加C创造了D。就像我们这里写的那样。最后，让我们将这个表达式的层次加深一层。因此，D将不再是最终输出节点。相反，在D之后，我们将创建一个新的值对象F。我们将开始。
- en: running out of variables soon。 F will be negative two point zero。 And it's label。
    Of course。 just D F。 And then L capital L will be the output of our graph。 And
    L will be T times F。 So L will be negative eight is the output。 So now we don't
    just draw a D， D draw L。 Okay。 And somehow the label of L is undefined。 Oops。
    All that label has to be explicitly。
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 变量很快就用完了。F的值将是负2.0。它的标签当然是D F。然后大写的L将是我们图的输出。L将是T乘以F。因此，L的值为负8，是输出。现在我们不仅要画D，还要画L。好吧。L的标签在某种情况下是未定义的。哎呀。所有标签必须明确。
- en: served given to it。 There we go。 So L is the output。 So let's quickly recap
    what we've done so far。 We are able to build out mathematical expressions using
    only plus and times so far。 They are scalar。 valued along the way。 And we can
    do this forward pass and build out a mathematical expression。 So we have multiple
    inputs here， A， B， C， and F going into a mathematical expression that produces。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 被提供给它的。好了。所以L是输出。让我们快速回顾一下到目前为止所做的工作。我们能够仅使用加法和乘法构建数学表达式。它们沿途都是标量值。我们可以执行正向传播并构建数学表达式。因此，我们在这里有多个输入，A、B、C和F进入一个产生。
- en: a single output L。 And this here is this， we're watching the forward pass。 So
    the output of the。 forward pass is negative eight。 That's the value。 Now， what
    we'd like to do next is we'd like to。 run back propagation。 And in back propagation，
    we are going to start here at the end。 And we're。 going to reverse and calculate
    the gradient along along all these intermediate values。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一个单一输出L。这里我们观察正向传播。因此，正向传播的输出是负8。这是值。接下来，我们想做的是运行反向传播。在反向传播中，我们将从末尾开始。然后，我们将反向计算沿着所有这些中间值的梯度。
- en: And really what we're computing for every single value here， we're going to
    compute the。 derivative of that node with respect to L。 So the derivative of L
    with respect to L is just one。 And then we're going to derive what is the derivative
    of L with respect to F with respect to D with。 respect to C with respect to E
    with respect to B and with respect to A。 And in neural network。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里计算的每一个值，实际上是计算该节点相对于L的导数。因此，L对L的导数就是1。接下来，我们将推导L相对于F、D、C、E、B和A的导数。在神经网络中。
- en: setting， you'd be very interested in the derivative of basically this loss function
    L with respect to。 the weights of a neural network。 And here， of course， we have
    just these variables A， B， C， and F。 But some of these will eventually represent
    the weights of a neural net。 And so we'll need to know。 how those weights are
    impacting the loss function。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 设置时，你会非常感兴趣于这个损失函数L相对于神经网络权重的导数。当然，在这里我们仅有这些变量A、B、C和F。但是其中一些最终将代表神经网络的权重。因此我们需要知道这些权重如何影响损失函数。
- en: So we'll be interested basically in the derivative。 of the output with respect
    to some of its leaf nodes。 And those leaf nodes will be the weights of。 the neural
    net。 And the other leaf nodes， of course， will be the data itself。 But usually。
    we will not want or use the derivative of the loss function with respect to data，
    because the data。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基本上会对输出相对于其某些叶节点的导数感兴趣。而这些叶节点将是神经网络的权重。其他叶节点当然将是数据本身。但是通常，我们不希望或使用损失函数相对于数据的导数，因为数据。
- en: is fixed。 But the weights will be iterated on using the gradient information。
    So next， we are。 going to create a variable inside the value class that maintains
    the derivative of L with respect to。 that value。 And we will call this variable
    grad。 So there's a data and there's a self that grad。 And initially， it will be
    zero。 And remember that zero is basically means no effect。 So at。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 是固定的。但是权重将基于梯度信息进行迭代。因此接下来，我们将创建一个变量在值类中，用于维护相对于该值的L的导数。我们将把这个变量称为grad。所以这里有数据，还有一个self，grad。最初，它将为零。请记住，零基本上意味着没有影响。因此在。
- en: initialization， we're assuming that every value does not impact does not affect
    the output。 Right。 because if the gradient is zero， that means that changing this
    variable is not changing the。 loss function。 So by default， we assume that the
    gradient is zero。 And then now that we have grad。 and it's 0。0， we are going to
    be able to visualize it here after data。 So here grad is 0。4。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化时，我们假设每个值不会影响输出。对，因为如果梯度为零，那意味着更改此变量并不会改变损失函数。因此默认情况下，我们假设梯度为零。现在既然我们有了grad，并且它是0.0，我们将在数据之后能够可视化它。因此这里grad是0.4。
- en: And this will be in that grad。 And now we are going to be showing both the data
    and the grad。 and initialize that zero。 And we are just about getting ready to
    calculate the back propagation。 And of course， this grad again， as I mentioned，
    is representing the derivative of the output。 In this， case， L with respect to
    this value。 So with respect to。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这将体现在grad中。现在我们将展示数据和grad，并将其初始化为零。我们即将准备计算反向传播。当然，这个grad再次，如我所提到的，代表输出的导数。在这种情况下，L相对于这个值的导数。因此相对于。
- en: so this is the derivative of L with respect to， F， respect to D and so on。 So
    let's now fill in those gradients and actually do back propagation， manually。
    So let's start filling in these gradients and start all the way at the end， as
    I mentioned， here。 First， we are interested to fill in this gradient here。 So
    what is the derivative of L with。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是L相对于F、D等的导数。现在让我们填入这些梯度，实际上手动进行反向传播。正如我提到的，从末尾开始填充这些梯度。首先，我们关注填入这里的这个梯度。那么L的导数是什么。
- en: respect to L？ In other words， if I change L by a tiny amount H， how much does
    L change？
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 相对于L呢？换句话说，如果我将L更改一个微小的量H，L会改变多少？
- en: It changes by H。 So it's proportional and therefore there ever will be one。
    We can of course measure， these or estimate these numerical gradients numerically。
    just like we've seen before。 So if I take this expression and I create a def LLL
    function here and put this here。 Now， the reason I'm creating a gating function
    LLL here is because I don't want to pollute or mess。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 它随着H变化。因此它是成比例的，因此永远会有一个。我们当然可以测量这些，或者像我们之前看到的那样数值估计这些数值梯度。因此，如果我采用这个表达式并在这里创建一个def
    LLL函数。现在，我创建这个门控函数LLL的原因是因为我不想污染或混淆。
- en: up the global scope here。 This is just kind of like a little staging area。 And
    as you know。 in Python， all of these will be local variables to this function。
    So I'm not changing any of the global， scope here。 So here。 L1 will be L and then
    copy-placing this expression， we're going to add a small amount H， in。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 全局作用域在这里。这就像一个小的暂存区。正如你所知道的，在Python中，所有这些都是此函数的局部变量。因此，我并没有更改任何全局作用域。在这里，L1将是L，然后复制此表达式，我们将增加一个小量H，在。
- en: for example， A， right？ And this would be measuring the derivative of L with
    respect to A。 So here。 this will be L2。 And then we want to print that derivative。
    So print L2 minus L1。 which is how much L changed and then normalize it by H。
    So this is the rise over run。 And we have。 to be careful because L is a value
    node。 So we actually want its data。 So that these are floats。
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，A，对吧？这将测量 L 关于 A 的导数。因此这里，这将是 L2。然后我们想要打印这个导数。所以打印 L2 减去 L1，这就是 L 变化的多少，然后用
    H 标准化。所以这是升高与运行。我们必须小心，因为 L 是一个值节点。因此我们实际上想要它的数据。所以这些是浮点数。
- en: dividing by H。 And this should print the derivative of L with respect to A because
    A is the one that。 we bumped a little bit by H。 So what is the derivative of L
    with respect to A？ It's six。 Okay。 And obviously， if we change L by H， then that
    would be here effectively。 This looks really awkward。 but changing， L by H， you
    see the derivative here is one。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 除以 H。这应该打印 L 关于 A 的导数，因为 A 是我们用 H 稍微改变的。那么 L 关于 A 的导数是什么？是六。好的。显然，如果我们将 L 改变
    H，那么这里实际上就是。这看起来真的很尴尬，但改变 L 以 H，你会看到这里的导数是 1。
- en: That's kind of like the base case of what we are doing here。 So basically。 we
    cannot copy here and we can manually set L dot grad to one。 This is our manual，
    backpropagation。 L dot grad is one。 And let's redraw。 And we'll see that we filled
    in grad is one for L。 We're now going to continue the backpropagation。 So let's
    here look at the derivatives of L with。
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这就像我们在这里做的基本情况。因此，基本上。我们不能复制这里，我们可以手动将 L 点梯度设为一。这是我们的手动反向传播。L 点梯度是 1。让我们重新绘制。我们看到我们为
    L 填充的梯度是 1。我们现在将继续反向传播。因此让我们在这里查看 L 的导数。
- en: respect to D and F。 Let's do a D first。 So what we are interested in。 if I create
    a markdown on here， is we'd like to know basically we have that L is D times F。
    And we'd like to know what is D L by， D D。 What is that？ And if you know you're
    a calculus。 L is D times F。 So what is D L by D D， it would be F。 And if you don't
    believe me。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 D 和 F。我们先做 D。那么我们感兴趣的是，如果我在这里创建一个 markdown，我们想知道基本上我们有 L 是 D 乘以 F。我们想知道 D
    L 对 D D 是什么？那是什么？如果你知道你在学习微积分，L 是 D 乘以 F。那么 D L 对 D D 就是 F。如果你不相信我。
- en: we can also just derive it because the proof would be， fairly straightforward。
    We go to the definition of the derivative， which is F of x plus H minus F of x。
    divided H as a limit of H goes to zero of this kind of expression。 So when we
    have L is D times F。 then increasing D by H would give us the output of D plus
    H times F。 That's basically F of x plus H。
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以直接推导，因为证明相对来说是相当简单的。我们去看导数的定义，即 F 的 x 加 H 减去 F 的 x，除以 H，作为 H 趋近于零时这种表达式的极限。因此，当
    L 等于 D 乘以 F 时，增加 D 的 H 会给我们 D 加 H 乘以 F 的输出。基本上就是 F 的 x 加 H。
- en: right？ Minus D times F and then divide H。 And symbolically expanding out here，
    we would have。 basically D times F plus H times F minus D times F divided H。 And
    then you see how the D F minus D。 F cancels。 So you're left with H times F divided
    H， which is F。 So in the limit。 as H goes to zero of， you know， derivative definition，
    we just get F in a case of D times F。
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对吧？减去 D 乘以 F 然后除以 H。象征性地展开，我们会得到。基本上是 D 乘以 F 加 H 乘以 F 减去 D 乘以 F 除以 H。然后你会看到
    D F 减去 D F 消去。因此你留下的是 H 乘以 F 除以 H，这就是 F。因此在极限中，当 H 趋近于零时，你知道，导数的定义，我们只得到 F 在 D
    乘以 F 的情况下。
- en: So symmetrically， D L by D F will just be D。 So what we have is that F dot grad
    we see now is just the value of D。 which is four。 And we see that D dot grad is
    just the value of F。 And so the value of F is negative two。 So we'll set those
    manually。 Let me erase this markdown node。 and then let's redraw what we have。
    Okay， and let's just make sure that these were correct。
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 所以对称地，D L 对 D F 将只是 D。因此我们得到的 F 点梯度现在就是 D 的值，即四。我们看到 D 点梯度就是 F 的值。因此 F 的值是负二。我们将手动设置这些。让我擦掉这个
    markdown 节点，然后让我们重新绘制我们拥有的内容。好的，让我们确保这些是正确的。
- en: So we seem to think that D L by D D is， negative two。 So let's double check。
    Let me erase this plus H from before。 And now we want to， derivative with respect
    to F。 So let's just come here when I create F and let's do a plus H here。 And
    this should print a derivative of L with respect to F。 So we expect to see four。
    Yeah。
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们似乎认为 D L 对 D D 是负二。因此让我们再检查一次。让我擦掉之前的加 H。现在我们想要关于 F 的导数。所以让我们来这里创建 F，然后在这里加一个
    H。这应该打印 L 关于 F 的导数。所以我们期待看到四。对。
- en: and this is four up to floating point funkiness。 And then D L by D D should
    be F。 which is negative two。 grad is negative two。 So if we again come here and
    we change D。 D dot data plus equals H right here。 So we expect， so we've added
    a little H and then we see how L。 changed。 And we expect to print negative two。
    There we go。 So we've numerically verified。
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这是四个关于浮点计算的问题。然后 D L 对 D D 应该等于 F。其值为负二。梯度是负二。所以如果我们再次来到这里并改变 D。D dot data 加上
    H 在这里。所以我们期望，我们添加了一个小 H，然后看到 L 的变化。我们期望打印负二。就是这样。我们已经在数值上验证了。
- en: What we're doing here is kind of like an inline gradient check。 Gradient check
    is when we are deriving this like back propagation and getting the derivative。
    with respect to all the intermediate results。 And then numerical gradient is just，
    you know。 estimating it using small step size。 Now we're going to the crux back
    propagation。
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里做的就像是一个内联梯度检查。梯度检查是当我们推导出反向传播并获得所有中间结果的导数时。而数值梯度就是，估计它，使用小步长。现在我们要进入反向传播的关键部分。
- en: So this will be the， most important node to understand。 because if you understand
    the gradient for this node。 you understand all of back propagation and all training
    of neural nets， basically。 So we need。 to derive D L by B C。 In other words， the
    derivative L with respect to C， because we've。
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是最重要的节点，因为如果你理解这个节点的梯度，你就理解了所有反向传播和神经网络的训练。因此，我们需要推导 D L 对 B C 的导数。换句话说，L
    关于 C 的导数，因为我们。
- en: computed all these other gradients already。 Now we're coming here and we're
    continuing the back。 propagation manually。 So we want D L by D C， and then we'll
    also derive D L by D E。 Now here's the problem。 How do we derive D L by D C？ We
    actually know the derivative L with respect。 to D。 So we know how L is sensitive
    to D。 But how is L sensitive to C？ So if we wiggle C， how does。
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经计算了所有其他梯度。现在我们来这里手动继续反向传播。所以我们想要 D L 对 C 的导数，然后我们也会推导 D L 对 E 的导数。现在问题来了。我们如何推导
    D L 对 C 的导数？我们实际上知道 L 关于 D 的导数。因此，我们知道 L 对 D 的敏感度。但是 L 对 C 的敏感度如何？所以如果我们微调 C，如何影响
    L 通过 D？
- en: that impact L through D？ So we know D L by D C。 And we also here know how C
    impacts D。 And so just。 very intuitively， if you know the impact that C is having
    on D。 and the impact that D is having on L， then you should be able to somehow
    put that information together to figure out how C impacts L。 And indeed， this
    is what we can actually do。 So in particular。
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们知道 D L 对 C 的导数。我们也知道 C 如何影响 D。因此，直观地说，如果你知道 C 对 D 的影响，以及 D 对 L 的影响，那么你应该能够以某种方式将这些信息结合起来，弄清楚
    C 如何影响 L。事实上，这正是我们可以做到的。特别地。
- en: we know just concentrating on D first。 Let's look at how what is derivative
    basically of D with respect to C。 So in other words， what is D D， by D C？ So here
    we know that D is C times C plus E。 That's what we know。 And now we're， interested
    in D D by D C。 If you just know your calculus again and you remember that the。
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们先专注于 D。让我们看看 D 对 C 的导数基本上是什么。所以换句话说，D D 对 D C 是什么？在这里，我们知道 D 是 C 乘以 C 加 E。这就是我们所知道的。现在我们对
    D D 对 D C 感兴趣。如果你再回想一下微积分。
- en: differentiating C plus E with respect to C， you know that that gives you 1 to
    0。 And we can also go。 back to the basics and derive this。 Because again， we can
    go to our f of x plus H minus f of x。 derived by h。 That's the definition of a
    derivative as h goes to 0。 And so here。 focusing on C and its， effect on D， we
    can basically do the f of x plus h will be C is incremented by h plus E。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 对 C 加 E 进行微分，你知道那给你 1 到 0。我们也可以回到基础知识并推导这个。因为我们可以使用 f 的 x 加 H 减去 f 的 x，由 h 导出的，这是导数的定义，h
    趋近于 0。因此，专注于 C 及其对 D 的影响，我们基本上可以得到 f 的 x 加 h 将是 C 增加 h 加 E。
- en: That's the， first evaluation of our function， minus C plus E。 And then divide
    h。 And so what is this？ Just， expanding this out， this will be C plus H plus E
    minus C minus E divided h。 And then you see here， how C minus E cancels E minus
    E cancels were left with h over h， which is 1。0。 And so， by symmetry also， D D
    by D， E will be 1。0 as well。 So basically the derivative of a sum。
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们函数的第一次评估，减去 C 加 E。然后除以 h。那么这是什么？展开来看，这将是 C 加 H 加 E 减去 C 减去 E 除以 h。然后你会看到
    C 减去 E 取消了 E 减去 E 取消了，剩下的就是 h 除以 h，即 1。0。因此，通过对称性，D D 对 D，E 也将是 1。0。因此，基本上一个和的导数。
- en: expression is very simple。 And this is the local derivative。 So I call this
    the local derivative because， we have the final output value all the way at the
    end of this graph。 And we're now like a small node， here。 And this is a little
    plus node。 And it。 the little plus node doesn't know anything about the， rest
    of the graph that it's embedded in。
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 表达式非常简单。这是局部导数。所以我称之为局部导数，因为我们在这个图的最末端有最终输出值。而我们现在像是这里的一个小节点。这是一个小加号节点。这个小加号节点对它所嵌入的图的其余部分一无所知。
- en: All it knows is that I did it plus。 It took a C and an E， added them and created
    a D。 And this plus node also knows the local influence of C on D。 rather than
    the derivative of D with respect to C。 And it also knows the derivative of D with
    respect， to E。 But that's not what we want。
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 它只知道我做了加法。它取了C和E，进行了相加，得到了D。而这个加号节点也知道C对D的局部影响，而不是D相对于C的导数。它还知道D相对于E的导数。但这不是我们想要的。
- en: That's just the local derivative。 What we actually want is D， L by D C。 And
    L could L is here just one step away。 But in a general case， this little plus。
    node is could be embedded in like a massive graph。 So again， we know how L impacts
    D。 And now we know， how C and E impact D。 How do we put that information together
    to write D L by D C？
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是局部导数。我们实际想要的是D L by D C。而L可以在这里仅一步之遥。但在一般情况下，这个小加号节点可能嵌入在一个庞大的图中。所以再一次，我们知道L如何影响D。而现在我们知道C和E如何影响D。我们如何将这些信息结合起来以写出D
    L by D C？
- en: And the answer， of course， is the chain rule in calculus。 And so I pulled up
    a chain rule here from Wikipedia。 And I'm going to go through this very briefly。
    So chain rule Wikipedia sometimes can be very， confusing and calculus can be very
    confusing。 Like。 this is the way I learned chain rule and， was very confusing。
    Like what is happening？
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 答案当然就是微积分中的链式法则。所以我从维基百科中拉出了链式法则。我将非常简要地介绍一下。所以维基百科中的链式法则有时会让人非常困惑，而微积分也会非常混乱。就像，这是我学习链式法则的方式，真的是非常混乱。到底发生了什么？
- en: It's just complicated。 So I like this expression much， better。 If a variable
    Z depends on a variable Y， which itself depends on a variable X。 then Z depends
    on X as well， obviously through the intermediate variable Y。 In this case， the
    chain。 rule is expressed as if you want D Z by DX， then you take the D Z by D
    Y and you multiply it by。
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是复杂之处。所以我更喜欢这个表达式。如果变量Z依赖于变量Y，而Y又依赖于变量X。那么显然，Z也依赖于X，通过中间变量Y。在这种情况下，链式法则表述为如果你想要D
    Z by DX，那么你就取D Z by D Y并将其相乘。
- en: D Y by DX。 So the chain rule fundamentally is telling you how we chain these
    derivatives together。 correctly。 So to differentiate through a function composition，
    we have to apply a。 multiplication of those derivatives。 So that's really what
    chain rule is telling us。 And there's a nice little intuitive explanation here，
    which I also think is kind of cute。
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: D Y by DX。所以链式法则基本上是告诉你如何正确地将这些导数连接在一起。因此，要通过函数复合体进行微分，我们必须对这些导数进行相乘。所以这正是链式法则告诉我们的。这里有一个很好的直观解释，我认为也挺可爱的。
- en: The chain rule says that knowing the instantaneous rate of change of Z with
    respect to Y and Y。 relative to X allows one to calculate the instantaneous rate
    of change of Z relative to X。 As a product of those two rates of change， simply
    the product of those two。 So here's a good one。 If a car travels twice as fast
    as a bicycle， and the bicycle is four times as fast as walking men。
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 链式法则表明，知道Z相对于Y的瞬时变化率和Y相对于X的变化率，可以计算Z相对于X的瞬时变化率。作为这两种变化率的乘积，简单地说就是这两者的乘积。所以这里有一个好的例子。如果汽车的速度是自行车的两倍，而自行车的速度是步行人的四倍。
- en: then the car travels two times four， eight times as fast as a man。 And so this
    makes it very clear。 that the correct thing to do sort of is to multiply。 So cars
    twice as fast as a bicycle。 and bicycle is four times as fast as men。 So the car
    will be eight times as fast as the men。 And so we can take these intermediate
    rates of change， if you will， and multiply them together。
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 然后汽车的速度是人的两倍，四倍于人。因此这非常明确，正确的做法是相乘。所以汽车的速度是自行车的两倍，而自行车的速度是人的四倍。所以汽车的速度将是人的八倍。所以我们可以将这些中间变化率相乘。
- en: And that justifies the chain rule intuitively。 So have a look at chain rule，
    but here， really。 what it means for us is there's a very simple recipe for deriving
    what we want， which is D L by D C。 And what we have so far is we know one， and
    we know what is the impact of D on L。 So we know D L by。 D D， the derivative of
    L with respect to D D。 We know that that's negative two。 And now because of。
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这从直观上证明了链规则。因此看看链规则，但在这里，实际上对我们意味着有一个非常简单的公式来推导我们想要的，即 D L 和 D C。到目前为止，我们知道一，并且知道
    D 对 L 的影响。因此我们知道 D L 和 D D，关于 D D 的 L 的导数。我们知道这是负二。现在因为。
- en: this local reasoning that we've done here， we know D D by D C。 So how does C
    impact D？ And in。 particular， this is a plus node。 So the local derivative is
    simply 1。0。 It's very simple。 And so the chain rule tells us that D L by D C，
    going through this intermediate variable。 will just be simply D L by D D times
    D D by D C。 That's chain rule。 So this is identical to what's。
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们做的局部推理中，我们知道 D D 和 D C。那么 C 如何影响 D？特别是，这是一个加节点。所以局部导数简单地是 1.0。这非常简单。因此链规则告诉我们，D
    L 和 D C，通过这个中间变量，只是 D L 和 D D 乘以 D D 和 D C。这是链规则。所以这与。
- en: happening here， except Z is R L， Y is R D， and X is R C。 So we literally just
    have to multiply these。 And because these local derivatives。 like D D by D C are
    just one， we basically just copy over D L by D D。 because this is just times one。
    So what is it？ So because， D L by D D is negative two。
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生的情况是，Z 是 R L，Y 是 R D，而 X 是 R C。所以我们实际上只需将这些相乘。因为这些局部导数，比如 D D 和 D C，都是一，所以我们基本上只是复制
    D L 和 D D，因为这只是乘以一。那么是什么呢？因为 D L 和 D D 是负二。
- en: What is D L by D C？ Well， it's the local gradient 1。0 times D L by D D， which
    is negative two。 So literally， what a plus node does， you can look at it that
    way。 is it literally just routes the gradient， because the plus nodes local derivatives
    are just one。 And so in the chain rule， one times D L by D D is just D L by D
    D。 And so that derivative。
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: D L 和 D C 是什么？嗯，这是局部梯度 1.0 乘以 D L 和 D D，即负二。所以字面上，加节点的作用可以这样看：它实际上只是路由梯度，因为加节点的局部导数都是一。因此在链规则中，一乘以
    D L 和 D D 就是 D L 和 D D。所以那个导数。
- en: just gets routed to both C and to E in the skates。 So basically， we have that
    E dot grad。 or let's start with C， since that's the one we looked at， is negative
    two times one， negative two。 And in the same way， my symmetry， E dot grad， will
    be negative two。 That's the claim。 So we can set those。 We can redraw。 And you
    see how we just assign negative two negative two。
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样被路由到 C 和 E 的滑板中。所以基本上，我们有 E.dot.grad，或者从 C 开始，因为这是我们查看的那个，是负二乘以一，负二。以同样的方式，我的对称性，E.dot.grad
    将是负二。这就是我们的主张。所以我们可以设置这些。我们可以重新绘制。你会看到我们只是分配负二负二。
- en: So this back propagating signal， which is carrying the information of like，
    what is the derivative。 of L with respect to all the intermediate nodes？ We can
    imagine it almost like flowing backwards。 through the graph。 And a plus node will
    simply distribute the derivative to all the leaf nodes。 sorry， to all the children
    nodes of it。 So this is the claim。 And now let's verify it。
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这个反向传播信号携带着有关 L 关于所有中间节点的导数的信息。我们几乎可以想象它像是向后流动，通过图。而加节点将简单地将导数分配给所有叶子节点，抱歉，是它的所有子节点。这就是我们的主张。现在让我们来验证一下。
- en: So let me remove the plus H here from before。 And now instead， what we're going
    to do is we want。 to incurrence C。 So C dot data will be incremented by H。 And
    when I run this。 we expect to see negative， two， negative two。 And then of course，
    for E。 So E dot data plus equals H。 And we expect to see， negatives here。 Simple。
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我把这里的加 H 去掉。现在，我们要做的是引入 C。所以 C.dot.data 将增加 H。当我运行这个时，我们期望看到负二，然后当然是 E。所以
    E.dot.data 加上 H，我们期望看到负值。简单。
- en: So those are the derivatives of these internal nodes。 And now we're going to。
    recurse our way backwards again。 And we're again going to apply the chain rule。
    So here we go。 our second application of chain rule。 And we will apply it all
    the way through the graph。 which just happened to only have one more node remaining。
    We have that dL by the E， as we have。
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这些是这些内部节点的导数。现在我们将再次递归回去。我们将再次应用链规则。所以这里我们来了。我们第二次应用链规则。我们将其应用到整个图中，而这个图恰好只剩下一个节点。我们有
    D L 和 E，如我们所知。
- en: just calculated is negative two。 So we know that。 So we know the derivative
    of L with respect to E。 And now we want dL by dA， right？ And the chain rule is
    telling us that that's just dL by dE。 negative two times the local gradient。 So
    what is the local gradient？ Basically dE by dA。 We have to look at that。 So I'm
    a little times node inside a massive graph。
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 刚计算出的结果是负二。所以我们知道了。因此我们知道 L 关于 E 的导数。现在我们想求 dL 对 dA，对吧？链式法则告诉我们这就是 dL 对 dE，负二乘以局部梯度。那么局部梯度是什么呢？基本上是
    dE 对 dA。我们得看看这个。所以我在一个庞大的图中的一个节点里。
- en: And I only know that I did， A times B and I produced an E。 So now what is dE
    by dA and dE by dB？
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 而且我只知道我做了 A 乘以 B，产生了 E。那么 dE 对 dA 和 dE 对 dB 是多少呢？
- en: That's the only thing that I， sort of know about。 That's my local gradient。
    So because we have that E is A times B， we're， asking what is dE by dA。 And of
    course we just did that here。 We had times， so I'm not going to， redrive it。 But
    if you want to differentiate this with respect to A， you'll just get B， right？
    The， value of B。
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我唯一知道的事情。这就是我的局部梯度。因为我们知道 E 是 A 乘以 B，所以我们在问 dE 对 dA。我们刚刚做了这个乘法，所以我不打算重新推导。但如果你想对
    A 求导，你会得到 B，对吧？B 的值。
- en: which in this case is negative 3。0。 So basically we have that dL by dA。 Well，
    let me。 just do it right here。 We have that A dot grad and we are applying chain
    rule here is dL by dE。 which we see here is negative two times what is dE by dA？
    It's the value of B， which is negative 3。 That's it。 And then we have B dot grad
    is again dL by dE， which is negative two。
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下是负 3.0。所以基本上我们有 dL 对 dA。让我就在这里做。我们有 A 点 grad，并且我们在应用链式法则，这里是 dL 对 dE。我们看到是负二乘以
    dE 对 dA，结果是 B 的值，负 3。这就是了。然后我们有 B 点 grad，再次是 dL 对 dE，结果是负二。
- en: just the same way times what is dE by dB is the value of A， which is two dot
    2。0。 That's the value。 of A。 So these are our claimed derivatives。 Let's redraw。
    And we see here that A dot grad turns out。 to be six because that is negative
    two times negative three。 And B dot grad is negative four， times。 sorry， is negative
    two times two， which is negative four。 So those are our claims。
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 以相同的方式，dE 对 dB 的值就是 A，结果是 2.0。这就是 A 的值。所以这是我们声称的导数。让我们重新绘制一下。我们看到 A 点 grad 的结果是
    6，因为那是负二乘以负三。而 B 点 grad 是负四，抱歉，是负二乘以二，结果是负四。这就是我们的声明。
- en: Let's delete this and let's verify them。 We have A here， A dot data plus equals
    H。 So the claim is that A dot grad is six。 Let's verify six。 And we have B dot
    data plus equals H。 So nudging B by H and looking at what happens， we claim it's
    negative four and indeed it's。 negative four plus minus again float oddness。 And
    that's it。 This， that was the manual back。
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们删除这些，来验证它们。我们这里有 A，A 点数据加上 H。因此，声明是 A 点 grad 是 6。让我们验证一下 6。我们有 B 点数据加上 H。所以通过
    H 轻推 B，看看发生了什么，我们声称是负四，确实是负四，再加上浮点数的奇异性。就这样。这是手动的反向传播。
- en: propagation all the way from here to all the leaf nodes。 And I've done it piece
    by piece。 And really all we've done is as you saw， we iterated through all the
    nodes one by one。 and locally applied the chain rule。 We always know what is the
    derivative of L with respect to。 this little output。 And then we look at how this
    output was produced。 This output was produced。
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 传播从这里到所有叶节点。我是逐步完成的。实际上，我们所做的就是，正如你所见，我们一个接一个地迭代所有节点，局部应用链式法则。我们始终知道 L 关于这个小输出的导数是什么。然后我们查看这个输出是如何产生的。这个输出是这样产生的。
- en: through some operation。 And we have the pointers to the children nodes of this
    operation。 And so in。 this little operation， we know what the local derivatives
    are。 And we just multiply them onto。 the derivative always。 So we just go through
    and recursively multiply on the local derivatives。 And that's what back propagation
    is， is just a recursive application of chain rule backwards。
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一些操作。我们有这个操作的子节点指针。在这个小操作中，我们知道局部导数是什么。我们只需将它们乘到导数上。因此，我们就这样递归地乘以局部导数。这就是反向传播，就是向后递归应用链式法则。
- en: through the computation graph。 Let's see this power in action， just very briefly。
    What we're。 going to do is we're going to nudge our inputs to try to make L go
    up。 So in particular， what we're。 doing is we want a data data， we're going to
    change it。 And if we want L to go up。 that means we just， have to go in the direction
    of the gradient。
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 通过计算图。让我们简要看看这个力量在运作，我们将要做的是轻推我们的输入，以尝试让 L 上升。因此，特别是，我们想要一个数据，我们要改变它。如果我们希望
    L 上升，那就意味着我们必须朝着梯度的方向前进。
- en: So a should increase in the direction of gradient， by like some small step amount。
    This is the step size。 And we don't just want this for being， but also for， being
    also for C。 Also for F， those are leaf nodes， which we usually have control over。
    And if we。 nudge in direction of the gradient， we expect a positive influence
    on L。 So we expect L to go up。
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，a应该朝着梯度的方向增加，像是一些小的步长。这是步长。我们不仅希望这适用于B，还希望它也适用于C。同样适用于F，那些是我们通常控制的叶子节点。如果我们朝梯度的方向轻微调整，我们期待对L产生积极影响。所以我们希望L上升。
- en: positively。 So it should become less negative。 It should go up to say negative，
    you know。 six or something like that。 It's hard to tell exactly。 And we'd have
    to rerun the forward pass。 So let me just do that here。 This would be the forward
    pass。 F would be unchanged。 This is。 effectively the forward pass。 And now if
    we print L dot data， we expect， because we nudged all the。
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 积极的。所以它应该变得不那么消极。它应该上升到负六之类的。具体来说很难判断。我们需要重新运行前向传播。那么让我在这里做一下。这就是前向传播。F将保持不变。这实际上是前向传播。如果我们打印L的数值，我们期待，因为我们轻微调整了所有的。
- en: values， all the inputs in the rational gradient， we expected a less negative
    L。 We expected to go up。 So maybe it's negative six or so。 Let's see what happens。
    Okay。 negative seven。 And this is， basically one step of an optimization that
    will end up running。 And really this gradient just， give us some power because
    we know how to influence the final outcome。
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 值，所有输入的合理梯度，我们预计L会变得不那么消极。我们预计会向上移动。所以也许是负六左右。让我们看看会发生什么。好的，负七。这基本上是一次优化步骤的结果，并且这梯度实际上给了我们一些力量，因为我们知道如何影响最终结果。
- en: And this will be extremely useful， for training， you know， that's as well as
    COC。 So now I would like to do one more example of， manual back propagation using
    a bit more complex and useful example。 We are going to back propagate， through
    a neuron。 So we want to eventually build out neural networks。 And in the simplest
    case， these are multilateral perceptrons， as they're called。
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于训练是非常有用的，你知道的，这同样适用于COC。因此现在我想做一个手动反向传播的例子，使用一个更复杂和有用的例子。我们将通过一个神经元进行反向传播。所以我们最终想要构建神经网络。在最简单的情况下，这些被称为多层感知器。
- en: So this is a two layer neural net。 And it's， got these hidden layers made up
    of neurons。 And these neurons are fully connected to each other。 Now biologically
    neurons are very complicated devices。 but we have very simple mathematical models
    of them。 And so this is a very simple mathematical model of a neuron。 You have
    some inputs， Xs。 and then you have these synapses that have weights on them。 So
    the Ws are weights。 And then the synapse interacts with the input to this neuron
    multiplicatively。 So what flows to the。 cell body of this neuron is W times X。
    But there's multiple inputs。
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种双层神经网络。它有这些由神经元组成的隐藏层。这些神经元彼此完全连接。现在生物学上，神经元是非常复杂的设备，但我们有它们的非常简单的数学模型。所以这是一个非常简单的神经元数学模型。你有一些输入，Xs，然后你有这些带权重的突触。所以Ws是权重。然后突触与这个神经元的输入以乘法方式相互作用。所以流向这个神经元细胞体的是W乘以X。但输入是多个。
- en: So there's many W times Xs flowing， to the cell body。 The cell body then has
    also like some bias。 So this is kind of like the inert innate， sort of trigger
    happiness of this neuron。 So this bias can make it a bit more trigger happy or
    with， less trigger happy。 regardless of the input。 But basically we're taking
    all the W times X， of all the inputs。
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 所以有很多W乘以Xs流向细胞体。细胞体也有一些偏置。因此这有点像这个神经元的内在触发快乐程度。这个偏置可以使其更容易触发或更不容易触发，无论输入如何。但基本上我们正在处理所有输入的W乘以X。
- en: adding the bias。 And then we take it through an activation function。 And this
    activation function is usually some kind of a squash function， like a sigmoid
    or 10H or。 something like that。 So as an example， we're going to use the 10H in
    this example。 NumPy has a np。10H。 So we can call it on a range， and we can plot
    it。 Those are the 10H function。
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 添加偏置。然后我们通过一个激活函数来处理它。这个激活函数通常是某种类型的压缩函数，比如sigmoid或10H或其他的。因此作为例子，我们将在这个例子中使用10H。NumPy有np。10H。所以我们可以在一个范围内调用它，并进行绘制。这些就是10H函数。
- en: And you see that the inputs as they come in， get squashed on the white coordinate
    here。 So right at zero， we're going to get exactly zero。 And then as you go more
    positive in the input。 then you'll see that the function will only go up to， one
    and then plateau out。 And so if you pass in very positive inputs， we're going
    to cap it， smoothly at one。
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到输入在这里进入时被压缩到白色坐标。因此在零点时，我们会得到确切的零。然后随着输入变得越来越正，你会看到函数仅会上升到一，然后趋于平稳。因此如果你输入非常正的值，我们将平滑地限制在一。
- en: And on a negative side， we're going to cap it smoothly to negative one。 So that's，
    10H。 And that's the squash function or an activation function。 And what comes
    out of this neuron is。 just the activation function applied to the dot product
    of the weights and the inputs。 So let's write one out。 I'm going to copy paste
    because， I don't want to type too much。 But okay。
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在负侧，我们将平滑地限制为负一。所以这是 10H。这就是压缩函数或激活函数。这个神经元输出的就是激活函数应用于权重和输入的点积。因此我们来写一个。我要复制粘贴，因为我不想打太多。但好吧。
- en: so here we have the inputs， x1， x2。 So this is a two dimensional neuron。 So
    two inputs are going to come in。 These are thought out as the weights of this
    neuron， weights w1。 w2。 And these weights again， are the synaptic strengths for
    each input。 And this is the bias of the neuron B。 And now we want to do is according
    to this model。
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们有输入，x1，x2。这是一个二维神经元。所以两个输入会进来。这些被视为这个神经元的权重，权重 w1，w2。这些权重再次是每个输入的突触强度。这是神经元的偏置
    B。现在我们想做的是根据这个模型。
- en: we need to multiply x1 times w1 and x2 times w2。 And then we need to add bias
    on top of it。 And it gets a little messy here， but all we are trying， to do is
    x1， w1 plus x2， w2 plus B。 And these are multiplied here。 Except I'm doing it
    in small steps。 so that we actually have pointers to all these intermediate nodes。
    So we have x1， w1 variable。
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将 x1 乘以 w1 和 x2 乘以 w2。然后我们需要在其上加上偏置。这里会有点复杂，但我们所要做的就是 x1，w1 加上 x2，w2 加上
    B。这里都是乘在一起的。只是我在分小步骤进行，以便我们实际有指向所有这些中间节点的指针。因此我们有 x1，w1 变量。
- en: x times x2， w2 variable， and I'm also labeling them。 So n is now the cell body
    raw activation。 without the activation function from now。 And this should be enough
    to basically plot it。 So draw a dot of n gives us x1 times w1， x2 times w2 being
    added。 then the bias gets added on top of this。 And this n is this sum。
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: x 乘以 x2，w2 变量，我也在标记它们。因此 n 现在是细胞体的原始激活，没有激活函数。从现在开始，这应该足够基本上绘制它。因此 n 的点给我们 x1
    乘以 w1，x2 乘以 w2 的和。然后偏置被加在这个基础上。这个 n 就是这个和。
- en: So we're now going to take it through an activation function。 And let's say
    we use the 10H so that we produce the output。 So what we'd like to do here is
    we'd。 like to do the output。 And I'll call it O is n dot 10H。 Okay， but we haven't
    yet written the 10H。 Now the reason that we need to implement another 10H function
    here is that 10H is a hyperbolic。
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们现在要通过激活函数。假设我们使用 10H 来产生输出。所以我们希望在这里做的是输出。我会称它为 O 是 n 点 10H。好的，但我们还没有写 10H。我们需要在这里实现另一个
    10H 函数的原因是，10H 是一个双曲函数。
- en: function。 And we've only so far implemented plus and the times。 And you can't
    make a 10H out of。 just pluses and times。 You also need exponentiation。 So 10H
    is this kind of a formula here。 You can。 use either one of these。 And you see
    that there's exponentiation involved。 which we have not implemented， yet for our
    low value node here。
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们到目前为止只实现了加法和乘法。你不能仅通过加法和乘法生成 10H。你还需要指数运算。所以 10H 是这样的一个公式。你可以使用这两个中的任何一个。你会看到涉及到指数运算，而这在我们这里的低值节点中尚未实现。
- en: So we're not going to be able to produce 10H yet。 And we have。 to go back up
    and implement something like it。 Now one option here is we could actually implement。
    exponentiation， right？ And we could return the x of a value instead of a 10H of
    a value。 Because if we had X， then we have everything else that we need。 So because
    we know how to add and。
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们还不能产生 10H。我们必须回去实现类似的东西。这里一个选项是我们可以实际实现指数运算，对吗？我们可以返回一个值的 x，而不是值的 10H。因为如果我们有
    X，那么我们就有了所需的一切。因此因为我们知道如何加法和。
- en: we know how to add and we know how to multiply。 So we'd be able to create 10H
    if we knew how to， X。 But for the purposes of this example， I specifically wanted
    to show you that we don't。 necessarily need to have the most atomic pieces in
    this value object。 We can actually like create。 functions at arbitrary points
    of abstraction。 They can be complicated functions， but they can be。
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道如何加法，也知道如何乘法。因此，如果我们知道如何进行X，我们就能创建10H。但为了这个例子的目的，我特别想告诉你，我们不一定需要在这个值对象中拥有最原子化的部分。我们实际上可以在任意抽象点创建函数。它们可以是复杂的函数，但它们也可以是。
- en: also very， very simple functions like a plus。 And it's totally up to us。 The
    only thing that matters， is that we know how to differentiate through any one
    function。 So we take some inputs and， we make an output。 The only thing that matters
    can be arbitrarily complex function。 As long as you， know how to create the local
    derivative。
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 也有非常非常简单的函数，比如加法。完全由我们决定。唯一重要的是，我们知道如何对任意一个函数进行微分。因此，我们输入一些值，输出一个结果。唯一重要的事情可以是任意复杂的函数。只要你知道如何创建局部导数。
- en: if you know the local derivative of how the inputs impact， the output， then
    that's all you need。 So we're going to cluster up all of this expression。 and
    we're not going to break it down to its atomic pieces。 We're just going to directly。
    implement 10H。 So let's do that。 Death 10H。 And then how it will be a value。
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你知道局部导数是如何影响输出的输入，那么这就是你所需要的全部。所以我们将把所有这些表达式聚集起来，而不打散到它的原子部分。我们将直接实现10H。让我们来做这个。死亡10H。然后它将是一个值。
- en: Of and we need this expression here。 So let me actually copy paste。 Let's grab
    N。 which is a cell data。 And then this， I believe is the 10H。 Math。x of， two，
    no， and my。 and minus one over two and plus one。 Maybe I can call this x。 Just
    so that it matches exactly。 Okay。 And now this will be t。 And children of this
    node， they're just one child。
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 而且我们需要这个表达式。所以让我复制粘贴一下。我们来抓取N，它是一个单元数据。然后这个，我相信是10H。Math.x的，两个，不，和我的。还有减去二分之一和加一。也许我可以叫它x。就这样，它完全匹配。好的。现在这将是t。这个节点的子节点只有一个孩子。
- en: And I'm wrapping it in a tuple。 So this， is a tuple of one object just self。
    And here the name of this operation will be 10H。 And we're， going to return that。
    Okay。 So now values should be implementing 10H。 And now we can scroll the， way
    down here。 And we can actually do n dot 10H。 And that's going to return the 10H
    output of N。
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我将它包装在一个元组中。所以这是一个包含一个对象的元组，仅仅是自我。在这里，这个操作的名称将是10H。我们将返回这个。好的。现在值应该实现10H。现在我们可以一直向下滚动。实际上我们可以做n.dot
    10H。那将返回N的10H输出。
- en: And now we should be able to draw that of， oh， not of N。 So let's see how that
    worked。 There we go。 N went through 10H to produce this up。 So now 10H is a sort
    of our little。 micro grad supported node here as an operation。 And as long as
    we know derivative of 10H。 then we'll be able to back propagate through it。 Now
    let's see this 10H in action。 Currently。
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们应该能够绘制那个，而不是N。让我们看看这是如何运作的。好了。N通过10H产生了这个输出。所以现在10H是我们这个操作的小微梯度支持节点。只要我们知道10H的导数，我们就能通过它进行反向传播。现在让我们看看10H的实际效果。目前。
- en: it's not squashing too much because the input to it is pretty low。 So the bias
    was increased。 to say eight。 Then we'll see that what's flowing into the 10H now
    is two。 And 10H is squashing。 it to 0。96。 So we're already hitting the tail of
    this 10H。 And it will sort of smoothly go up to one， and then plateau out over
    there。 Okay。
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 由于输入相对较低，所以并没有过多地压缩。因此，偏差增加到了8。然后我们会看到流入10H的值现在是2。而10H将其压缩到0.96。因此，我们已经触及到10H的尾部。它将平滑地上升到1，然后在那里达到平稳。好的。
- en: so now I'm going to do something slightly strange。 I'm going to change this
    bias from eight to this number， 6。88， et cetera。 And I'm going to do this for，
    specific reasons because we're about to start back propagation。 And I want to
    make sure that our， numbers come out nice。 They're not like very crazy numbers。
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我将做一些稍微奇怪的事情。我将把这个偏差从8改为这个数字，6.88，等等。我这样做是有特定原因的，因为我们即将开始反向传播。我想确保我们的数字看起来不错，不会是很疯狂的数字。
- en: They're nice numbers that we can sort of， understand in our head。 Let me also
    add O's label。 O is short for output here。 So that's the， okay。 So point eight
    flows into 10H comes up 0。7。 So now we're going to do back propagation。 And， we're
    going to fill in all the gradients。 So what is the derivative O with respect to
    all the， inputs here？ And of course。
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们能在脑海中理解的不错的数字。让我也加上O的标签。O在这里是输出的缩写。所以，好的，0.8流入10H变为0.7。现在我们要进行反向传播，并填入所有梯度。那么O对所有输入的导数是什么？当然。
- en: in a typical neural network setting， what we really care about the most。 is
    the derivative of these neurons on the weights specifically， the W2 and W1。 Because
    those are。 the weights that we're going to be changing， partly optimization。 And
    the other thing that we。 have to remember is here we have only single neuron，
    but in the neural not to typically have。
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的神经网络设置中，我们最关心的是这些神经元对权重的导数，特别是W2和W1。因为这些是我们要改变的权重，部分是优化。而我们必须记住的是，这里只有单个神经元，但神经网络中通常有。
- en: many neurons and they're connected。 So this is only like a one small neuron，
    a piece of a much。 bigger puzzle。 And eventually there's a loss function that
    sort of measures the accuracy of。 the neural net。 And we're back propagating with
    respect to that accuracy and trying to increase it。 So let's start off back propagation
    here and what is the derivative of O with respect to O。
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多神经元，它们是相互连接的。所以这只是一个小神经元，是一个更大拼图的一部分。最终有一个损失函数，用来测量神经网络的准确性。我们正在针对该准确性进行反向传播并试图提高它。那么让我们从这里开始反向传播，O关于O的导数是什么。
- en: The base case sort of we know always is that the gradient is just one point
    zero。 So let me fill it。 in and then let me split out the drawing function here。
    And then here cell， clear this output here。 Okay。 So now when we draw O， we'll
    see that O and the grad is one。 So now we're going to back propagate through the
    10H。 So to back propagate through 10H。
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 基本情况我们知道梯度总是1.0。所以让我填入，然后让我分开这个绘图函数。然后在这里清除这个输出。好的。现在当我们绘制O时，会看到O和梯度是1。所以现在我们将通过10H进行反向传播。为了通过10H进行反向传播。
- en: we need to know the local derivative of 10H。 So if we have that O is 10H of
    N， then what is。 D O by D N。 Now what you could do is you could come here and
    you could take this expression and。 you could do your calculus derivative taking。
    And that would work。 But we can also just scroll down。 with the PDI here into
    a section that hopefully tells us that derivative D by D X of 10H of X is。
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要知道10H的局部导数。因此，如果O是N的10H，那么D O对D N是什么？你可以过来，用这个表达式进行微积分导数运算。这是可行的。但我们也可以向下滚动，看看PDI这里的一个部分，
    hopefully能够告诉我们10H对X的导数D对D X是。
- en: any of these。 I like this one one minus 10H square of X。 So this is one minus
    10H of X squared。 So basically what this is saying is that D O by D N is one minus
    10H of N squared。 And we already have 10H of N。 It's just O。 So it's one minus
    O squared。 So O is the output here。 So the output is this number。 O dot data is
    this number。
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这些中的任何一个。我喜欢这个1减去10H平方的X。所以这是1减去10H的X平方。基本上这表明D O对D N是1减去10H的N平方。我们已经有了10H的N。这就是O。所以是1减去O平方。O是这里的输出。因此，输出就是这个数字。O点数据是这个数字。
- en: And then what this is saying is that D O by， D N is one minus this squared。
    So one minus O dot data squared is 0。5 conveniently。 So the local。 derivative
    of this 10H operation here is 0。5。 And so that would be D O by D N。 So we can
    fill in that， N dot grad is 0。5。 We'll just fill it in。 So this is exactly 0。5。
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 然后这表明D O对D N是1减去这个平方。所以1减去O点数据平方是0.5，恰好。所以这个10H操作的局部导数是0.5。因此，D O对D N是0.5。我们可以填入，N点梯度是0.5。我们就填上去。这正好是0.5。
- en: So now we're going to continue the back propagation。 This is 0。5。 And this is
    a plus node。 So how is backprop going to？ What is backprop going to do here？
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将继续反向传播。这是0.5。这是一个加法节点。那么反向传播将会怎么做？反向传播会在这里做什么？
- en: And if you remember our previous example， a plus is just a distributor of gradient。
    So this。 gradient will simply flow to both of these equally。 And that's because
    the local derivative of this。 operation is one for every one of its nodes。 So
    one times 0。5 is 0。5。 So therefore we know that。 this node here， which we called
    this， it's grad is just 0。5。 And we know that B dot grad is also 0。
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得我们之前的例子，加法只是梯度的分发器。因此，这个梯度将简单地平均流向这两个。这是因为这个操作的局部导数对于每个节点都是1。因此1乘以0。5是0。5。因此我们知道这个节点，这个grad只是0。5。而我们知道B点grad也是0。
- en: 5。 So let's set those and let's draw。 So those are 0。5。 Continuing， we have
    another plus 0。5。 Again。 we'll just distribute。 So 0。5 will flow to both of these。
    So we can set theirs。 X2W2 as well。 grad is 0。5。 And let's redraw。 Plus this are
    my favorite operations to backpropagate。 through because it's very simple。 So
    now it's flowing into these expressions is 0。5。 And so really。
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 5。所以让我们设定这些并绘制图形。所以这些是0。5。继续，我们还有另一个加0。5。再次，我们只需分配。所以0。5将流向这两个。因此，我们也可以设置X2W2。grad是0。5。让我们重新绘制。加上这是我最喜欢的反向传播操作，因为它非常简单。所以现在流入这些表达式的是0。5。实际上。
- en: again， keep in mind what the derivative is telling us at every point in time
    along here。 This is saying that if we want the output of this neuron to increase，
    then the influence on these。 expressions is positive on the output。 Both of them
    are positive contribution to the output。 So now backpropagating to X2 and W2 first。
    This is a times node。 So we know that the local。
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，请记住导数在这里每个时间点告诉我们的是什么。这表示如果我们希望这个神经元的输出增加，那么对这些表达式的影响是正向的。两者都对输出有正贡献。所以现在首先反向传播到X2和W2。这是一个乘法节点。所以我们知道局部。
- en: derivative is the other term。 So if we want to calculate X2 dot grad， then can
    you think through。 what it's going to be？ So X2 dot grad will be W2 dot data times
    this X2W2 dot grad。 Right？
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 导数是另一个项。因此，如果我们想计算X2点grad，那么你能想一下它将是什么吗？所以X2点grad将是W2点data乘以这个X2W2点grad。对吧？
- en: And W2 dot grad will be X2 dot data times X2W2 dot grad。 Right？
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 而W2点grad将是X2点data乘以X2W2点grad。对吗？
- en: So that's the local piece of chain rule。 Let's set them and let's redraw。 So
    here we see that the gradient on our weight 2 is 0 because X2's， data was 0。 Right？
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是链式法则的局部部分。让我们设定它们并重新绘制。所以在这里我们看到我们的权重2的梯度是0，因为X2的数据是0。对吗？
- en: But X2 will have the gradient 0。5 because data here was 1。 And so what's interesting，
    here， right。 is because the input X2 was 0， then because of the way the times
    works， of course。 this gradient will be 0。 And think about intuitively why that
    is。 Derivative always tells us the。 influence of this on the final output。 If
    I wiggle W2， how is the output changing？
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 但X2将具有梯度0。5，因为这里的数据是1。所以有趣的是，因为输入X2是0，所以由于乘法的方式，这个梯度当然会是0。想想为什么直观上是这样的。导数总是告诉我们这个对最终输出的影响。如果我
    wiggle W2，输出是如何变化的？
- en: It's not changing， because we're multiplying by 0。 So because it's not changing。
    there is no derivative。 And 0 is the， correct answer because we're splashing with
    that 0。 And let's do it here。 0。5 should come here and， flow through this times。
    And so we'll have that X1 dot grad is， can you think through a little bit， what。
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 它没有变化，因为我们乘以0。因此，因为没有变化，所以没有导数。而0是正确的答案，因为我们与0相乘。让我们在这里做。0。5应该放在这里，并流过这个乘法。所以我们将有X1点grad，你能稍微想一下是什么吗？
- en: what this should be？ The local derivative of times with respect to X1 is going
    to be， w1。 So w1's data times X1 w1 dot grad。 And w1 dot grad will be X1 dot data
    times X1 w2 w1 dot grad。 Let's see what those came out to be。 So this is 0。5。
    So this would be negative 1。5 and this would be 1。 And we back propagate it through
    this expression。
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该是什么？关于X1的乘法的局部导数将是w1。因此w1的数据乘以X1 w1点grad。而w1点grad将是X1点data乘以X1 w2 w1点grad。让我们看看它们的结果是什么。因此这是0。5。所以这将是负1。5，而这将是1。我们通过这个表达式反向传播。
- en: These are the actual final derivatives。 So if we， want this neuron's output
    to increase。 we know that what's necessary is that w2， we have no gradient。 w2
    doesn't actually matter to this neuron right now。 But this neuron， this weight
    should go up。 So if this weight goes up， then this neuron's output would have
    gone up and proportionally。
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是实际的最终导数。因此，如果我们希望这个神经元的输出增加，我们知道所需的是w2，我们没有梯度。w2对这个神经元现在并不重要。但这个神经元，这个权重应该增加。所以如果这个权重增加，那么这个神经元的输出就会相应增加。
- en: because the gradient is 1。 Okay， so doing the back propagation manually is obviously
    ridiculous。 so we are now going to put an end to this suffering。 And we're going
    to see how we can implement。 the backward pass a bit more automatically。 We're
    not going to be doing all of it manually out here。 It's now pretty obvious to
    us by example how these pluses and times are back rapidly ingredients。
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 因为梯度是1。好吧，所以手动执行反向传播显然是荒谬的。因此我们现在要结束这种痛苦。我们将看看如何更自动地实现反向传播。我们不会在这里手动做所有的事情。通过示例现在对我们来说非常明显，这些加法和乘法是如何迅速影响梯度的。
- en: So let's go up to the value object and we're going to start codifying what we've
    seen。 in the examples below。 So we're going to do this by storing a special self
    dot backward。 And underscore backward。 And this will be a function。 which is going
    to do that little piece of chain， rule at each little node that took inputs and
    produced output。
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们去看看值对象，并开始对我们所看到的进行编码。在下面的示例中。我们将通过存储一个特殊的自我反向（self dot backward）和下划线反向（underscore
    backward）来做到这一点。这个将是一个函数，它将在每一个小节点上执行链式法则的小部分，输入并产生输出。
- en: We're going to store， how we are going to chain the outputs gradient into the
    inputs gradients。 So by default， this will be a function that doesn't do anything。
    So。 and you can also see that here in the value， in micro grad。 So with this backward
    function。 by default， doesn't do anything。 This is a empty function。 And that
    would be sort of the case。
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将存储如何将输出的梯度链接到输入的梯度。因此默认情况下，这将是一个什么都不做的函数。并且你也可以在值中看到这一点，在`micro grad`中。所以默认情况下，这个反向函数什么都不做。这是一个空函数。这大致就是情况。
- en: for example， for a leaf node。 For leaf node， there's nothing to do。 But now
    if when we're creating these out values， these out values are an addition of self
    and other。 And so we're going to set outs backward， to be the function that propagates
    the gradient。 So let's define what should happen。 And we're going to store it
    in a closure。
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于叶节点，没什么可做的。但是现在当我们创建这些输出值时，这些输出值是`self`和`other`的和。因此我们将把`outs backward`设置为传播梯度的函数。所以让我们定义应该发生的事情。我们将把它存储在一个闭包中。
- en: Let's define what should happen when we call， outscrad。 For addition。 our job
    is to take outscrad and propagate it into self-scrad and other， grad。 So basically
    we want to solve self-grad to something。 And we want to set others that grad。
    to something。 Okay。 And the way we saw below how chain rule works。
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义调用`outscrad`时应该发生的事情。对于加法，我们的任务是将`outscrad`传播到`self-scrad`和其他的`grad`。所以基本上我们想要将`self-grad`解决为某个值。并且我们想要将其他的`grad`设为某个值。好吧。我们在下面看到链式法则是如何工作的。
- en: we want to take the local derivative， times the sort of global derivative， I
    should call it。 which is the derivative of the final， output of the expression
    with respect to outs data。 with respect to out。 So the local derivative， of self
    in an addition is 1。0。 So it's just 1。0 times outs grad。 That's the chain rule。
    And others that grad will be 1。0 times outscrad。
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要取局部导数，乘以某种全局导数，我应该称之为。这是表达式最终输出相对于`outs data`的导数。相对于`out`。因此在加法中，`self`的局部导数是1.0。所以就是1.0乘以`outs
    grad`。这就是链式法则。而其他的`grad`将是1.0乘以`outscrad`。
- en: And what you basically what you're seeing here， is that outs grad will simply
    be copied onto self-scrad and others grad。 as we saw happens for an addition，
    operation。 So we're going to later call this function to propagate the gradient
    having done an addition。
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上你在这里看到的是，`outs grad`将简单地被复制到`self-scrad`和其他的`grad`中。正如我们在加法操作中看到的那样。因此，我们稍后将调用这个函数来传播梯度，完成加法后。
- en: Let's now do multiplication。 We're going to also define that backward。 And we're
    going to set。 its backward to be backward。 And we want to chain out grad into
    self-that grad and others。 that grad。 And this would be a little piece of chain
    rule for multiplication。 So we'll have。 so what should it be？ Can you think through？
    So what is the local derivative？ Here。
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们进行乘法。我们也将定义反向。并且我们将把它的反向设为反向。我们想要将`out grad`链接到`self-that grad`和其他的`that
    grad`。这将是乘法的一个小链式法则。因此，我们将有。那么应该是什么呢？你能思考一下吗？局部导数是什么？这里。
- en: the local derivative was others that data。 And then others that data。 And then
    times out that grad。 that's chain rule。 And here we have self-that data times
    out that grad。 That's what we've been doing。 And finally here for 10H， that backward。
    And then we want to set outs backwards to be just backward。 And here we need to
    back propagate。
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 局部导数是其他数据。然后是其他数据。然后乘以输出梯度。那就是链式法则。在这里我们有自我数据乘以输出梯度。这就是我们一直在做的。最后，在这里对于10H，进行反向传播。然后我们想将输出反向传播设置为仅仅是反向传播。在这里我们需要反向传播。
- en: We have out that grad and we want to chain it into self-that grad。 And self-that
    grad will be the local derivative of this operation that we've done here。 which
    is 10H。 And so we saw that the local gradient is 1 minus the 10H of x squared。
    which here is t。 That's the local derivative because that's t is the output of
    this 10H。
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经找到了梯度，并希望将其链接到自我梯度。自我梯度将是我们在这里进行的这个操作的局部导数，也就是10H。因此，我们看到局部梯度是1减去x平方的10H，这里的t就是这个局部导数，因为t是这个10H的输出。
- en: So 1 minus t square is， the local derivative。 And then gradients has to be multiplied
    because of the chain rule。 So out grad is chained through the local gradient into
    self-that grad。 And that should be basically it。 So we're going to redefine our
    value node。 We're going to swing all the way down here。 And we're going to redefine
    our expression。
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 所以1减去t平方就是局部导数。然后梯度需要因为链式法则而相乘。因此，输出梯度通过局部梯度链接到自我梯度。这基本上就是了。所以我们将重新定义我们的值节点。我们将一直向下移动，并重新定义我们的表达式。
- en: Make sure that all the grads are zero。 Okay。 But now we don't have to do this
    manually anymore。 We are going to basically be calling the dot backward in the
    right order。 So first we want to call o's dot backward。 So o was the outcome of
    10H。 Right。 So calling o's that those goes backward， will be this function。 This
    is what it will do。
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 确保所有的梯度都是零。好的。但是现在我们不再需要手动执行这个了。我们基本上会按正确的顺序调用dot反向传播。所以首先我们想调用o的dot反向传播。o是10H的结果。对。调用o的反向传播将会是这个函数。这就是它要做的。
- en: Now we have to be careful because there's a times。 out that grad and out that
    grad remember is initialized to zero。 So here we see grad zero。 So as a base case
    we need to set both that grad to 1。0， to initialize this with one。 And then once
    this is one we can call o dot backward。
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们必须小心，因为有一个乘法。输出梯度，记住初始化为零。因此我们在这里看到梯度为零。所以作为基本情况，我们需要将两个梯度都设置为1.0，以用1来初始化它。一旦这个是1，我们就可以调用o点反向传播。
- en: And what that should do is it should propagate this grad through 10H。 So the
    local derivative times the global derivative which is initialized at one。 So this
    should。 So I thought about redoing it but I figured I should just leave the error
    in here because。 it's pretty funny。 Why is non-tia object not callable？ It's because，
    I screwed up。
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 而这应该做的是将这个梯度传播通过10H。因此，局部导数乘以全局导数，而全局导数初始化为1。所以这应该是这样。我考虑重新做一遍，但我想我应该把错误留在这里，因为……这很搞笑。为什么non-tia对象不可调用？因为我搞砸了。
- en: We're trying to save these functions。 So this is correct。 This here。 we don't
    want to call the function because that returns none。 These functions return none。
    We just want to store the function。 So let me redefine the value object。 And then
    we're going to come back in redefine the expression draw dot。 Everything is great。
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在尝试保存这些函数。所以这是正确的。这里。我们不想调用这个函数，因为那样会返回None。这些函数返回None。我们只想存储这个函数。所以让我重新定义这个值对象。然后我们将回来重新定义表达式draw
    dot。一切都很好。
- en: O dot grad is one。 O dot grad is one。 And now now this should work of course。
    Okay。 So all that backward should have this grad should now be 0。5 if we redraw。
    And if everything went correctly 0。5。 Yay。 Okay。 So now we need to call n-stat-grad。
    n-stat- backward， sorry。 n-stat- backward。 So that seems to have worked。
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: O点梯度是1。O点梯度是1。现在，这应该可以正常工作。好的。所有的反向传播应该使这个梯度现在为0.5，如果我们重新绘制的话。如果一切都正确，那么就是0.5。太好了。现在我们需要调用n-stat-梯度，n-stat-反向传播，抱歉，n-stat-反向传播。看起来这应该是有效的。
- en: So n-stat- backward wrapped the gradient to both of these。 So this is looking
    great。 Now we could of course call called v-stat-grad。 v-stat- backward， sorry。
    What's going to happen？
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 所以n-stat-反向传播将梯度包裹到这两个中。这看起来很好。现在我们当然可以调用v-stat-梯度。v-stat-反向传播，抱歉。接下来会发生什么？
- en: Well， b doesn't have it backward。 b is backward because b is a leaf node。 b
    is backward is by initialization dm t function。 So nothing would happen but we
    can call call it on it。 But when we call this one is backward。 Then we expect
    this point five to get further around it。 Right。 So there we go。 0。5。 And then
    finally， we want to call it here on x2w2。 And on x1w1。
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，b没有反向。b是反向的，因为b是一个叶节点。b的反向是通过初始化dm t函数。因此不会发生任何事情，但我们可以在其上调用它。但当我们调用这个反向时，我们希望这个0.5进一步围绕它。因此，0.5。最后，我们想在x2w2和x1w1上调用它。
- en: Let's do both of those。 And there we go。 So we get 0。5， negative 1。5。 and 1
    exactly as we did before。 But now we've done it through calling that backward
    sort of manually。 So we have the， one last piece to get rid of which is us calling
    underscore backward manually。 So let's think， through what we are actually doing。
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们同时做这两件事。好了，我们得到了0.5，负1.5，和1，正如我们之前所做的。但现在我们是通过手动调用反向排序来完成的。所以我们还有最后一个要处理的部分，就是手动调用下划线反向。让我们想一想，我们实际上在做什么。
- en: We've laid out a mathematical expression and now we're trying。 to go backwards
    through that expression。 So going backwards through the expression just means
    that。 we never want to call a dot backward for any node before we've done sort
    of everything after it。 So we have to do everything after it before ever going
    to call dot backward on any one node。 We。
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经列出了一个数学表达式，现在我们试图逆向通过这个表达式。逆向通过表达式意味着，我们在对任何节点调用点之前，必须先完成它后面的所有内容。因此，在对任何节点调用点之前，我们必须完成所有后面的内容。
- en: have to get all of its full dependencies。 Everything that it depends on has
    to propagate to it before。 we can continue back propagation。 So this ordering
    of graphs can be achieved using something called。 topological sort。 So topological
    sort is basically a laying out of a graph such that all the edges。 go only from
    left to right， basically。 So here we have a graph， it's a direction， a cyclic
    graph。
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须获取它的所有完整依赖关系。它所依赖的所有内容必须在我们继续反向传播之前传播到它。因此，这种图的排序可以通过一种称为拓扑排序的方式实现。拓扑排序基本上是图的排列，使得所有的边只从左到右走。
- en: a DAG。 And this is two different topological orders of it， I believe。 where
    basically you'll see that， it's a laying out of the nodes such that all the edges
    go only one way from left to right。 And， implementing topological sort， you can
    look in Wikipedia and so on。 I'm not going to go through it， in detail。 But basically，
    this is what builds a topological graph。
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有向无环图（DAG）。我相信这是它的两种不同的拓扑顺序。基本上，你会看到它是一个节点的排列，使得所有的边仅向一个方向从左到右走。实施拓扑排序，你可以查阅维基百科等。我不会详细讲解。但基本上，这就是构建拓扑图的过程。
- en: We maintain a set of visited， nodes。 And then we are going through starting
    at some root node。 which for us is oh， that's what， I want to start a topological
    sort。 And starting at oh。 we go through all of its children， and we， need to lay
    them out from left to right。 And basically。 this starts at oh， if it's not visited，
    then it marks it as visited。
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们维护一个已访问节点的集合。然后我们从某个根节点开始，对于我们来说，这是哦，我想开始一个拓扑排序。从哦开始，我们遍历它的所有子节点，需要将它们从左到右排列。基本上，如果它没有被访问，那么就将其标记为已访问。
- en: And then it iterates through all of its children， and calls built， topological
    on them。 And then after it's gone through all the children， it adds itself。 So
    basically。 this node that we're going to call it on， like say， oh， is only going
    to add itself。 to the topo list after all of the children have been processed。
    And that's how this function is。
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 然后它遍历所有的子节点，并对它们调用构建拓扑函数。在遍历完所有子节点后，它会添加自己。因此，基本上，我们要调用的这个节点，比如说哦，只有在所有子节点都被处理后，才会将自己添加到拓扑列表中。这就是这个函数的工作原理。
- en: guaranteeing that you're only going to be in the list once all your children
    are in the list。 And。 that's the invariant that is being maintained。 So if we
    built up on Oh， and then inspect this list。 we're going to see that it ordered
    our value objects。 And the last one is the value of 0。7。 which is the output。
    So this is oh， and then this is n。 And then all the other nodes that laid out。
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 保证你只会在列表中出现一次，所有子节点也在列表中。这就是保持的不可变性。所以如果我们在哦的基础上构建，然后检查这个列表，我们会看到它对我们的值对象进行了排序。最后一个是0.7的值，这是输出。因此这是哦，然后这是n，接下来是所有其他的节点排列。
- en: before it。 So that built the topological graph。 And really， what we're doing
    now is we're just。 calling that underscore backward on all of the nodes in a topological
    order。 So if we just reset。 the gradients， they're all zero。 What did we do？ We
    started by setting o dot grad to be one。 That's the base case。 Then we built the
    topological order。
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在它之前。所以构建了拓扑图。现在，我们所做的就是在所有节点上以拓扑顺序调用下划线反向传播。如果我们重置梯度，它们都是零。我们做了什么？我们首先将 o dot
    grad 设置为一。这是基本情况。然后我们构建了拓扑顺序。
- en: And then we went for node in reversed of topo。 Now， in the reverse order。 because
    this list goes from， you know， we need to go through it in reversed order。 So
    starting at Oh， node backward。 And that， should be it。 There we go。 Those are
    the correct derivatives。 Finally， we are going to hide this， functionality。
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们对 topo 的反向节点进行迭代。现在，按反向顺序，因为这个列表是从……我们需要按反向顺序处理它。所以从 Oh，节点反向开始。就这样。这些是正确的导数。最后，我们将隐藏这个功能。
- en: So I'm going to copy this。 And we're going to hide it inside the value class。
    because we don't want to have all that code lying around。 So instead of an underscore
    backward。 we're now going to define an actual backward。 So that backward without
    the underscore。 And that's going to do all the stuff that we just arrived。
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我将复制这个。我们会把它隐藏在值类中。因为我们不想让所有代码都暴露在外。所以我们将定义一个实际的反向传播，而不是下划线反向传播。这个没有下划线的反向传播将处理我们刚刚到达的所有内容。
- en: So let me just clean this up a little bit。 So we're first going to build the
    topological graph。 starting at self。 So build topo of self， will populate the
    topological order into the topo list。 which is a local variable。 Then we set self
    that grad to be one。 And then for each node in the reversed list， so starting
    at us and going to all the children。
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我稍微整理一下。我们首先将从 self 开始构建拓扑图。所以构建 self 的拓扑将把拓扑顺序填充到 topo 列表中，这是一个局部变量。然后我们将
    self 的梯度设置为一。然后对于逆向列表中的每个节点，从我们开始，走向所有子节点。
- en: underscore backward。 And that should be it。 So save。 Come down here， redefine。
    Okay。 all the grads are zero。 And now we can do is without backward， without the
    underscore。 And there we go。 And that's back propagation。 Please for one neuron。
    We shouldn't be too happy with ourselves， actually， because we have a bad bug。
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 下划线反向传播。这就是了。保存。到这里重新定义。好的，所有的梯度都是零。现在我们可以在没有下划线的情况下进行反向传播。就这样。这是反向传播。对于一个神经元，我们其实不应该太自满，因为我们有一个严重的
    bug。
- en: And we have not surfaced the bug because of some specific conditions that we
    have to think about。 right now。 So here's the simplest case that shows the bug。
    Say I create a single node a。 And then I create a B that is a plus a。 And then
    I call backward。 So what's going to happen is a is three。 And then a is a plus
    a。 So there's two arrows on top of。
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有显现出这个 bug，因为有一些特定条件我们需要考虑。现在。这是显示 bug 的最简单情况。假设我创建一个单节点 a。然后我创建一个 B，它是 a
    加 a。然后我调用反向传播。发生的事情是 a 是三。然后 a 是 a 加 a。所以这里有两个箭头重叠在一起。
- en: each other here。 Then we can see that B is， of course， the forward pass works。
    B is just， a plus a。 which is six。 But the gradient here is not actually correct
    that we calculated automatically。 And that's because， of course， just doing calculus
    in your head。 the derivative of B with respect to a， should be two。 One plus one。
    It's not one。 Intuitively。
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到 B 当然是前向传播有效。B 只是 a 加 a，这就是六。但这里的梯度实际上不是我们自动计算的正确值。这是因为，仅仅在脑海中进行微积分。B
    关于 a 的导数应该是二。一个加一个。不是一。从直觉上看。
- en: what's happening here， right？ So B is the result， of a plus a。 And then we call
    backward on it。 So let's go up and see what that does。 B is a result of addition。
    So out is B。 And then when we call backward， what happened is， self that grad
    was set to one。 And then other that grad was set to one。 But because we're doing
    a， plus a。
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生了什么，对吧？所以 B 是 a 加 a 的结果。然后我们在它上面调用反向传播。让我们上去看看那会发生什么。B 是加法的结果。所以输出是 B。然后当我们调用反向传播时，self
    的梯度被设置为一。然后其他的梯度也被设置为一。但因为我们做的是 a，加 a。
- en: self and other are actually the exact same object。 So we are overriding the
    gradient。 We are。 setting it to one。 And then we are setting it again to one。
    And that's why it stays at one。 So that's a problem。 There's another way to see
    this in a little bit more complicated expression。 So here we have a and B。 And
    then D will be the multiplication of the two， and E will be the。
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 自我和他人实际上是同一个对象。因此我们正在覆盖梯度。我们是。将其设置为一。然后我们再次将其设置为一。这就是它保持为一的原因。因此这是个问题。还有另一种方法可以通过稍微复杂的表达来看待这一点。所以这里我们有A和B。然后D将是这两者的乘积，而E将是。
- en: addition of the two。 And then we multiply it to get F。 And then we call it F
    that backward。 And these gradients， if you check， will be incorrect。 So fundamentally，
    what's happening here。 again， is basically we're going to see an issue anytime
    we use a variable more than once。 Until now。 in these expressions above， every
    variable is used exactly once。 So we didn't see the issue。
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个的相加。然后我们乘以得到F。然后我们调用F的向后传播。如果你检查这些梯度，将是错误的。因此，根本上，发生的事情是，每次我们多次使用一个变量时，我们会看到问题。到现在为止，在上述表达式中，每个变量仅使用一次。因此我们没有看到问题。
- en: But here， if a variable is used more than once， what's going to happen during
    backward pass。 we're back propagating from F to E to D。 So far so good。 But now
    E calls a backward。 and it deposits its gradients to a and B。 But then we come
    back to D and call backward。 And it overrides those gradients at a and B。 So that's
    obviously a problem。 And the solution here。
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 但是这里，如果一个变量被多次使用，向后传播期间会发生什么。我们正在从F反向传播到E到D。到目前为止一切都很好。但是现在E调用向后传播。并将其梯度存储到A和B。但是然后我们回到D并调用向后传播。它覆盖了A和B的那些梯度。因此这显然是个问题。而这里的解决方案。
- en: if you look at the multivariate case of the chain rule and its generalization
    there。 the solution there is basically that we have to accumulate these gradients，
    these gradients add。 And so instead of setting those gradients， we can simply
    do plus equals。 We need to accumulate。 those gradients plus equals plus equals
    plus equals plus equals。 And this will be okay， remember。
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看多变量链式法则及其推广，那里的解决方案基本上是我们必须累积这些梯度，这些梯度相加。因此，我们可以简单地使用加等于，而不是设置那些梯度。我们需要累积。那些梯度加等于加等于加等于加等于。这将没问题，请记住。
- en: because we are initializing them at zero。 So they started zero。 And then any。
    contribution that flows backwards， we'll simply add。 So now if we redefine this
    one。 because the plus equals this now works。 Because a that grad started at zero，
    and we call beta。 backward， we deposit one， and then we deposit one again。 And
    now this is two， which is correct。
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们将它们初始化为零。所以它们从零开始。然后任何向后流动的贡献，我们将简单地相加。因此现在如果我们重新定义这个。因为加等于现在可以正常工作。因为A的梯度从零开始，我们调用beta向后传播，我们存储一个，然后再次存储一个。现在这是二，这就是正确的。
- en: And here this will also work。 And we'll get correct gradients。 Because when
    we call， eta backward。 we will deposit the gradients from this branch。 And then
    we get to back to， data backward。 it will deposit its own gradients。 And then
    those gradients simply add on top of， each other。 And so we just accumulate those
    gradients and that fixes the issue。 Okay， now， before we move on。
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这里这也将有效。我们将获得正确的梯度。因为当我们调用eta向后传播时，我们将从这个分支存储梯度。然后我们回到数据向后传播。它将存储自己的梯度。然后这些梯度只是叠加在一起。因此我们只是累积那些梯度，这解决了问题。好了，现在，在我们继续之前。
- en: let me actually do a bit of cleanup here and delete some of these， some of，
    this intermediate work。 So I'm not going to need any of this now that we've derived
    all of it。 We are going to keep this because I want to come back to it。 Delete
    the 10H， delete our。 remote again， example， and need this step。 Delete this， keep
    the code that draws， and then delete。
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 让我实际上清理一下，删除一些这些中间工作。所以现在我们不需要这些，因为我们已经推导出所有内容。我们将保留这个，因为我想回到它。删除10H，删除我们的远程再次，示例，并需要这一步。删除这个，保留绘制的代码，然后删除。
- en: this example and leave behind only the definition of value。 And now let's come
    back to this nonlinearity， here that we implemented the 10H。 Now I told you that
    we could have broken down 10H into its。 explicit atoms in terms of other expressions
    if we had the exp function。 So if you remember。
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子只留下值的定义。现在让我们回到这个非线性，这里我们实现了10H。现在我告诉你，如果我们有exp函数，我们可以将10H分解为其显式原子。如果你记得。
- en: 10H is defined like this， and we chose to develop 10H as a single function，
    and we can do that。 because we know it's derivative and we can back propagate
    through it。 But we can also break down。 10H into an expressative function of exp。
    And I would like to do that now because I want to prove。 to you that you get all
    the same results and all the same gradients。 But also because it forces。
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 10H 定义如下，我们选择将 10H 作为单一函数来开发，我们可以这样做。因为我们知道它的导数，我们可以通过它反向传播。但我们也可以将 10H 分解为
    exp 的表达函数。我现在想这样做，因为我想证明给你看，你会得到所有相同的结果和所有相同的梯度。但也是因为这强迫。
- en: us to implement a few more expressions。 It forces us to do exponentiation， addition，
    subtraction。 division， and things like that。 And I think it's a good exercise
    to go through a few more of these。 Okay， so let's scroll up to the definition
    of value。 And here， one thing that we currently can't。 do is we can do like a
    value of say 2。0。 But we can't do， you know， here， for example， want to add。
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现更多的表达式。这强迫我们进行指数运算、加法、减法、除法等。我认为这是一种很好的练习，去处理更多这些内容。好的，那么让我们向上滚动到值的定义。在这里，我们目前无法做到的一件事是，我们可以做像
    2.0 这样的值。但我们不能这样做，你知道，这里，例如，我们想要添加。
- en: constant one， and we can't do something like this。 And we can't do it because
    it says into object has， no attribute data。 That's because a plus one comes right
    here to add。 And then other is the integer one。 And then here Python is trying
    to access one dot data。 And that's not a thing。 That's because。
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 常数一，我们不能做这样的事情。我们不能这样做，因为它表示对象没有属性数据。这是因为一加一恰好在这里添加。而其他是整数一。在这里，Python 尝试访问
    one dot data。而那并不存在。这是因为。
- en: basically one is not a value object。 And we only have addition form value objects。
    So as a matter。 of convenience， so that we can create expressions like this and
    make them make sense， we can simply。 do something like this。 Basically， we let
    other alone， if other is an instance of value。 but if it's， not an instance of
    value， we're going to assume that it's a number like an integer or float。
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，一不是一个值对象。我们只拥有加法形式的值对象。因此，为了方便起见，以便我们可以创建像这样的表达式并使其合理，我们可以简单地这样做。基本上，如果其他是值的实例，我们就让其他保持不变，但如果它不是值的实例，我们将假设它是一个数字，比如整数或浮点数。
- en: And， we're going to simply wrap it in in value。 And then other will just become
    value of other。 And then， other will have a data attribute。 And this should work。
    So if I just say this read a foreign value， then this should work。 There we go。
    Okay。 now let's do the exact same thing for multiply， because we can't do something
    like this。 Again。
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将简单地将其包裹在值中。然后其他将变成其他的值。然后，其他将拥有数据属性。这应该可以工作。所以如果我只是说读取一个外部值，那么这应该可以工作。就这样。好的，现在让我们对乘法做完全相同的事情，因为我们不能做这样的事情。再一次。
- en: for the exact same reason。 So we just have to go to， mall。 And if other is not
    a value。 then let's wrap it in value。 Let's read the fine value。 And， now this
    works。 Now here's a kind of unfortunate and not obvious part。 Eight times two
    works， we saw， that。 But two times a is that gonna work。 You'd expect it to write，
    but actually it will not。 And。
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 基于完全相同的原因。因此，我们只需去 mall。如果其他不是一个值，那么我们就把它包裹在值中。让我们读取该值。现在这可以工作。现在有一个有些不幸且不明显的部分。八乘以二可以工作，我们看到了。但二乘以
    a 会工作吗？你会期望它能够运行，但实际上不会。
- en: the reason it won't is because Python doesn't know。 Like when you do a times
    two， basically。 so a times two， Python will go and it will basically do something
    like a dot mall of two。 That's。 basically what we'll call。 But to it， two times
    a is the same as two dot mall of a。 And it doesn't。 to can't multiply value。 And
    so it's really confused about that。 So instead， what happens。
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 不这样做的原因是因为 Python 不知道。就像你做两倍一样，基本上。也就是说，a 乘以二，Python 会去执行类似 dot mall of two
    的操作。这就是我们所称的基本内容。但对它而言，二乘以 a 和二 dot mall of a 是一样的。它无法乘以值。因此，它对此感到非常困惑。那么，接下来会发生什么。
- en: is in Python， the way this works is you are free to define something called
    the armal。 And our mall。 is kind of like a fallback。 So if the Python can't do
    two times a， it will check if， if by any。 chance a knows how to multiply two，
    and that will be called into our mall。 So because Python can't do， two times a，
    it will check is there an armal in value。
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，这种方式的工作是你可以自由定义一个叫做 armal 的东西。我们的 mall 有点像一个回退。因此，如果 Python 无法执行二乘以
    a，它会检查是否有机会 a 知道如何乘以二，这将被调用到我们的 mall 中。所以因为 Python 无法做二乘以 a，它会检查值中是否存在 armal。
- en: And because there is， it will now call that。 And what we'll do here is we will
    swap the order of the operands。 So basically， two times a will， redirect to our
    mall， and our mobile basically call a times two。 And that's how that will work。
    So redefining that with our mall， two times a becomes four。 Okay。 now looking
    at the other， elements that we still need。
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这样，它将现在调用这个。我们将在这里交换操作数的顺序。因此，基本上，2 乘以 a 将重定向到我们的 mall，而我们的 mall 基本上会调用 a
    乘以 2。这就是它的工作方式。因此，使用我们的 mall 重新定义，2 乘以 a 变成 4。好的，现在看看我们仍然需要的其他元素。
- en: we need to know how to exponentiate and how to divide。 So let's first。 the explanation
    to the exponentiation part， we're going to introduce a single function X here。
    And X is going to mirror 10H in a sense that it's a simple， simple function that
    transforms。 single scale of value and outputs a single scale of value。 So we pop
    out the Python number。
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要知道如何进行指数运算和如何进行除法。因此，让我们首先解释指数部分，我们将在这里引入一个单一的函数 X。X 将在某种程度上反映 10H，因为它是一个简单的函数，转换单一的值尺度并输出单一的值尺度。因此我们弹出
    Python 数字。
- en: we use method X to exponentiate it， create a new value object， everything that
    we've seen before。 Tricky part of course is how do you back propagate through
    e to the X。 And so here you can potentially， pause the video and think about what
    should go here。 Okay。 so basically， I'm going to need to know， what is the local
    derivative of e to the X。
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用方法 X 进行指数运算，创建一个新的值对象，所有我们之前看到的内容。当然，棘手的部分是我们如何通过 e 的 X 进行反向传播。因此在这里，你可以潜在地暂停视频，思考这里应该是什么。好的。所以基本上，我需要知道
    e 的 X 的局部导数是什么。
- en: So d by dx of e to the X is famously just e to the X。 And we've already just
    calculated e to the X。 and it's inside out that data。 So we can do， out that data
    times and out that grad that's a chain。 So we're just chaining on to the current，
    running grad。 And this is what the expression looks like。 It looks a little confusing，
    but this is， what it is。 And that's the explanation。 So redefining。
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 所以 e 的 X 对 x 的导数著名地就是 e 的 X。我们已经计算过 e 的 X，并且它在里面的数据。因此，我们可以将外面的数据与外面的梯度相乘，这是一个链式法则。因此我们只是将其链接到当前的运行梯度。这就是表达式的样子。它看起来有点困惑，但这就是它的样子。这就是解释。所以重新定义。
- en: we should not be able to call it a dot X。 And， hopefully the backward pass works
    as well。 Okay。 and the last thing we'd like to do， of course， is we'd like to
    be able to divide。 Now。 I actually will implement something slightly more powerful，
    than division。 because division is just a special case of something a bit more
    powerful。 So in particular。
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不应该称之为 dot X。希望反向传播也能正常工作。好的，最后一件我们希望做的事情，当然是我们希望能够进行除法。现在，我实际上会实现一些比除法稍微强大的东西，因为除法只是更强大的一种特殊情况。因此，特别是。
- en: just by rearranging， if we have some kind of a b equals value of 4。0 here。 we'd
    like to basically be able to do a divided b， and we'd like this to be able to
    give us 0。5。 Now。 division actually can be reshuffled as follows。 If we have a
    divided b， that's actually。 the same as a multiplying one over b， and that's the
    same as a multiplying b to the power of negative。
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 通过重新排列，如果我们有某种形式的 a 除以 b 等于 4.0，我们希望基本上能够进行 a 除以 b，并且希望这个结果能够给我们 0.5。现在，除法实际上可以重新排列如下。如果我们有
    a 除以 b，那实际上就是 a 乘以 b 的倒数，这和 a 乘以 b 的负一次方是一样的。
- en: one。 And so what I'd like to do instead is I basically like to implement the
    operation of X。 to the k for some constant k。 So it's an integer or a float。 And
    we would like to be able to differentiate， this。 And then as a special case。 negative
    one will be division。 And so I'm doing that just because， it's more general， and
    yeah。
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我想要做的是基本上实现 X 的 k 次方的操作，其中 k 是某个常数。所以它可以是整数或浮点数。我们希望能够对其进行微分。作为一个特例，负一次方将是除法。因此，我这样做是因为这更通用，是的。
- en: you might as well do it that way。 So basically what I'm saying is we can， redefine
    division。 which we will put here somewhere。 Yeah， we can put this here somewhere。
    What I'm saying。 is that we can redefine division。 So self divide other can actually
    be rewritten itself times other。 to the power of negative one。 And now value raised
    to the power of negative one。
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 你不妨这样做。因此，基本上我所说的是我们可以重新定义除法。我们会在这里的某个地方放置这个。是的，我们可以在这里的某个地方放这个。我所说的是，我们可以重新定义除法。所以
    self 除以 other 实际上可以重写为自身乘以 other 的负一次方。现在，值的负一次方。
- en: we have now defined， that。 So here's， so we need to implement the power function。
    Where am I going to put the power， function maybe here somewhere？ Let's just call
    it for it。 So this function will be called when we try， to raise a value to some
    power。 And other will be that power。 Now I'd like to make sure that other is，
    only an int or a float。
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经定义了这个。所以在这里，我们需要实现幂函数。我将把幂函数放在哪里，也许在这里的某个地方？我们就叫它这个吧。因此，当我们试图将一个值提高到某个幂时，这个函数将被调用。其他将是那个幂。现在我想确保其他仅仅是一个整数或浮点数。
- en: Usually other is some kind of a different value object。 But here other will
    be。 forced to be an int or a float。 Otherwise， the math won't work for for trying
    to achieve in the。 specific case。 That would be a different derivative expression
    if we wanted other to be a value。 So here we create the output value， which is
    just， you know， this data raised to the power of other。
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，其他的值是某种不同的值对象。但在这里，其他必须被强制转换为一个整数或浮点数。否则，数学计算将无法在特定情况下实现。如果我们希望其他是一个值，那么这将是一个不同的导数表达式。因此，在这里我们创建输出值，这就是你知道的数据提高到其他的幂。
- en: And other here could be， for example， negative one。 That's what we are hoping
    to achieve。 And then this is the backward stub。 And this is the fun part， which
    is what is the chain rule。 expression here for back for back propagating through
    the power function。 where the power is to the power， of some kind of a constant。
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 而这里的其他可以是负一。这就是我们希望实现的目标。这是反向推导的部分。这是有趣的部分，链式法则在这里的表达是什么，用于通过幂函数的反向传播，其中幂是某种常数的幂。
- en: So this is the exercise and maybe pause the video here and see if you can。 figure
    it out yourself as to what we should put here。 Okay， so you can actually go here
    and look at。 derivative rules as an example。 And we see lots of derivatives that
    you can hopefully know from。 calculus。 In particular， what we're looking for is
    the power rule。
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这是练习，也许在这里暂停视频，看看你是否能自己找出我们应该放什么。好的，你可以在这里查看导数规则作为一个例子。我们看到很多你希望从微积分中知道的导数。特别是，我们要找的是幂法则。
- en: Because that's telling us that if， we're trying to take d by dx of x to the
    n。 which is what we're doing here， then that is just n times， x to the n minus
    one， right？ Okay。 so that's telling us about the local derivative of this power，
    operation。 So all we want here。 basically n is now other and self dot data is
    x。 And so this now， becomes other。
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这告诉我们，如果我们试图对 x 的 n 次方进行 d by dx 的求导，这是我们在这里所做的，那么这就是 n 乘以 x 的 n 减一，对吧？好的。这告诉我们这个幂运算的局部导数。因此我们想要的基本上是
    n 现在是其他，而 self.dot.data 是 x。因此，这现在变成了其他。
- en: which is n times self dot data， which is now a Python int or a float。 It's not
    a， valid object。 We're accessing the data attribute raised to the power of other
    minus one or n minus， one。 I can put brackets around this， but this doesn't matter
    because power takes precedence over。 multiply and by him。 So that would have been
    okay。 And that's the local derivative only。 But now we。
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 n 乘以 self.dot.data，现在是一个 Python 整数或浮点数。这不是一个有效对象。我们正在访问数据属性，提升到其他减一或 n 减一的幂。我可以在这周围加上括号，但这并不重要，因为幂运算优先于乘法。所以这样是可以的。这只是局部导数。但现在我们。
- en: have to chain it。 And we change it just simply by multiplying by a top grad，
    that's chain rule。 And this should technically work。 And we're gonna find out
    soon。 But now if we do this， this should。 now work。 And we get point five。 So
    the forward pass works， but does the backward password。 And I。 realized that we
    actually also have to know how to subtract。 So right now， a minus b will not work。
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 必须进行链式操作。我们通过乘以一个顶部梯度来简单地改变它，这就是链式法则。这应该在技术上有效。我们很快会发现。但是现在如果我们这样做，这应该现在有效。我们得到
    0.5。因此，前向传播有效，但反向传播也有效吗？我意识到我们实际上也需要知道如何减法。因此，目前 a 减 b 将无法工作。
- en: To make it work， we need one more piece of code here。 And basically， this is
    the， subtraction。 And the way we're going to implement subtraction is we're going
    to implement it by。 addition of a negation。 And then to implement negation， we're
    going to multiply by negative one。 So just again， using the stuff we've already
    built and just expressing it in terms of what we have。
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使其有效，我们需要在这里添加一段代码。基本上，这就是减法。我们实现减法的方法是通过加上一个负数。然后为了实现负数，我们将乘以负一。因此，再次使用我们已经构建的东西，只是以我们已有的形式表达出来。
- en: and a minus b does not work。 Okay， so now let's scroll again to this expression
    here for this neuron。 And let's just compute the backward pass here once we've
    defined O and let's draw it。 So here's the gradients for all these leaf nodes
    for this two dimensional neuron that has a 10。 H that we've seen before。 So now
    what I'd like to do is I'd like to break up this 10 H into this。
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 而且a减b是不可行的。好的，那么现在让我们再次滚动到这个神经元的表达式这里。让我们在定义O后计算反向传递，并绘制它。所以这是我们之前看到的这个二维神经元的所有叶节点的梯度，其中包含10
    H。因此我想做的是将这个10 H分解为这个。
- en: expression here。 So let me copy paste this here。 And now instead of， we'll preserve
    the label。 And we， will change how we define O。 So in particular， we're going
    to implement this formula here。 So we need e to the 2x minus one over e to the
    x plus one。 So e to the 2x， we need to take 2 times。 m and we need to exponentiate
    it。 That's e to the 2x。 And then because we're using it twice。
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的表达式。所以让我复制粘贴到这里。现在代替，我们将保留标签。我们将改变定义O的方式。特别是，我们要实现这个公式。因此我们需要e的2x减去一除以e的x加一。所以e的2x，我们需要取2乘m，然后我们需要将其指数化。这是e的2x。然后因为我们使用了两次。
- en: let's create an intermediate variable， e， and then define O as e plus one over
    e minus one over e plus。 one， e minus one over e plus one。 And that should be
    it。 And then we should be able to draw。 that above。 So now before I run this，
    what do we expect to see？ Number one， we're expecting to see。 a much longer graph
    here because we've broken up 10 H into a bunch of other operations。 But those。
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个中间变量e，然后定义O为e加一除以e减一再除以e加一，e减一再除以e加一。这就应该这样了。然后我们应该能够绘制上面的内容。那么在我运行这个之前，我们预期会看到什么？首先，我们期待看到一个更长的图，因为我们将10
    H拆分成了一堆其他操作。但这些。
- en: operations are mathematically equivalent。 And so what we're expecting to see
    is number one， the。 same result here。 So the forward pass works。 And number two，
    because of that mathematical。 equivalence， we expect to see the same backward
    pass and the same gradients on these leaf nodes。 So these gradients should be
    identical。 So let's run this。 So number one， let's verify that instead。
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 操作在数学上是等价的。所以我们期待看到的是，第一，结果应该是相同的。因此正向传递有效。第二，由于这种数学等价性，我们期望在这些叶节点上看到相同的反向传递和相同的梯度。因此这些梯度应该是相同的。让我们运行这个。所以首先，让我们验证一下。
- en: of a single 10 H node， we have now exp and we have plus， we have times negative
    one。 This is the。 division。 And we end up with the same forward pass here。 And
    then the gradients， we have to be。 careful because they're in slightly different
    order potentially。 The gradients for w two x two。 should be zero and point five，
    w two and x two are zero and point five。 And w one x one are one。
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单个10 H节点，我们现在有了exp，有加法，还有负一乘法。这是除法。我们在这里得到了相同的正向传递。然后梯度方面，我们必须小心，因为它们的顺序可能稍有不同。w2和x2的梯度应该是零和0.5，而w2和x2都是零和0.5。w1和x1是1。
- en: and negative one point five， one and negative one point five。 So that means
    that both our forward。 passes and backward passes were correct， because this turned
    out to be equivalent to 10 H before。 And so the reason I wanted to go through
    this exercise is number one。 We got to practice a few。 more operations and writing
    more backwards passes。 And number two。
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 和负1.5，1和负1.5。这意味着我们的正向传递和反向传递都是正确的，因为这结果与之前的10 H是等价的。因此，我想进行这个练习的原因是，第一，我们需要练习更多的操作和编写更多的反向传递；第二。
- en: I wanted to illustrate the point that， the the level at which you implement
    your operations is totally up to you。 You can implement backward， passes for tiny
    expressions like a single individual plus or single times。 or you can implement
    them， for say 10 H， which is a kind of a potentially you can see it as a composite
    operation because it's。 made up of all these more atomic operations。 But really，
    all of this is kind of like a fake concept。
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我想说明的观点是，你实施操作的级别完全取决于你自己。你可以为像单个加法或单个乘法这样的微小表达式实施反向传递，或者你可以为例如10 H实施，因为这可以被视为一种复合操作，因为它由所有这些更原子的操作组成。但实际上，这一切有点像是一个虚假的概念。
- en: All that matters is we have some kind of inputs and some kind of an output and
    this output is a。 function of the inputs in some way。 And as long as you can do
    forward pass and the backward。 pass of that little operation， it doesn't matter
    what that operation is and how composite it is。 If you can write the local gradients，
    you can change the gradient and you can continue back。
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是我们有某种输入和某种输出，而这个输出在某种程度上是输入的函数。只要你能够进行前向传播和后向传播，无论这个操作是什么以及它有多复杂都无所谓。如果你可以写出局部梯度，你就可以改变梯度，并且可以继续反向传播。
- en: application。 So the design of what those functions are is completely up to you。
    So now I would like to show you how you can do the exact same thing by using a
    modern deep neural。 network library， like for example PyTorch， which I've roughly
    modeled micro grad by。 And so PyTorch。 is something you would use in production。
    And I'll show you how you can do the exact same thing。
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序。因此，这些函数的设计完全取决于你。所以现在我想向你展示如何通过使用现代深度神经网络库来做完全相同的事情，例如PyTorch，我大致根据它建模了micro
    grad。因此，PyTorch是你在生产中会使用的东西。我将向你展示如何做到完全相同的事情。
- en: but in PyTorch API。 So I'm just going to copy paste it in and walk you through
    it a little bit。 This is what it looks like。 So we're going to import PyTorch。
    And then we need to define these。 value objects like we have here。 Now micro grad
    is a scalar valued engine。 So we only have scalar。 values like 2。0。 But in PyTorch，
    everything is based around tensors。 And like I mentioned tensors。
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 但在PyTorch API中。所以我只需将其复制粘贴，并带你了解一下。这就是它的样子。所以我们将导入PyTorch。然后我们需要定义这些值对象，就像我们这里所做的那样。现在micro
    grad是一个标量值引擎。所以我们只有像2.0这样的标量值。但在PyTorch中，一切都围绕张量进行。正如我提到的张量。
- en: are just n dimensional arrays of scalars。 So that's why things get a little
    bit more complicated here。 I just need a scalar value to tensor， a tensor with
    just a single element。 But by default。 when you work with PyTorch， you would use
    more complicated tensors like this。 So if I import PyTorch， then I can create
    tensors like this。 And this tensor， for example。
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 只是标量的n维数组。所以这就是为什么事情在这里变得有点复杂。我只需要一个标量值来张量，一个只有单个元素的张量。但是默认情况下，当你使用PyTorch时，你会使用更复杂的张量。所以如果我导入PyTorch，那么我可以像这样创建张量。比如说这个张量。
- en: is a 2x3 array of scalars， in a single compact representation。 So we can check
    its shape。 We see that it's a 2x3 array。 So this is usually what you would work
    with in the actual libraries。 So here I'm creating， a tensor that has only a single
    element 2。0。 And then I'm casting it to be double。 Because Python。
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 是一个2x3的标量数组，以单一紧凑的表示方式呈现。所以我们可以检查它的形状。我们看到它是一个2x3的数组。所以这通常是你在实际库中会处理的内容。在这里，我创建了一个只有单个元素2.0的张量。然后我将其转换为双精度。因为Python。
- en: is by default using double precision for its floating point numbers。 So I'd
    like everything to。 be identical。 By default， the data type of these tensors will
    be float 32。 So it's only using a。 single precision float。 So I'm casting it to
    double so that we have float 64 just like in Python。 So I'm casting to double。
    And then we get something similar to value of two。
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，它使用双精度浮点数。所以我希望一切都是一致的。默认情况下，这些张量的数据类型是float 32。所以它只使用单精度浮点数。所以我将其转换为双精度，以便我们有float
    64，就像在Python中一样。所以我将其转换为双精度。然后我们得到类似于值2的东西。
- en: The next thing I have to， do is because these are leaf nodes。 by default PyTorch
    assumes that they do not require gradients。 So I need to explicitly say that all
    of these nodes require gradients。 Okay， so this is going。 to construct scalar
    valued one element tensors。 Make sure that PyTorch knows that they require。
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我必须做的是，因为这些是叶节点，默认情况下PyTorch假设它们不需要梯度。所以我需要明确表示所有这些节点都需要梯度。好的，这将构建标量值的单元素张量。确保PyTorch知道它们需要。
- en: gradients。 Now by default， these are set to false by the way， because of efficiency
    reasons。 because usually you would not want gradients for leaf nodes， like the
    inputs to the network。 And this is just trying to be efficient in the most common
    cases。 So once we've defined all of。 our values in PyTorch land， we can perform
    arithmetic just like we can here in micrograd land。
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度。现在默认情况下，这些是设置为false的，出于效率考虑。因为通常你不希望叶节点，如网络的输入，具有梯度。这只是为了在最常见的情况下尽量提高效率。因此，一旦我们在PyTorch中定义了所有值，我们就可以像在micrograd中那样进行算术运算。
- en: So this， would just work。 And then there's a torch dot 10 H also。 And what we
    get back is a tensor again。 And we can just like in micrograd。 it's got a data
    attribute and it's got grad attributes。 So these tensor objects just like in micrograd
    have a dot data and a dot grad。 And the only。
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这个可以正常工作。然后还有一个 torch dot 10 H，我们得到的结果也是一个张量。就像在 micrograd 中一样，它具有 data 属性和
    grad 属性。因此，这些张量对象就像在 micrograd 中一样，有 dot data 和 dot grad。唯一的区别是。
- en: difference here is that we need to call a dot item， because otherwise PyTorch
    dot item basically。 takes a single tensor of one element and it just returns that
    element stripping out the tensor。 So let me just run this and hopefully we are
    going to get this is going to print the forward pass。 which is 0。707。 And this
    will be the gradients， which hopefully are 0。50 negative 1。5 and 1。
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的区别在于我们需要调用 dot item，因为否则 PyTorch 的 dot item 基本上会取一个单元素的张量，并返回那个元素，同时去掉张量。因此，让我运行一下，希望我们能得到这个将打印前向传递，结果是
    0.707。而这将是梯度，希望是 0.50、-1.5 和 1。
- en: So if we just run this， there we go。 0。7。 So the forward pass agrees and then
    0。50， a 1。5 and 1。 So PyTorch agrees with us。 And just to show you here， basically。
    here's a tensor with a single element and it's a double。 And we can call that
    item on it to。 just get the single number out。 So that's what it does。 And O is
    a tensor object like I mentioned。
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果我们运行这个，就这样。0.7。因此前向传递一致，然后 0.50、1.5 和 1。所以 PyTorch 与我们一致。为了向你展示，基本上，这里是一个单元素的张量，它是一个双精度浮点数。我们可以在其上调用
    item，只获取单个数字。这就是它的作用。而 O 是一个张量对象，正如我提到的。
- en: and it's got a backward function just like we've implemented。 And then all of
    these also have a。 dot grad。 So like X2 for example， as a grad， and it's a tensor，
    and we can pop out the individual。 number with dot item。 So basically， tortuous，
    tort can do what we did in micrograd as a special。 case when your tensors are
    all single element tensors。 But the big deal with PyTorch is that。
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 它具有一个向后函数，就像我们实现的那样。然后所有这些也都有一个 dot grad。例如，X2 作为一个 grad，它是一个张量，我们可以通过 dot item
    弹出单个数字。因此，基本上，tortuous，tort 可以做到我们在 micrograd 中所做的作为特例，当你的张量都是单元素张量时。但 PyTorch
    的大问题是。
- en: everything is significantly more efficient because we are working with these
    tensor objects。 And we。 can do lots of operations in parallel on all of these
    tensors。 But otherwise， what we've built very。 much agrees with the API of PyTorch。
    Okay， so now that we have some machinery to build out pretty。 complicated mathematical
    expressions， we can also start building up neural nets。 And as I。
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 一切都显著更高效，因为我们在处理这些张量对象。我们可以在所有这些张量上进行大量并行操作。但否则，我们所构建的非常符合 PyTorch 的 API。好吧，现在我们有一些构建复杂数学表达式的机制，我们也可以开始构建神经网络。正如我所说。
- en: mentioned， neural nets are just a specific class of mathematical expressions。
    So we're going to start， building out a neural net piece by piece and eventually
    we'll build out a two layer multi-layer。 perceptron as it's called。 And I'll show
    you exactly what that means。 Let's start with a single。 individual neuron。 We've
    implemented one here， but here I'm going to implement one that also。
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 提到，神经网络只是特定类别的数学表达式。因此我们将开始逐步构建一个神经网络，最终我们将构建一个两层的多层感知机（multi-layer perceptron），我将向你展示这究竟意味着什么。让我们从一个单独的神经元开始。我们在这里实现了一个，但这里我要实现一个也有。
- en: subscribes to the PyTorch API and how it designs its neural network modules。
    So just like we saw。 that we can like match the API of PyTorch on the autograd
    side， we're going to try to do that on。 the neural network modules。 So here's
    class neuron。 And just for the sake of efficiency。 I'm going to， copy paste some
    sections that are relatively straightforward。
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 订阅 PyTorch API 以及它如何设计神经网络模块。所以就像我们看到的那样，我们可以在 autograd 方面匹配 PyTorch 的 API，我们将尝试在神经网络模块上做到这一点。所以这里是类
    neuron。为了效率，我将复制粘贴一些相对简单的部分。
- en: So the constructor will take， number of inputs to this neuron。 which is how
    many inputs come to a neuron。 So this one， for， example， is three inputs。 And
    then it's going to create a weight that is some random number。 between negative
    one and one for every one of those inputs and a bias that controls the overall。
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 构造函数将接受该神经元的输入数量，即有多少输入传入神经元。因此这个，举个例子，有三个输入。然后它会为每个输入创建一个在负一和一之间的随机权重，以及一个控制整体的偏置。
- en: trigger happiness of this neuron。 And then we're going to implement a def_call，
    of self and x。 some input x。 And really what we don't do here is w times x plus
    b， or w times。 x here is a dot product specifically。 Now if you haven't seen call，
    let me just return 0。0 here from， now。 The way this works now is we can have an
    x which is say like 2。0， 3。0。
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 触发这个神经元的幸福感。然后我们将实现一个def_call，包含self和x，即一些输入x。我们在这里真正要做的就是w乘以x加上b，或者说w乘以x，这里是点积。现在如果你还没有看到调用，让我这里返回0。0。现在它的工作方式是我们可以有一个x，例如2。0，3。0。
- en: then we can initialize， a neuron that is two-dimensional because these are two
    numbers。 And then we can feed those two， numbers into that neuron to get an output。
    And so when you use this notation， n of x， Python will use call。 So currently
    call just returns 0。0。 Now we'd like to actually do the forward pass of this neuron
    instead。
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以初始化一个二维神经元，因为这些是两个数字。然后我们可以将这两个数字输入到神经元中以获取输出。所以当你使用这种表示法，n of x时，Python会使用调用。现在调用只返回0。0。现在我们实际上想进行这个神经元的前向传播。
- en: So what we're going to do here， first is we need to basically multiply all of
    the elements of w with all of the elements of x。 pairwise。 We need to multiply
    them。 So the first thing we're going to do is we're going to zip up。 uh， salta
    w and x。 And in Python， zip takes two iterators and it creates a new iterator
    that。 iterates over the topples of their corresponding entries。 So for example，
    just to show you we can。
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们在这里要做的第一件事是，基本上将w的所有元素与x的所有元素成对相乘。我们需要逐对相乘。所以我们要做的第一件事是zip上salta w和x。在Python中，zip接收两个迭代器，并创建一个新的迭代器，迭代它们对应条目的元组。例如，只是为了展示我们可以。
- en: print this list and still return 0。0 here。 Sorry， I'm in my。 So we see that
    these w's are paired。 up with the x's w with x。 And now what we're going to do
    is， for wixi in。 we want to multiply w times， wi times xi。 And then we want to
    sum all of that together to come up with an activation。 and add also salta b on
    top。 So that's the raw activation。 And then of course we need to pass。
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 打印这个列表并仍然返回0。0。抱歉，我在我的。所以我们看到这些w与x配对。w与x。现在我们要做的是，对于wixi，我们希望将wi乘以xi。然后我们希望将所有这些加在一起以得到一个激活值。并且还要加上salta
    b。所以这就是原始激活值。当然我们还需要传递。
- en: that through a null minority。 So what we're going to be returning is act。10h。
    And here's out。 So now， we see that we are getting some outputs and we get a different
    output from neural each time because。 we are initializing different weights and
    biases。 And then to be a bit more efficient here actually。 sum by the way takes
    a second optional parameter， which is the start。 And by default， the start is。
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一个空的少数群体。所以我们将返回的是act。10h。这里就是输出。所以现在，我们看到我们获得了一些输出，并且每次从神经元得到不同的输出，因为我们初始化了不同的权重和偏差。而且为了更高效一点，实际上，sum顺便需要一个第二个可选参数，即起始值。默认情况下，起始值是。
- en: 0。 So these elements of this sum will be added on top of zero to begin with。
    But actually， we can。 just start with salta b。 And then we just have an expression
    like this。 And then the generator。 expression here must be parenthesized by thumb。
    There we go。 Yep。 So now we can forward a single。 neuron。 Next up， we're going
    to define a layer of neurons。 So here we have a schematic for a， MLP。
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 0。这个和的元素将首先加到零上。但是实际上，我们可以。直接从salta b开始。然后我们就有了这样的一个表达式。然后这里的生成器表达式必须被括起来。好了。现在我们可以前向传播一个单独的神经元。接下来，我们将定义一层神经元。所以这里我们有一个多层感知器（MLP）的示意图。
- en: So we see that these MLPs each layer， this is one layer， has actually a number
    of neurons。 and they're not connected to each other。 But all of them are fully
    connected to the input。 So what is a layer of neurons？ It's just it's just a set
    of neurons evaluated independently。 So in the interest of time， I'm going to do
    something fairly straightforward here。
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们看到这些MLP的每一层，这是一层，实际上有多个神经元。它们彼此之间没有连接。但是它们都与输入完全连接。那么神经元的一层是什么？它只是独立评估的一组神经元。为了节省时间，我将在这里做一些相当简单的事情。
- en: It's literally a layer is just a list of neurons。 And then how many neurons
    do we have？
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 它实际上是一层神经元的列表。那么我们有多少个神经元呢？
- en: We take that， as an input argument here。 How many neurons do you want in your
    layer number of outputs in this。 layer？ And so we just initialize completely independent
    neurons with this given dimensionality。 And when we call on it， we just independently
    evaluate them。 So now instead of a neuron。 we can make a layer of neurons。 There
    are two dimensional neurons and let's have three of them。
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里将其作为输入参数。你希望在这一层中有多少个神经元和输出？因此，我们完全独立地用这个给定的维度初始化神经元。当我们调用它时，我们独立地评估它们。所以现在，我们不再是一个神经元，而是可以制作一个神经元层。这里有两个维度的神经元，让我们有三个。
- en: And now we see that we have three independent evaluations of three different
    neurons。 Okay， finally。 let's complete this picture and define an entire multilateral
    perception or MLP。 And as we can see here in an MLP， these layers just feed into
    each other sequentially。 So let's come here and I'm just going to copy the code
    here in the interest of time。
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们看到我们有三个不同神经元的独立评估。好的，最后。让我们完善这个图像，定义一个完整的多层感知器或 MLP。正如我们在 MLP 中看到的，这些层依次互相输入。因此，让我们过来，我只是为了节省时间复制代码。
- en: So an MLP is very similar。 We're taking the number of inputs as before， but
    now instead of。 taking the single and out， which is number of neurons in a single
    layer， we're going to take。 a list of and outs。 And this list defines the sizes
    of all the layers that we want in our MLP。 So here we just put them all together
    and then iterate over consecutive pairs of these sizes and。
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，MLP 非常相似。我们依然使用之前的输入数量，但现在不是取单一的神经元数量，而是要取一系列的输出。这份列表定义了我们在 MLP 中希望的所有层的大小。我们将它们全部放在一起，然后对这些大小的连续对进行迭代。
- en: create layer objects for them。 And then in the call function， we are just calling
    them sequentially。 So that's an MLP really。 And let's actually re implement this
    picture。 So we want three input。 neurons and then two layers of four and an output
    unit。 So we want three dimensional input。 say this is an example input， we want
    three inputs into two layers of four and one output。 And this。
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 为它们创建层对象。然后在调用函数中，我们只是依次调用它们。所以这真的就是一个 MLP。让我们实际重新实现这个图像。因此，我们希望有三个输入神经元，然后是两个四个神经元的层和一个输出单元。因此我们希望有三个维度的输入，假设这是一个示例输入，我们希望有三个输入到两个四个神经元的层和一个输出。这。
- en: of course， is an MLP。 And there we go。 That's a forward passive in MLP。 To make
    this a little bit nicer， you see how we have just a single element， but it's wrapped
    in a。 list because layer always returns lists。 Circum for convenience return outs
    at zero。 If。 when out is exactly a single element else return full list。 And this
    will allow us to just get a。
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这是一个 MLP。好了。这就是 MLP 中的前向传递。为了让这一切看起来更好，你会看到我们只有一个单一元素，但它被包装在一个列表中，因为层始终返回列表。为了方便，返回
    outs 的零索引。如果输出正好是单一元素，则返回单一元素，否则返回完整列表。这将允许我们获取一个输出。
- en: single value out at the last layer that only has a single neuron。 And finally，
    we should be able to。 prod out of N of X。 And as you might imagine， these expressions
    are now getting relatively involved。 So this is an entire MLP that we're defining
    now。 All the way until a single output。 Okay。 And so obviously you would never
    differentiate on pen and， paper these expressions。
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一层只有一个神经元的情况下，最终我们应该能够从 N 的 X 中获得输出。正如你所想象的，这些表达式现在变得相对复杂。所以这是我们现在定义的整个 MLP，直到得到单一输出。好吧，因此显然你不会在纸上对这些表达式进行微分。
- en: but with micro grad， we will be able to back propagate all the way through。
    this and back propagate into these weights of all these neurons。 So let's see
    how that works。 Okay。 so let's create ourselves a very simple example data set
    here。 So this data set has four， examples。 And so we have four possible inputs
    into the neural net。 And we have four desired targets。
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 但是通过微型梯度，我们将能够完全向后传播。这一切，并向这些神经元的权重进行反向传播。那么让我们看看这是如何工作的。好的，因此让我们创建一个非常简单的示例数据集。这个数据集有四个示例。因此，我们有四个可能的输入到神经网络中。我们有四个期望的目标。
- en: So we'd like the neural net to assign or output 1， 1， 0 when it's fed this example。
    negative one when it's fed these examples， and one when it's fed this example。
    So it's a very simple binary classifier neural net， basically that we would like
    here。 Now let's think what the neural net currently thinks about these four examples。
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们希望神经网络在输入这个示例时输出 1，1，0，在输入这些示例时输出负一，在输入这个示例时输出一。因此，这基本上是一个非常简单的二元分类器神经网络。现在让我们想想神经网络目前对这四个示例的看法。
- en: We can just get their predictions。 Basically we can just call N of X for X in
    Xs。 And then we can。 print。 So these are the outputs of the neural net on those
    four examples。 So the first one is 0。91。 but we like it to be one。 So we should
    push this one higher。 This one we want to be higher。 This one says 0。88 and we
    want this to be negative one。 This is 0。88， we want it to be negative one。
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接获得它们的预测。基本上，我们可以对每个 `X` 在 `Xs` 中调用 `N of X`。然后我们可以打印出来。这些是神经网络在这四个例子上的输出。第一个是
    0.91，但我们希望它为 1。所以我们应该把这个值提高。这个我们希望更高。这个是 0.88，我们希望它为负一。这个是 0.88，我们希望它为负一。
- en: And this one is 0。88， we want it to be one。 So how do we make the neural net
    and how do we tune the。 weights to better predict the desired targets？ And the
    trick used in deep learning to achieve this。 is to calculate a single number that
    somehow measures the total performance of your neural net。 And we call this single
    number the loss。 So the loss first is a single number that we're going to。
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 这个是 0.88，我们希望它为 1。那么我们该如何让神经网络更好地预测期望的目标呢？深度学习中实现这一点的技巧是计算一个单一数字，这个数字以某种方式衡量神经网络的整体表现。我们将这个单一数字称为损失。因此，损失首先是一个我们要计算的单一数字。
- en: define that basically measures how well the neural net is performing。 Right
    now we have the。 intuitive sense that it's not performing very well because we're
    not very much close to this。 So the loss will be high and we'll want to minimize
    the loss。 So in particular， in this case。 what we're going to do is we're going
    to implement the mean squared error loss。
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定义基本上衡量了神经网络的表现。目前我们直观上感觉它的表现并不好，因为我们离目标值还很远。因此损失会很高，我们希望最小化损失。在这种情况下，我们将实现均方误差损失。
- en: So what this is doing， is we're going to basically iterate for y ground truth
    and y output in zip of y's and y-thread。 So we're going to pair up the ground
    truths with the predictions and this zip iterates over。 tuples of them。 And for
    each y ground truth and y output， we're going to subtract them。 and square them。
    So let's first see what these losses are。 These are individual loss components。
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作的基本内容是，我们将对 `y ground truth` 和 `y output` 在 `y` 和 `y-thread` 的 zip 中进行迭代。我们将把真实值与预测值配对，这个
    zip 会遍历它们的元组。对于每个 `y ground truth` 和 `y output`，我们将它们相减并平方。所以让我们先看看这些损失是什么。这些是单个损失组件。
- en: And so basically for each one of the four， we are taking the prediction and
    the ground truth。 We are subtracting them and squaring them。 So because this one
    is so close to its target， 0。91 is almost one， subtracting them gives a very small
    number。 So here we would get like a。 negative point one and then squaring it just
    makes sure that regardless of whether we are more。
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于这四个中的每一个，我们都在取预测和真实值。我们相减并平方它们。因为这个值与目标非常接近，0.91 几乎是 1，减去后会得到一个非常小的数字。所以在这里，我们会得到大约
    -0.1，然后平方它确保不论我们是更接近还是更远。
- en: negative or more positive， we always get a positive number。 Instead of squaring，
    we should。 hope we could also take， for example， the absolute value。 We need to
    discard the sign。 And so you see that the expression is arranged so that you only
    get 0 exactly when y out is equal。 to y ground truth。 When those two are equal，
    so your prediction is exactly the target。
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 不管是负数还是正数，我们总是得到一个正数。与其平方，我们应该希望能够取绝对值。例如，我们需要舍弃符号。因此你会看到这个表达式的排列方式是，当 `y out`
    等于 `y ground truth` 时，只有在那时你才能准确得到 0。当这两个相等时，你的预测正好是目标值。
- en: you are going to get 0。 And if your prediction is not the target， you are going
    to get some other。 number。 So here， for example， we are way off。 And so that's
    why the loss is quite high。 And the more off we are， the greater the loss will
    be。 So we don't want high loss， we want low loss。 And so the final loss here will
    be just the sum of all of these numbers。 So you see that this。
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的预测是目标，你会得到 0。如果你的预测不是目标，你会得到其他某个数字。例如，在这里，我们偏差很大。这就是为什么损失很高。我们偏差越大，损失就越大。因此我们不希望高损失，而希望低损失。所以最终的损失将是所有这些数字的总和。你会看到这一点。
- en: should be 0 roughly plus 0 roughly， but plus 7。 So loss should be about 7 here。
    And now we want to。 minimize the loss。 We want the loss to be low。 Because if
    loss is low， then every one of the。 predictions is equal to its target。 So the
    loss， the lowest it can be is 0。 And the greater it is。 the worse off the neural
    net is predicting。 So now of course， if we do loss that backward。
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 因为如果损失低，那么每一个预测都等于它的目标。损失最低可以为0，越高则神经网络的预测越糟糕。所以现在当然，如果我们进行损失反向传播。
- en: something magical happened when I hit enter。 And the magical thing， of course，
    that happened is。 that we can look at and add layers that neuron and that layers
    at say like the first layer。 that neurons at zero。 Because remember that MLP has
    the layers， which is a list。 And each layer has neurons， which is a list。 And
    that gives us individual neuron。 And then it's got。
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 当我按下回车时发生了一些神奇的事情。发生的神奇事情是，我们可以查看并添加层，这个神经元和这个层，比如说第一层。这个神经元在零的位置。因为记住，MLP有层，这是一种列表。每一层都有神经元，这也是一个列表。这样我们就得到了单个神经元。然后它有。
- en: some weights。 And so we can， for example， look at the weights at zero。 Oops。
    it's not called weights， it's called W。 And that's a value。 But now this value
    also has a graph because of the backward pass。 And so we see that because this
    gradient here on this particular weight of this particular neuron。
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 一些权重。所以我们可以，比如说，看一下零处的权重。哦，不是叫权重，是叫W。这是一个值。但现在这个值也有一个图，因为反向传播。所以我们看到这个特定神经元的这个特定权重的梯度应该大约是0，基本上是0，但加上7。因此，这里的损失应该是大约7。现在我们想要最小化损失。我们希望损失低。
- en: of this particular layer is negative， we see that its influence on the loss
    is also negative。 So。 slightly increasing this particular weight of this neuron
    of this layer would make the loss go down。 And we actually had this information
    for every single one of our neurons and all of their parameters。 Actually， it's
    worth looking at also the draw dot loss， by the way。 So previously， we looked
    at the。
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个特定层的值是负的，我们会发现它对损失的影响也是负的。所以，稍微增加这个层的神经元的特定权重会使损失降低。实际上，我们对每一个神经元及其所有参数都有这样的信息。顺便说一句，查看绘制的点损失也是值得的。
- en: draw dot of a single neural neural forward pass。 And that was already a large
    expression。 But。 what is this expression？ We actually forwarded every one of those
    four examples。 And then we。 have the loss on top of them with the mean squared
    error。 And so this is a really massive graph。 Because this graph that we built
    up now， oh my gosh， this graph that we built up now。
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们看过一个单一神经网络的前向传播的绘制点。那已经是一个庞大的表达式了。但是，这个表达式是什么呢？我们实际上对那四个例子进行了前向传播。然后在它们上面加上了均方误差的损失。所以这是一个非常庞大的图。因为我们现在构建的这个图，哦我的天，这个图是如此庞大。
- en: which is kind of excessive。 It's excessive because it has four forward passes
    of a neural net for。 every one of the examples。 And then it has the loss on top。
    And it ends with the value of the loss。 which for seven point one two。 And this
    loss will now back propagate through all the forward forward。 passes， all the
    way through just every single intermediate value of the neural net， all the way。
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这有点过于复杂。它之所以复杂，是因为每一个例子都有四次神经网络的前向传播。然后它还有损失在上面。最后是损失的值，约为七点一二。这个损失将反向传播通过所有的前向传播，直到神经网络的每一个中间值。
- en: back to of course the parameters of the weights， which are the input。 So these
    weight parameters。 here are inputs to this neural net。 And these numbers here。
    these scalars are inputs to the neural net。 So if we went around here。 we will
    probably find some of these examples， this 1。0， potentially maybe， this 1。0 or
    you know。
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 回到当然是权重的参数，这些是输入。因此，这些权重参数是神经网络的输入。这些数字，这些标量是神经网络的输入。所以如果我们在这里转一圈，可能会发现一些这样的例子，可能是这个1.0，或者其他的。
- en: some of the others。 And you'll see that they all have gradients as well。 The
    thing is these gradients on the input data are not that useful to us。 And that's
    because。 the input data seems to be not changeable。 It's a given to the problem。
    And so it's a fixed input。 We're not going to be changing it or messing with it，
    even though we do have gradients for it。
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到它们都有梯度。问题是，这些输入数据的梯度对我们并不太有用。这是因为输入数据似乎是不可改变的。它是问题所给定的。因此，它是一个固定的输入。我们不会改变或干扰它，尽管我们确实有它的梯度。
- en: But some of these gradients here will be for the neural network parameters，
    the Ws and the Bs。 And those sweep， of course， we want to change。 Okay， so now
    we're going to want some convenience。 code to gather up all the parameters of
    the neural net so that we can operate on all of them。 simultaneously。 And every
    one of them， we will nudge a tiny amount based on the gradient information。
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 但这些梯度中有一些将是神经网络参数，Ws和Bs。当然，我们想要更改这些。好吧，现在我们希望一些方便的代码来收集神经网络的所有参数，以便我们可以同时操作它们。每一个参数，我们将根据梯度信息微调一点。
- en: So let's collect the parameters of the neural net all in one array。 So let's
    create a parameters of。 self that just returns self that W which is a list， concatenated
    with a list of self that B。 So this will just return a list list plus list just
    gives you a list。 So that's parameters of。 neuron。 And I'm calling it this way
    because also PyTorch has a parameters on every single and in。
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们把神经网络的参数全部收集到一个数组中。我们创建self的参数，返回self的W，这是一个列表，与self的B列表连接。因此，这将只返回一个列表，列表加列表只给你一个列表。这就是神经元的参数。我这样称呼是因为PyTorch在每个神经元中都有参数。
- en: module。 And it does exactly what we're doing here。 It just returns the parameter
    tensors for us is the， parameter scalars。 Now layer is also a module。 So it will
    have parameters self。 And basically what。 we want to do here is something like
    this like params is here and then for neuron in self that。
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 模块。它正好做我们在这里做的事情。它只会为我们返回参数张量，即参数标量。现在layer也是一个模块。所以它将拥有self的参数。基本上我们想在这里做的事情是params在这里，然后对于self.that中的每个神经元。
- en: neurons， we want to get neuron parameters。 And we want to params。extend。 So
    these are the parameters， of this neuron。 And then we want to put them on top
    of params。 So params。extend of piece。 And then， we want to return params。 So this
    is way too much code。 So actually there's a way to simplify this， which is return
    P for neuron in self that neurons for P in neuron dot parameters。
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元，我们想要获取神经元参数。我们希望params.extend。这些是这个神经元的参数。然后我们想把它们添加到params上。所以params.extend这个片段。然后，我们想返回params。这段代码太多了。其实有办法简化，就是返回P，对于self.that.neurons中的每个神经元，对于P在neuron.dot.parameters中。
- en: So it's a single list comprehension in Python。 You can sort of nest them like
    this。 And you can。 then create the desired array。 So these are identical。 We can
    take this out。 And then let's do the same， here。 Death parameters， self。 and return
    a parameter for layer in self dot layers for P in layer dot， parameters。
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在Python中，这是一个单一的列表推导。你可以像这样嵌套它们。然后你可以创建所需的数组。这些是相同的。我们可以把这个拿掉。然后我们在这里做同样的事情。死亡参数，self。并返回一个参数，对每一层在self.dot.layers中，对于P在layer.dot.parameters中。
- en: And that should be good。 Now let me pop out this so we don't re initialize our
    network。 because we need to re initialize our。 Okay， so unfortunately， we will
    have to probably。 re initialize network because we just had functionality。 Because
    this class， of course。 I want to get， all the end up parameters。 That's not going
    to work because this is the old class。
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 这样应该可以。现在让我把这个弹出，以免我们重新初始化网络，因为我们需要重新初始化它。好吧，不幸的是，我们可能需要重新初始化网络，因为我们刚刚有了功能。因为这个类，我想获取所有的最终参数。这将不起作用，因为这是旧的类。
- en: Okay。 So unfortunately， we do have to re initialize the network。 which will
    change some of the numbers。 But let me do that so that we pick up the new API。
    we can now do end up parameters。 And these are all the weights and biases inside
    the entire neural net。 So in total， this MLP has 41 parameters。 And now we'll
    be able to change them。
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧。不幸的是，我们必须重新初始化网络，这将改变一些数字。但让我来做这个，以便我们能获取新的API。现在我们可以处理所有参数。这些都是整个神经网络内部的权重和偏置。因此，总共有41个参数。现在我们将能够更改它们。
- en: If we recalculate the loss here， we see that unfortunately。 we have slightly
    different predictions and slightly different loss。 But that's okay。 Okay。 so we
    see that this neurons gradient is slightly negative。 We can also look， at its
    data right now。 which is 0。85。 So this is the current value of this neuron， and
    this is its gradient on the loss。
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在这里重新计算损失，我们会发现不幸的是，我们的预测和损失略有不同。但这没关系。好吧，所以我们看到这个神经元的梯度稍微是负的。我们现在也可以查看它的数据，当前值是0.85。这是这个神经元的当前值，也是它在损失上的梯度。
- en: So what we want to do now is we want to iterate for every， p in and that parameters。
    So for all the 41 parameters of this neural net， we actually want to change。 p
    data slightly according to the gradient information。 Okay， so dot dot dot to do
    here。 But this will be basically a tiny update in this gradient descent scheme。
    And gradient descent。
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们现在想要做的是对每个 p 和这些参数进行迭代。因此，对于这个神经网络的所有 41 个参数，我们实际上想要根据梯度信息稍微改变 p 数据。好的，所以这里还有更多要做的。但这基本上是在这个梯度下降方案中的一个微小更新。梯度下降。
- en: we are thinking of the gradient as a vector pointing in the direction of increased
    loss。 And so in gradient descent， we are modifying p data by a small step size
    in the direction of。 the gradient。 So the step size as an example could be like
    a very small number， 0。01 is the step size。 Times d dot grad， right。 But we have
    to think through some of the signs here。
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将梯度视为一个指向损失增加方向的向量。因此，在梯度下降中，我们通过一个小步长来调整 p 数据，方向是梯度。举个例子，步长可以是一个非常小的数字，0.01就是步长。乘以
    d dot grad，对吧。但我们必须考虑这里的一些符号。
- en: So in particular， working with this specific example here， we see that if we
    just left it like this。 then this， neurons value would be currently increased
    by a tiny amount of the gradient。 The gradient is， negative。 So this value of
    this neuron would go slightly down。 It would become like 0。84 or something， like
    that。 But if this neurons value goes lower。
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，特别是针对这个特定示例，我们看到如果我们就这样保持它，那么这个神经元的值将因梯度的微小量而增加。梯度是负的。因此，这个神经元的值会略微降低。它将变成大约
    0.84 之类的。但是如果这个神经元的值变得更低。
- en: that would actually increase the loss。 That's because， the derivative of this
    neuron is negative。 So increasing this makes the loss go down。 So increasing。
    it is what we want to do instead of decreasing it。 So basically what we're missing
    here is we're。 actually missing a negative sign。 And again， this other interpretation，
    and that's because we want。
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上会增加损失。这是因为这个神经元的导数是负的。因此，增加这个值会使损失降低。因此，增加它是我们想要做的，而不是减少它。因此，基本上我们在这里缺少的是一个负号。还有另一种解释，这就是我们想要的。
- en: to minimize the loss。 We don't want to maximize the loss。 We want to decrease
    it。 And the other。 interpretation， as I mentioned， is you can think of the gradient
    vector。 So basically， just the。 vector of all the gradients as pointing in the
    direction of increasing the loss。 But then we want。 to decrease it。 So we actually
    want to go in the opposite direction。
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 目的是最小化损失。我们不想最大化损失。我们想要减少它。另一种解释，正如我提到的，你可以将梯度向量视为指向损失增加方向的向量。但我们想要减少它。因此，我们实际上想朝相反的方向前进。
- en: And so you can convince yourself， that this sort of like does the right thing
    here with the negative because we want to minimize the。 loss。 So if we notch all
    the parameters by tiny amount， then we'll see that this data will have。 changed
    a little bit。 So now this neuron is a tiny amount greater value。 So 0。854 when
    it's 0。857。 And that's a good thing because slightly increasing this neuron data
    makes the loss go down。
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可以说服自己，这种做法是正确的，因为我们想要最小化损失。所以如果我们将所有参数稍微调整一下，那么我们会看到这些数据会有所改变。因此，现在这个神经元的值稍微增加了。所以
    0.854 当它是 0.857 时。这是件好事，因为稍微增加这个神经元的数据会使损失降低。
- en: according to the gradient。 And so the correcting has happened signwise。 And
    so now what we would。 expect， of course， is that because we've changed all these
    parameters， we expect that the loss。 should have gone down a bit。 So we want to
    reevaluate the loss。 Let me basically。 this is just a data definition that hasn't
    changed。 But the forward pass here of the network。
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 根据梯度。因此，修正是按符号发生的。因此，现在我们当然期望，因为我们已经改变了所有这些参数，我们希望损失应该略微下降。所以我们想重新评估损失。基本上，这是一个没有变化的数据定义。但网络的前向传递在这里。
- en: we can recalculate。 And actually， let me do it outside here so that we can compare
    the two。 loss values。 So here， if I recalculate the loss， we'd expect the new
    loss now to be slightly lower。 than this number。 So hopefully， what we're getting
    now is a tiny bit lower than 4。84。 4。36。 Okay。 And remember， the way we've arranged
    this is that low loss means that our predictions。
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以重新计算。实际上，让我在这里外部进行计算，以便我们可以比较两个损失值。因此在这里，如果我重新计算损失，我们预期新的损失现在会稍微低于这个数字。因此，希望我们现在得到的结果会稍微低于
    4.84，4.36。好的。请记住，我们安排这个的方式是低损失意味着我们的预测。
- en: are matching the targets。 So our predictions now are probably slightly closer
    to the targets。 And now all we have to do is we have to iterate this process。
    So again， we've done the forward pass。 and this is the loss。 Now we can lost that
    backward。 Let me take these out。 And we can do a step size。 And now we should
    have a slightly lower loss。 4。36 goes to 3。9。 And okay。
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 预测现在与目标匹配。所以我们的预测现在可能稍微接近目标。现在我们需要做的就是迭代这个过程。所以我们再次进行了前向传播，这是损失。现在我们可以向后推。让我把这些去掉。我们可以做一个步长。现在损失应该稍微降低，从4.36降到3.9。好的。
- en: so we've done the forward， pass。 Here's the backward pass， nudge。 And now the
    loss is 3。66。 3。47。 And you get the idea。 We just， continue doing this。 And this
    is a gradient descent。 We're just iteratively doing forward pass， backward， pass
    update， forward pass。 backward pass update。 And the neural net is improving its
    predictions。
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们完成了前向传播，接下来是反向传播，轻推一下。现在损失是3.66，3.47。你明白这个意思了吧。我们只需继续这样做。这就是梯度下降。我们只是反复进行前向传播、反向传播更新、前向传播、反向传播更新。神经网络在改善其预测。
- en: So here if we look at y-pred now， y-pred， we see that this value should be getting
    closer to one。 So this value should be getting more positive。 These should be
    getting more negative。 And this。 one should be also getting more positive。 So
    if we just iterate this a few more times。 actually we'll be able to afford to
    go a bit faster。 Let's try a slightly higher learning rate。
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这里如果我们看看y-pred，现在y-pred的值应该更接近于一。这个值应该变得更积极。这些值应该变得更消极。这个值也应该变得更积极。所以如果我们再迭代几次，实际上我们可以加快一点速度。让我们尝试一个稍高的学习率。
- en: Oops。 Okay， there we go。 So now we're at 0。31。 If you go too fast， by the way，
    if you try to make。 it too big of a step， you may actually overstep over confidence。
    Because again， remember， we don't。 actually know exactly about the loss function。
    The loss function has all kinds of structure。 And we only know about the very
    local dependence of all these parameters on the loss。
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 哎呀。好的，现在我们在0.31。如果你走得太快，顺便说一下，如果你试图一步走得太大，你可能会过于自信地超步。因为记住，我们实际上并不知道损失函数的确切情况。损失函数有各种结构，而我们只知道这些参数对损失的非常局部依赖。
- en: But if we step， too far， we may step into a part of the loss that is completely
    different。 And that can， destabilize training and make your loss actually blow
    up even。 So the loss is now 0。04。 So actually， the predictions should be really
    quite close。 Let's take a look。 So you see how this is almost one， almost negative
    one， almost one。 We can continue going。 So yep。
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如果我们走得太远，可能会进入一个完全不同的损失区域。这可能会破坏训练，甚至让损失飙升。所以损失现在是0.04。实际上，预测应该非常接近。我们来看一下。你会看到这几乎是一个，几乎是负一，几乎是一个。我们可以继续进行。所以，是的。
- en: backward update。 Oops， there we go。 So we went way too fast。 And we actually
    overstepped。 So we got two， two eager。 Where are we now？ Oops。 Okay。 7e negative
    nine。 So this is very。 very low loss。 And the predictions are basically perfect。
    So somehow we， basically。 we were doing way， to the updates and we briefly exploded。
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 反向更新。哎呀，走得太快了。我们实际上超步了。所以我们有点急。我们现在在哪里？哎呀。好的，7e负9。这是非常非常低的损失，预测基本上是完美的。所以不知怎么的，我们实际上进行了过快的更新，瞬间爆炸了。
- en: But then somehow we ended up getting into a really good spot。 So usually this
    learning rate and the tuning of it is a subtle art。 You want to set your learning
    rate。 If it's too low， you're going to take way too long to converge。 But if it's
    too high， the whole thing gets unstable and you might actually even explode the
    loss。
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 但不知怎么的，我们最终进入了一个非常好的状态。所以通常来说，这个学习率及其调节是一门微妙的艺术。你想设置你的学习率。如果它太低，你会花太长时间收敛。但如果太高，整个过程会变得不稳定，你甚至可能会使损失爆炸。
- en: depending on your loss function。 So finding the step size to be just right，
    it's a pretty subtle。 art sometimes when you're using sort of vanilla gradient
    descent。 But we happen to get into a good。 spot。 We can look at end up parameters。
    So this is the setting of weights and biases that makes our。 network predict the
    desired targets very， very close。 And basically we've successfully trained。
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 这取决于你的损失函数。所以找到合适的步长有时是一门相当微妙的艺术，尤其是在使用常规的梯度下降时。但我们恰好进入了一个不错的状态。我们可以看看最终的参数。这是设置权重和偏置，使我们的网络预测到期望的目标非常接近。基本上，我们已经成功训练。
- en: a neural nut。 Okay， let's make this a tiny bit more respectable and implement
    an actual training。 loop and what that looks like。 So this is the data definition
    that stays。 This is the forward pass。 So for K in range， you know， we're going
    to take a bunch of steps。 First， you do the forward pass。 We evaluate the loss。
    Let's reinitialize the neural line from scratch。 And here's the data。
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 一个神经网络的坚果。好的，让我们稍微严肃一点，实施一个实际的训练循环，以及它的样子。这是保持不变的数据定义。这是前向传播。对于 K 在范围内，我们将进行一系列步骤。首先，进行前向传播。我们评估损失。让我们从头开始重新初始化神经线。这里是数据。
- en: And we first do forward pass， then we do the backward pass。 And then we do an
    update。 That's。 great in descent。 And then we should be able to iterate this and
    we should be able to print the。 current step， the current loss。 Let's just print
    the sort of number of the loss。 And that should be it。 And then the learning rate
    0。01 is a little too small 0。1。
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先进行前向传播，然后进行反向传播。接着我们进行更新。这是在下降中很好的。然后我们应该能够迭代这个，并能够打印当前步骤和当前损失。让我们只打印损失的数字。应该就是这样。然后学习率0.01有点太小，0.1。
- en: We saw is like a little bit dangerous， with UI。 Let's go somewhere between and
    we'll optimize this for not 10 steps。 but let's go for， say 20 steps。 Let me erase
    all of this junk。 And let's run the optimization。 And you see how we've actually
    converged slower in a more controlled manner and got to a loss that。 is very low。
    So I expect white bread to be quite good。 There we go。 And that's it。 Okay， so
    this is。
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到有点危险，涉及到用户界面。让我们选择一个中间值，我们将优化这个，不是10步，而是20步。让我擦掉所有这些垃圾。让我们运行优化。你会看到我们实际上以更慢的、更可控的方式收敛，并达到了一个非常低的损失值。所以我期待白面包会很好。好了，就这样。
- en: kind of embarrassing， but we actually have a really terrible bug in here。 And
    it's a subtle bug。 And it's a very common bug。 And I can't believe I've done it
    for the 20th time in my life。 especially on camera。 And I could have reshot the
    whole thing， but I think it's pretty funny。 And you get to appreciate a bit what
    working with neural nets maybe is like sometimes。 We are。
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 有点尴尬，但这里面实际上有一个非常糟糕的错误。这是一个微妙的错误，也是一个非常常见的错误。我无法相信我这一生中已经犯了20次这个错误，尤其是在镜头前。我本可以重新拍摄整个过程，但我觉得这很有趣。而且你也能稍微体会到有时与神经网络合作的感觉。我们是。
- en: guilty of a common bug。 I've actually tweeted the most common neural mistakes
    a long time ago now。 And I'm not really going to explain any of these except for
    we are guilty of number three。 You。 forgot to zero grad before Doug backward。
    What is that？ Basically what's happening。 and it's a subtle bug and I'm not sure
    if you saw it， is that all of these weights here have a。
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 犯了一个常见错误。我实际上很久以前就发过关于最常见神经网络错误的推文。我不会详细解释这些，除了我们在第三个错误上有过失。你在进行反向传播之前忘记了归零梯度。这是什么呢？基本上发生的事情是，这些权重都有一个。
- en: dot data and a dot grad。 And dot grad starts at zero。 And then we do backward
    and we fill in the。 gradients。 And then we do an update on the data， but we don't
    flush the grad。 It stays there。 So when we do the second forward pass and we do
    backward again， remember that all the backward。 operations do a plus equals on
    the grad。 And so these gradients just add up and they never get。
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: dot.data 和 dot.grad。dot.grad 从零开始。然后我们进行反向传播，填充梯度。接着我们对数据进行更新，但不清空梯度。它保持在那里。所以当我们进行第二次前向传播并再次进行反向传播时，记住所有的反向操作在梯度上执行加法赋值。因此，这些梯度只是累加，从未被。
- en: reset to zero。 So basically we didn't zero grad。 So here's how we zero grad
    before backward。 We need to iterate over all the parameters。 And we need to make
    sure that p dot grad is set to zero。 We need to reset it to zero just like it
    is in the constructor。 So remember all the way here for。 all these value nodes，
    grad is reset to zero。 And then all these backward passes do a plus equals。
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 重置为零。所以基本上我们没有将梯度归零。那么，在反向传播之前我们是如何归零梯度的呢？我们需要遍历所有参数。我们需要确保 p.dot.grad 被设置为零。我们需要像构造函数中那样将其重置为零。所以记住，所有这些值节点的梯度都被重置为零。然后所有这些反向传播都执行加法赋值。
- en: not grad。 But we need to make sure that we reset these grads to zero so that
    when we do backward。 all of them start at zero and the actual backward pass accumulates
    the loss derivatives into the。 grads。 So this is zero grad in PyTorch。 And we
    will get a slightly different optimization。 Let's reset the neural net。 The data
    is the same。 This is now， I think， correct。
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 不是梯度。但是我们需要确保将这些梯度重置为零，这样当我们进行反向传播时，所有的梯度都从零开始，实际的反向传递将损失导数累积到梯度中。这在 PyTorch
    中是零梯度。我们将得到稍微不同的优化。让我们重置神经网络。数据是相同的。这现在，我认为是正确的。
- en: And we get a much more， you know， we get a much more slower descent。 We still
    end up with pretty good results。 And we can continue this a bit more to get down
    lower and lower and lower。 Yeah。 So the only reason， that the previous thing worked，
    it's extremely buggy。 The only reason that worked is that， this is a very， very
    simple problem。
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到一个更加缓慢的下降。我们最终仍然会得到相当不错的结果。我们可以继续这样做，让损失进一步降低。是的。因此，前面那个有效的唯一原因是，它极其不稳定。它有效的唯一原因是，这是一个非常非常简单的问题。
- en: And it's very easy for this neural net to fit this data。 And so the grads ended
    up accumulating and it effectively gave us a massive step size。 And it made us
    converge extremely fast。 But basically， now we have to do more steps。 to get to
    very low values of loss and get Y-pred to be really good。
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 这个神经网络很容易适应这些数据。因此，梯度最终积累起来，有效地给了我们一个巨大的步长。它让我们收敛得极快。但基本上，现在我们必须进行更多的步骤，以达到非常低的损失值，并使预测的
    Y 变得很好。
- en: We can try to step a bit greater。 Yeah， we're going to get closer and closer
    to one minus one。 So we're going to do all that sometimes， tricky because you
    may have lots of bugs in the code and your network might actually work。 just like
    ours worked。 But chances are is that if we had a more complex problem。 then actually
    this bug would have made us not optimize the loss very well。 And we were only。
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以尝试更进一步。是的，我们会越来越接近一减一。所以我们有时会做这些事情，这很棘手，因为代码中可能会有很多错误，而你的网络实际上可能会工作。就像我们的网络一样。但如果我们有一个更复杂的问题，实际上这个错误会使我们无法很好地优化损失。
- en: able to get away with it because the problem is very simple。 So let's now bring
    everything together。 and summarize what we learned。 What are neural nets？ Neural
    nets are these mathematical expressions。 Fairly simple mathematical expressions，
    in case of multi-layo perceptron that take input as the。 data and they take input
    the weights and the parameters of the neural net。 Mathematical expression。
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 能够做到这一点，因为这个问题非常简单。现在让我们把所有内容结合起来，总结我们学到的。什么是神经网络？神经网络是这些数学表达式。在多层感知器的情况下，这些相对简单的数学表达式将数据作为输入，并接收神经网络的权重和参数作为输入。数学表达式。
- en: for the forward pass， followed by a loss function。 And the loss function tries
    to measure the accuracy， of the predictions。 And usually the loss will be low
    when your predictions are matching your targets。 or where the new network is basically
    behaving well。 So we manipulate the loss function so that。
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 对于前向传递，后面跟着一个损失函数。损失函数试图衡量预测的准确性。通常当你的预测与目标匹配时，损失会较低。或者新的网络基本上表现良好。所以我们操控损失函数，以便。
- en: when the loss is low， the network is doing what you wanted to do on your problem。
    And then we backward， the loss， use back propagation to get the gradient。 And
    then we know how to tune all the parameters to， decrease the loss locally。 But
    then we have to iterate that process many times in what's called， the gradient
    descent。
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 当损失低时，网络在你的问题上做了你希望它做的事情。然后我们进行反向传播，使用反向传播获取梯度。然后我们知道如何调整所有参数，以局部减少损失。但接着我们必须在所谓的梯度下降中多次迭代这个过程。
- en: So we simply follow the gradient information and that minimizes the loss。 and
    the losses arranged so that when the loss is minimized。 the network is doing what
    you want it to do。 And yeah。 so we just have a blob of neural stuff and we can
    make it do arbitrary things。 And that's。
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们只是遵循梯度信息来最小化损失，并且损失排列成这样：当损失被最小化时，网络正按照你希望的方式工作。是的。所以我们只需有一堆神经元的东西，我们可以让它做任意的事情。这就是。
- en: what gives neural net their power。 It's you know， this is a very tiny network
    with 41 parameters。 But you can build significantly more complicated neural nets
    with billions at this point， almost。 trillions of parameters。 And it's a massive
    blob of neural tissue， simulated neural tissue。 roughly speaking。 And you can
    make it do extremely complex problems。 And these neural nets then。
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的力量来源于此。你知道，这只是一个拥有41个参数的非常小的网络。但你可以构建更复杂的神经网络，目前几乎有数十亿、甚至万亿个参数。这是一团巨大的神经组织，模拟的神经组织。粗略来说。你可以让它解决极其复杂的问题。然后这些神经网络。
- en: have all kinds of very fascinating emergent properties in when you try to make
    them do。 significantly hard problems。 As in the case of GPT， for example， we have
    massive amounts of。 text from the internet。 And we're trying to get a neural nets
    to predict to take like a few words。 and try to predict the next word in a sequence。
    That's the learning problem。 And it turns out that。
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 当你试图让它们解决显著困难的问题时，它们会表现出各种非常迷人的涌现特性。以GPT为例，我们从互联网获取了大量文本。我们试图让神经网络预测，从几个单词开始，预测序列中的下一个单词。这就是学习问题。结果发现。
- en: when you train this on all of internet， the neural net actually has like really
    remarkable。 emergent properties。 But that neural net would have hundreds of billions
    of parameters。 But it。 works on fundamentally these axing principles。 The neural
    net， of course。 will be a bit more complex。 But otherwise， the value in the gradient
    is there and will be identical。
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在整个互联网中训练时，神经网络实际上具有非常显著的涌现特性。但这个神经网络会有数百亿个参数。但它基本上是基于这些基本原则工作的。神经网络当然会复杂一些。但否则，梯度中的值仍然存在并且是相同的。
- en: And the gradient descent， would be there and would be basically identical。 But
    people usually use slightly different updates。 This is a very simple stochastic
    gradient descent update。 And loss function would not be in。 least squared error。
    They would be using something called the cross entropy loss for predicting the。
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降在这里是基本相同的。但人们通常使用略微不同的更新。这是一个非常简单的随机梯度下降更新。损失函数不会是最小平方误差。他们会使用一种称为交叉熵损失的函数来进行预测。
- en: next token。 So there's a few more details， but fundamentally， the neural network
    setup and neural。 network training is identical and pervasive。 And now you understand
    intuitively how that works。 under the hood。 In the beginning of this video， I
    told you that by the end of it。 you would understand， everything in micro grad
    and that would slowly build it up。
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个词。因此还有一些细节，但从根本上说，神经网络的设置和训练是相同且普遍的。现在你直观地理解了它在后台如何工作。在这个视频的开头，我告诉你到最后你会理解微梯度中的一切，并且这会慢慢地建立起来。
- en: Let me briefly prove that to you。 So I'm going to step through all the code
    that is in micro grad as of today。 Actually， potentially some of the code will
    change by the time you watch this video。 because I intend to， continue developing
    micro grad。 But let's look at what we have so far at least。 In it。py is empty。
    When you go to engine。py， that has the value。
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 让我简单证明一下这一点。因此，我将逐步展示截至今天在微梯度中的所有代码。实际上，等你观看这个视频时，某些代码可能会有所变化，因为我打算继续开发微梯度。但让我们看看目前为止的内容。在`__init__.py`中是空的。当你去`engine.py`时，它包含值。
- en: everything here you should mostly recognize。 So， we have the data data that
    grad attributes with the backward function。 We have the previous set of， children
    and the operation that produced this value。 We have addition multiplication and
    raising to a， scalar power。 We have the relo nonlinearity。 which is slightly different
    type of nonlinearity than， 10 H that we used in this video。
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的一切你应该大致上都能认出来。因此，我们有带有向后函数的梯度属性的数据数据。我们有先前的一组子项和产生该值的操作。我们有加法、乘法和提升到标量幂。我们有ReLU非线性函数，这是一种与我们在本视频中使用的10H稍有不同的非线性函数。
- en: Both of them are nonlinearities。 And notably， 10 H is not。 actually present
    in micro grad as of right now， but I intend to add it later。 With the backward。
    which is identical。 And then all of these other operations， which are built up
    on top of operations。 here。 So value should be very recognizable， except for the
    nonlinearity used in this video。
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个都是非线性函数。值得注意的是，10H实际上目前并不在微梯度中，但我打算稍后添加。与向后传播相同。然后所有这些其他操作都是基于这里的操作构建的。因此，值应该是非常可识别的，除了本视频中使用的非线性函数。
- en: There's no massive difference between relu and 10 H and sigmoid and these other
    nonlinearities。 They're all roughly equivalent and can be used in MLPs。 So I use
    10 H because it's a bit smoother。 and because it's a little bit more complicated
    than relu。 And therefore it's stressed a little bit more， the the local gradients
    and working with those derivatives。
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU和tanh、sigmoid及其他非线性函数之间没有太大区别。它们在多层感知器中都大致等效。因此，我使用tanh，因为它稍微平滑一点，并且比ReLU稍微复杂一些。因此，它在处理局部梯度和导数时更加突出。
- en: which I thought would be useful。 And in the pie is the neural networks library，
    as I mentioned。 So you should recognize identical， implementation of their own
    layer and MLP。 Notably。 or not so much， we have a class module here。 There's，
    a parent class of all these modules。 I did that because there's an end up module
    class in PyTorch。
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这会很有用。在图中是神经网络库，正如我提到的。所以你应该能识别出它们自己的层和多层感知器的相同实现。值得注意的是，我们这里有一个类模块。所有这些模块的父类。我这样做是因为在PyTorch中有一个结束模块类。
- en: And so this exactly matches that API and end up module in PyTorch has also a
    zero grad。 which I refactored out here。 So that's the end of micro grad， really。
    Then there's a test。 which you'll see basically creates two chunks of code， one
    in micro grad and one in PyTorch。 and we'll make sure that the forward and the
    backward paths agree identically。 For a slightly。
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这完全匹配PyTorch中的API，并且结束模块在PyTorch中还有一个零梯度的功能。我在这里进行了重构。因此，这就是微型梯度的结束。然后有一个测试，基本上创建了两段代码，一段在微型梯度中，另一段在PyTorch中。我们将确保前向和后向路径完全一致。
- en: less complicated expression and slightly more complicated expression， everything
    agrees。 So we。 agree with PyTorch and all of these operations。 And finally， there's
    a demo that I， by Y and B here。 and it's a bit more complicated binary classification
    demo than the one I covered in this lecture。 So we only had a tiny dataset of
    four examples。 Here we have a bit more complicated example。
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 更简单的表达式和稍微复杂的表达式，一切都一致。因此，我们与PyTorch及所有这些操作达成一致。最后，有一个演示，我在这里用Y和B制作的。这是一个比我在这次讲座中讲解的更复杂的二元分类演示。因此，我们只有一个包含四个示例的小数据集。这里我们有一个稍微复杂一些的示例。
- en: with lots of blue points and lots of red points。 And we're trying to， again。
    build a binary classifier， to distinguish two dimensional points as red or blue。
    It's a bit more complicated than MLP here with， it's bigger MLP。 The loss is a
    bit more complicated because it supports batches。 So because our data， civil so
    tiny。
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多蓝点和很多红点。我们再次尝试建立一个二元分类器，以区分红色或蓝色的二维点。这里比多层感知器稍微复杂，因为它是一个更大的多层感知器。损失函数也更复杂一些，因为它支持批处理。因此，由于我们的数据太小了。
- en: we always did a forward pass on the entire dataset of four examples。 But when
    your。 dataset is like a million examples， what we usually do in practice is we
    basically pick out some random。 subset， we call that a batch， and then we only
    process the batch forward， backward， and update。 So we don't have to forward the
    entire training set。 So this supports batching because there's a。
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 我们总是在四个示例的数据集上进行前向传播。但当你的数据集有一百万个示例时，我们在实践中通常做的就是挑选一些随机子集，我们称之为批量，然后我们只处理这一批的前向、后向和更新。因此我们不需要前向整个训练集。因此，这支持批处理。
- en: lot more examples here。 We do a forward pass。 The loss is slightly more different。
    This is a。 max margin loss that I implement here。 The one that we used was the
    mean squared error loss。 because it's simplest one。 There's also the binary cross
    entropy loss。 All of them can be used for。 binary classification and don't make
    too much of a difference in the simple examples that we looked。
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有更多的示例。我们进行一次前向传播。损失略有不同。这是我在这里实现的最大边际损失。我们使用的是均方误差损失，因为它是最简单的一个。还有二元交叉熵损失。所有这些都可以用于二元分类，对于我们查看的简单示例，它们之间没有太大差别。
- en: at so far。 There's something called L2 regularization used here。 This has to
    do with generalization of， the neural net and controls the overfitting in machine
    learning setting。 But I did not cover these， concepts in this video potentially
    later。 And the training loop you should recognize。 So forward， backward， with
    zero grad， and update。
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，有一种叫做L2正则化的东西被使用。这与神经网络的泛化有关，并控制机器学习中的过拟合。但我在这个视频中没有涵盖这些概念，可能稍后会提到。你应该能识别训练循环。因此，前向、后向、零梯度和更新。
- en: and so on。 You'll notice that in the update here， the learning rate is scaled
    as a function of number of iterations。 and it shrinks。 And this is， something
    called learning rate decay。 So in the beginning。 you have a high learning rate，
    and as， the network sort of stabilizes near the end。 you bring down the learning
    rate to get some of the， fine details in the end。 And in the end。
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 等等。你会注意到在这里的更新中，学习率被作为迭代次数的函数进行缩放，并且会减少。这被称为学习率衰减。所以一开始，你有一个较高的学习率，当网络在最后趋于稳定时，你会降低学习率，以获取最终的一些细节。最终。
- en: we see the decision surface of the neural net， and we see that it learned to
    separate out the red and the blue area based on the data points。 So that's the
    slightly more complicated example in the demo that I by Y and B that you're free
    to。 go over。 But yeah， as of today， that is micro grad。 I also wanted to show
    you a little bit of real。 stuff so that you get to see how this is actually implemented
    in production grade library like PyTorch。
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到神经网络的决策面，并且看到它学会根据数据点将红色和蓝色区域分开。这是我通过 Y 和 B 演示的稍微复杂一点的例子，你可以自由查看。但截至今天，这就是
    micro grad。我还想给你展示一点真实的东西，让你看到它是如何在像 PyTorch 这样的生产级库中实现的。
- en: So in particular， I wanted to show I wanted to find and show you the backward
    pass for 10H。 in PyTorch。 So here in micro grad， we see that the backward pass
    for 10H is one minus T square。 where T is the output of the 10H of X times out
    that grad， which is the chain rule。 So we're looking， for something that looks
    like this。 Now， I went to PyTorch。
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，我想展示和查找 10H 在 PyTorch 中的反向传播。所以在 micro grad 中，我们看到 10H 的反向传播是 1 减去 T 平方，其中
    T 是 10H 的输出，X 乘以该 grad，这就是链式法则。所以我们在寻找看起来像这样的东西。现在，我去查阅 PyTorch。
- en: which has an open source GitHub code base， and I looked through a lot of its
    code。 And honestly。 I spent about 15 minutes and I couldn't find 10H。
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 它有一个开源的 GitHub 代码库，我浏览了很多代码。老实说，我花了大约 15 分钟，还是找不到 10H。
- en: '![](img/cc7869f47d1be0913690256270daf4d3_1.png)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cc7869f47d1be0913690256270daf4d3_1.png)'
- en: And that's because these libraries， unfortunately， they grow in size and entropy。
    And if you just。
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这些库的规模和复杂性不断增加。如果你仅仅。
- en: '![](img/cc7869f47d1be0913690256270daf4d3_3.png)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cc7869f47d1be0913690256270daf4d3_3.png)'
- en: search for 10H， you get apparently 2800 results and 400 and 406 files。 So I
    don't know what these。 files are doing， honestly。 And why there are so many mentions
    of 10H。 But unfortunately。 these libraries are quite complex。 They're meant to
    be used， not really inspected。 Eventually。 I did stumble on someone who tries
    to change the 10H backward code for some reason。
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索 10H，你会发现明显有 2800 个结果和 400 和 406 个文件。所以我不知道这些文件在做什么，老实说。而且为什么会有这么多提及 10H。但不幸的是，这些库相当复杂。它们的设计目的是使用，而不是检查。最后，我确实偶然发现有人尝试修改
    10H 的反向代码出于某种原因。
- en: And someone here， pointed to the CPU kernel and the CUDA kernel for 10H backward。
    So this basically depends on if， you're using PyTorch on a CPU device or on a
    GPU。 which these are different devices and I haven't， covered this。 But this is
    the 10H backward kernel for CPU。 And the reason it's so large is that， number
    one。
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有人提到了 CPU 核心和 10H 的 CUDA 核心。所以这基本上取决于你是使用 CPU 设备还是 GPU，而这两者是不同的设备，我还没有涉及到。但这是
    CPU 的 10H 反向核心。之所以如此庞大，首先是因为。
- en: this is like if you're using a complex type， which we haven't even talked about。
    if you're using a specific data type of B float 16， which we haven't talked about。
    And then if you're not， then this is the kernel and deep here， we see something
    that resembles our。 backward pass。 So they have eight times one minus B square。
    So this B， B here， must be the output。
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是复杂类型，我们甚至还没有讨论过。如果你使用的是 B float 16 的特定数据类型，我们也还没有讨论过。如果没有，这就是内核，深入看，我们看到一些类似于我们的反向传播的内容。所以它们有八倍的
    1 减去 B 平方。因此，这里的 B，必须是输出。
- en: of the 10H。 And this is the help that grad。 So here we found it deep inside
    PyTorch on this location。 for some reason， inside binary ops kernel， when 10H
    is not actually a binary op。 And then this is。 the GPU kernel。 We're not complex。
    We're here。 And here we go with online。 So we did find it， but。 basically， unfortunately，
    these code bases are very large。 And micrograd is very， very simple。
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 10H 的输出。这是帮助 grad。所以在这里我们在 PyTorch 的这个位置深处找到了它，出于某种原因，在二元运算内核中，当 10H 实际上并不是一个二元运算时。然后这是
    GPU 核心。我们并不复杂。我们在这里。然后我们在线。所以我们确实找到了它，但不幸的是，这些代码库非常庞大，而 micrograd 非常非常简单。
- en: but we actually want to use real stuff， finding the code for it。 you'll actually
    find that difficult。 I also wanted to show you a little example here。 where PyTorch
    is showing you how you can register。 a new type of function that you want to add
    to PyTorch as a Lego building walk。 So here， if you。
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们实际上想使用真实的东西，找到它的代码。你会发现这很困难。我还想在这里给你展示一个小例子，PyTorch向你展示如何注册你想添加到PyTorch的新类型函数，作为一个乐高积木。因此在这里，如果你。
- en: want to， for example， add a like gender polynomial three。 Here's how you could
    do it。 you will register， it as a class that， subclass is torch。org， that function。
    And then you have to tell PyTorch how to， forward your new function and how to
    backward through it。 So as long as you can do the forward， pass of this little
    function piece that you want to add。
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 比如说，我想添加一个像性别多项式三。以下是你可以做到的方法。你需要将它注册为一个类，子类是torch。org，那个函数。然后你必须告诉PyTorch如何前向你的新函数，以及如何通过它反向传播。因此，只要你能做到这个小函数的前向传递。
- en: and as long as you know， the local derivative， local gradients。 which are implemented
    in the backward， PyTorch will be able to back propagate。 through your function。
    And then you can use this as a Lego block in a larger Lego castle of all the。
    different Lego blocks that PyTorch already has。 And so that's the only thing you
    have to tell。
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 只要你知道局部导数和局部梯度，它们是在反向传播中实现的，PyTorch就能够通过你的函数进行反向传播。然后你可以把它当作一个乐高块，在更大的乐高城堡中，与所有PyTorch已经拥有的不同乐高块结合。因此这就是你需要告诉的唯一内容。
- en: PyTorch and everything would just work。 And you can register new types of functions
    in this way。 following this example。 And that is everything that I wanted to cover
    in this lecture。 So I hope。 you enjoyed building out my program with me。 I hope
    you find it interesting， insightful。 And， yeah。 I will post a lot of the links
    that are related to this video in the video description below。
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch和所有内容将正常工作。你可以通过这种方式注册新类型的函数，遵循这个例子。这就是我想在这次讲座中涵盖的所有内容。因此我希望你享受与我一起构建我的程序。我希望你觉得它有趣、富有启发性。是的。我会在视频描述中发布许多与此视频相关的链接。
- en: I will also probably post a link to a discussion forum or discussion group where
    you can ask。 questions related to this video。 And then I can answer or someone
    else can answer your questions。 And I may also do a follow-up video that answers
    some of the most common questions。 But for now。 that's it。 I hope you enjoyed
    it。 If you did， then please like and subscribe so that。
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 我也可能会发布一个讨论论坛或讨论组的链接，你可以在那儿提问与这个视频相关的问题。然后我可以回答或者其他人可以回答你的问题。我可能还会制作一个后续视频，回答一些最常见的问题。但目前为止，就到这里。我希望你喜欢它。如果你喜欢，请点赞和订阅。
- en: YouTube knows to feature this video to more people。 And that's it for now。 I'll
    see you later。
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: YouTube知道要向更多人推荐这个视频。现在就到这里，我稍后再见。
- en: '![](img/cc7869f47d1be0913690256270daf4d3_5.png)'
  id: totrans-361
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cc7869f47d1be0913690256270daf4d3_5.png)'
- en: '![](img/cc7869f47d1be0913690256270daf4d3_6.png)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cc7869f47d1be0913690256270daf4d3_6.png)'
- en: Now here's the problem。 We know dl by。 Wait， what is the problem？
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 现在问题来了。我们知道dl是通过。等一下，这个问题是什么？
- en: And that's everything I wanted to cover in this lecture。 So I hope you enjoyed
    us building out。 micro-grabbed micro-grab。 Okay， now let's do the exact same thing
    for multiply because we。 can't do something like eight times two。 Oops。 I know
    what happened there。
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我想在这次讲座中涵盖的所有内容。因此我希望你喜欢我们构建的微抓取微抓取。好吧，现在我们为乘法做完全相同的事情，因为我们不能做类似八乘以二的事情。哦，糟糕。我知道发生了什么。
- en: '![](img/cc7869f47d1be0913690256270daf4d3_8.png)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cc7869f47d1be0913690256270daf4d3_8.png)'
