# P74：p74 CS 285： Lecture 18, Variational Inference, Part 2 - 加加zero - BV1NjH4eYEyZ

好的，那么现在让我们进入今天的讲座的主要技术部分，那就是讨论可变推断框架。

![](img/8e71fb473d0996cd530cc9460e2ebd70_1.png)

所以这个框架基本上关注的是这个问题。

![](img/8e71fb473d0996cd530cc9460e2ebd70_3.png)

我们如何计算给定x i的p(c)，但在这个过程中，我们也会看到预期对数似然实际上是一个合理的目标。

![](img/8e71fb473d0996cd530cc9460e2ebd70_5.png)

所以让我们思考做一些粗糙的近似，所以给定x i的p(z)通常是一个相当复杂的分布，对，因为单个点x可能来自z空间中许多不同的地方，但是让我们做一个非常简化的近似，假设我们要根据x i来近似z的p。

使用z的分布q i，它是高斯分布的一种。

![](img/8e71fb473d0996cd530cc9460e2ebd70_7.png)

或者一般上，使用一些非常易于处理的参数化分布类，并注意我正在叫它q子脚本i的z，所以它是z的分布，并且是，它是一种将特定于这个点的分布，x i，好的，所以，而不是有这个复杂的东西。

"我们将尝试用单个峰值来近似它"，"并且我故意选择了这张照片"，"只是为了明确指出，这个近似值并不一定是一个很好的值"，"但我们将尝试在这个简单的分布类中找到最佳的可能匹配"，"高斯分布类"。

"结果发现，如果你用任何q i来近似给定x i的z的p，那么"，"Z"的中文翻译是"Z"。"实际上，你可以对x i的日志概率构建一个下界。"，"这就是一个极其强大的想法"。

"因为如果你能构建出x i的日志概率下界"，然后最大化这个边界将推动对x i的日志概率的上升，现在，一般而言，最大化下界不会增加你关心的量。



![](img/8e71fb473d0996cd530cc9460e2ebd70_9.png)

但如果边界足够紧，那么它就会，我们稍后将看到在一些条件下，边界实际上很紧。

![](img/8e71fb473d0996cd530cc9460e2ebd70_11.png)

但现在让我们只是不要，让我们不担心紧度，让我们只看看我们如何使用q ioc来获取一个边界，所以，我们可以写出x i的日志概率，作为对变量z的所有值积分的p(x_i)的日志，给定z乘以z的p和uh。



![](img/8e71fb473d0996cd530cc9460e2ebd70_13.png)

你知道，通常的技巧，如果你想引入当前方程中不存在的量。

![](img/8e71fb473d0996cd530cc9460e2ebd70_15.png)

是乘以那个量除以自己，所以q_i，z除以q_i，z等于一，所以我们可以在任何时候乘以那个，所以现在嗯，我们可以注意到我们有一些数量乘以q i的z，所以我们可以写这个数量作为在q i下z的期望值。



![](img/8e71fb473d0996cd530cc9460e2ebd70_17.png)

所以我们基本上取那个比例的 Numerator，变成期望值。

![](img/8e71fb473d0996cd530cc9460e2ebd70_19.png)

然后其他所有的都被留下，所以我们有在q i下z的期望值p的log，给定z时，z的p除以q i的z。

![](img/8e71fb473d0996cd530cc9460e2ebd70_21.png)

好的，所以到目前为止，我们没有做任何近似。

![](img/8e71fb473d0996cd530cc9460e2ebd70_23.png)

这些只是，嗯，这只是一些代数运算，接下来，我们要做的是我们将使用，Jensen的不等式，Jensen的不等式是一种关系方式，嗯，凸或凹函数适用于线性组合，我在幻灯片上写的。



![](img/8e71fb473d0996cd530cc9460e2ebd70_25.png)

是Jensen不等式对对数的特殊情况，这是一个凹函数，但一般来说，这个不等式对于任何凹函数都成立，如果你有一个凸函数，那么它就成立，但不等式方向相反，对于对数情况，Jensen的不等式说。

对某个变量的期望值的对数，大于等于该变量对数的期望值，如果你觉得这个有点反直觉，嗯，你可以考虑尝试画个图，对数是一个凹函数，所以它像这样，如果你想象对数一个函数的和。



![](img/8e71fb473d0996cd530cc9460e2ebd70_27.png)

因为对数。

![](img/8e71fb473d0996cd530cc9460e2ebd70_29.png)

对数增加的速率总是减少，那么这些函数的和，对数的和，将大于等于对数这些函数的和，因为速率的减少，如果你对这个还不太清楚，试着在这个上画个图，你知道有多个不同的，嗯，对数函数被相加在一起。



![](img/8e71fb473d0996cd530cc9460e2ebd70_31.png)

好的，我们可以直接应用Jensen的不等式到前一个幻灯片的结果。

![](img/8e71fb473d0996cd530cc9460e2ebd70_33.png)

我们这样做的方式是，通过注意到我们有期望值某个数量的log，那就是在q i下z的期望值。

![](img/8e71fb473d0996cd530cc9460e2ebd70_35.png)

所以，应用詹的不等式仅仅将期望值推出了对数的范围之外，并替换了等式为一个大于或等于的符号。

![](img/8e71fb473d0996cd530cc9460e2ebd70_37.png)

所以这意味着我们的先前结果被期望值下q的限制所下界。

![](img/8e71fb473d0996cd530cc9460e2ebd70_39.png)

I of z of the logarithm of the ratio p of x i，给定z乘以p的z除以q的z。



![](img/8e71fb473d0996cd530cc9460e2ebd70_41.png)

但现在，当然，我们知道，那对数的乘积可以写成对数的和的形式。

![](img/8e71fb473d0996cd530cc9460e2ebd70_43.png)

所以我们可以等价地写出这个作为q的i下期望值z，"给定log p x i和log p z，求log p (z+xi)"。



![](img/8e71fb473d0996cd530cc9460e2ebd70_45.png)

减去在q_i下z的期望值，以及对数q_i下的z。

![](img/8e71fb473d0996cd530cc9460e2ebd70_47.png)

"所以我这样写出来的原因是"，"这是因为我想要收集所有依赖于关键词的术语"。

![](img/8e71fb473d0996cd530cc9460e2ebd70_49.png)

"嗯，在第一部分"，"并且第二部分中依赖于q的所有术语"。

![](img/8e71fb473d0996cd530cc9460e2ebd70_51.png)

"现在，这个方程的一个好处是所有事情都是可追踪的，所以"。

![](img/8e71fb473d0996cd530cc9460e2ebd70_53.png)

"对于z中的任意q i，这个都是真实的"。

![](img/8e71fb473d0996cd530cc9460e2ebd70_55.png)

"所以我们可以选择z中的任意一个q i，然后我们有一个下界"，"尽管不是所有的q都"。

![](img/8e71fb473d0996cd530cc9460e2ebd70_57.png)

当然会导致最佳的下界，但我们可以从样本集合z中挑选一些qi，来评估第一个期望，然后评估第二个期望，你会发现实际上这是qi对z的熵的方程。



![](img/8e71fb473d0996cd530cc9460e2ebd70_59.png)

对于许多简单的分布如高斯分布，有一个封闭形式的解。

![](img/8e71fb473d0996cd530cc9460e2ebd70_61.png)

好的，所以我们可以替换第二个术语为仅qi的熵，所以嗯，最大化这个可能会最大化logpxi。

![](img/8e71fb473d0996cd530cc9460e2ebd70_63.png)

尽管，正如我之前提到的，你需要证明界限不是太松。

![](img/8e71fb473d0996cd530cc9460e2ebd70_65.png)

现在让我来做一个简短的插曲，只是为了总结一下一些信息，我们在这里遇到的理论量，其中大部分我们已经见过，我们已经谈论过熵，例如，在讲座的探索中，但我只是想简要回顾一下。



![](img/8e71fb473d0996cd530cc9460e2ebd70_67.png)

因为这东西对于获得良好的直觉真的很重要，所以变分推断实际上在做什么，熵，某个分布的熵是其对数概率的负预期值，这里是一个伯努利分布的熵方程，所以二进制事件的概率，你可以看到，随着事件概率接近0。5。

熵增加，并下降到0，如果事件被保证会发生，所以概率等于1，或者保证不会发生，概率等于零，所以对熵的一种直觉是随机变量的随机程度如何，所以这在伯努利变量的案例中非常有道理。



![](img/8e71fb473d0996cd530cc9460e2ebd70_69.png)

当它是0。5时，变量在某种意义上是最随机的，最不可预测的，并且它具有最高的熵，第二个直觉是，它是在期望下自身对数概率的大小。



![](img/8e71fb473d0996cd530cc9460e2ebd70_71.png)

所以如果你在期望下主要看到低的对数概率。

![](img/8e71fb473d0996cd530cc9460e2ebd70_73.png)

这意味着有许多，许多你分配的地方，你知道，如果你主要看到非常高的对数概率，这意味着你真的集中在几个点上，你分配高概率的地方。



![](img/8e71fb473d0996cd530cc9460e2ebd70_75.png)

所以顶部的例子具有高熵，因为对数概率普遍较低。

![](img/8e71fb473d0996cd530cc9460e2ebd70_77.png)

而底部的例子具有更高的熵。

![](img/8e71fb473d0996cd530cc9460e2ebd70_79.png)

或者更低的熵，因为对数概率只在几个地方非常高，嗯，这就是一个低熵的分布，好的，嗯，然后我们可以，嗯，问关于我们之前在幻灯片上看到的变分下界问题，我们期待它会做什么，它是某个量的期望加上q的熵。

所以如果这张图显示p x i，z。

![](img/8e71fb473d0996cd530cc9460e2ebd70_81.png)

所以第一个期望中的东西。

![](img/8e71fb473d0996cd530cc9460e2ebd70_83.png)

你可以想象这个函数的期望会被最大化，仅仅通过在最高的峰上分配大量的概率质量，这就是如果我们只最大化第一部分会得到的。



![](img/8e71fb473d0996cd530cc9460e2ebd70_85.png)

我们只是想找到一个在z上的分布。

![](img/8e71fb473d0996cd530cc9460e2ebd70_87.png)

其中我们具有p x i，z的最大值，但我们也在试图最大化这个分布的熵。

![](img/8e71fb473d0996cd530cc9460e2ebd70_89.png)

所以我们不想让它太瘦，如果我们也在试图最大化熵，那。

![](img/8e71fb473d0996cd530cc9460e2ebd70_91.png)

我们想要尽可能地分散它。

![](img/8e71fb473d0996cd530cc9460e2ebd70_93.png)

同时仍然保持在p x i，z值大的区域，因为第二个术语，我们得到了一种分散的情况。

![](img/8e71fb473d0996cd530cc9460e2ebd70_95.png)

因为我们有这个第二个术语，所以我们得到了一种分散的情况，因为第二个术语，我们得到了一种分散的情况。

![](img/8e71fb473d0996cd530cc9460e2ebd70_97.png)

直觉是，由于这个，在期望下最大化这个量的qi z将覆盖uh。

![](img/8e71fb473d0996cd530cc9460e2ebd70_99.png)

p x i，z分布。

![](img/8e71fb473d0996cd530cc9460e2ebd70_101.png)

现在，我想在这里回顾的另一个概念是KL散度，两个分布的KL散度，Q和p由在第一个分布下对log的期望值给出。



![](img/8e71fb473d0996cd530cc9460e2ebd70_103.png)

第一个分布概率除以第二个分布的比例，再次，利用，对数乘积的事实是对数和的和。

![](img/8e71fb473d0996cd530cc9460e2ebd70_105.png)

我们可以将这个写出来为在q下q对x的期望值，减去在q下logp对x的期望值。

![](img/8e71fb473d0996cd530cc9460e2ebd70_107.png)

我们可以重新写为，以一种看起来更像前一张幻灯片公式的方式，如果我们只是交换位置并识别在q下log q的预期值，就是负熵，所以KL散度是q下log p(x)的预期值的负值，和q的熵。

对于KL散度测量什么意味的一种直觉是，两个分布有多不同，你会注意到，当q和p相等时，嗯，KL散度趋近于零，它为什么容易为零是显而易见的，因为你有q除以p等于。



![](img/8e71fb473d0996cd530cc9460e2ebd70_109.png)

一个数的log是零，并且第二个直觉是如何小是预期log概率一个分布相对于另一个，减去熵，现在为什么熵好对于同样的原因我们之前看到。



![](img/8e71fb473d0996cd530cc9460e2ebd70_111.png)

因为如果你没有熵项，那么q只会想要坐在p下最可能的点。

![](img/8e71fb473d0996cd530cc9460e2ebd70_113.png)

但如果我们有熵。

![](img/8e71fb473d0996cd530cc9460e2ebd70_115.png)

那么它想要覆盖它，所以变分近似说log p of x i是大于，等于在q_i下，z的期望值对log pi(x_i)的贡献，加上log z加上q的熵，我们称这个为证据下界或变分下界。

我将其表示为li(p)，逗号q_i，嗯，正如我们在上一页看到的，它也是负数，嗯，KL散度，那么，什么是一个好的z气？嗯，直觉是，一个好的z气应该近似于z的p，给定xi。

因为那样你就可以得到最紧的近似在何种意义上。

![](img/8e71fb473d0996cd530cc9460e2ebd70_117.png)

嗯，嗯，你可以根据KL散度来比较它们，所以可以说。

![](img/8e71fb473d0996cd530cc9460e2ebd70_119.png)

KL散度测量两个分布的差异，当KL多样性为零时，那么两个分布完全相等，所以让我们选择$q_i$，以最小化$q_i$对$z$的KL多样性，和给定$x$的$z$后验$p$的KL多样性。



![](img/8e71fb473d0996cd530cc9460e2ebd70_121.png)

为什么，因为如果我们使用前面的定义来写出这个KL多样性。

![](img/8e71fb473d0996cd530cc9460e2ebd70_123.png)

我们将看到它等于$q_i$对$z$的期望值，嗯，在$q_i$对$z$下的$log q_i$对$z$除以给定$x_i$的$p$，现在，给定x和i，p(z|x，i)可以写成p(x，i)的形式，由p(x。

i)得到，逗号，z除以p(x，i)，因为我们正在处理这个的倒数，我们翻转比例，得到这里和这里的方程。

![](img/8e71fb473d0996cd530cc9460e2ebd70_125.png)

应用性质，即对数的和。

![](img/8e71fb473d0996cd530cc9460e2ebd70_127.png)

对数的积等于对数的和，我们在这一侧得到方程。

![](img/8e71fb473d0996cd530cc9460e2ebd70_129.png)

所以，我们有第一个项，q_i下z的负期望值对log p(x，i|z)。

![](img/8e71fb473d0996cd530cc9460e2ebd70_131.png)

加上对z的logp，然后我们有熵项。

![](img/8e71fb473d0996cd530cc9460e2ebd70_133.png)

然后我们有这个先验项，所以将熵的方程代入。

![](img/8e71fb473d0996cd530cc9460e2ebd70_135.png)

我们得到这个，所以这意味着这两个量之间的KL散度等于负数。

![](img/8e71fb473d0996cd530cc9460e2ebd70_137.png)

嗯，变分下界加上x_i的log概率我注意到，但是x_i的log概率，嗯实际上并不等于q_i。

![](img/8e71fb473d0996cd530cc9460e2ebd70_139.png)

所以我们可以稍微重新排列这些术语。

![](img/8e71fb473d0996cd530cc9460e2ebd70_141.png)

并且我们可以将log p(x_i)表示为q和p之间的KL散度，加上证据下界，而且这不是一个不等式，这现在是精确的，我们知道KL散度总是正的，所以这实际上是另一种得到证据下界的方法。



![](img/8e71fb473d0996cd530cc9460e2ebd70_143.png)

因为知道log p(x_i)等于一些正数加上l，这意味着l是log p(x_i)的下界。

![](img/8e71fb473d0996cd530cc9460e2ebd70_145.png)

这意味着l是log p(x_i)的下界，但是除此之外，这个方程表明如果我们让kl发散到零。

![](img/8e71fb473d0996cd530cc9460e2ebd70_147.png)

那么证据的下限实际上等于log p of x i，这意味着最小化那个kl发散。

![](img/8e71fb473d0996cd530cc9460e2ebd70_149.png)

嗯是一种有效的方法来紧缩界限。

![](img/8e71fb473d0996cd530cc9460e2ebd70_151.png)

所以这就解释了我们为什么想要选择q。

![](img/8e71fb473d0996cd530cc9460e2ebd70_153.png)

I of z来近似p of z给定x i，并且它也解释了我们为什么想要使用预期的对数似然，因为当我们使用预期的对数似然。



![](img/8e71fb473d0996cd530cc9460e2ebd70_155.png)

那就是像取p下z给定x的期望，哪个点是边界最紧的。

![](img/8e71fb473d0996cd530cc9460e2ebd70_157.png)

好的，所以在我们使用它来推导出这个边界之前，我们有这个方程。

![](img/8e71fb473d0996cd530cc9460e2ebd70_159.png)

嗯，并且我们可以将KL散度写成这样，这是我们在前一滑片中看到的，所以嗯，这意味着KL散度由负变分下界给出，加上现在这个log p of x i项，log p of x i不依赖于q i，所以。

如果我们想要优化q i of x，嗯，关于z到u的优化，最小化KL散度，我们可以等价地优化相同的证据，下界。



![](img/8e71fb473d0996cd530cc9460e2ebd70_161.png)

这看起来很吸引人，以q i为参数最大化相同的证据下界。

![](img/8e71fb473d0996cd530cc9460e2ebd70_163.png)

最小化KL散度并使边界紧密，以p为参数最大化会增加log似然度。

![](img/8e71fb473d0996cd530cc9460e2ebd70_165.png)

所以现在这立即建议一个实用的学习算法。

![](img/8e71fb473d0996cd530cc9460e2ebd70_167.png)

取你的变分下界，你的证据下界。

![](img/8e71fb473d0996cd530cc9460e2ebd70_169.png)

以q i为参数最大化以获取最紧的边界，然后以uh为参数最大化。

![](img/8e71fb473d0996cd530cc9460e2ebd70_171.png)

P，以提高你的模式来提高你的log似然度。

![](img/8e71fb473d0996cd530cc9460e2ebd70_173.png)

然后交替这些两步，好的，所以总结一下，我们的目标是最大化log p theta of x i。

![](img/8e71fb473d0996cd530cc9460e2ebd70_175.png)

但这是不可行的，所以相反，我们将最大化证据下界。

![](img/8e71fb473d0996cd530cc9460e2ebd70_177.png)

对于每个x i，我们将计算模型参数关于的梯度um。

![](img/8e71fb473d0996cd530cc9460e2ebd70_179.png)

通过从qi of x i采样z的，然后使用这些样本来估计梯度，所以这是单个样本版本，所以你从qi of x i采样一个z，嗯，并假设你的先验p of z不依赖于theta。

然后梯度就是log p theta x i给定z的梯度，然后你改进你的theta。

![](img/8e71fb473d0996cd530cc9460e2ebd70_181.png)

然后更新qi以最大化相同的证据下界，所以这就是变分推断的随机梯度下降版本。

![](img/8e71fb473d0996cd530cc9460e2ebd70_183.png)

再次声明，以确保我们都在同一页面上。

![](img/8e71fb473d0996cd530cc9460e2ebd70_185.png)

为了估计grad theta of l i p p comma q i的梯度，从近似后验qi中采样一个z，计算grad theta l i p，逗号，给定z。

q i和log p theta x i的关系是什么，然后取这个梯度步，然后更新你的qi以最大化，嗯，L i p，逗号，q a可以吗，所以这里的一切都很直接，除了最后一行，你实际上如何改善你的qi很好。

假设qi由a给出，嗯，一个均值为μi，方差为σi的高斯分布。

![](img/8e71fb473d0996cd530cc9460e2ebd70_187.png)

σi很好。

![](img/8e71fb473d0996cd530cc9460e2ebd70_189.png)

实际上，你可以根据证据的均值和方差计算梯度，下界，然后，对μi和σi进行梯度上升。

![](img/8e71fb473d0996cd530cc9460e2ebd70_191.png)

我，嗯，那么这个问题在哪里，嗯，我们有多少参数要记住，因为我们对每个数据点都有一个单独的qi，Xi和它们各有不同的平均值和标准差，如果你有数千个数据点，这不算大问题。



![](img/8e71fb473d0996cd530cc9460e2ebd70_193.png)

但如果你有数百万个数据点，这就成为一个大问题，而且在深度学习设置中，我们通常将有一个非常大的数据集。

![](img/8e71fb473d0996cd530cc9460e2ebd70_195.png)

所以如果我们有这个，嗯，高斯分布，模型的参数数量就是参数theta的数量。

![](img/8e71fb473d0996cd530cc9460e2ebd70_197.png)

加上均值的维度，再加上方差的维度乘以。

![](img/8e71fb473d0996cd530cc9460e2ebd70_199.png)

数据点数n。

![](img/8e71fb473d0996cd530cc9460e2ebd70_201.png)

好的，所以这可能有点太大，嗯n可能是一个很大的数字。

![](img/8e71fb473d0996cd530cc9460e2ebd70_203.png)

嗯，这可能是不可解的，但是记住我们的直觉是，z的qi需要，在某种程度上近似给定x i的后验p(z)，那么如果我们不学习z的每个单独qi，对于每个数据点，都有一个单独的均值和方差。

如果您训练一个单独的神经网络模型，该模型近似于z的qi。

![](img/8e71fb473d0996cd530cc9460e2ebd70_205.png)

因此，我们不再为每个数据点有一个单独的zi的q。

![](img/8e71fb473d0996cd530cc9460e2ebd70_207.png)

xi，我们有一个网络q给定xi，表示z，它旨在近似后验。

![](img/8e71fb473d0996cd530cc9460e2ebd70_209.png)

所以然后，在我们的生成模型中，我们将有一个神经网络，从z映射到x，另一个神经网络从x映射到z。

![](img/8e71fb473d0996cd530cc9460e2ebd70_211.png)

那就是神经网络，嗯，它给我们提供了一个后验估计，其均值和方差由神经网络函数x给出。

![](img/8e71fb473d0996cd530cc9460e2ebd70_213.png)

![](img/8e71fb473d0996cd530cc9460e2ebd70_214.png)

这就是变分推断的 amortized 理念。

![](img/8e71fb473d0996cd530cc9460e2ebd70_216.png)