# 【深度强化学习 CS285 2023】伯克利—中英字幕 - P58：p58 CS 285： Lecture 13, Part 5 - 加加zero - BV1NjH4eYEyZ

好的，接下来，我将讨论深度强化学习中的探索算法，这些算法基于后验采样或汤普森采样的思想，我之前已经讨论过。



![](img/830c1bf7cf86e351e64894a9edd8a7a6_1.png)

所以作为提醒，在棒手设置中的后验采样或汤普森采样，指的是我们实际上估计我们的棒手的模型的情况，所以如果theta是参数化我们棒手奖励分布的theta，我们将实际上维护对theta的信念。

然后在探索的每一步中，我们将基于我们的信念采样theta，并采取行动，那就是由该模型描述的强盗的最大值，所以在深度强化学习设置中，我们可以问，我们应该采样什么，我们如何表示分布，在强盗设置中。

实际上并没有选择要做，唯一未知的是奖励模型的模型，然后你知道那个模型很简单，所以在深度强化学习设置中，不是很难表示，这当然比这复杂得多，所以，在强盗设置中。

P 假设 theta 一至 theta n 是奖励分布的一个实例，类似和 MDPs 将是一个 q 函数，因为在强盗中，即时奖励基本上就是你需要了解的所有。

所以你可以选择你的行动作为 MDP 中奖励的最大值，我们不选择我们的行动作为奖励的最大值，我们选择行动作为 q 函数的最大值，所以，我们可以通过以下方式适应，采用后验采样或汤普森采样。

而且这并不是唯一的方法，但是，有一种特别简单的方法是从q函数的分布中采样一个q函数，然后，根据那个q函数进行一集动作，然后更新你的q函数分布，然后重复，现在，由于q学习是非策略的。

我们实际上不关心是哪个q函数被用来收集那集数据，你可以训练你知道的所有，我们的整个q函数分布都可以在同一个数据集上进行训练，所以这是可以的，如果我们使用不同的策略或对每个回滚使用不同的政策。



![](img/830c1bf7cf86e351e64894a9edd8a7a6_3.png)

我们如何很好地表示函数分布，我们可以做的事情之一是回顾基于模型的rl讲座，在那里我们学习如何使用bootstrap ensembles来代表分布，并基本上尝试同样的事情，所以给定一个数据集。

D我们以替换的方式从该数据集中采样n次，以获取n个数据集d1到dn，然后我们在每个数据集上训练一个单独的模型，这基本上意味着我们对每个数据集都训练一个单独的q函数，然后从后验中采样。

我们简单地从那些模型中随机选择一个。

![](img/830c1bf7cf86e351e64894a9edd8a7a6_5.png)

然后使用那个模型，所以这里是一个显示不确定性的小例子，现在，由这些Bootstrap神经网络估计的区间，当然，训练和大型神经网络是昂贵的，我们如何避免它好。

我们再次使用我们在基于模型的RL讲座中使用的同一个技巧。

![](img/830c1bf7cf86e351e64894a9edd8a7a6_7.png)

你知道的，不，嗯，样本替换的重采样，只是使用相同的数据集，此外，嗯，我们可以做的事情之一，这在这篇论文的底部描述，叫做Bootstrap深度探索，DQN是，我们可以实际上训练一个网络有多个头。

那不是理想的，因为现在，那些不同头的输出将会相关，但在实践中，它们可能会足够不同，给我们一些探索的变异性，这可能不是一个很好的方式来估计一个非常准确的后验，但它可能足够好。

以确保每个头都有稍微不同的行为，顺便说一句，对于那些不熟悉深度学习术语的人，当我说多个头时，我的意思是，网络的所有层都是共享的，除了最后一层，所以有多个复制的最后一层。



![](img/830c1bf7cf86e351e64894a9edd8a7a6_9.png)

我们称之为不同的头，好的。

![](img/830c1bf7cf86e351e64894a9edd8a7a6_11.png)

所以，为什么这种方法在随机行动探索中工作得好，如。

![](img/830c1bf7cf86e351e64894a9edd8a7a6_13.png)

例如，使用像epsilon-greedy的结果，你知道一个问题结果是，你可能会来回摆动，你可能不会到达一个有趣或连贯的地方，只是通过随机摆动。



![](img/830c1bf7cf86e351e64894a9edd8a7a6_15.png)

例如，这里有一个被称为Sequestyou的Atari游戏的困难之处。

![](img/830c1bf7cf86e351e64894a9edd8a7a6_17.png)

你控制潜水艇，为某种原因，你应该做的事情是射击鱼，或者捡起潜水员。

![](img/830c1bf7cf86e351e64894a9edd8a7a6_19.png)

或者可能是反过来，我不知道，但它是一种非常不友好的生态行为。

![](img/830c1bf7cf86e351e64894a9edd8a7a6_21.png)

但潜水艇耗尽了氧气，所以如果它呆在水下太久，那么你就会失败。

![](img/830c1bf7cf86e351e64894a9edd8a7a6_23.png)

因为你没有空气，为了正确地玩这个游戏，你应该射击所有的鱼。

![](img/830c1bf7cf86e351e64894a9edd8a7a6_25.png)

一旦氧气条变得太低，然后返回并恢复一些空气。

![](img/830c1bf7cf86e351e64894a9edd8a7a6_27.png)

问题是，如果你随机探索。

![](img/830c1bf7cf86e351e64894a9edd8a7a6_29.png)

那么一旦你到达海洋的底部，你随机浮出水面的可能性极小，因为那需要随机按下上升按钮很多次，实际上，一旦你到达底部，你浮出水面的可能性就呈指数级下降。



![](img/830c1bf7cf86e351e64894a9edd8a7a6_31.png)

并且由于游戏的机制，实际上，玩起来稍微容易一些。

![](img/830c1bf7cf86e351e64894a9edd8a7a6_33.png)

如果你稍微深入一些，所以，通过epsilon贪婪探索发现呼吸空气的机会非常困难。

![](img/830c1bf7cf86e351e64894a9edd8a7a6_35.png)

当你使用随机q函数探索时。

![](img/830c1bf7cf86e351e64894a9edd8a7a6_37.png)

你将承诺在整个episode内执行一个随机但内部一致的策略，所以，q函数可能会得出稍微不同的结论，例如，在你的集合中，一个q函数可能决定更深入是好的。



![](img/830c1bf7cf86e351e64894a9edd8a7a6_39.png)

另一个可能决定向上是好的。

![](img/830c1bf7cf86e351e64894a9edd8a7a6_41.png)

如果你只是随机选择决定向上是好的那个，那么它就会持续向上，你将实际上浮出水面以纠正错误。

![](img/830c1bf7cf86e351e64894a9edd8a7a6_43.png)

你不会在每个episode中都服务海绵，但它更有可能在你随机样本中的一个发生。

![](img/830c1bf7cf86e351e64894a9edd8a7a6_45.png)

然后，你将得到一个实际上会上升的策略。

![](img/830c1bf7cf86e351e64894a9edd8a7a6_47.png)

嗯，在论文中的实验中，他们确实显示了这一点。

![](img/830c1bf7cf86e351e64894a9edd8a7a6_49.png)

嗯，一个Bootstrap技巧实际上在 some games 上确实有所帮助。

![](img/830c1bf7cf86e351e64894a9edd8a7a6_51.png)

尽管不是其他的所有，它并不工作得很好，我想要放大以复仇，例如，一般来说，这种方法不如好的方法工作得那么好。



![](img/830c1bf7cf86e351e64894a9edd8a7a6_53.png)

基于计数的探索或伪计数。

![](img/830c1bf7cf86e351e64894a9edd8a7a6_55.png)

但它有一些重大优势，所以它不需要对原始奖励函数进行任何更改。

![](img/830c1bf7cf86e351e64894a9edd8a7a6_57.png)

实际上，在收敛时，你期待你的集合中的所有你的q函数都会很好。

![](img/830c1bf7cf86e351e64894a9edd8a7a6_59.png)

你不实际上不需要调整任何超参数来权衡探索和利用。

![](img/830c1bf7cf86e351e64894a9edd8a7a6_61.png)

所以它是非常简单和方便的，它是，它是一种非常不侵入的方式来进行探索。

![](img/830c1bf7cf86e351e64894a9edd8a7a6_63.png)

奖励通常做得很好的方法往往做得很好，然而，所以这不是最好的探索方法。

![](img/830c1bf7cf86e351e64894a9edd8a7a6_65.png)

在实际应用中，它实际上并没有被使用得很多，仅仅因为如果你真的有一个困难的探索问题。

![](img/830c1bf7cf86e351e64894a9edd8a7a6_67.png)

分配奖励通常会更好。

![](img/830c1bf7cf86e351e64894a9edd8a7a6_69.png)

但这是探索算法的一个相当广泛研究的类别。

![](img/830c1bf7cf86e351e64894a9edd8a7a6_71.png)