# P55：p55 CS 285： Lecture 13, Part 2 - 加加zero - BV1NjH4eYEyZ

好的，那么让我们谈谈一些实际的探索算法，目前我们还将处于多臂强盗设置中，我们将关注理论上的原则性策略，那些理论上可以获得良好后悔的策略，意味着好。



![](img/531c8f57ac5f870aa15913c3b06975f0_1.png)

并不像实际解决pom dp那样远，那么我们如何打败强盗。

![](img/531c8f57ac5f870aa15913c3b06975f0_3.png)

我们如何最小化这种后悔的度量，嗯。

![](img/531c8f57ac5f870aa15913c3b06975f0_5.png)

结果发现，存在许多证明能够减少后悔的相对简单的策略。

![](img/531c8f57ac5f870aa15913c3b06975f0_7.png)

从大O的角度来看，这是最优的，我们通常可以对这种后悔提供理论上的保证。

![](img/531c8f57ac5f870aa15913c3b06975f0_9.png)

现在，这些算法在最大常数因子上是最优的。

![](img/531c8f57ac5f870aa15913c3b06975f0_11.png)

所以我们要做一个大O的事情，但是，这些算法的实际性能可能会有所不同，并非所有的它们在实际使用中都能表现出相同的性能，嗯，在数值模拟中。



![](img/531c8f57ac5f870aa15913c3b06975f0_13.png)

我们将从更复杂的策略中学习到探索策略。

![](img/531c8f57ac5f870aa15913c3b06975f0_15.png)

然后，MDP领域的策略将受到这些可处理的策略的启发。

![](img/531c8f57ac5f870aa15913c3b06975f0_17.png)

好的，所以，我要讲的第一个是乐观探索，所以，在嗯，乐观探索，这就是我们要做的，通常，如果你只是嗯，试图做纯粹的利用，你可以这样做的一种方式是，你可以为每个你的行动估计出该行动获得的平均奖励。

如果你只是想利用，如果你不是，如果你不关心探索得非常好，你可以只选择当前平均奖励最大的行动，如果你不会被允许后来更新你的策略，这基本上是你能做的最好的，如果你处于纯粹的利用模式。

最佳的行动是只是继续选择看起来平均上最好的行动。

![](img/531c8f57ac5f870aa15913c3b06975f0_19.png)

你可以相反地构建一个乐观的估计，通过将该动作的奖励平均值加上一些常数c乘以的倍数，标准差，所以这会做什么，它会选择那些具有非常高平均值的动作，或者那些平均值较低的动作，但标准差非常高的动作。

这意味着对于你可能得到的奖励，你非常不确定的动作，所以sigma是一种某种方差估计，如果你这样做，你是，你基本上是乐观的，你在说，我猜想如果我没有彻底学习过的任何事情，那么它可能是好的。

所以如果你认为它可能是好的，就试试看，如果你确定它是坏的，就不要做，但如果你认为它可能是好的，那么就试试看，直觉是你尝试每个手臂，直到你确定它不是很棒的，一旦你确信一个手臂是坏的，那么你就停止尝试它。

所以啊，结果发现，有很多。

![](img/531c8f57ac5f870aa15913c3b06975f0_21.png)

非常易于估计的不确定性的方法，它们仍然工作得非常好。

![](img/531c8f57ac5f870aa15913c3b06975f0_23.png)

其中一些方法真的很简单，所以，一个非常非常简单的方法来估计这种不确定性是简单地将一些量添加到你的平均值中，这个量与你从中抽取的次数成反比。



![](img/531c8f57ac5f870aa15913c3b06975f0_25.png)

所以，这个特定的奖励是根号2乘以，自然对数的时间步长除以n的次数，其中的n是a的次数。

![](img/531c8f57ac5f870aa15913c3b06975f0_27.png)

其中a是你从rma中抽取的次数。

![](img/531c8f57ac5f870aa15913c3b06975f0_29.png)

所以，直觉上，这个奖励会随着你拉臂的频率增加而减少，但对于你很少拉臂的臂，奖励非常大，并且在分母中有对t的log。



![](img/531c8f57ac5f870aa15913c3b06975f0_31.png)

基本上以确保随着你走了更多的步，你会探索的越来越少，这种非常简单的。

![](img/531c8f57ac5f870aa15913c3b06975f0_33.png)

嗯，你添加到你平均值上的奖励最终理论上会得到log t后悔。

![](img/531c8f57ac5f870aa15913c3b06975f0_35.png)

这可能比任何算法都好，所以实际上，log t后悔的所有部分都是最好的，对于多臂强盗，你可以近似地做到这一点。



![](img/531c8f57ac5f870aa15913c3b06975f0_37.png)

这个算法得到了这种后悔，所以，这与实际上解决手掌dp的悔恨是一样的，嗯，一般来说。

![](img/531c8f57ac5f870aa15913c3b06975f0_39.png)

所以这非常棒，这表明一个非常，非常简单的策略，只是简单地拉那个只是向手臂添加奖励。

![](img/531c8f57ac5f870aa15913c3b06975f0_41.png)

你没有拉得很多，最终会得到近似最优的悔恨。

![](img/531c8f57ac5f870aa15913c3b06975f0_43.png)

并且，我们将学习到的一些深度强化学习的实际探索算法。

![](img/531c8f57ac5f870aa15913c3b06975f0_45.png)

基于这种乐观探索的直觉，这也是有时被称为乐观主义的。

![](img/531c8f57ac5f870aa15913c3b06975f0_47.png)

在面对不确定性时，好的，我们可以用于探索的另一项策略是，嗯，被称为概率匹配或后验采样，如我之前描述的乐观探索是一种非常模型无关的方法，它不试图明确模型任何不确定性，仅仅在计算你拉每条手臂的次数，M。

它是渐近最优的，但在实践中，它并不总是实际上最好的方法，在实践中，有一些经验差异，所以有一种方式你可以，嗯，作为替代进行探索的方法，实际上是做一些稍微接近那个泵dp的事情。

所以你实际上可以维持对theta的信念状态，所以你可以说，很好，你有一个状态为theta的泵dp，你将以一种非常近似的方式维护对这些thetas的信念，所以这是你的强盗模型。

这个p hat of theta是对可能强盗的分布，你认为你可能有。

![](img/531c8f57ac5f870aa15913c3b06975f0_49.png)

后验采样或概率匹配策略说，你探索的方式应该是。

![](img/531c8f57ac5f870aa15913c3b06975f0_51.png)

你从你的信念中采样一个thetas向量，然后假装那是真正的mdp并采取最佳行动，所以如果你采样了许多thetas。



![](img/531c8f57ac5f870aa15913c3b06975f0_53.png)

然后按照那个模型采取最佳的行动，那么你会发现你得到了正确的答案。

![](img/531c8f57ac5f870aa15913c3b06975f0_55.png)

意味着你会，你会发现你抽样的模型相当准确，你做了，实际上得到了你预期的更高奖励。

![](img/531c8f57ac5f870aa15913c3b06975f0_57.png)

或者你会得到一个与那个模型相反的例子，所以如果你抽样那个模型，并且它说行动一真的很好。

![](img/531c8f57ac5f870aa15913c3b06975f0_59.png)

你拉起手臂一，你会发现手臂一实际上很糟糕，现在，你的信念将会改变。

![](img/531c8f57ac5f870aa15913c3b06975f0_61.png)

下次你采样那个模型的概率会大大降低，因为它的概率会非常低，这并不是真正解决手掌dp的难度。

![](img/531c8f57ac5f870aa15913c3b06975f0_63.png)

因为这个策略不考虑你从实际拉出那个手臂时获得的信息，在某种意义上，它有点像贪婪的，但事实证明，用这种方式贪婪行事实际上效果很好。



![](img/531c8f57ac5f870aa15913c3b06975f0_65.png)

嗯，然后当然你会更新你的模式，然后重复，这就是后验采样，概率匹配，有时候它也被称为汤普森采样。

![](img/531c8f57ac5f870aa15913c3b06975f0_67.png)

所以如果有人说汤普森采样，他们实际上的意思是保持对你的模型的信念，采样模型。

![](img/531c8f57ac5f870aa15913c3b06975f0_69.png)

假装那个模型是正确的，在那个模型下采取最佳行动。

![](img/531c8f57ac5f870aa15913c3b06975f0_71.png)

然后根据你观察到的更新模型分布，现在从理论上分析这要困难得多，但它在实践中可以工作得非常好，所以，为了了解更多关于这个，查看查普莱和李的这篇论文，叫做'汤普森采样的实证评估'。



![](img/531c8f57ac5f870aa15913c3b06975f0_73.png)

总的来说，基于汤普森采样的探索方法构成了一个非常大且广泛研究的探索方法类别。

![](img/531c8f57ac5f870aa15913c3b06975f0_75.png)

无论是在浴缸中还是在深度强化学习中都是如此。

![](img/531c8f57ac5f870aa15913c3b06975f0_77.png)

我们将讨论的第三类方法，是使用某种信息增益的概念的方法，所以这些方法甚至更加明确地基于模型，这里的想法基于被称为贝叶斯实验设计的东西。



![](img/531c8f57ac5f870aa15913c3b06975f0_79.png)

所以嗯，我将以抽象的方式演示贝叶斯实验设计，然后我将将其与探索联系起来，假设我们要确定一些潜伏变量，Z，让我们不担心Z是什么，我们只是想知道它的值尽可能准确，但我们不能直接看Z。

所以Z可能可能是最佳行动或最佳行动的价值，一些未知的量。

![](img/531c8f57ac5f870aa15913c3b06975f0_81.png)

但我们可以采取行动，问题是我们应该采取什么行动来学习Z，所以嗯，我们将使用h(p hat of Z)来denote我们当前对Z估计的熵，所以这是多么不确定我们对p hat of Z的估计。

然后我们可以使用h(p hat of Z given y)来denote我们的Z估计在观察y后的熵，所以这是多么不确定我们对p hat of Z given y的估计。



![](img/531c8f57ac5f870aa15913c3b06975f0_83.png)

如果y对Z有信息性，那么Z给定y的熵将低于Z的熵，所以为什么我们实际上观察到的奖励可能是，所以，这个条件熵越低，我们越精确地知道Z，所以直觉上，我们想要做的事情是导致y的。



![](img/531c8f57ac5f870aa15913c3b06975f0_85.png)

对于哪些y，其给定Z的熵尽可能低，信息增益被量化为现在p of Z的熵与观察y后的熵之间的差异，问题在于我们不知道我们将观察哪个y，如果我们知道我们将观察哪个y。



![](img/531c8f57ac5f870aa15913c3b06975f0_87.png)

我们就已经观察到它了。

![](img/531c8f57ac5f870aa15913c3b06975f0_89.png)

我们的信念已经改变了，从y获取Z的信息增益被定义为在y分布上的预期值，的。

![](img/531c8f57ac5f870aa15913c3b06975f0_91.png)

p of Z和p of Z given y的熵差异，所以它是说你不知道y。

![](img/531c8f57ac5f870aa15913c3b06975f0_93.png)

但如果你知道你将观察的y，你就已经观察到它了，从y获取Z的信息增益被定义为在y分布上的预期值。

![](img/531c8f57ac5f870aa15913c3b06975f0_95.png)

的，p of Z和p of Z given y的熵差异，所以它是说你不知道y，但你可以有一些关于y的信念，并且你可以根据你对为什么如何你的信息熵在z上的改变进行测量，因此。

这种信息增益将允许我们量化我们想要观察y的程度，如果我们可以选择观察y，那么这将告诉我们关于z的很多吗。



![](img/531c8f57ac5f870aa15913c3b06975f0_97.png)

现在，通常，如果我们在做某种探索性的事情。

![](img/531c8f57ac5f870aa15913c3b06975f0_99.png)

我们想要这取决于行动，所以，我们将有关于z从y给出的动作a的信息增益。

![](img/531c8f57ac5f870aa15913c3b06975f0_101.png)

在这种情况下，我们将使所有这些分布条件于a。

![](img/531c8f57ac5f870aa15913c3b06975f0_103.png)

所以，这就是我们从当前信念中学习到的动作a关于z的多少，所以，你将使用条件期望，所以，描述这个想法的算法在这个由rousseau和van roy撰写的论文中被描述，叫做通过信息定向采样来优化学习。



![](img/531c8f57ac5f870aa15913c3b06975f0_105.png)

他们必须做出的选择是，嗯，你获取了关于什么信息的信息，以及你将使用的变量。

![](img/531c8f57ac5f870aa15913c3b06975f0_107.png)

你将实际观察的变量，他们说，你将观察的变量是奖励分数a。

![](img/531c8f57ac5f870aa15913c3b06975f0_109.png)

你想要学习的变量是theta a，意味着。

![](img/531c8f57ac5f870aa15913c3b06975f0_111.png)

对于动作a的模型参数，当你观察到奖励时。

![](img/531c8f57ac5f870aa15913c3b06975f0_113.png)

你实际上不知道来自这个奖励的分布，所以你想要学习的是关于这个奖励对于这个动作的参数。

![](img/531c8f57ac5f870aa15913c3b06975f0_115.png)

你所观察到的是来自这个分布的样本。

![](img/531c8f57ac5f870aa15913c3b06975f0_117.png)

他们定义信息增益，嗯，关于从观察r a中获得theta i的信息，给定动作a。

![](img/531c8f57ac5f870aa15913c3b06975f0_119.png)

所以，这是某个动作a的信息增益。

![](img/531c8f57ac5f870aa15913c3b06975f0_121.png)

他们定义这个被称为delta a的量，这是某个动作a的预期子最优性。

![](img/531c8f57ac5f870aa15913c3b06975f0_123.png)

这是说在你的当前关于mdp的信念下。

![](img/531c8f57ac5f870aa15913c3b06975f0_125.png)

你认为模型可能可能是的最优行动与您当前考虑的行动的差异是多少。

![](img/531c8f57ac5f870aa15913c3b06975f0_127.png)

所以，这是delta a现在关键，你不知道a星。

![](img/531c8f57ac5f870aa15913c3b06975f0_129.png)

所以，delta a是在模型分布上的期望，所以，ga是说这个行动的信息量是多少。

![](img/531c8f57ac5f870aa15913c3b06975f0_131.png)

delta是说你认为这个行动可能多差。

![](img/531c8f57ac5f870aa15913c3b06975f0_133.png)

直觉是，你想要采取信息量大的行动，但你不想采取子最优的行动。

![](img/531c8f57ac5f870aa15913c3b06975f0_135.png)

所以，这个他们在这篇论文中分析的特定决策规则。

![](img/531c8f57ac5f870aa15913c3b06975f0_137.png)

并显示出相当好，是delta a的平方除以ga的a。

![](img/531c8f57ac5f870aa15913c3b06975f0_139.png)

然后您取这个的最小值，所以，直觉上你想要选择最不差的行动，但你除以信息增益，所以，'如果'信息增益非常。



![](img/531c8f57ac5f870aa15913c3b06975f0_141.png)

非常大，因为因为你在除以。

![](img/531c8f57ac5f870aa15913c3b06975f0_143.png)

非常large，那么，因为因为你在除以，你将有一个小的值，这意味着这可能是最小值。

![](img/531c8f57ac5f870aa15913c3b06975f0_145.png)

即使其次优性很大，所以不要在意采取行动，如果你知道你不会学到任何东西，所以如果a的g非常小，那么这个值会爆炸。



![](img/531c8f57ac5f870aa15913c3b06975f0_147.png)

但不要采取行动，如果你确定它们是次优的，因为如果delta a极其小。

![](img/531c8f57ac5f870aa15913c3b06975f0_149.png)

那么你也不会采取行动。

![](img/531c8f57ac5f870aa15913c3b06975f0_151.png)

好的，所以嗯在，如果你想了解更多关于策略的信息，查看russo和van roy的论文。

![](img/531c8f57ac5f870aa15913c3b06975f0_153.png)

叫做通过信息导向采样学习优化，但他们短版本显示的策略也非常非常好。

![](img/531c8f57ac5f870aa15913c3b06975f0_155.png)

尽管它稍微更数学复杂。

![](img/531c8f57ac5f870aa15913c3b06975f0_157.png)

好的，所以我们从学习中学到的一般主题是关于上界置信区间或乐观探索，这是你对一个动作的平均预期奖励，并添加到一个奖励的奖金，它按你取该动作的次数比例缩放，这意味着行动没有被取得很多次的。

将获得非常大的奖金，然后你被强烈激励去取它们更多，我们学习了thompson采样，你在那里维护对theta的信念，你从那个信念中采样，然后根据那个样本采取最佳行动，我们学习了信息增益，你学习。

你试图估计你根据某些观察y对某些感兴趣的量z的信息增益，给定某些行动a，你可能想要，例如，使用奖励作为你的观察来获取关于模型的信息，现在，大多数探索策略。



![](img/531c8f57ac5f870aa15913c3b06975f0_159.png)

嗯。

![](img/531c8f57ac5f870aa15913c3b06975f0_161.png)

都需要某种程度的不确定性估计，正如我们所见，所以每个这些三个都需要估计不确定性，即使你做得相当粗略，如在ucb的案例中。



![](img/531c8f57ac5f870aa15913c3b06975f0_163.png)

你的不确定性估计仅仅是你取该行动的次数，通常你假设新信息的某种价值。

![](img/531c8f57ac5f870aa15913c3b06975f0_165.png)

因为这是至关重要的，因为如果你不知道奖励在哪里，最好的事情你可能能做的就是说学习东西是好的。

![](img/531c8f57ac5f870aa15913c3b06975f0_167.png)

因为你不能只是说探索以最大化奖励，因为整个点就是你不知道奖励是什么，所以，所以你不能说探索以最大化奖励，因为整个点就是你不知道奖励是什么，通常，你需要假设一些值来获取新信息。



![](img/531c8f57ac5f870aa15913c3b06975f0_169.png)

所以，在乐观的情况下，你假设未知事物在集合中是好的。

![](img/531c8f57ac5f870aa15913c3b06975f0_171.png)

在汤普森采样的情况下，你假设你的样本在某种程度上是基础，在信息增益的情况下。

![](img/531c8f57ac5f870aa15913c3b06975f0_173.png)

你假设信息增益是可取的，现在，这些假设可能看起来有些任意。

![](img/531c8f57ac5f870aa15913c3b06975f0_175.png)

但我们之所以舒适地做出那些假设，是因为，在这些理论上可处理的班迪特设置中。

![](img/531c8f57ac5f870aa15913c3b06975f0_177.png)

我们可以证明产生的算法是证明最优的。

![](img/531c8f57ac5f870aa15913c3b06975f0_179.png)

我们不能在更复杂的领域证明同样的事情，我会在下一个讨论，但我们将从这些中获得一些直觉来引导我们，更原则的算法。



![](img/531c8f57ac5f870aa15913c3b06975f0_181.png)

好的，我们为什么应该关心这些多臂班迪特设置。

![](img/531c8f57ac5f870aa15913c3b06975f0_183.png)

因为班迪特更容易分析和理解，并且你可以用它们来推导出更实用的探索算法。

![](img/531c8f57ac5f870aa15913c3b06975f0_185.png)

然后，你可以将这些方法应用于更复杂的mdps。

![](img/531c8f57ac5f870aa15913c3b06975f0_187.png)

在那里，这些保证不适用，现在，这里有许多其他与探索相关的主题我们没有在这里覆盖。

![](img/531c8f57ac5f870aa15913c3b06975f0_189.png)

我只会提到它们以完整性，我们没有真正谈论上下文班迪特，所以，这些都是有状态的班迪特，本质上是一个一步mdp，我们没有谈论在小mdps中的优化探索，所以我没有深入研究理论。



![](img/531c8f57ac5f870aa15913c3b06975f0_191.png)

这有很多理论，我们没有真正谈论基于贝叶斯模型的强化学习。

![](img/531c8f57ac5f870aa15913c3b06975f0_193.png)

这是从信息增益的自然进展。

![](img/531c8f57ac5f870aa15913c3b06975f0_195.png)

所以你可以完全基于贝叶斯来做出优化探索的决策。

![](img/531c8f57ac5f870aa15913c3b06975f0_197.png)

这将更接近那个pdp设置，我没有真正谈论那个。

![](img/531c8f57ac5f870aa15913c3b06975f0_199.png)

我们还没有谈论，嗯，嗯，基于包的探索，你可以使用包理论来开发具有某些非常吸引人保证的探索方法。

![](img/531c8f57ac5f870aa15913c3b06975f0_201.png)

这深入到理论中去了，但知道这存在，如果你感兴趣。

![](img/531c8f57ac5f870aa15913c3b06975f0_203.png)