# 【深度强化学习 CS285 2023】伯克利—中英字幕 - P24：p24 CS 285： Lecture 6, Part 4 - 加加zero - BV1NjH4eYEyZ

今天的讲座下一部分一切顺利，接下来我要讨论另一种方法，我们可以使用批评家的方法，通过将批评家作为政策的基线来集成政策梯度，这将有一些有趣的权衡，与迄今为止我们讨论的标准演员批评算法相比。



![](img/40b2701213d42f21b0cf546fe0996825_1.png)

在这张幻灯片上，我有我们今天讲座讨论的演员批评的方程式。

![](img/40b2701213d42f21b0cf546fe0996825_3.png)

以及我们在上一堂课看到的政策梯度的方程式。

![](img/40b2701213d42f21b0cf546fe0996825_5.png)

演员批评包括grad log pi项，乘以奖励加上gamma乘以，下一个值减去当前值，策略梯度由grad log pi项乘以。



![](img/40b2701213d42f21b0cf546fe0996825_7.png)

未来奖励之和减去基线，所以未来奖励之和是一个无偏的单个样本，在s i t a i t处的q值估计，演员批评策略梯度估计器。



![](img/40b2701213d42f21b0cf546fe0996825_9.png)

它有优势，大大降低了方差，因为批评者的功能近似器集成了所有可能性。

![](img/40b2701213d42f21b0cf546fe0996825_11.png)

可能发生的事情。

![](img/40b2701213d42f21b0cf546fe0996825_13.png)

而不是依赖于单个样本，不幸的是，演员，批评家，梯度估计器，也有其缺点，即它不再无偏，其原因是如果你的价值函数稍有错误。



![](img/40b2701213d42f21b0cf546fe0996825_15.png)

这可能是，因为它是一个基于有限样本训练的功能逼近器。

![](img/40b2701213d42f21b0cf546fe0996825_17.png)

那么你就不能，嗯，嗯，展示，超过预期吗。

![](img/40b2701213d42f21b0cf546fe0996825_19.png)

这个梯度实际上会收敛到真实策略梯度，原始策略梯度是无偏的。

![](img/40b2701213d42f21b0cf546fe0996825_21.png)

所以即使它可能有高方差，即使每个单独的评估可能在预期上偏离，它会得出正确的值。

![](img/40b2701213d42f21b0cf546fe0996825_23.png)

但它有更高的方差，这意味着你通常需要使用更多的样本或较小的学习率，所以，我们可以问一个问题是。

![](img/40b2701213d42f21b0cf546fe0996825_25.png)

我们能否得到一个仍然无偏的政策梯度估计器，但使用批评者以另一种方式降低方差。

![](img/40b2701213d42f21b0cf546fe0996825_27.png)

答案是我们可以，我们可以实际上构建一个政策梯度估计器。

![](img/40b2701213d42f21b0cf546fe0996825_29.png)

它的方差略高于演员批评者版本，但没有像政策梯度版本那样的偏差。

![](img/40b2701213d42f21b0cf546fe0996825_31.png)

我们如何能做到这一点，嗯，是通过使用被称为状态依赖基线的东西。

![](img/40b2701213d42f21b0cf546fe0996825_33.png)

因此，我们发现通过扩展前一堂课的证明，我们可以证明。

![](img/40b2701213d42f21b0cf546fe0996825_35.png)

政策梯度不仅当您减去任何常数时保持无偏。

![](img/40b2701213d42f21b0cf546fe0996825_37.png)

而且实际上仍然保持无偏，如果您减去仅依赖于状态而不依赖于动作的任何函数。

![](img/40b2701213d42f21b0cf546fe0996825_39.png)

实际上构建这个证明非常容易，它几乎完全遵循我们在前一堂课中的证明。

![](img/40b2701213d42f21b0cf546fe0996825_41.png)

我鼓励那些感兴趣的人去回顾并重新推导。

![](img/40b2701213d42f21b0cf546fe0996825_43.png)

以证明这一点，这是非常直接的事情可以做，但总的来说，您可以使用任何依赖于状态的基线，一个非常好的选择是价值函数，因为你会期望这个单一样本估计器。



![](img/40b2701213d42f21b0cf546fe0996825_45.png)

在期望上应该等于价值函数，所以如果你使用价值函数作为基准，然后乘以log pi的梯度的数字在期望上应该更小。



![](img/40b2701213d42f21b0cf546fe0996825_47.png)

这意味着他们的方差更小，这意味着你整个策略梯度的方差更小。

![](img/40b2701213d42f21b0cf546fe0996825_49.png)

所以实际上使用价值函数作为基准是一个相当好的想法，但这不像完整的演员批评算法那样降低方差。

![](img/40b2701213d42f21b0cf546fe0996825_51.png)

因为你仍然有未来奖励的总和，但是，它比常数基线低得多，而且，它仍然无偏。

![](img/40b2701213d42f21b0cf546fe0996825_53.png)

现在，一些人可能会在这个点上感到疑惑，嗯，好的，我们使用价值函数作为基线。

![](img/40b2701213d42f21b0cf546fe0996825_55.png)

我们使它依赖于更多的东西，并且我们得到了更低的方差，如果我们使它依赖于更多的东西，如果我们使基线依赖于状态和动作，那样我们的方差会更低。



![](img/40b2701213d42f21b0cf546fe0996825_57.png)

答案是肯定的，但到那时事情会变得更加复杂。

![](img/40b2701213d42f21b0cf546fe0996825_59.png)

所以我们会讨论这个，在文献中，有时被称为控制变体的方法，它们是使用状态和动作依赖的基础线的方法。

![](img/40b2701213d42f21b0cf546fe0996825_61.png)

真正的优势是q值减去价值函数，我们在政策梯度中使用的近似优势。

![](img/40b2701213d42f21b0cf546fe0996825_63.png)

当我们有一个状态依赖的基础线时，它是所有未来奖励的总和减去当前价值，这是因为它有更低的方差，但如果我们减去q值，我们可以使方差更低。



![](img/40b2701213d42f21b0cf546fe0996825_65.png)

所以这版本没有偏见，但它的方差比演员-批评家版本高。

![](img/40b2701213d42f21b0cf546fe0996825_67.png)

这是因为单一样本估计，如果我们减去q值。

![](img/40b2701213d42f21b0cf546fe0996825_69.png)

这具有很好的性质，实际上它会趋近于零，如果你的批评者是正确的，所以如果你的批评者是正确的，如果你的未来行为不是太随机，那么你应该期待这些量最终只会收敛到零，不幸的是。

如果你将这个优势估计器插入到你的政策梯度中，策略梯度不再正确。

![](img/40b2701213d42f21b0cf546fe0996825_71.png)

它实际上不会给你正确的梯度，因为存在一个误差项，你需要补偿它，看，与标准基线不同，它在期望中集成到零。



![](img/40b2701213d42f21b0cf546fe0996825_73.png)

依赖于动作的基线不再集成到零，它集成到一个误差项，你需要考虑这个误差项。

![](img/40b2701213d42f21b0cf546fe0996825_75.png)

所以如果你包含一个依赖于状态和动作的基线。

![](img/40b2701213d42f21b0cf546fe0996825_77.png)

并考虑误差项，那么你就得到这个方程，这是一次有效的策略梯度估计器。

![](img/40b2701213d42f21b0cf546fe0996825_79.png)

即使你的基础线不依赖于动作，但在这种情况下，第二个术语基本上消失了，因为第二个术语等于零，但如果你的基础线依赖于动作。



![](img/40b2701213d42f21b0cf546fe0996825_81.png)

第二个术语不再为零，所以第一个术语只是你的策略评分与你的基础线，第二个术语是剩余的，它是预期的值，嗯，这是你基准策略下预期值的梯度，现在有些人可能会看到这一点并思考，嗯，我们真的买了自己像是的东西吗。

第一个术语将会很小因为它将趋近于零，但是第二个术语看起来像原始的政策梯度，所以这真的比较好吗，结果在某些情况下，这实际上要更好得多，例如，在许多情况下。



![](img/40b2701213d42f21b0cf546fe0996825_83.png)

第二个术语实际上可以非常准确地被评估，非常准确地，如果你有离散动作。

![](img/40b2701213d42f21b0cf546fe0996825_85.png)

你可以对所有可能的动作进行求和，如果你有连续动作。

![](img/40b2701213d42f21b0cf546fe0996825_87.png)

那么你可以采样一个非常大数量的动作，因为对动作的期望评估不需要采样新的州。

![](img/40b2701213d42f21b0cf546fe0996825_89.png)

所以它不需要实际上与世界交互。

![](img/40b2701213d42f21b0cf546fe0996825_91.png)

这意味着你可以从同一状态生成更多的样本，这是我们以前不能做到的。

![](img/40b2701213d42f21b0cf546fe0996825_93.png)

当我们必须实际进行整个部署时，而且，在许多连续行动情况下，如果你必须选择一个分布的类别，和q函数的类别，这个积分也有一个解析解。



![](img/40b2701213d42f21b0cf546fe0996825_95.png)

例如，在正态分布下二次函数的期望值。

![](img/40b2701213d42f21b0cf546fe0996825_97.png)

有一个解析解，所以在许多情况下，第二个术语可以以这种方式被评估，使其方差为零。

![](img/40b2701213d42f21b0cf546fe0996825_99.png)

或者非常接近零，并且第一个术语的方差较低。

![](img/40b2701213d42f21b0cf546fe0996825_101.png)

因为q hat减去q pi通常将是一个小数字，所以嗯，这种技巧可以用于提供一个非常低的方差政策梯度。

![](img/40b2701213d42f21b0cf546fe0996825_103.png)

尤其是如果你可以得到一个良好的q函数估计器，如果你想了解更多关于这种控制变体的信息。

![](img/40b2701213d42f21b0cf546fe0996825_105.png)

以及如何在不引入偏差的情况下使用批评者。

![](img/40b2701213d42f21b0cf546fe0996825_107.png)

提供，第二个术语可以评估，然后看看uh写的这篇文章，Shishanu叫q pro。

![](img/40b2701213d42f21b0cf546fe0996825_109.png)

好的，所以到目前为止，我们讨论了我们如何使用批评家和获取政策评分估计器，它们是无偏的，但我们也可以使用批评家和获取仍然偏斜的政策梯度吗，但只有一点点偏斜。



![](img/40b2701213d42f21b0cf546fe0996825_111.png)

所以接下来我们将谈论eligibility traces和n步回报，让我们嗯，看一下演员批评算法中的优点估计器，我将这个表示为帽子c，所以帽子c是当前奖励加上下一个值减去当前值，正如我们所见。

这比政策梯度有更低的方差，但更高的偏置，如果值错误，并且总是至少有一点点错误。

![](img/40b2701213d42f21b0cf546fe0996825_113.png)

我们也可以看看蒙特卡洛优点估计器，我们在政策梯度算法中使用它。

![](img/40b2701213d42f21b0cf546fe0996825_115.png)

我已经在这里写出来，以价值函数作为基线，以保持一致。

![](img/40b2701213d42f21b0cf546fe0996825_117.png)

这样两者都有一个减去v的项，所以这里我们有未来奖励的总和。

![](img/40b2701213d42f21b0cf546fe0996825_119.png)

减去我们的基线，在这个情况下是价值函数，这没有偏差，但它的方差因为单一样本估计而大大增加。

![](img/40b2701213d42f21b0cf546fe0996825_121.png)

所以到这个时候你可能会想，我们能否得到中间值。

![](img/40b2701213d42f21b0cf546fe0996825_123.png)

我们能否，例如，而不是无限时间步，计算未来五个时间步的奖励总和。

![](img/40b2701213d42f21b0cf546fe0996825_125.png)

然后将其值放在那个的末尾，结果我们发现我们可以。

![](img/40b2701213d42f21b0cf546fe0996825_127.png)

我们可以用这个来获得一个非常漂亮的方法来控制偏置方差，升级，有几个因素使这很有趣，首先，当你在使用折扣时。



![](img/40b2701213d42f21b0cf546fe0996825_129.png)

通常你的回报会随着时间的推移而减少，因为因为你在折扣它，所以这意味着你从价值函数中得到的偏差问题要小得多。



![](img/40b2701213d42f21b0cf546fe0996825_131.png)

如果你将价值函数放在下一步，但更远的未来，在哪里奖励更小，另一方面。

![](img/40b2701213d42f21b0cf546fe0996825_133.png)

你从单一样本估计器得到的方差问题也要大得多。

![](img/40b2701213d42f21b0cf546fe0996825_135.png)

更远的未来，为了理解这可能为什么，考虑进行单一样本预测。

![](img/40b2701213d42f21b0cf546fe0996825_137.png)

让我们假设你问我做一个单一样本预测。

![](img/40b2701213d42f21b0cf546fe0996825_139.png)

你希望在五分钟后的城市是哪个，你将在那里，嗯，如果你问我这个问题，我的单一样本将是伯克利，因为我将在五分钟后到达伯克利。



![](img/40b2701213d42f21b0cf546fe0996825_141.png)

但如果你问我，我在二十年后将在哪个城市。

![](img/40b2701213d42f21b0cf546fe0996825_143.png)

我可能会说好吧，你想要单一样本，让我们说旧金山，但我真的不知道它可能是任何地方。

![](img/40b2701213d42f21b0cf546fe0996825_145.png)

所以，未来很远的可能性比现在近得多。

![](img/40b2701213d42f21b0cf546fe0996825_147.png)

所以，如果你从当前状态发出这么多可能的轨迹，它们将在未来分支出更多，并且在现在会更紧密地聚集在一起。

![](img/40b2701213d42f21b0cf546fe0996825_149.png)

这意味着你的未来方差将很高，而接近现在的方差将很低。

![](img/40b2701213d42f21b0cf546fe0996825_151.png)

所以这意味着，如果你要用你的单一样本估计器来处理这段轨迹的一部分。

![](img/40b2701213d42f21b0cf546fe0996825_153.png)

你更喜欢在现在使用它，然后在方差变得太大之前切断它，然后使用你的价值函数来替换它。

![](img/40b2701213d42f21b0cf546fe0996825_155.png)

哪个方差更低，价值函数不会考虑所有这些不同的可能性，但可能带有一些偏差。

![](img/40b2701213d42f21b0cf546fe0996825_157.png)

所以你可以做到这一点，通过构建被称为n步回报估计器的东西。

![](img/40b2701213d42f21b0cf546fe0996825_159.png)

在n步回报估计器中，你直到时间步n为止地累积奖励。

![](img/40b2701213d42f21b0cf546fe0996825_161.png)

然后切断并使用价值函数来替换它。

![](img/40b2701213d42f21b0cf546fe0996825_163.png)

这里是n步回报估计器的方程，第一部分看起来像amc。

![](img/40b2701213d42f21b0cf546fe0996825_165.png)

所以你累积你的奖励，但现在你不累积到无穷大。

![](img/40b2701213d42f21b0cf546fe0996825_167.png)

你从t累积到t加上n，你仍然减去基础线。

![](img/40b2701213d42f21b0cf546fe0996825_169.png)

但然后你有那个剩余部分，从n加上一直到无穷大。

![](img/40b2701213d42f21b0cf546fe0996825_171.png)

你使用你的价值函数来替换它，所以你评估你的回报函数在st加上n。

![](img/40b2701213d42f21b0cf546fe0996825_173.png)

并且你将其乘以伽马到n的幂次方，这就是一个n步返回估计器，并且常常选择n大于一比n等于一更好。

![](img/40b2701213d42f21b0cf546fe0996825_175.png)

所以n等于一是这个的极限情况，一般来说，n越大。

![](img/40b2701213d42f21b0cf546fe0996825_177.png)

偏差越小，因为值函数前面的系数变得越来越小。

![](img/40b2701213d42f21b0cf546fe0996825_179.png)

但方差更高，因为你正在使用单个样本估计器来求和更多的时间步。

![](img/40b2701213d42f21b0cf546fe0996825_181.png)

这是第一项，所以第一项贡献方差，最后一项贡献偏差，并且n越大，第三项越小。

![](img/40b2701213d42f21b0cf546fe0996825_183.png)

第一项越大，这意味着n越大，偏差越小。

![](img/40b2701213d42f21b0cf546fe0996825_185.png)

方差越高，但往往甜点不在n等于一也不在n等于无穷大。

![](img/40b2701213d42f21b0cf546fe0996825_187.png)

你可能想要使用中间值。

![](img/40b2701213d42f21b0cf546fe0996825_189.png)

现在，我要讨论的最后一个技巧是，一种实际将n步返回估计器泛化的方法。

![](img/40b2701213d42f21b0cf546fe0996825_191.png)

嗯，并且实际上构建一种混合估计器，该估计器使用许多不同n步返回，我们是否必须选择一个端点，我们是否必须在某个时间点的特定点进行硬切。



![](img/40b2701213d42f21b0cf546fe0996825_193.png)

如果我们想要实际上构建所有可能的n步返回估计器并平均它们。

![](img/40b2701213d42f21b0cf546fe0996825_195.png)

这里是n步返回估计器的方程，我们可以构建一种融合估计器。

![](img/40b2701213d42f21b0cf546fe0996825_197.png)

我们将其称为gae，通用优势估计，它由所有可能n步返回估计器的加权平均组成，与不同n的权重。

![](img/40b2701213d42f21b0cf546fe0996825_199.png)

你选择权重的方法是利用你的理解，如果你使用小n，你将有更多的偏差，如果你使用大n，所以你更倾向于在早期切割，因为这样你可以减少方差。



![](img/40b2701213d42f21b0cf546fe0996825_201.png)

![](img/40b2701213d42f21b0cf546fe0996825_202.png)

但你想要保留一些未来轨迹，所以一个相当好的选择，导致特别简单的算法，是使用指数衰减。

![](img/40b2701213d42f21b0cf546fe0996825_204.png)

是将权重w n设置为lambda的n减一的幂次方，如果你认为这有点像折扣因子，因为它确实如此，如果你使用指数衰减。



![](img/40b2701213d42f21b0cf546fe0996825_206.png)

这意味着w n是lambda的n减一，那么实际上，有一个非常方便的方法来评估这个无限和，如果使用指数衰减。



![](img/40b2701213d42f21b0cf546fe0996825_208.png)

这意味着w n是lambda的n减一，那么实际上，有一个非常方便的方法来评估这个无限和，在所有可能的结局中返回，如果w n是lambda到n减一的函数，那么你可以写出aae作为曲线奖励。



![](img/40b2701213d42f21b0cf546fe0996825_210.png)

加上gamma乘以一减去lambda，下一个值，加上lambda的下一个奖励和它的下一个值。

![](img/40b2701213d42f21b0cf546fe0996825_212.png)

就这样，所以每次时间步你都有一个减去lambda的值函数时间步，加上从那以后lambda的ge估计值。



![](img/40b2701213d42f21b0cf546fe0996825_214.png)

这就给你所有可能的n步回报估计值的加权和。

![](img/40b2701213d42f21b0cf546fe0996825_216.png)

这给你一个递归计算的公式。

![](img/40b2701213d42f21b0cf546fe0996825_218.png)

但事实证明，我们可以收集术语并得到甚至更简单的形式，通过将gae估计值表达为简单地gamma乘以lambda的t'次方。



![](img/40b2701213d42f21b0cf546fe0996825_220.png)

减去t次，我将要称为delta t'的量，delta t'只是t'处的奖励，加上t'处的价值，加上一减去t'处的价值，它有点像t'步的单步优势估计器。



![](img/40b2701213d42f21b0cf546fe0996825_222.png)

所以这真的很酷，如果你只是构建每个时间步的这些一步优势估计器的加权和。

![](img/40b2701213d42f21b0cf546fe0996825_224.png)

加权为gamma乘以lambda的t'次方，减去t，你将精确地恢复所有可能的n步回报的总和。

![](img/40b2701213d42f21b0cf546fe0996825_226.png)

这将允许你通过选择这个lambda来交易偏差方差。

![](img/40b2701213d42f21b0cf546fe0996825_228.png)

它作用得非常相似于折扣，所以更大的lambda看更远的未来。

![](img/40b2701213d42f21b0cf546fe0996825_230.png)

较小的lambda使用更接近现在的价值函数。

![](img/40b2701213d42f21b0cf546fe0996825_232.png)

所以它有类似的效果作为折扣，这也可能揭示折扣在政策梯度中的角色。

![](img/40b2701213d42f21b0cf546fe0996825_234.png)

所以这表明即使我们没有lambda。

![](img/40b2701213d42f21b0cf546fe0996825_236.png)

gamma的角色将是一种偏差-方差交易，实际上就是这样。

![](img/40b2701213d42f21b0cf546fe0996825_238.png)

所以折扣也可以被解释为自身的一种方差减少。

![](img/40b2701213d42f21b0cf546fe0996825_240.png)

因为较小的折扣会导致你的马尔可夫单样本估计器有。

![](img/40b2701213d42f21b0cf546fe0996825_242.png)

嗯，你知道，对远在未来的奖励给予较低的权重。

![](img/40b2701213d42f21b0cf546fe0996825_244.png)

这正是你期望具有高方差的奖励，仅使用折扣。

![](img/40b2701213d42f21b0cf546fe0996825_246.png)

当然，使用小折扣会引入更多的偏差，与使用正确的更高折扣值相比，使用lambda可以减轻这种偏差。

![](img/40b2701213d42f21b0cf546fe0996825_248.png)

![](img/40b2701213d42f21b0cf546fe0996825_249.png)