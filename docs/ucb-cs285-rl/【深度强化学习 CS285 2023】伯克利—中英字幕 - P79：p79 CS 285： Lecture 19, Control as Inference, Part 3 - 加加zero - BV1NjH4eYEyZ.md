# 【深度强化学习 CS285 2023】伯克利—中英字幕 - P79：p79 CS 285： Lecture 19, Control as Inference, Part 3 - 加加zero - BV1NjH4eYEyZ

好的，到目前为止，我们已经看到了如何将控制框架视为特定图形模型中的推断，然后，我们讨论了如何在那个图形模型中做精确推断，并理解了三种可能的推断问题，计算反向消息，计算使用这些反向消息的策略。

并计算向前消息，正如我所提到的，这将在后续的讨论中有用，当我们谈论逆向强化学习时，现在，我们所讨论的所有推断程序都是精确推断，但是，当然，在复杂的，高维或连续状态空间或设置中，动态未知，其中。

过渡概率对我们不可用，而且，我们只能通过进行模拟来采样它们，我们需要做近似推断，这就是我在下一节要谈论的，实际上，我会使用我们从上周学到的工具，变分推断的工具展示了如何显示模型。



![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_1.png)

现在，在设计这些近似算法的过程中，我们还将看到如何，我们可以为以前提出的特定问题设计一个解决方案，那就是我之前提到的乐观问题，如果你记得之前的讲座部分，我们讨论了如何状态向后消息和状态动作向后消息。

他们的对数可以被解释为非常类似于价值函数和q函数，当我们将这些方程写在对数空间时，我们得到了一个与价值迭代非常相似的算法，"除了动作的最大值被替换为软最大值之外"，"并且。

备用铃人的系统有一个期望值为指数形式的日志"，"现在，softmax并不是一个真正的问题"，"实际上，这就是我们获得软最优性这个概念的地方。"，所以我们实际上想要那个，"但这种备份方式有点问题"。

"这个问题的麻烦在于，预期指数值的日志文件"，"下一个状态的值将由最幸运的状态主导"，"看到这一点的最容易方法是想象"，这一行动相当于购买彩票，所以您有一千分之一的机会获得极其巨大的回报。

并且有九百九十九点九九九九的机会获得 nothing，现在，这个的影响将是，你知道，零乘以九九九九点九九九九和，嗯，一 million 乘以零，零一，所以这意味着它只是 one million 乘以零。

零一，当你取那个嗯，的指数，然后对数，零点，它们的影响基本上会消失，并且最终的值会被那个积极的结果主导，而且这真的是坏消息因为，当然，购买水票并不是个好主意，而且其预期值不高，但是其对数预期，嗯。

指数值高，所以本质上这种备份结果导致了一种乐观，偏差。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_3.png)

现在为什么会发生这种情况呢，我们正在解决的问题是推断出最可能的轨迹。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_5.png)

给定最优性，然后边缘化和条件化这个，我们得到给定状态s、时间t和观察o的t的政策p。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_7.png)

嗯，这个推断问题直观上所问的是。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_9.png)

你获得了高奖励，你的行动概率是多少，现在回想彩票的例子，如果你知道你已经有了一百万美元。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_11.png)

这使你更可能玩彩票，但这并不意味着玩彩票是个好主意，所以本质上这里的紧张在于我们正在问的推断问题并不完全是。



![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_13.png)

我们真正想要得到的答案是什么。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_15.png)

我们想知道的是如果你试图做到最佳，你会做什么。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_17.png)

如果试图最优化，不是我认为你做了什么。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_19.png)

假设你获得了一百万，这个问题的真正根源是，给定s t，后验概率s t的加一并不等于其先验概率。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_21.png)

加上给定s t，a t和o one通过capital t的加一，并不等于其先验概率。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_23.png)

所以当我们进行这个推断过程时，我们实际上是改变动态以符合我们的证据，这里的直觉再次从彩票例子中跟随得非常好，如果你知道你获得了一百万块钱，并且你买了彩票。



![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_25.png)

你赢得彩票的可能性更高。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_27.png)

因为获得百万块钱的证据增加了你实际上想要水的信念。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_29.png)

当然，在现实中，动态不允许改变，在现实中，我们想要找出在原始实际动态中大约最优的事情是什么。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_31.png)

所以，这个问题是基于你获得高奖励的。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_33.png)

你的状态转移概率是什么，但从某种意义上说，我们对这个问题并不在意。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_35.png)

你的状态转移概率应该保持固定，所以让我们思考一下我们如何解决这个乐观主义问题，所以我们想要的是我们想要政策，但我们不想我们的政策推断过程，允许我们改变动态。



![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_37.png)

所以直觉上我们想要，假设你获得高奖励。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_39.png)

在你状态转移概率没有改变的情况下，你的行动概率是多少。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_41.png)

所以我们可以这样接近这个问题，我们可以说，我们能找到另一个分布吗，嗯，Q，在接近后验的分布上，跨越状态和动作，在给定1到t的状态和动作上，但具有相同的原始动态。



![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_43.png)

所以，在这个近似后验q中，我们想要动态与原始相同，嗯，就像它们最初不受到你对奖励的了解的影响，但是我们想要行动概率发生变化。



![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_45.png)

那么我们在哪里见过这种情况，我们在哪里见过近似一个概率分布用另一个有一些特性的分布来近似的概念。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_47.png)

所以如果我们暂时说x是从大t到t的1到n。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_49.png)

并且z是从大t到t的1到n，a是从大t到t的1到n，那么这个问题相当于说找到z的q来近似z的p，给定x。



![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_51.png)

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_52.png)

基本上找到一种近似分布，那准确地近似未观察到的变量的后验分布，这就是变分推断解决的问题。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_54.png)

所以能否将这个问题塞进去，我们能找到另一个分布吗，从s到t的q是多少，从s到t的a是多少，那就是接近后验p的分布。



![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_56.png)

但它有给定s、d、a和t的s_t+1动态p的分布，我们能将这个框架塞进变分推断中吗。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_58.png)

花几分钟思考这个问题，思考一下如何使用变分推断来解决这个问题。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_60.png)

也许暂停视频并思考一下，然后检查你的回答与我在下一个幻灯片上要告诉你的内容是否一致。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_62.png)

好的，为了使用变分推断进行控制，我们将做以下事情，我们将定义一个对于q有些特殊的分布类，我们将定义q从s1到t，以及a从1到t，作为p(s1)的乘积，过渡概率的乘积，在每一步时间步长，给定s_t。

a_t，p(s_t+1|s_t，a_t)的乘积，以及给定s_t，q(a_t|s_t)的分布，这个分布，变分分布的这个定义相当特殊，因为通常当我们使用变分推断时，我们学习整个变分分布，但是这里。

我们实际上在固定变分分布的一些部分使它们与p相同，并只学习动作条件，所以我们将有相同的动态和相同的初始状态p，这将对我们对抗这种乐观主义非常重要，偏差，所以对于学习这个近似的后验，我们只学习了一件事。

嗯，后验q是s给定t的条件分布。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_64.png)

我们可以以下方式图形化表示，我们正在尝试推断的真实图形模型在这里显示，所以我们有观察到的变量o一至大t，和无观察到的变量，s的和a的，所以我们有初始状态，过渡概率和最优性变量的概率。

近似对应于这个图形模型，记住在变分推断中，a的变分分布不包含观察到的变量。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_66.png)

所以移除o的是有意义的，只有s的和a的剩余，我们有相同的初始状态分布，相同的过渡概率，我们不再有o的，但我们有q的a给定s t，顺便说一句，这就是我们要学习的唯一部分。



![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_68.png)

我应该提到所有这些推导都是为情况呈现的，其中s一是未观察到的，通常你可能实际上知道s一，在这种情况下，p的s一消失，s一节点在所有地方都会被阴影，它实际上不会成为你变分分布的一部分，这是非常直接的。

它只是添加了一些混乱的符号，这就是为什么我在这些幻灯片上省略了它，并将s一视为潜在变量，但是请记住，如果你处于一个知道当前状态的情况，并只是想找出未来的状态和动作，那么s一将被观察。

但这很容易扩展到这个设置，我鼓励你在自己的时间去做这个练习，好的，现在让我们再次将这与上周的变分推断讨论联系起来，我们将说x是观察到的，变量是o一至大t，Z是潜在变量，对应于s一至大t和a一至大t。

所以第一个图形模型是p的z给定x，第二个一个是q的z，然后我们将我们的变分下界以这些东西写成，然后我们将优化这个变分下界，我们将看到实际上这与我们已经学到的很多rl算法非常接近，我们之前已经学过的。



![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_70.png)

好的，所以这里是我们在上周讲座中看到的变分下界。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_72.png)

x的日志概率大于或等于在q下z的期望值，的日志p(x)，逗号z和减去q下z的日志，而且这实际上对于任何z的q都是真实的，但是当然，正如我们上周学到的，"给定x，z的后验p与z的q越近"。

"这个界限变得越来越紧"，"因此，最后这个术语就是q的熵"，"所以，将前一张幻灯片中对于x和z的定义代入我们的公式中"，我们可以说让q等于这个，"然后我们可以写出对数p从o一到t的值"。

"我们证据作为超过某值的对数概率"，"或等于在时间区间s到t和a到t下的预期值"，"按照我们图形模型中所有概率的q进行分配"。



![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_74.png)

"对数s的一"，再加上转换概率对数的和，再加上最优性变量对数的和，减去熵，也就是说，将是减去s1的logp，所以，这个s1来自我们对q的定义，减去转换概率的对数。



![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_76.png)

再次，这来自我们对q条件的定义，然后减去给定s和t的logq的t，所以现在我们可以看到为什么我们选择了这个特定的q，我们选择了q，因此，初始状态概率和转移概率非常方便地相互抵消。



![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_78.png)

这意味着我们的边界，现在仅仅对应于最优性变量的日志概率和。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_80.png)

在q下的动作的日志概率的和。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_82.png)

将o t给定s t a t的定义代入p中，我们得到这个表达式，我们对我们似然度的下限的估计仅仅是总奖励的期望值。



![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_84.png)

减去在每个时间步长给定s t at时q a t的日志概率，嗯，而且我可以移动x轴，期望之外的和由期望线性性给出。



![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_86.png)

并将log q项替换为熵，现在我们可以看到，这个下限正好等于最大化奖励，并最大化动作熵。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_88.png)

记住，q与原始问题的初始状态分布和转移概率相同。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_90.png)

这意味着这就是预期的奖励，我们的原始强化学习目标。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_92.png)

加上这些额外的熵项，并且额外的熵项，用于解释你为什么不想要单一的最优解。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_94.png)

但你可能想要一些随机行为，这也模型化了稍微低于最优的思考，回到我们的稍微低于最优的猴子。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_96.png)

这基本上是优化主观意愿，会给我们稍微低于最优的猴子，所以这很酷的是，只要应用变分下界。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_98.png)

我们恢复了一个看起来非常像原始强化学习目标的对象。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_100.png)

但加上这些额外的熵项，好的，所以如何优化这个变分下界，所以这是我们的q，这是我们的界限，从最后一张幻灯片，花一点时间思考我们如何优化它，我们能吗，例如，我们可以使用我们已经从之前的讲座中学到的一些算法。

所以我们可以采取动态规划的方法，与您所知道的一样，我们学习到的价值迭代风格方法，我们可以解决最后一个时间步。



![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_102.png)

嗯，它只有一个奖励函数，当解决最后一个时间步时，我们可以将术语分组，以便我们有q of S capital t下S capital t的预期值。

S capital t的预期值下a capital t的奖励的预期值，加上熵，实际上，你可以证明，只要你有一个最大化目标。



![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_104.png)

它具有形式为在某个量的分布下的平均值，减去该分布的对数概率，解决方案总是有该量的指数形式，很容易证明这一点，只要设置导数，你知道，取导数。

设置导数为零并解出q a capital t给定s a capital t，但一般来说，一个良好的规则之一是，如果你的目标是某事物的预期值。



![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_106.png)

减去在该事物下取预期值的东西的对数概率，解决方案总是该量的指数期望。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_108.png)

所以最后一个时间步总是优化的，当q of a capital t，给定s t是比例于最后一个时间步奖励的指数时。



![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_110.png)

特别是，如果我们写出归一化，你可以看到，分母只是所有动作的指数奖励的积分。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_112.png)

当然，当然，正是q函数减去价值函数的指数，当然，在最后一个时间步，函数q有点简单，函数q就是奖励。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_114.png)

而价值函数就是q函数的对数积分指数指数化，这是指数归一化常数，所以这就是价值函数，嗯现在，如果我要嗯，然后将这个表达式中的q替换为，嗯，然后我，嗯，我知道奖励和log q的差就是价值，因为log q。

log小q是大q减去v所以大q在这里是奖励，所以奖励减去奖励加上v，我最终在右边得到这个表达式。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_116.png)

所以这与我们在lqr中做的有些类似，我们从后面开始，解决最优策略，然后替换相应的表达式。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_118.png)

所以这告诉我们对于最后一个时间步。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_120.png)

这个最后时间步对总体目标的贡献是v(s_t， t)。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_122.png)

其中q(a_t|s_t)由这个表达式给出。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_124.png)

嗯然后我们可以继续递归的案例，我们可以说在任何给定时间步，嗯，q_t(s_t， a_t)在那个时间步的s_t下是arg max的预期价值。



![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_126.png)

在q_t(a_t|s_t)下的预期价值，加上那个时间步的奖励。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_128.png)

加上下一个时间步的价值函数的预期值，加上q_t(a_t|s_t)的熵，当然，嗯，如果我们这样做，嗯，我们可以总是说我们有这个量q_t(s_t， a_t)。



![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_130.png)

这是r加上下一个v那就是常规的贝尔曼备份。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_132.png)

这不再乐观了，我们替换他们到这个方程中。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_134.png)

再次，我们得到看起来像q_t(a_t)下的某个量的预期值，减去log q，所以我们知道再次解决方案是exponentiated q值和归一化常数。



![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_136.png)

再次价值函数，所以再次我们得到相同的表达式q_t(a_t|s_t)，我们可以重复这个递归从时间步向后，所以这给我们一个动态规划解决方案。



![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_138.png)

当然我们可以，嗯正式化这个作为后向传递，这里是从最后一个时间步到开始的后向传递总结，将你的q函数设置为r，加上下一个v的预期值，所以这就是常规贝尔曼备份，将你的v设置为软最大，所以这就是软，嗯最大。

就像在常规值迭代算法中一样。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_140.png)

我们将重复这些备份，现在我们有一个软值迭代算法，一切都与常规算法相同，除了v是softmax而不是硬最大，最后的政策是q减去v的指数。



![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_142.png)

好的。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_144.png)

所以，总结来说，我们有原始的模型，我们做了变分近似，我们在每一步的价值函数是指数化q值的对数积分。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_146.png)

我们的q值是以正态分布的方式备份的，就像在常规的钟和备份中。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_148.png)

你可以在这个来自2018年的教程文章中阅读更多关于这个的主题，叫做强化学习，学习和控制是概率推断，教程和回顾，但这基本上给我们一个动态规划算法，它是价值迭代的软模拟，现在，有这个变体的许多版本，你可以。

例如，构建一个折扣变体。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_150.png)

其中，你在下一个价值函数的预期值前面放一个伽马。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_152.png)

那就是对应于将你的动态更改为死亡概率为一减伽马的概率，你也可以添加一个明确的温度，所以，当您执行这个价值函数计算时，你可以在你乘以q值并除以alpha的地方添加一个alpha，然后。

在最后乘以alpha，当alpha趋近于零时，这将接近一个硬最大。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_154.png)

当然，你也可以构建这个的无限期形式，其中，而不是真正地从轨迹的末尾开始动态编程。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_156.png)

你实际上运行一个无限期软价值迭代程序。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_158.png)

这也是一个完全合理的，完全正确的事情要做于无限期情况。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_160.png)

基本上按照幻灯片上描述的程序工作，完全符合你的预期。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_162.png)

好的，这就是动态规划做某事的方法，在下一部分中，我们将讨论如何实现这种想法。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_164.png)

我将谈论如何实例化这个想法，以及一些其他设计理念。

![](img/2ccabd0b5daa7dec84d4c547f1bd89ad_166.png)