# 【深度强化学习 CS285 2023】伯克利—中英字幕 - P23：p23 CS 285： Lecture 6, Part 3 - 加加zero - BV1NjH4eYEyZ

好的，接下来，我们将讨论一些实际实现演员的设计决策，批评算法。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_1.png)

所以我们将从神经网络架构的讨论开始，为了实际上实例化这些算法作为深度强化学习算法，我们必须决定我们如何表示价值函数和策略，所以，在上一节课中，我们只有了处理策略的。



![](img/d842ad3eeced91a1e3bc013e2f9751d2_3.png)

现在，我们必须代表这两个对象，并且我们有几个选择可以做，所以选择一个非常合理的起始选择，这就是我推荐的一个，如果你刚刚开始，有一个完全分开的网络是一个好主意。



![](img/d842ad3eeced91a1e3bc013e2f9751d2_5.png)

所以你有一个网络，它映射一个状态到一个值。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_7.png)

然后，你有另一个完全分开的网络，它映射同一个状态到一个动作的分布，这些网络没有任何共同点，这是一个方便的选择，因为它相对简单来实现，而且它通常在训练时比较稳定，缺点是，它可能被认为是有些效率低下的。



![](img/d842ad3eeced91a1e3bc013e2f9751d2_9.png)

因为演员和批评者之间没有特征的共享，这可能是一个更重要的问题，例如，如果你直接从图像中学习。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_11.png)

并且这两个网络都是卷积神经网络，你可能真的希望他们共享他们的内部表示，所以例如，如果价值函数找出了良好的表示。



![](img/d842ad3eeced91a1e3bc013e2f9751d2_13.png)

首先，策略可以从中受益，在这种情况下，你可能选择共享网络设计，你有一个主干，也许这代表了卷积层。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_15.png)

然后你有单独的头，一个用于值和策略动作分布政策的样本。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_17.png)

这种共享网络设计稍微难于训练。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_19.png)

它可能会稍微不稳定，因为那些共享层正在被非常不同的分量更新。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_21.png)

来自价值回归的分量和来自策略梯度的分量，它们的规模可能不同，它们可能会有不同的分布，因此，可能需要更多的超参数调整才能使这种方法稳定。



![](img/d842ad3eeced91a1e3bc013e2f9751d2_23.png)

但在原则上可以更有效，因为你有这些共享表示，现在，在我们得到实际操作之前，还有一个重要的点我们需要讨论，深度强化学习演员，批评方法。



![](img/d842ad3eeced91a1e3bc013e2f9751d2_25.png)

那就是批次大小的问题，正如这里所述，这个算法是完全在线的，意味着它一次学习一个样本，所以它采取一个动作，获取一个过渡，在过渡上更新价值函数，然后在过渡上更新策略，并且两个更新现在都只使用一个样本。

我们从深度学习的基本原理中知道，使用随机梯度下降更新深度神经网络，仅仅使用一个样本不会很好工作，所以这些更新将具有太多的方差，因此，这些更新在所有情况下都最好如果我们有一个批次。

并且我们可以通过使用并行工人来获取批次的一种方式，这就是想法，这是最基本的一种并行演员-批评者，它是一种同步的并行演员-批评者，而不是只有一根数据收集线程，而不是只运行一个模拟器，你可能运行多个模拟器。

并且每个都将在步骤一中选择一个动作并生成过渡，但他们将使用不同的随机种子，所以它们做一些稍微不同的事情。



![](img/d842ad3eeced91a1e3bc013e2f9751d2_27.png)

然后在步骤二和步骤四中更新，使用所有线程的数据一起，因此，更新是同步的，意味着在步骤一中，每个线程都会执行一步。



![](img/d842ad3eeced91a1e3bc013e2f9751d2_29.png)

然后收集所有数据并将其用于更新价值函数，然后使用它同步更新策略，然后重复这个过程，这将给您一个与工人线程数量相同的批次大小，这可能有点昂贵，因为如果您想要一个批次大小，如3或2。

那么您需要3或2个工人线程，但它工作得很好，如果我们将其转换为异步并行演员-批评者，它可以更快。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_31.png)

这意味着我们基本上删除了同步点，所以现在我们有这些不同线程，它们以自己的速度运行，当更新时，我们将做什么。



![](img/d842ad3eeced91a1e3bc013e2f9751d2_33.png)

我们将拉取最新的参数，并为该线程进行更新。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_35.png)

但我们不会实际同步所有线程，所以只要积累一定量的过渡。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_37.png)

例如，我们从所有工人那里获取32个过渡，我们现在将进行更新，这种方法的问题，当然，是实际的过渡可能不是由完全相同的参数收集的，如果其中一个线程落后，也许它的过渡是由较旧的演员生成的。

然后您基本上不会实际更新，直到您从更快的线程那里获取过渡，这些将使用更新的演员，总的来说，在这里，您将所有过渡池积聚在一起，可能由稍微不同的角色生成，它们不会太不同，因为这些线程不会运行得如此截然不同。

但是，会有一点延迟，所以，在这里有一个显而易见的问题，这种更新方式，异步更新与标准同步更新在数学上是等价的，答案是，它并不仅仅是你有一个小的延迟，这与使用异步SGD时得到的结果相似，但在实践中。

通常情况下，使方法异步会导致性能提升，这超过了由于使用稍微较旧的演员而产生的偏差，关键的是稍微较旧，因为演员不会太旧如果他们太旧，那么当然这不会工作，但只要没有线程挂起。



![](img/d842ad3eeced91a1e3bc013e2f9751d2_39.png)

那么你就会是安全的，嗯，但这可能会让我们思考另一个问题。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_41.png)

在异步演员批评算法中，整个目的是我们可以使用由稍微较旧的演员生成的过渡。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_43.png)

如果我们能找到一种方法可以使用由非常旧的演员生成的过渡，那么也许我们甚至不需要多个线程，也许我们可以使用同一个演员的老过渡，基本上，也许我们可以使用历史，并加载来自该历史的过渡，甚至不需要担心多个线程。

这就是离线策略演员批评的原则。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_45.png)

所以，离线策略演员批评的设计是，你现在将有一个线程。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_47.png)

你会更新那个线程，但当你更新时。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_49.png)

你会使用一个包含旧过渡的复用缓冲区，你将实际上从复用缓冲区加载您的批处理，所以你实际上并不总是使用最新的过渡，您将收集一个过渡存储在复用缓冲区中，然后从该复用缓冲区中采样整个批处理，也许32个过渡。

而不是一个，并根据该批处理进行更新，到这个点，我们必须修改算法，因为这样做不会直接工作，我们从复用缓冲区加载的批处理。



![](img/d842ad3eeced91a1e3bc013e2f9751d2_51.png)

肯定来自非常旧的策略，所以它不像之前的异步演员批评，其中过渡来自稍微较旧的演员，如果我们可以忽视这一点，现在，它来自非常旧的演员，我们不能忽视这一点，我们必须实际上改变我们的算法，好的，所以我想说。

复用缓冲区，基本上，我只是意味着一个包含我们在先前时间步中看到的过渡的缓冲区，最直接的方式实现复用缓冲区是实现它作为一个环。



![](img/d842ad3eeced91a1e3bc013e2f9751d2_53.png)

缓冲区，一个先进先出缓冲器，您可以批量处理，让我们假设有一百万次转换，我在这里要说的是，我们将会深入讨论重放缓冲区。



![](img/d842ad3eeced91a1e3bc013e2f9751d2_55.png)

"在接下来的讲座中会有更多内容"，所以嗯，"不要对这个过于执着"，"目前"，嗯，"它就是一个缓存，存储所有数据"，你所看到的所有经验，"然后当然，我们将为这些更新形成每个批次"。



![](img/d842ad3eeced91a1e3bc013e2f9751d2_57.png)

通过使用先前看到的过渡，好的，所以嗯，让我们看看这种策略在离线策略批评算法中可能看起来什么样子。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_59.png)

我们将像往常一样采取行动，从我们最新的策略，获取相应的过渡，但是，而不是使用那个过渡进行学习。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_61.png)

我们将实际上将其存储在我们的回放缓冲区中，然后，我们将从该回放缓冲区中采样一个批次，所以这种标记表示一个包含n个转换的集合，每个都被索引为，我，它甚至可能不包含我们最新的转换。

所以当我们从缓冲区加载这个批处理时，它可能不包含我们采样的最新转换，这没关系。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_63.png)

然后我们将更新我们的价值函数，使用批处理中的每个转换的目标，所以我们有大写的n个转换，这就意味着我们有n个目标，所以我们将计算损失在批次上的均值梯度，所以n在这里是批次大小，它不是总缓冲大小。

它只是批次的大小，所以可能是三，两或六十四，然后我们将对我们批次中的每个样本重新评估我们的优势。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_65.png)

然后我们将更新我们的梯度，我们的策略梯度使用那个批次。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_67.png)

所以现在政策梯度也被平均过了n个样本，然后我们会像以前那样应用这个政策梯度。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_69.png)

所以这个算法不会以我描述的那样工作实际上它相当有问题，我们必须做很多事情来修复它。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_71.png)

在这里我建议作为一个练习，暂停视频。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_73.png)

看看这个算法并尝试猜出它哪里出了问题，我现在告诉你，它至少在两个地方有问题。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_75.png)

这意味着在伪代码中至少两个地方，有一些东西并不合理，尝试暂停视频并找到它，然后你可以继续，我会告诉你是什么。



![](img/d842ad3eeced91a1e3bc013e2f9751d2_77.png)

所以第一个问题是当你从回放缓冲器加载这些过渡时。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_79.png)

记住在那些过渡中的行动是由更老的演员执行的。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_81.png)

所以当你使用那些更老的演员来获取动作并计算目标值时。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_83.png)

![](img/d842ad3eeced91a1e3bc013e2f9751d2_84.png)

那不会给你正确的目标值，它会给你其他演员的值，不是你最新的演员，而且那不是你想要的，所以正式上，问题是我没有来自最新的pi theta，它来自一些更旧的πθ，因此。

π'也不是通过与最新版本演员采取行动的结果，这是一个问题。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_86.png)

第二个问题是由于同样的原因。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_88.png)

因为我不是来自最新版本的πθ。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_90.png)

你不能用这种方式计算政策梯度，记得之前的讲座，在计算政策梯度时，非常非常重要。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_92.png)

因为我们实际上得到了从我们的政策中采样的行动。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_94.png)

因为这需要基于πθ的期望值，如果那不是这样，我们需要使用某种类型的纠正，例如，重要采样，我们实际上可以用重要采样来做到这一点。



![](img/d842ad3eeced91a1e3bc013e2f9751d2_96.png)

但结果是，对于离政策演员的算法，实际上有更好的方法来做它，批评家。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_98.png)

我会在下一次告诉你，嗯，但首先，让我们谈谈修复价值函数，"所以，我先解决步骤三的问题。"，"然后，我在步骤五中会解决这个问题"，"所以，来解决步骤三的问题"，嗯。



![](img/d842ad3eeced91a1e3bc013e2f9751d2_100.png)

"而不是与价值函数一起工作"，"让我们相反地回到我们的第四堂课"，"我们还引入了这个q函数的概念"，"如果价值函数告诉你从状态st开始你将获得的预期奖励"。



![](img/d842ad3eeced91a1e3bc013e2f9751d2_102.png)

"然后遵循政策π"，"q函数告诉你从状态st开始你将获得的奖励"。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_104.png)

然后执行动作a，然后遵循策略pi。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_106.png)

现在请注意，我们没有假设动作a在时间t实际上来自你的策略。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_108.png)

所以q函数是对任何动作的有效函数，只是在所有有序的步骤中，你遵循pi。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_110.png)

所以我们将做一件事来适应我们的过渡，S i a i s i prime没有来自我们的最新策略pi theta的事实。



![](img/d842ad3eeced91a1e3bc013e2f9751d2_112.png)

是实际上我们不会学习v，但我们将学习q。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_114.png)

所以我们不会跟踪v hat pi phi将跟踪q hat pi phi，它，这将是一个不同的神经网络，它将输入一个状态和一个动作，并输出一个q值，但是其他方面，更新原则仍然是相同的。

所以我们将计算目标值，然后，我们将回归到这些目标值，只是现在，我们将将动作作为输入传递给q函数。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_116.png)

另一种思考方式是，我们不能再假设我们的行动来自我们的最新策略pi theta，所以，我们将学习一个状态动作值函数，它对任何动作都有效。



![](img/d842ad3eeced91a1e3bc013e2f9751d2_118.png)

这样我们就可以训练它，即使使用不来自于pi theta的动作，然后使用来自pi theta的动作查询它，好的，现在注意听的你们，可能会注意到这里有一点问题，因为我在学习v hat。

我在目标中使用v hat，这是可以的，因为我在学习v hat，所以我有可用的v hat来使用我的目标，但现在我正在学习q hat，但我仍然需要v hat来为我的目标值，那么我从哪里可以得到那个，嗯。



![](img/d842ad3eeced91a1e3bc013e2f9751d2_120.png)

记住，价值函数也可以被表达为q函数的预期值。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_122.png)

其中，期望是在您的策略下取的。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_124.png)

所以我们可以做的是，我们可以将我们的目标值中的v替换为在动作上评估的q，Ai prime，除了ai prime现在不是我们回放缓冲区的动作。



![](img/d842ad3eeced91a1e3bc013e2f9751d2_126.png)

i prime实际上是您当前策略pi theta将采取的行动。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_128.png)

如果它发现自己在si prime，所以你实际上将从你的回放缓冲器中采样s i a i s i prime。



![](img/d842ad3eeced91a1e3bc013e2f9751d2_130.png)

然后你将采样ai prime，但实际上运行你的最新策略，你可以这样做，因为你的策略只是一个神经网络。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_132.png)

你不必实际上与模拟器交互来询问策略，它将采取什么行动，所以我们在这里拉了一个小把戏，我们实际上是利用我们实际上对政策的功能访问。



![](img/d842ad3eeced91a1e3bc013e2f9751d2_134.png)

所以我们可以问我们的策略它将做什么，如果它发现自己处于这个旧的状态。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_136.png)

如果，即使那从来没有真正发生过，那么然后我们就得到这个动作ai prime，我们将它插入到q值中，这就给我们得到了一个目标值，那实际上代表了在这个旧状态si prime上最新的政策的价值。



![](img/d842ad3eeced91a1e3bc013e2f9751d2_138.png)

![](img/d842ad3eeced91a1e3bc013e2f9751d2_139.png)

这真的很酷，好的，所以我们已经解决了价值函数的问题，而不是学习v，我们将学习q，我们将利用我们可以评估价值函数的事实。



![](img/d842ad3eeced91a1e3bc013e2f9751d2_141.png)

就是当前策略下q函数的预期值，我们如何处理步骤五，我们如何处理策略梯度，嗯，我们所要做的就是，我们将使用同样的技巧，但这次我们将用它来代替AI prime而不是AI。



![](img/d842ad3eeced91a1e3bc013e2f9751d2_143.png)

为了评估策略梯度，我们需要找出从最新策略πθ在状态si采样的动作，但是当然我们可以做到，我们可以只是问我们的政策在州si的情况下会做什么，如果它，嗯如果有选择在那里采取行动的机会。

然后我们会称这个行动为ai pi，以区别它与ai。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_145.png)

所以ai实际上来自缓冲区，Ai prime是政策在缓冲区状态si时如果存在的话会做的事情。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_147.png)

现在，我们将只是将这个ai pi插入到我们的政策梯度方程中。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_149.png)

因为这是正确的，因为ai prime确实来自pi theta，所以这实际上是对pi theta期望的无偏估计者，所以记住，在这里的ai pi并不是来自回放缓冲区的动作。

它是从回放缓冲区中的状态采样的你的策略动作。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_151.png)

现在在实际中，当我们做这种离线策略批评者时，我们实际上不使用优势值。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_153.png)

我们只是直接将我们的q hat直接插入到这个方程中，我们不必这样做，我们实际上可以计算优势，没有人阻止我们做这件事。



![](img/d842ad3eeced91a1e3bc013e2f9751d2_155.png)

但是，事实证明，直接插入q值是非常方便的。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_157.png)

他们的方差更高，因为他们没有基线，但是，更高的方差在这里实际上是可以的，为什么那样，这是因为我们不需要与模拟器交互来采样这些动作。



![](img/d842ad3eeced91a1e3bc013e2f9751d2_159.png)

Ai prime，所以实际上很容易降低我们的方差，仅仅通过生成更多的动作样本，而不实际生成更多的采样状态，所以它不需要任何模拟。



![](img/d842ad3eeced91a1e3bc013e2f9751d2_161.png)

它只需要运行网络几次，所以实际上我们在这里对更高的方差是接受的。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_163.png)

因为交换后我们得到了更大的批量大小，一切都很好，而且它节省了我们计算优势的复杂性，步骤四，所以对于离线策略批评算法，我们将完全跳过步骤四，我们将使用q hat而不是a hat，这仍然是无偏的。



![](img/d842ad3eeced91a1e3bc013e2f9751d2_165.png)

它只是缺乏基线，所以这给我们带来了，更或更完整的算法用于离线策略批评。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_167.png)

还有什么是完好无损的，还有一些小问题，因为实际上我们正在使用的状态本身，它不是来自最新的政策的状态边缘。



![](img/d842ad3eeced91a1e3bc013e2f9751d2_169.png)

它是来自旧政策的状态边缘，不幸的是，基本上我们 here 没有什么可以做的，所以这将是这个程序的一个偏差来源。



![](img/d842ad3eeced91a1e3bc013e2f9751d2_171.png)

我们只好接受它，直觉上为什么它不这么坏是。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_173.png)

因为最终我们想要 p theta 在 s 上的最优政策，但是，我们在更广泛的分布上得到了最优策略，所以，我们的回放缓冲区将包含来自最新策略的样本。



![](img/d842ad3eeced91a1e3bc013e2f9751d2_175.png)

以及来自许多其他较旧策略的许多样本，所以，分布大致比我们要求的更广，所以我们不会错过我们最新策略的状态，我们只是还需要在其他我们可能永远不会访问的状态上做得好。



![](img/d842ad3eeced91a1e3bc013e2f9751d2_177.png)

所以我们在做额外的工作，但我们没有错过重要的东西。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_179.png)

这就是为什么这基本上倾向于工作的直觉，好的，所以嗯，这里有一些细节，如果你实际上阅读一些论文。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_181.png)

我在这里将引用一篇即将实施的论文的参考，你会注意到的一件事是，在步骤四中，我们常常可以做一些更复杂的事情。



![](img/d842ad3eeced91a1e3bc013e2f9751d2_183.png)

例如，我们可以使用被称为重参数化技巧的东西，我将在课程的后半部分稍后讨论，所以不要为此担心现在，但是，这可能是一种更好的估计这个积分的方法，还有许多更复杂的方法来拟合q函数，在下两堂课中。

当我们讨论q学习时，我们将讨论这个问题。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_185.png)

所以我描述了一种非常朴素的方法来拟合q函数。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_187.png)

但实际上，有其他更好的方法可以做到，如果你想要一个基于这个想法的实用算法的例子。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_189.png)

查看被称为软演员批评的算法，这实际上是今天最广泛使用的演员批评方法之一。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_191.png)

尽管在线基于价值的演员批评方法更古典，基于q值的离线策略演员批评方法更常见。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_193.png)

我们还将学习做这种事的算法。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_195.png)

稍后我们将讨论确定性策略，所以这是为了后来的随机演员，当我们谈论Q学习时，实际上将重新审视离策略。

![](img/d842ad3eeced91a1e3bc013e2f9751d2_197.png)

![](img/d842ad3eeced91a1e3bc013e2f9751d2_198.png)