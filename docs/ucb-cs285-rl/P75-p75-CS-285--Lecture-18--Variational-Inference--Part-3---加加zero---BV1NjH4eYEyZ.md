# P75：p75 CS 285： Lecture 18, Variational Inference, Part 3 - 加加zero - BV1NjH4eYEyZ

"现在好了"，"让我们谈谈如何通过摊销的变分推断使我们能够"。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_1.png)

"基本上，我已经通过x学习了这个z的问题"，"由此实现可变推断并构建实用的工具"，即使在数据集规模极其庞大的环境中。



![](img/9b9fef0e9784f3aeae82f48e0153c28b_3.png)

"所以，让我们回顾一下在我们进行模型参数训练的变分推断程序之前我们所讨论的内容"。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_5.png)

"西格玛基本上看起来像这样"，"对于我们数据集中的每个图像x_i或数据点x_i"。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_7.png)

或者更一般地，对于每个小批量，"你将根据你的变分下界lp对theta进行估计。"，逗号 q，并且你这样做的方法是通过从近似后验分布q中采样一个z。



![](img/9b9fef0e9784f3aeae82f48e0153c28b_9.png)

我关于z，然后估计梯度，这将仅仅是关于theta对log p theta给定zi的x的梯度，这是单一样本估计器。



![](img/9b9fef0e9784f3aeae82f48e0153c28b_11.png)

使用你从qi得到的一个样本来计算期望，然后您使用此估计的梯度grad theta i来执行梯度步骤。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_13.png)

然后您更新qi以最大化界限li并因此紧缩它。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_15.png)

并且问题，当然，是与你如何最大化边界有关，所以我们在讲座的前一部分看到的是，如果你将qi表示为每个数据点xi的单独高斯分布。



![](img/9b9fef0e9784f3aeae82f48e0153c28b_17.png)

那么你可以简单地计算i关于theta的导数。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_19.png)

但是，现在参数的总数随着n的增加而增加，所以，在平均化变分推断的想法中会是，基本上要摊销推断这个近似后验q的成本，我对所有数据点的z的i，通过使用能给出任意x的后验分布的单一模型。



![](img/9b9fef0e9784f3aeae82f48e0153c28b_21.png)

并且那个单一模型可以是神经网络模型。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_23.png)

所以，在这种情况下，我们对于每个x仍然会有一个高斯后验分布。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_25.png)

但是，我们不再为每个数据点存储后验分布的均值和方差。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_27.png)

我们将有一个神经网络，该网络接受该数据点，并输出x的phi的均值和5x的方差，数据点x的高斯后验分布的参数。



![](img/9b9fef0e9784f3aeae82f48e0153c28b_29.png)

好的，这就是 amortized variational inference 的基本思想，你有两个网络，你正在尝试学习的网络，你生成模型p theta给定z的x。



![](img/9b9fef0e9784f3aeae82f48e0153c28b_31.png)

以及我们称之为你推断网络q phi给定x的c，所以，我们知道，从我们之前的讨论中，你可以形成一个对log p的x i的下界，使用任何线索，但是最慢的下界在队列接近后验时最紧，所以，现在。

就像以前我们有q i的z，现在我们有q phi的z给定x i，现在，变分下界是一个关于两个分布的函数，在给定z和q phi的情况下，x i的分布为P theta。



![](img/9b9fef0e9784f3aeae82f48e0153c28b_33.png)

I，所以，我们的训练过程，只是前一个幻灯片的修改，将像这样首先计算变分下界对theta的梯度。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_35.png)

这将基本上与以前相同，从q中采样z，给定x i，z的分布为Phi of z，然后计算对theta的梯度，这被近似为对theta的log p的梯度，给定z，x i的theta值，其中z是你刚刚采样的z。

然后你可以对theta进行梯度上升步。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_37.png)

生成模型的参数，然后你需要做的就是对phi进行梯度步。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_39.png)

对phi的梯度步也是变分下界l的梯度下降，所以我们必须回答的问题。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_41.png)

为了完成这个算法是，我们如何计算l关于phi的梯度。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_43.png)

所以这是我们现在关心的量。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_45.png)

所以这就是对l的表达式，你可以看到phi出现在两个地方。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_47.png)

首先，phi是你期望值所遵循的分布，期望值，其次phi出现在熵项中，所以这里有一个问题要问大家，嗯，好吧，让我们先考虑那个第一个项，这是对由参数phi参数化的分布所期望的值。



![](img/9b9fef0e9784f3aeae82f48e0153c28b_49.png)

对于与phi无关的一些量，我们想要计算它的梯度，我们在哪里见过这个，我们实际上已经讨论了一个可以计算这个梯度部分的算法，我们在课程的早期就讨论了这个算法，花一点时间来思考这个。

并试着思考这个术语的梯度将看起来什么样子，基于我们在课堂上已经学到的，所以，我们的嗯，Q，当给定x，Q下标五z由高斯分布给出，其中，均值和方差是x的神经网络函数。



![](img/9b9fef0e9784f3aeae82f48e0153c28b_51.png)

高斯的熵可以以封闭形式表达，它是一个涉及mu的封闭形式方程。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_53.png)

Phi和sigma i的方程，你可以在教科书或维基百科上查找这个，这是一个非常标准的方程。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_55.png)

所以那个实际上是相当容易做的，你只需要以mu和sigma的形式写下方程，并且你可以取它的导数，实际上，第一个术语是最有问题的，所以我们可以建议性地重写那个术语，称它为phi的j。

它等于关于z按照q分布的期望值。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_57.png)

给定x_i的一些量，我将其称为r，phi的z是。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_59.png)

这是关于x_i和z的函数，重要的是，r仅仅不依赖于phi，那么，我们如何计算这个关于phi的导数。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_61.png)

嗯，我们可以简单地使用策略梯度，所以我建议在这里有意识地使用j和r，只是为了明确这一点，这种方程的形式与我们之前有过的完全相同。



![](img/9b9fef0e9784f3aeae82f48e0153c28b_63.png)

使用政策梯度，并且，这意味着，我们可以通过从q phi中采样z来估计phi的梯度grad j。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_65.png)

我们平均价量的是样本的grad phi log q phi乘以r。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_67.png)

与政策梯度不同。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_69.png)

这些样本不需要实际上与真实世界进行交互，这些样本只需要 uh。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_71.png)

从你的q模型采样，然后评估在p模型下的对数似然。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_73.png)

所以你可以廉价地生成这些样本。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_75.png)

这是计算第一个术语梯度的一个非常合理的方法，好的，这个梯度有什么错吗。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_77.png)

花一点时间回顾政策梯度讲座，想一想我们可能想要改进这个梯度的原因。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_79.png)

一点，所以我现在就告诉你，你可以使用梯度。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_81.png)

这个策略梯度，实施 amortized 变异推断是完全合理的。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_83.png)

使用策略梯度优化推断网络的参数，你可能需要绘制多个样本才能得到准确的梯度。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_85.png)

但这是一个完全可行的方法，但这不是最好的方法。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_87.png)

所以就像我们在策略梯度讲座中学到的，这个梯度估计器往往容易受到高方差的影响。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_89.png)

高方差意味着你的梯度将会是嘈杂的。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_91.png)

或者你需要抽取更多的样本才能得到同样准确的梯度。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_93.png)

这可能会有些不便，幸运的是，有一种特定的技巧，你可以在与平均化变分推断一起使用时使用，这在我们通常做常规强化学习时并不可用，这就是叫做重新参数化技巧的东西。



![](img/9b9fef0e9784f3aeae82f48e0153c28b_95.png)

高层次的想法是，在强化学习中。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_97.png)

我们使用政策梯度，因为我们无法通过动态计算导数，但是，与平均化变分推断一起使用时。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_99.png)

没有未知的动态，只有q，通过q的平均值和方差计算导数实际上相当可行，所以，我们可以使用这个技巧，它利用了这一点，再次给出phi中的j的方程，记住，困难的部分是计算这个术语的梯度，熵的梯度很容易得到。

因为熵是以mu和sigma的形式表达的封闭形式方程。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_101.png)

你可以为那个计算导数，使用任何自动微分软件。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_103.png)

困难的部分是计算phi中的j的梯度，"当q phi再次是一个高斯分布时"，"所以，如果你有一个按照正态分布分布的变量z"，"你可以总是将那个变量重写为确定性项的和"。



![](img/9b9fef0e9784f3aeae82f48e0153c28b_105.png)

"x的缪菲和由某个随机数epsilon乘以的随机项"，"西格玛斐的x"，"如果epsilon按照均值为零，方差为一的高斯分布进行分配"，"然后，将此公式插入将使z对应于均值为mu的高斯样本。"。



![](img/9b9fef0e9784f3aeae82f48e0153c28b_107.png)

"并且方差sigma"，所以你实际上是将一个样本从零转化为。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_109.png)

"将均值为mu，方差为sigma的均值单位方差正态分布转换为样本"，关于这个方程，需要注意的是，epsilon并不依赖于phi，所以，用这种方式写z，表示z为一个确定性函数。

由随机变量epsilon的phi参数化，该epsilon与phi独立。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_111.png)

这就是为什么我们称这个为重新参数化技巧。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_113.png)

因为我们正在重新参数化随机变量z，以成为另一个随机变量的确定性函数，与phi独立的epsilon，当我们这样做时，我们可以得到更好的梯度估计器，所以我们可以做的是，我们可以以关于z的期望来写我们的期望。



![](img/9b9fef0e9784f3aeae82f48e0153c28b_115.png)

而不是以关于epsilon的期望。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_117.png)

其中我简单地将z以epsilon表示的方程代入r，所以这是严格的等式，这不是近似，根据均值为mu，方差为sigma的高斯分布，z的期望值，sigma等于对epsilon的期望值，按照均值为零。

方差为一的高斯分布分布，并在xi处评估你的r，xi的均值为mu加上epsilon乘以sigmax乘以psi，所以现在，我们已经非常接近能够计算出关于i的j的更好梯度。



![](img/9b9fef0e9784f3aeae82f48e0153c28b_119.png)

因为phi在这里现在只参数化了确定性的量，所以想一下我们如何能写一个更好的梯度估计器，基于这个关于epsilon的期望，好的，所以这就是我们以phi为参数，关于phi的j的梯度估计方法。



![](img/9b9fef0e9784f3aeae82f48e0153c28b_121.png)

首先从零均值单位方差的高斯分布中采样epsilon，采样epsilon1到epsilonm的高斯样本。



![](img/9b9fef0e9784f3aeae82f48e0153c28b_123.png)

一个样本实际上工作得很好，所以你可以为每个数据点在你的迷你批量中生成一个样本。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_125.png)

实际上这就是我们通常做的事情，然后以phi为变量计算梯度，仅仅作为平均值，对所有评价x_i，mu的grad phi的样本进行求和，phi x_i加上epsilon j sigma phi x_i。

所以这要求r必须对z可微，当然，它要求mu phi和sigma phi对phi可微，因为它们是神经网络，我们通常不能在强化学习中使用这个，因为在强化学习中，我们不假设我们知道我们可以计算你的回报的导数。

但是，我们可以，因为r只是生成模型下的逻辑概率，它是另一个神经网络，我们知道它是什么，所以我们可以计算它对phi的导数，这个梯度估计器的方差更低，因为我们实际上在使用r的导数。

我们以前使用的策略评估没有使用r的导数。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_127.png)

所以这是一个更好的梯度估计器，而且像tensorflow或pi torch这样的大多数自动微分软件会为你计算这个。



![](img/9b9fef0e9784f3aeae82f48e0153c28b_129.png)

所以你不需要，你不需要了解如何对x_i的p_theta进行微分。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_131.png)

逗号自己，你只需要在auditive中实现它。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_133.png)

并且auditive会处理所有事情，所以这实际上是一种非常简单的计算导数的方法，唯一的不寻常之处是你必须采样这些epsilons，否则它看起来就像任何其他神经网络。



![](img/9b9fef0e9784f3aeae82f48e0153c28b_135.png)

嗯，这就是另一种看待它的方式，你可以取你的原始变分下界。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_137.png)

关于z的期望值，给定z，log p theta x i的p(theta|x，i)的期望值，加上log p z加上q的熵，把它写成三个术语，所以，给定z，解码器的期望值是，p(theta|x，i)给定z。

加上给定z的先验log p c以及熵的期望值，第二个术语本质上是q和p(i|z)之间的KL散度方程，给定x和p(z)的z的i，因为KL散度有一个熵项和一个期望值项。

并且如果我们的先验p(z)也是正态分布，KL，两个正态分布的KL散度有一个方便的解析形式。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_139.png)

所以如果你查找两个正态分布的KL散度，你会找到一条方程，以那些正态分布的均值和方差来表达。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_141.png)

就像你会找到熵的方程一样，这意味着你不需要任何复杂的梯度估计器，你只需要写出在tensorflow或pytorch中实现的那条方程，然后对之应用点梯度，所以没有，计算这个术语不需要随机采样，所以嗯。

我们剩下什么，现在将对第一个术语进行重新参数化。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_143.png)

将其表达为在误差下的期望值，现在方程中的所有项都是确定的，除了误差。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_145.png)

所以我们将用单个样本近似第一个期望值，以一个单样本高斯误差epsilon，我们留下的方程是，现在方程中的所有项都可以在你的自动微分软件中直接编码。

log p theta 给定 q phi x i 加 epsilon 的 x i，Sigma phi x i 那就是调用神经网络表示 p theta 以一个输入。

取决于来自 mu phi 和 sigma phi 的神经网络表示，这里所有的都可以反向传播，epsilon 仅仅被当作常数，第二个术语，KL 散度有一个对它的闭合形式方程。

以 q 和 p 对 z 的平均值和方差来表达。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_147.png)

你可以这样想，你有你的推断网络参数化为 phi，它接受 x i，并产生两个量。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_149.png)

一个均值 mu mu phi 的 x i，和一个 sigma phi 的 x i，然后你从零均值单位方差高斯中采样 epsilon。



![](img/9b9fef0e9784f3aeae82f48e0153c28b_151.png)

并将它与 mu 和 sigma 组合起来得到 z，然后 z 被喂入 p theta 给定 z 产生对 x 的分布。



![](img/9b9fef0e9784f3aeae82f48e0153c28b_153.png)

![](img/9b9fef0e9784f3aeae82f48e0153c28b_154.png)

这是完整的计算图，这里所有的都有已知的导数，所以你可以反向传播通过这个整个东西，相对于 both phi 和 theta。



![](img/9b9fef0e9784f3aeae82f48e0153c28b_156.png)

这意味着你可以实际上在你的 uh。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_158.png)

自动微分软件中编码这个，并且只是调用对这个整个东西的梯度，这将给你关于 l 对 both theta 和 phi 的导数。



![](img/9b9fef0e9784f3aeae82f48e0153c28b_160.png)

好的，这个三参数化技巧如何比较政策梯度。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_162.png)

这是政策梯度估计器对于变分下界导数的衍生，只是第一个术语，不是熵术语，这个可以处理连续和离散的潜在变量，所以这实际上不关心 z 是否是一个连续数字，q 不需要是高斯，q 可以是任何分布。

只要它有定义良好的对数概率。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_164.png)

但是，它具有高方差，通常需要为每个 x 绘制多个样本，并且学习率较小。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_166.png)

![](img/9b9fef0e9784f3aeae82f48e0153c28b_167.png)

由这个方程给出的重新参数化技巧仅适用于连续潜在变量。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_169.png)

因为你必须能够对 r 相对于 z 的导数。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_171.png)

如果 z 是离散的，那个导数就不清晰。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_173.png)

它确实，但是，非常容易实现，通常具有低方差，只需要每个数据点一个样本就能很好地工作。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_175.png)

![](img/9b9fef0e9784f3aeae82f48e0153c28b_176.png)

所以，如果你在疑惑，在你的 amortized variational inference 实现中应该使用哪种梯度估计器。



![](img/9b9fef0e9784f3aeae82f48e0153c28b_178.png)

如果你有连续变量，可能一个好的主意是使用参数化技巧，如果你有离散变量。

![](img/9b9fef0e9784f3aeae82f48e0153c28b_180.png)