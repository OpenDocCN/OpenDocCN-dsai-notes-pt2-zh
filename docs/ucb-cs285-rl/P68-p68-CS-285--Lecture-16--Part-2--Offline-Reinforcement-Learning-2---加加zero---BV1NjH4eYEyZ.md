# P68：p68 CS 285： Lecture 16, Part 2： Offline Reinforcement Learning 2 - 加加zero - BV1NjH4eYEyZ

接下来我要描述的方法类别，对于外分布动作问题的解决采取不同的方法，而不是试图某种程度地控制演员，这些方法将要做的就是直接修复在q函数中过高估计的行动。



![](img/fe7fa072ba65b87d0244db21544a2a41_1.png)

正如我们之前讨论的。

![](img/fe7fa072ba65b87d0244db21544a2a41_3.png)

问题是当你在贝尔曼备份中选出分布外的动作时，你选择的是价值最大的那些，因此它们是那些，错误地最大过估计的，所以，我之前的照片，在周一的讲座中，老师说如果这条绿色曲线代表真实函数。



![](img/fe7fa072ba65b87d0244db21544a2a41_5.png)

而蓝色曲线是你的拟合，尽管蓝色曲线在大多数地方都是好的拟合，如果你选择蓝色曲线最大化的点，它将是正方向上误差最大的一个。



![](img/fe7fa072ba65b87d0244db21544a2a41_7.png)

所以有一个想法，什么，如果我们修改训练q函数的目标函数，使其具有我们的标准贝尔曼误差。

![](img/fe7fa072ba65b87d0244db21544a2a41_9.png)

就像以前那样，最小化。

![](img/fe7fa072ba65b87d0244db21544a2a41_11.png)

然后另一个明确寻求找到高q值动作的术语。

![](img/fe7fa072ba65b87d0244db21544a2a41_13.png)

然后最小化它们的值，所以请注意，这里的mu被选择，所以，期望在mu下的q值尽可能大。

![](img/fe7fa072ba65b87d0244db21544a2a41_15.png)

然后q被训练以最小化这些值，所以，直觉上。

![](img/fe7fa072ba65b87d0244db21544a2a41_17.png)

这个程序应该做的事情是找到这些错误的峰值并推它们下去，实际上，实际上，如果alpha被选择适当。

![](img/fe7fa072ba65b87d0244db21544a2a41_19.png)

结果得到的q函数是真实q函数的下界，现在嗯，实际上我们可以计算出一个比这更好的嗯。

![](img/fe7fa072ba65b87d0244db21544a2a41_21.png)

目标，这个目标的问题是它可能会有些过于悲观，因为我们正在向下推所有q值。

![](img/fe7fa072ba65b87d0244db21544a2a41_23.png)

这意味着q函数恢复的基本上永远不会是正确的q函数。

![](img/fe7fa072ba65b87d0244db21544a2a41_25.png)

无论我们有多少数据，所以我们可以做的是我们可以有这个总是向下推q值的术语。

![](img/fe7fa072ba65b87d0244db21544a2a41_27.png)

我们可以在数据集中添加一个额外的术语来推高q值。

![](img/fe7fa072ba65b87d0244db21544a2a41_29.png)

现在这可能最初看起来像是一个非常奇怪的事情因为我们在对抗过度估计。

![](img/fe7fa072ba65b87d0244db21544a2a41_31.png)

但是，我们实际上通过在数据集中最大化q值来做到这一点。

![](img/fe7fa072ba65b87d0244db21544a2a41_33.png)

让我们想象一下这个程序在实际中会做什么，如果所有大的q值都是对数据集中的动作，那么这两个术语应该大致平衡，因为mu会选择数据集中的动作来最小化它们的价值，然后，第二个术语会最大化它们的价值。

并且这两个会基本上平衡，如果有很小的净效果，但是，如果大的q值是对与数据集中的动作非常不同的动作，第一个术语会推它们下去，并且第二个术语将相反地推动数据集中的行动，这意味着下一次当mu选择行动时。

它们将更接近数据集，所以从某种意义上说，你可以把它看作是一种反馈过程，如果我们最终得到的q值越远离分布，这两个术语一起就越多地将它们推回分布，一旦它们被完全推回，然后这两个术语与大约抵消这个。



![](img/fe7fa072ba65b87d0244db21544a2a41_35.png)

嗯，目标函数的更精细版本，不再保证学习到的q函数是下界。

![](img/fe7fa072ba65b87d0244db21544a2a41_37.png)

对于所有情况和动作的真值函数。

![](img/fe7fa072ba65b87d0244db21544a2a41_39.png)

但是，结果发现，即使在策略的期望值上，它仍然得到保证。

![](img/fe7fa072ba65b87d0244db21544a2a41_41.png)

对于所有情况，这就是我们真正想要的，因为最终我们关心的不是过高估计当前政策的价值，嗯，接下来我将讨论如何实际实现这个。



![](img/fe7fa072ba65b87d0244db21544a2a41_43.png)

所以使用这一概念的算法的一般结构将像这样。

![](img/fe7fa072ba65b87d0244db21544a2a41_45.png)

使用SQL和您的数据集更新您的近似q函数。

![](img/fe7fa072ba65b87d0244db21544a2a41_47.png)

然后第二步是按照常规方式更新您的政策，以最大化预期值。

![](img/fe7fa072ba65b87d0244db21544a2a41_49.png)

如果动作是离散的，政策就是argmax政策。

![](img/fe7fa072ba65b87d0244db21544a2a41_51.png)

所以基本上与实践中的q学习完全相同。

![](img/fe7fa072ba65b87d0244db21544a2a41_53.png)

当你编写子程序时，在第二步中，甚至不会有明确的政策计算，您只会插入最大值，但如果动作是连续的，然后您可以有一个单独的明确演员pi theta，并按照常规方式更新该演员，所以，在离散动作的情况下。

在这种情况下，实际上看起来就像Q学习，但在连续动作的情况下有两个额外的术语，通常，你会这样做作为一个Q函数演员-批评家算法。



![](img/fe7fa072ba65b87d0244db21544a2a41_55.png)

所以在这种情况下，演员的训练与常规演员-批评家方法完全相同。

![](img/fe7fa072ba65b87d0244db21544a2a41_57.png)

唯一改变的是批评者的损失函数，加上，这两个术语，向下推和向上推的术语。

![](img/fe7fa072ba65b87d0244db21544a2a41_59.png)

所以现在在实践中，这就是批评者的目标，如果你想实际实现这个，你会做的就是。

![](img/fe7fa072ba65b87d0244db21544a2a41_61.png)

你将添加一些正则化项，嗯，从μ，让我们称它为μ的r。

![](img/fe7fa072ba65b87d0244db21544a2a41_63.png)

对于那个正则化项的适当选择，结果发现，有很多方便的方法来实现主观的，这不需要实际上计算μ，因此，这个正则化项的一个非常常见的选择是使用μ的熵。



![](img/fe7fa072ba65b87d0244db21544a2a41_65.png)

这基本上意味着我们想要μ的熵高。

![](img/fe7fa072ba65b87d0244db21544a2a41_67.png)

嗯，我们想要捕获q值高的动作，所以这是一种最大熵正则化，结果如果你这样做，你从中选择的最优选项与q值的指数成正比。



![](img/fe7fa072ba65b87d0244db21544a2a41_69.png)

嗯，此外，结果也证明在mu下q函数的期望值。

![](img/fe7fa072ba65b87d0244db21544a2a41_71.png)

是q值的对数和指数和的和。

![](img/fe7fa072ba65b87d0244db21544a2a41_73.png)

所以这是一种方便的属性，嗯，如果你有离散动作，你们都可以使用最大熵正则化。

![](img/fe7fa072ba65b87d0244db21544a2a41_75.png)

然后甚至不需要明确构造μ，你可以简单地最小化每个状态下q值的对数和。

![](img/fe7fa072ba65b87d0244db21544a2a41_77.png)

所以这非常方便，所以对于离散动作，只需直接计算这个量对于连续动作，你可以使用重要性采样来估计在μ下q函数的预期值。



![](img/fe7fa072ba65b87d0244db21544a2a41_79.png)

所以你可以随机从当前策略或均匀地采样动作。

![](img/fe7fa072ba65b87d0244db21544a2a41_81.png)

然后使用指数q来重新加权它们，基本上利用最优的μ与指数q成正比的事实。

![](img/fe7fa072ba65b87d0244db21544a2a41_83.png)

然后使用这个来估计这个期望。

![](img/fe7fa072ba65b87d0244db21544a2a41_85.png)

并且这两个选择都是完全有效的，所以对于离散动作，就直接使用这个对数。

![](img/fe7fa072ba65b87d0244db21544a2a41_87.png)

对于连续动作，有些x公式工作得非常好，像重要采样这样的东西通常工作得很好。

![](img/fe7fa072ba65b87d0244db21544a2a41_89.png)

即使它是重要采样，并且我们通常会对高方差感到担心，记住在这里，我们不是乘以许多时间步骤的重要权重，这只是一步的时间步，这样实际上工作得很好，嗯，这就是你需要实现的基本内容。



![](img/fe7fa072ba65b87d0244db21544a2a41_91.png)

SQL，嗯，你将使用这张幻灯片的一般设计来实施批评更新。

![](img/fe7fa072ba65b87d0244db21544a2a41_93.png)

然后，算法的结构将所有训练在。

![](img/fe7fa072ba65b87d0244db21544a2a41_95.png)

更新批评者和更新演员之间，如果你在做连续动作，或者如果你在做离散动作。

![](img/fe7fa072ba65b87d0244db21544a2a41_97.png)

只需要在批评的第二行上使用argmax策略。

![](img/fe7fa072ba65b87d0244db21544a2a41_99.png)